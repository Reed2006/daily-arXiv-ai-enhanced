<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 105]
- [econ.GN](#econ.GN) [Total: 7]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.LG](#cs.LG) [Total: 101]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: OpenStaxQA是一个基于43本开放大学教科书的评估基准，用于大学教育应用，支持英语、西班牙语和波兰语。研究使用QLoRa方法对约70亿参数的大语言模型进行微调和评估，并在AI2推理挑战数据集上进行零样本评估。


<details>
  <summary>Details</summary>
Motivation: 为大学教育应用创建一个专门的评估基准，利用开放许可的教科书资源，支持多语言环境，并探索该数据集对其他任务的迁移效果。

Method: 基于43本开放大学教科书构建OpenStaxQA数据集，使用量化低秩适配器（QLoRa）对约70亿参数的大语言模型进行微调，并在AI2推理挑战开发数据集上进行零样本评估。

Result: 开发了OpenStaxQA评估基准，包含多语言大学教科书内容，成功对大语言模型进行了微调，并验证了该数据集对其他推理任务的潜在迁移价值。

Conclusion: OpenStaxQA为大学教育应用提供了一个有效的多语言评估基准，QLoRa方法能够有效微调大语言模型，且该数据集可能对其他推理任务有积极影响。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出KG-MASD方法，通过知识图谱引导的多智能体系统蒸馏，解决工业QA系统中多智能体模型推理不可控和不可验证的问题，将协作推理能力蒸馏到轻量级学生模型中。


<details>
  <summary>Details</summary>
Motivation: 工业QA系统需要比通用对话模型更高的安全性和可靠性，因为高风险场景（如设备故障诊断）中的错误可能造成严重后果。多智能体大语言模型虽然增强了推理深度，但存在迭代不可控和输出不可验证的问题。

Method: 将蒸馏过程建模为马尔可夫决策过程，并引入知识图谱作为可验证的结构化先验，丰富状态表示并确保收敛。通过结合协作推理和知识基础，生成高置信度的指令调优数据，并将推理深度和可验证性联合蒸馏到紧凑的学生模型中。

Result: 在工业QA数据集上的实验表明，KG-MASD相比基线方法准确率提升了2.4%到20.1%，并显著增强了可靠性，能够在安全关键的工业场景中实现可信AI部署。

Conclusion: KG-MASD方法成功解决了工业QA系统中多智能体模型的可控性和可验证性问题，为安全关键场景提供了可靠的轻量级部署方案。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [3] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 提出了一个专门针对人类调查回复的两阶段评估框架，包括垃圾回复过滤和三个维度（努力程度、相关性和完整性）的评估，在英语和韩语数据集上验证了优于现有指标的性能。


<details>
  <summary>Details</summary>
Motivation: 开放式调查回复在营销研究中提供宝贵见解，但低质量回复不仅增加人工筛选负担，还可能导致误导性结论，需要有效的自动评估方法。现有方法主要针对LLM生成文本，无法充分评估具有独特特征的人类书面回复。

Method: 两阶段评估框架：第一阶段垃圾回复过滤去除无意义回复；第二阶段使用LLM能力评估三个维度（努力程度、相关性和完整性），基于真实调查数据的实证分析。

Result: 在英语和韩语数据集上的验证表明，该框架不仅优于现有指标，而且在响应质量预测和响应拒绝等实际应用中表现出高实用性，与专家评估具有强相关性。

Conclusion: 该研究提出的专门针对人类调查回复的评估框架有效解决了现有方法的不足，在实际应用中表现出优越性能和实用性。

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [4] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 提出CoT Referring策略，通过结构化思维链训练数据增强多模态大语言模型在指代表达理解和分割任务中的推理能力，在复杂查询场景下提升准确性


<details>
  <summary>Details</summary>
Motivation: 指代表达理解和分割是评估多模态大语言模型语言理解和图像理解整合能力的关键任务，需要解决复杂查询场景下的准确性问题

Method: 将文本结构解析为顺序指代步骤，在每个步骤中识别关系并确保一致的对齐；重构训练数据强制新输出形式，为现有数据集提供新标注；集成检测和分割能力到统一MLLM框架，使用自适应加权损失优化性能

Result: 在自建基准和RefCOCO/+/g数据集上的实验结果显示，相比基线模型有2.5%以上的显著提升

Conclusion: CoT Referring策略通过结构化思维链训练有效提升了多模态大语言模型在复杂指代任务中的性能

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [5] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 该研究旨在为科学领域寻找最优的词表示算法和分词方法，并构建一个综合评估套件来评估各种算法在科学领域NLP任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 在特定领域中，同一单词可能具有不同含义和表示，而生成式AI和transformer架构虽然能生成上下文嵌入，但训练成本高。因此需要为科学领域寻找更高效的词表示和分词方法。

Method: 构建包含多个下游任务和相关数据集的评估套件，使用该套件测试各种词表示和分词算法。

Result: 研究构建了一个综合评估套件，能够系统评估不同词表示和分词算法在科学领域NLP任务中的性能。

Conclusion: 该工作为科学领域NLP提供了有效的词表示和分词方法评估框架，有助于选择最适合科学领域下游任务的算法。

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [6] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 提出TRepLiNa方法，通过强制多语言大语言模型特定内部层的跨语言相似性来改善低资源语言翻译质量


<details>
  <summary>Details</summary>
Motivation: 解决印度多样低资源语言缺乏资源的问题，改善低资源语言到高资源语言的翻译质量

Method: 结合中心核对齐(CKA)和REPINA正则化方法，在Aya-23 8B模型上使用QLoRA进行零样本、少样本和微调实验

Result: 使用TRepLiNa方法对齐中间层能有效改善低资源语言翻译，特别是在数据稀缺场景下

Conclusion: TRepLiNa是一种低成本、实用的方法，能有效提升低资源语言的翻译性能

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [7] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 提出了一个可扩展的多语言数据管理框架，用于在13个代表性不足的地区进行高质量的个人身份信息标注，显著提高了召回率和降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，确保其在不同监管环境下可靠处理个人身份信息变得至关重要。

Method: 采用分阶段、人在回路的多语言标注方法，结合语言专业知识和严格的质量保证，利用标注者间一致性度量和根本原因分析来发现和解决标注不一致问题。

Result: 框架在试点、训练和生产阶段显著提高了召回率和降低了误报率，生成了适合监督式大语言模型微调的高保真数据集。

Conclusion: 迭代式、分析驱动的流程可以提升多语言PII标注质量，并增强下游模型的可靠性。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [8] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 这是一个双语（英语-印地语）Prakriti评估问卷数据集，基于阿育吠陀原则评估个体的身体、生理和心理特征，包含24个多选题，用于计算Vata、Pitta、Kapha三种体质得分。


<details>
  <summary>Details</summary>
Motivation: 为阿育吠陀研究、计算智能和个性化健康分析提供标准化数据平台，支持特质分布分析、相关性研究和预测建模。

Method: 遵循AYUSH/CCRAS指南开发24项多选题问卷，通过Google Forms收集数据，隐藏体质标签以避免偏见，自动计算体质得分。

Result: 创建了一个结构化数据集，可用于研究体质分布、特质相关性，并为智能健康应用开发提供参考。

Conclusion: 该数据集为阿育吠陀研究和计算健康分析提供了有价值的资源，支持未来基于Prakriti的研究和智能健康应用开发。

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [9] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 提出了一种在嵌入式设备上运行的两阶段临床摘要系统，能够在保护患者隐私的同时离线生成结构化临床摘要。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含大量非结构化临床数据，急诊医生难以快速识别关键信息，需要一种能保护隐私的离线摘要解决方案。

Method: 采用双设备架构：Jetson Nano-R负责检索相关患者记录片段，Jetson Nano-S使用小型语言模型生成结构化摘要，通过轻量级socket链接通信。

Result: 在MIMIC-IV和真实匿名电子健康记录上的初步结果显示，系统能在30秒内有效生成有用的临床摘要。

Conclusion: 该系统证明了在嵌入式设备上实现完全离线临床摘要的可行性，为保护患者隐私的临床决策支持提供了新途径。

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [10] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 这篇论文是关于大语言模型幻觉问题的综述研究，系统分析了幻觉的类型、成因、检测方法和缓解策略，旨在提高LLMs的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然表现出色，但经常产生虚假或捏造的信息（幻觉现象），这严重影响了模型在需要事实准确性的领域中的可靠性和可信度。

Method: 采用综述研究方法，首先建立幻觉类型的分类体系，分析整个LLM开发周期中的根本原因，然后构建检测方法和缓解策略的结构化分类法，并评估现有基准和指标。

Result: 提出了全面的幻觉分类体系，系统梳理了检测和缓解方法，分析了现有方法的优缺点，并总结了评估基准和指标。

Conclusion: 幻觉是LLMs面临的关键挑战，需要从数据、架构到推理的整个开发周期进行系统性解决，论文为开发更真实可信的LLMs提供了研究基础和未来方向。

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [11] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 使用深度学习分析70年来Billboard音乐榜单歌曲，发现1990年后流行音乐中的露骨内容显著增加，包括粗俗、性暗示等不当语言。


<details>
  <summary>Details</summary>
Motivation: 音乐中滥用和性露骨内容急剧增加，特别是Billboard榜单，但缺乏验证趋势的研究来制定有效政策，因为这些内容对儿童和青少年有有害行为影响。

Method: 利用深度学习方法和语言模型分析美国Billboard榜单过去70年的歌曲歌词，通过情感分析和滥用检测进行纵向研究。

Result: 结果显示1990年后流行音乐中露骨内容显著上升，含有粗俗、性露骨和其他不当语言的歌曲越来越普遍。

Conclusion: 语言模型能够捕捉歌词内容中的细微模式，反映了社会规范和语言使用随时间的变化。

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [12] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 这篇论文复现了XRec框架，使用Llama 3替代GPT-3.5-turbo进行评估，并扩展分析了混合专家模块嵌入对解释结构的影响。


<details>
  <summary>Details</summary>
Motivation: 复现XRec框架的成果，验证其在可解释推荐中的有效性，同时探索混合专家模块嵌入在解释生成中的作用。

Method: 基于原始XRec源代码，使用Llama 3作为语言模型，修改或删除混合专家模块的输入/输出嵌入来评估框架性能。

Result: XRec能有效生成个性化解释，融入协同信息提高了稳定性，但并非在所有指标上都优于基线模型。混合专家嵌入对解释结构有重要影响。

Conclusion: XRec框架在可解释推荐中表现良好，混合专家嵌入对解释结构形成至关重要，研究提供了开源实现增强了可访问性。

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [13] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 该研究探讨多语言Transformer模型如何表示疑问句的形态句法特征，通过QTC数据集和回归探针方法分析七种语言中疑问句的类型和结构复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究多语言Transformer模型对疑问句形态句法特征的表示能力，评估上下文表示何时优于统计基线，以及参数更新是否减少预训练语言信息的可用性。

Method: 使用QTC数据集（包含七种语言的疑问句标注），采用回归探针方法配合选择性控制，比较冻结的Glot500-m模型表示与子词TF-IDF基线以及微调模型。

Result: 统计特征在具有显式标记的语言中能有效分类疑问句，而神经探针能更好地捕捉细粒度的结构复杂度模式。

Conclusion: 上下文表示在捕捉结构复杂度方面优于统计基线，但参数更新可能会减少预训练语言信息的可用性。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [14] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 提出了一种基于加权自适应损失的微调方法，用于将LLM的性别-职业输出分布与期望分布对齐，同时保持语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有偏见缓解研究主要关注促进社会平等和人口统计均等，但较少关注将LLM输出与期望分布对齐，例如与现实世界分布对齐以支持事实基础。

Method: 使用加权自适应损失进行微调，基于美国劳动力统计数据构建三个职业集（男性主导、女性主导和性别平衡），评估自适应方法（反映现实）和非自适应变体（追求平等）。

Result: 在三个掩码语言模型中，两种分布下都观察到偏见。在平等设置下实现近乎完全缓解，在现实世界设置下实现30-75%的减少。自回归LLM在平等设置下无偏见，但在现实世界设置下存在显著偏见，Llama Instruct模型实现50-62%的减少。

Conclusion: 该方法能有效将LLM输出与期望分布对齐，无论是追求平等还是反映现实，同时保持语言建模能力。

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [15] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: EVALUESTEER是一个评估LLMs和奖励模型在用户价值和风格偏好方面可操控性的基准，通过系统生成的165,888个偏好对进行测试，发现当前模型在识别和适应相关用户信息方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在全球部署，需要创建能够适应全球用户多样化偏好和价值观的多元化系统，而现有数据集不支持对奖励模型操控性的受控评估。

Method: 基于心理学和人机交互文献，系统生成165,888个偏好对，涵盖4个价值维度（传统、世俗理性、生存、自我表达）和4个风格维度（冗长性、可读性、自信度、温暖度），评估6个开源和专有LLMs在16种提示条件和6种偏好比较场景下的表现。

Result: 当提供完整的用户价值观和风格偏好时，最佳模型的准确率低于75%，而仅提供相关风格和价值偏好时准确率超过99%，突显当前奖励模型在识别和适应相关用户信息方面的局限性。

Conclusion: EVALUESTEER揭示了当前奖励模型在识别和适应相关用户信息方面的不足，为开发能够操控到多样化人类价值观和偏好的奖励模型提供了具有挑战性的测试平台。

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [16] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 提出了EverydayMMQA框架和OASIS数据集，用于解决多模态模型在文化背景和日常知识推理方面的不足，特别关注低资源语言和阿拉伯语变体。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态模型在视觉问答任务上表现良好，但在需要文化背景和日常知识的查询中表现不佳，特别是在低资源和代表性不足的语言中。

Method: 开发了EverydayMMQA框架来创建大规模、文化基础的数据集，并构建了OASIS多模态数据集，包含图像、语音和文本，支持四种输入组合。

Result: OASIS数据集包含约92万张图像和1480万个问答对，其中370万个是语音问题，覆盖18个国家的英语和阿拉伯语变体，测试模型在实用推理、常识和文化意识方面的能力。

Conclusion: EverydayMMQA和OASIS为构建具有文化背景理解能力的多模态大语言模型提供了基准和训练数据集，将向社区公开。

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [17] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: 提出了语义正则表达式（semantic regexes）作为LLM特征的结构化描述方法，比自然语言描述更精确、简洁和一致，支持量化特征复杂度和模型级分析。


<details>
  <summary>Details</summary>
Motivation: 现有LLM特征的自然语言描述存在模糊、不一致的问题，需要人工重新标注，需要更精确的结构化描述方法。

Method: 结合捕捉语言和语义特征模式的基本元素，以及用于上下文化、组合和量化的修饰符，构建语义正则表达式来描述LLM特征。

Result: 语义正则表达式在准确性上与自然语言相当，但提供了更简洁一致的特征描述，支持特征复杂度量化和模型级模式分析。

Conclusion: 语义正则表达式是LLM特征描述的有效结构化方法，能帮助人们建立准确的特征激活心智模型，推动从单个特征到模型级模式的自动化可解释性分析。

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [18] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 提出一种对抗基于搜索的链接攻击的方法，通过识别文档中罕见的N-gram片段，并使用LLM重写器迭代重写这些片段，以保护文本隐私同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的去识别化模型无法解决链接风险，即可能将去识别化文本映射回原始来源。基于搜索的链接攻击通过提取短语并检查其在原始数据集中的存在来执行链接。

Method: 方法分为两步：首先构建文档集合中N-gram的倒排索引，识别出现在少于k个文档中的N-gram；然后使用基于LLM的重写器迭代重写这些片段，直到无法进行链接。

Result: 在法院案例集合上的实验结果表明，该方法能有效防止基于搜索的链接，同时保持对原始内容的忠实度。

Conclusion: 该方法成功解决了去识别化中的链接风险问题，在保护隐私的同时维持了文本的语义质量。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [19] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: 提出了RegDiff框架，一种用于可控文本生成的规范化扩散方法，无需在采样时使用预训练分类器，降低了计算成本


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：无分类器引导(CFG)方法语义保持好但属性控制弱，分类器引导(CG)方法属性对齐好但计算成本高且存在泛化问题

Method: 采用VAE编码器-解码器架构保证重建保真度，结合属性监督训练的潜在扩散模型，仅在训练时注入属性信息

Result: 在五个涵盖多种风格属性的数据集上实验表明，RegDiff在生成风格化文本方面优于强基线方法

Conclusion: RegDiff是属性可控文本扩散的高效解决方案，验证了该方法的有效性

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [20] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 该论文研究了奖励模型(RMs)在语言模型对齐中的社会人口统计偏见问题，发现RMs与多个群体存在对齐不足，会系统性地奖励有害刻板印象，且仅靠提示引导不足以克服这些限制。


<details>
  <summary>Details</summary>
Motivation: 理解奖励模型的行为有限，需要研究RMs是否表现出社会人口统计偏见，以及如何引导奖励朝向目标群体的偏好。

Method: 建立测量RMs观点对齐的框架，研究RMs在争议话题上的主观多样化视角，探索提示引导对奖励的影响。

Result: RMs与多个人口统计群体对齐不佳，会系统性地奖励有害刻板印象，仅靠提示引导无法有效克服这些偏见限制。

Conclusion: 需要在偏好学习过程中更仔细地考虑RM行为，以防止在语言技术中传播不良社会偏见。

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [21] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 提出了一个基于大语言模型的框架，用于生成与虚拟实验室教学目标对齐的问题，通过教师-LLM对话、实验室理解、问题分类法和TELeR提示优化来提高问题质量。


<details>
  <summary>Details</summary>
Motivation: 虚拟实验室为探究式科学学习提供了机会，但教师难以调整第三方材料以满足教学目标，开发定制资源又耗时且难以扩展。

Method: 开发了包含四个组件的对齐框架：教学目标理解（教师-LLM对话）、实验室理解（知识单元和关系分析）、问题分类法（结构化认知和教学意图）、TELeR分类法（控制提示细节）。

Result: 评估了19个开源LLM生成的1100多个问题，结果显示：问题分类法提升了认知需求（开放格式和关系类型使质量提高0.29-0.39分），优化的TELeR提示提高了格式一致性（80%可解析性，>90%一致性），大型模型表现最佳（可解析性+37.1%，一致性+25.7%，平均质量+0.8分）。

Conclusion: 该框架成功利用LLM生成与虚拟实验室教学目标对齐的高质量问题，大型模型在问题生成方面表现尤为出色，为教师提供了有效的教学资源定制工具。

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [22] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: 提出了FinLFQA基准，用于评估LLMs在复杂金融问题中生成带可靠归因的长篇答案的能力，涵盖证据提取、数值推理和领域知识三个关键方面。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注简单的文本证据归因，但在金融等现实场景中，归因需要更细粒度的支持，包括数值推理和领域知识。

Method: 构建FinLFQA基准，通过人工标注评估三个归因方面：财务报告证据提取、中间数值推理步骤、领域特定金融知识。提供自动评估框架，在八个LLMs上进行多范式实验。

Result: 细粒度指标能更好区分模型能力；端到端生成与后处理方法性能相当；仅在有外部反馈时迭代优化才有效。

Conclusion: FinLFQA为金融领域的LLM归因提供了更全面的评估标准，揭示了当前模型在复杂归因任务中的局限性。

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [23] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST是首个统一的RST风格话语解析器，能够处理11种语言的18个树库，无需修改其关系清单。提出了两种训练策略：多头方法和掩码联合方法，后者在参数效率和性能上都表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决不同语言树库中关系清单不兼容的问题，实现单一模型的多语言话语解析，避免为每个树库单独训练模型。

Method: 提出了两种训练策略：多头方法为每个清单分配单独的关系分类层；掩码联合方法通过选择性标签掩码实现共享参数训练。还使用了简单有效的增强技术来处理低资源设置。

Result: 掩码联合方法不仅参数效率高，而且性能最强。UniRST在18个单树库基线中超越了16个，展示了单一模型在多语言端到端话语解析中的优势。

Conclusion: UniRST证明了统一的多语言话语解析器的可行性，掩码联合方法是最佳的训练策略，能够在保持参数效率的同时实现卓越性能。

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [24] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 论文提出MathRobust-LV测试集，评估语言模型对数学问题语言变体的鲁棒性，发现在保持数值结构和答案不变的情况下改变表面细节会导致模型准确率下降。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在数学基准测试中表现优异，但对语言变体的鲁棒性研究不足。教育场景中教师会以不同方式表述相同概念，因此语言鲁棒性对教育应用部署至关重要。

Method: 构建MathRobust-LV测试集，保持数学问题难度和答案不变，仅改变表面细节（如名称、上下文、变量），评估34个模型在基准版本和变体版本上的表现差异。

Result: 所有模型从基准版本到变体版本准确率都下降，小型模型下降9-11%，强模型也有可测量的退化，只有前沿模型如GPT-5、Gemini-2.5pro相对稳定。

Conclusion: 语言变体鲁棒性是基本挑战，暴露了模型推理的脆弱性，需要在教育应用部署中重视这一问题。

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [25] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 这是关于LLM智能体在网络安全领域应用的首次全面调查，围绕应用、威胁和防御三大支柱构建了包含150多篇论文的分类体系。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从被动工具向自主智能体的快速转变，这种智能体上下文引入了一类新的固有安全风险，需要系统性地理解这一新兴领域的整体安全态势。

Method: 通过构建包含应用、威胁和防御三大支柱的综合分类体系，对150多篇相关论文进行了全面调查和分类分析。

Result: 详细的分析揭示了智能体架构的新兴趋势，同时在模型和模态覆盖方面发现了关键的研究空白。

Conclusion: 这项工作为理解LLM智能体在网络安全领域的整体安全格局提供了首个系统性框架，识别了当前研究的重要空白。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [26] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 该研究在澳大利亚土著语言Yan-nhangu上微调wav2vec2 ASR模型，比较音位和正字法标记化策略，发现音位标记化显著提升性能，且ASR输出的人工修正比手动转录快得多。


<details>
  <summary>Details</summary>
Motivation: 现代ASR系统依赖数据密集型transformer架构，不适用于资源匮乏语言。研究旨在探索ASR在语言文档化流程中的可行性，特别是针对濒危语言。

Method: 在Yan-nhangu语言上微调wav2vec2 ASR模型，比较音位标记化和正字法标记化两种策略对性能的影响。

Result: 语言学驱动的音位标记化系统相比基线正字法标记化方案，显著降低了词错误率(WER)和字符错误率(CER)。ASR输出的人工修正速度远快于从头手动转录音频。

Conclusion: ASR可以成功应用于资源匮乏语言，音位标记化策略能有效提升性能，ASR可作为语言文档化流程中的实用工具。

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [27] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 测试时缩放(TTS)对通用推理模型在机器翻译中的效果有限，但在领域特定微调或后编辑场景下能显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 探索测试时缩放(TTS)在机器翻译任务中的有效性，目前该技术在数学和编程任务中表现良好，但在机器翻译领域的应用尚未充分研究。

Method: 评估12个推理模型在多个机器翻译基准上的表现，考察三种场景：直接翻译、强制推理外推和后编辑。

Result: 通用模型的TTS效果有限且不一致，性能很快达到平台期；领域特定微调能解锁TTS效果，带来持续改进；强制过度推理会降低质量；后编辑场景下TTS效果显著。

Conclusion: 推理时间计算在机器翻译中的价值不在于用通用模型增强单次翻译，而在于多步骤自校正工作流和任务专用模型的针对性应用。

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [28] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了Webscale-RL管道，将大规模预训练文档转化为120万个多样化、可验证的问答对用于强化学习，显著提升了模型性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs模仿学习造成的训练-生成差距和推理能力限制，以及强化学习面临的数据瓶颈问题——现有RL数据集规模远小于预训练语料。

Method: 开发了可扩展的Webscale-RL数据管道，系统地将大规模预训练文档转化为多样化的问答对，构建了包含120万个样本、覆盖9个以上领域的Webscale-RL数据集。

Result: 使用该数据集训练的模型在多个基准测试中显著优于持续预训练和强数据精炼基线，RL训练效率大幅提升，达到相同性能所需的token数量减少高达100倍。

Conclusion: 这项工作为实现强化学习扩展到预训练规模提供了可行路径，能够开发更强大、更高效的语言模型。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [29] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 论文实证研究发现，基于预训练模型的引导式预训练（bootstrapped pretraining）存在可预测的缩放效率递减规律：随着基础模型预训练token数量的增加，第二阶段预训练的缩放指数呈对数下降。


<details>
  <summary>Details</summary>
Motivation: 研究引导式预训练（如持续预训练或模型增长）在降低从头训练语言模型成本方面的有效性，特别是在应用于过度训练的基础模型时效果如何。

Method: 通过实证研究分析引导式预训练的缩放行为，建立了基于第一阶段和第二阶段token数量的简单缩放定律模型。

Result: 发现引导式预训练的缩放效率会随着基础模型预训练token数量的增加而递减，呈现出饱和效应。

Conclusion: 多阶段预训练策略存在基本权衡：模型预训练越充分，引导式预训练带来的额外收益越少，这对高效语言模型训练和过度训练模型的复用具有重要启示。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [30] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 论文发现原本训练为助手的语言模型不适合模拟真实用户，因此开发了专门用于模拟多轮对话中人类用户的User LMs，这些模型能更准确地反映人类行为并导致助手模型性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法使用助手语言模型来模拟用户，但这些模型与真实用户行为存在差异，需要更真实的用户模拟器来准确评估助手模型在多轮对话中的表现。

Method: 开发了专门用于模拟人类用户的User LMs模型，通过后训练使模型能够模拟真实用户在对话中的不完美表达、部分努力和实时修正等行为特征。

Result: User LMs比现有模拟方法更好地对齐人类行为，具有更好的模拟鲁棒性。在编程和数学对话模拟中，强助手模型（GPT-4o）的性能从74.6%降至57.4%。

Conclusion: 更真实的用户模拟环境揭示了助手模型在多轮对话设置中难以应对用户细微差别的局限性，表明需要专门设计的用户模拟器来准确评估助手模型性能。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [31] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 本文提出将语言模型的对齐问题重新定义为解析问题，通过神经符号架构将自然语言编译为Montague风格的逻辑形式，并映射到带有道义算子和司法管辖上下文的类型化本体，以解决幻觉、脆弱的内容审核和不透明的合规结果等问题。


<details>
  <summary>Details</summary>
Motivation: 当代语言模型虽然流畅，但经常错误处理其输出所蕴含的意义类型。作者认为幻觉、脆弱的内容审核和不透明的合规结果是缺乏类型论语义学的结果，而非数据或规模限制。

Method: 提出Savassan神经符号架构：神经组件从非结构化输入中提取候选结构；符号组件执行类型检查、约束推理和跨司法管辖映射，生成合规感知的指导而非二元审查。

Result: 系统能够"一次解析"并在多个法律本体中投影结果（如韩国/日本的诽谤风险、美国的受保护意见、欧盟的GDPR检查），将结果组合成单一可解释的决策。

Conclusion: 可信赖的自主性需要对意义进行组合类型化，使系统能够在统一的语义代数中推理描述性内容、规范性内容和责任承担。

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [32] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist是一个交互式、可扩展、可控的自动研究框架，旨在解决LLM自动研究工作流日益复杂的问题，提供开源代码库、Web演示和Python包。


<details>
  <summary>Details</summary>
Motivation: 随着LLM自动研究的快速发展，多智能体系统、规划、工具使用等复杂工作流给扩展和维护带来了显著挑战，需要解决这些复杂性。

Method: 识别自动研究工作流的关键组件，提出交互式、可扩展、可控的框架，支持新工具的适配和迭代增长。

Result: 开发了开源代码库、交互式Web演示和PyPI Python包，使最先进的自动研究管道对研究人员和开发者广泛可用。

Conclusion: TinyScientist框架有效解决了自动研究工作流的复杂性问题，为研究社区提供了易于使用和扩展的解决方案。

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [33] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 该研究分析了LLM的内部表示，重点关注隐藏层对越狱提示与良性提示的响应差异，为基于内部模型动态的越狱检测和防御提供了初步发现。


<details>
  <summary>Details</summary>
Motivation: 随着对话式LLM的普及，越狱攻击成为紧迫问题。攻击者通过精心设计的提示词获取受限输出，现有防御机制难以完全抵抗新型攻击技术。

Method: 分析开源LLM GPT-J和状态空间模型Mamba2的内部表示，比较隐藏层对越狱提示与良性提示的响应模式。

Result: 初步发现显示不同层在面对越狱和良性提示时表现出明显不同的行为特征。

Conclusion: 研究结果表明利用内部模型动态进行越狱检测和防御具有良好前景，为后续研究指明了方向。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [34] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 本文首次对状态空间模型(SSMs)和Transformer模型(TBMs)进行了统一的表示传播分析，发现两者在表示演化上存在关键差异：TBMs早期快速同质化token表示，后期重新多样化；SSMs早期保持token独特性，深层才趋于同质化。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型作为Transformer模型的高效替代方案在长序列处理中表现出色，但其跨层和跨token的上下文信息流动机制尚未得到充分研究。

Method: 使用中心核对齐、稳定性指标和探针分析等方法，系统分析了SSMs和TBMs中表示在层内和层间的演化过程。

Result: 发现TBMs的过平滑源于架构设计，而SSMs的过平滑主要来自训练动态。TBMs早期快速同质化token表示，后期重新多样化；SSMs早期保持token独特性，深层才趋于同质化。

Conclusion: 这些发现阐明了两种架构的归纳偏置，为未来长上下文推理的模型设计和训练提供了指导。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [35] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: SAO是一个完全自合成的LLM对齐框架，通过模型自身生成提示、回复和偏好数据，无需人工标注或外部奖励模型，有效提升聊天能力并保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要昂贵的人工标注数据，RLAIF也需要收集多样提示和回复并使用外部奖励模型，成本高昂。

Method: SAO框架让LLM进行角色扮演生成多样提示和回复，然后进行自我评估以优化偏好。

Result: 在AlpacaEval~2.0等基准测试中有效提升模型聊天能力，同时在下游客观任务（如问答、数学推理）上保持强劲性能。

Conclusion: 为LLM对齐的自我改进提供了实用解决方案，代码已开源。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [36] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: ToolMem是一个让智能体通过记忆工具能力来选择最优工具的框架，在文本和多模态生成任务中显著提升了工具选择和性能预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的智能体通常依赖固定工具，而神经工具在不同任务场景中表现不确定。人类通过交互积累对工具能力的理解并选择最优工具，因此需要让智能体也能从类似过程中受益。

Method: 提出ToolMem框架，让智能体从之前的交互中总结工具的优势和弱点并存储在记忆中；在推理时检索相关记忆条目，为单个任务选择最佳工具。

Result: 在文本生成和文本到图像生成任务中，ToolMem增强的智能体比无记忆通用智能体在工具性能预测上分别准确14.8%和28.7%；在多工具选择场景中，最优工具选择率分别提高了21%和24%。

Conclusion: ToolMem通过记忆工具能力有效提升了智能体在复杂任务中的工具选择和性能表现，证明了从交互中学习工具能力的重要性。

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [37] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: PiKa是一个数据高效的对齐数据集家族，仅使用3万SFT样本就能训练出超越官方Llama-3-8B-Instruct模型的性能，显著降低了LLM对齐的数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有对齐数据集要么是私有的，要么需要昂贵的人工标注，限制了可复现性和可扩展性。即使使用AI反馈，数据质量问题依然存在，且不清楚到底需要多少数据才能将基础模型微调成强大的指令跟随模型。

Method: 提出了PiKa数据高效对齐数据集家族，特别是PiKa-SFT数据集仅使用3万个SFT样本，远少于现有数据集。通过在Llama-3-8B-Base和Qwen2.5系列模型上进行微调实验验证效果。

Result: 在AlpacaEval 2.0和Arena-Hard基准测试中，PiKa-SFT微调甚至超越了官方Llama-3-8B-Instruct模型（使用超过1000万个专有样本训练）。Qwen2.5系列模型在PiKa-SFT上训练也获得了一致的性能提升。

Conclusion: 高质量的对齐可以通过显著更少的数据实现，为开源LLM对齐提供了可扩展的路径。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [38] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 提出了一个面向客服人员的增量式摘要系统，通过智能判断对话时机生成简洁要点，减少上下文切换和重复查看，结合微调模型和分类器过滤无关内容，通过用户编辑反馈持续优化模型。


<details>
  <summary>Details</summary>
Motivation: 减少客服人员在对话过程中的上下文切换负担和冗余查看，提升工作效率和摘要质量。

Method: 结合微调的Mixtral-8x7B模型进行持续笔记生成，使用DeBERTa分类器过滤琐碎内容，通过客服人员编辑反馈实现在线笔记优化和离线模型重训练。

Result: 在生产环境中，相比批量摘要方法，系统实现了3%的案件处理时间减少（在高度复杂情况下可达9%），同时获得较高的客服满意度评分。

Conclusion: 增量式摘要结合持续反馈机制能有效提升摘要质量和客服人员生产力，具有规模化应用价值。

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [39] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 提出了一种专门针对机器翻译任务的提示优化方法，使用基于回译策略训练的小参数模型，显著降低单任务优化的训练开销，同时保持高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法主要优化指令组件，但在机器翻译等任务中，输入组件更为关键，且现有方法通常需要大参数LLM作为辅助工具，适用性有限。

Method: 使用基于回译策略训练的小参数模型进行提示优化，专门针对机器翻译任务设计。

Result: 该方法显著降低了训练开销，同时提供了高效的性能表现。

Conclusion: 该方法可有效解决机器翻译任务中的提示优化问题，并可通过适当调整扩展到其他下游任务。

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [40] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: 研究发现LLMs在内部表示中线性编码了有效性和合理性概念，且两者在表示几何中高度对齐，导致模型混淆合理性与有效性。通过操纵表示向量可以因果性地影响判断，并构建解偏向量来减少内容效应。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs中内容效应的机制，即语义内容的合理性如何影响逻辑有效性判断，这与人类推理中的双过程理论相似但机制不明。

Method: 分析LLMs内部表示中有效性和合理性的编码方式，使用操纵向量技术验证因果关系，构建解偏向量来分离这两个概念。

Result: 发现有效性和合理性在LLMs表示中线性编码且高度对齐，这种对齐程度预测了行为内容效应的强度，解偏向量能有效减少内容效应并提高推理准确性。

Conclusion: LLMs中抽象逻辑概念的表示方式导致内容效应，通过表示干预可以构建更符合逻辑的系统，这推进了对LLMs推理机制的理解。

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [41] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出了SUPO算法，通过总结增强的上下文管理来解决LLM智能体在长序列多轮工具使用中的上下文长度瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在长序列多轮工具使用中面临上下文长度限制、指令跟随退化和高昂的rollout成本等挑战

Method: 引入基于总结的上下文管理，定期压缩工具使用历史，通过LLM生成的总结保留任务相关信息，在保持紧凑上下文的同时让智能体能够超越固定上下文窗口

Result: 在交互式函数调用和搜索任务中，SUPO显著提高了成功率，同时保持相同或更低的上下文长度；在复杂搜索任务中，当测试时的最大总结轮数超过训练时，SUPO能进一步提升评估性能

Conclusion: 基于总结的上下文管理为训练超越固定上下文长度限制的RL智能体提供了一种原则性和可扩展的方法

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [42] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 提出了PTEB动态评估协议，通过生成语义保留的释义变体来测试句子嵌入模型的鲁棒性，发现模型性能对词汇变化敏感，建议转向动态评估范式。


<details>
  <summary>Details</summary>
Motivation: 当前句子嵌入模型评估依赖静态测试集（如MTEB），重复调优会夸大性能并掩盖真实世界的鲁棒性问题。

Method: 使用基于LLM的成本效益方法生成语义保留的释义变体，在7个MTEB任务上进行动态评估，并扩展到3个多语言数据集。

Result: 验证了句子编码器性能对词汇空间变化的敏感性，发现较小模型受影响程度与较大模型相当，结果在多次运行中具有统计鲁棒性。

Conclusion: 建议NLP评估从静态基准转向动态、随机评估范式，利用评估时计算来更真实地反映模型性能。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [43] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: RAF是一种两阶段令牌优化方法，通过生成简洁的文本扰动来操纵LLM的重新排序行为，显著提升目标项目在排名中的位置，同时保持语言的自然性。


<details>
  <summary>Details</summary>
Motivation: 暴露LLM在信息检索中作为重新排序器的脆弱性，展示其排名行为容易被自然语言提示操纵的安全隐患。

Method: 两阶段令牌优化：阶段1使用贪婪坐标梯度结合可读性评分筛选候选令牌；阶段2通过基于熵的动态加权方案评估候选令牌，使用温度控制采样选择最佳令牌。

Result: 实验表明RAF能显著提升目标项目的排名，比现有方法在提升排名和保持自然性方面更具鲁棒性。

Conclusion: LLM基于的重新排序本质上容易受到对抗性操纵，这对现代检索系统的可信度和鲁棒性提出了新的挑战。

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [44] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种基于权重矩阵的训练无关指纹识别方法，用于验证语言模型的血统，能够有效抵抗多种后训练操作的干扰。


<details>
  <summary>Details</summary>
Motivation: 保护大语言模型的知识产权至关重要，需要可靠的方法来识别模型是否基于现有基础模型训练，但后训练过程给识别带来巨大挑战。

Method: 利用线性分配问题(LAP)和无偏中心核对齐(CKA)相似度来消除参数操作的影响，获得高度鲁棒和保真的相似度度量。

Result: 在60个正样本和90个负样本模型对的测试中，该方法对所有六类后训练操作都表现出卓越的鲁棒性，假阳性风险接近零，所有分类指标均达到完美分数。

Conclusion: 该方法为可靠的模型血统验证建立了坚实基础，计算效率高，在NVIDIA 3090 GPU上30秒内完成。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [45] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出一种无需训练和标签的短文本聚类方法，可在任何现有嵌入器上使用，通过迭代向量更新和LLM指导实现意图聚类。


<details>
  <summary>Details</summary>
Motivation: 在面向客户的聊天机器人场景中，公司需要处理大量用户话语并按意图聚类，但商业环境中通常没有标注数据且聚类数量未知。

Method: 基于迭代向量更新：构建基于代表性文本的稀疏向量，然后通过LLM指导迭代优化，无需假设聚类数量或标签的先验知识。

Result: 在多样化数据集和较小LLM上的实验表明，该方法与使用对比学习的最先进方法相比具有可比或更优结果，且模型无关、可扩展到大型数据集。

Conclusion: 该方法在低资源、适应性强的设置下具有可扩展性，比现有聚类方法更符合现实世界场景。

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [46] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 提出了一个基于流畅度的多参考评估框架，将n-gram相似度作为多个合法修正的聚合问题，通过四种聚合策略来评估语法错误修正。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要基于编辑操作且以英语为中心，依赖于系统和参考编辑之间的刚性对齐，限制了在多语言和生成式设置中的适用性。需要反映人类修正多样性的评估指标。

Method: 引入基于流畅度的多参考评估框架，将n-gram相似度作为聚合问题，实例化了四种聚合策略：选择最佳、简单平均、加权平均和合并计数。

Result: 在捷克语、爱沙尼亚语、乌克兰语和中文语料库上的实证结果表明，这些策略捕捉了流畅性和覆盖率的互补方面。

Conclusion: 该框架将多参考评估统一为一个原则性的、以流畅度为导向的方法，在不惩罚合法变体的情况下纳入语言多样性。

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [47] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: 提出一种叠加部署策略，通过轻量级、无需训练的调控机制，在推理时选择性遗忘LRM的某些部分，从而在保持推理能力的同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在结构化任务中表现出色，但容易过度思考，导致性能下降和资源浪费。现有方法需要部署多个模型进行路由，成本高且不实用。

Method: 通过分析奇异值的累积能量，识别最优低秩投影，在推理时调整LRM的推理程度，实现计算缩放。

Result: 该方法能够在保持推理能力的同时有效减少计算资源消耗。

Conclusion: 提出的叠加部署策略和调控机制为优化大型推理模型的推理效率提供了实用解决方案。

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [48] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出自适应多范式神经符号推理框架，自动识别自然语言问题中的形式推理策略，动态选择和应用专门的形式逻辑求解器，在多项推理任务中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号NLP方法大多是静态的，目标求解器的集成在设计时就已确定，这限制了使用多样化形式推理策略的能力。

Method: 开发自适应多范式神经符号推理框架：(1)自动识别自然语言问题中的形式推理策略；(2)通过自动形式化接口动态选择和应用专门的形式逻辑求解器。

Result: LLMs在预测必要形式推理策略方面准确率超过90%；框架性能比GPT-4o和DeepSeek-V3.1分别高出27%和6%；自适应推理还能提升纯LLM方法，在零样本、CoT和符号CoT设置下分别获得10%、5%和6%的增益。

Conclusion: 这项工作为自适应LLM符号推理奠定了基础，为在异构推理挑战中统一物质和形式推理提供了前进路径。

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [49] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 该研究系统评估了从大语言模型中提取结构化知识的可行性，分析了终止性、可重现性和鲁棒性，发现虽然能可靠提取核心知识，但在可重现性和某些扰动类型下存在局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型蕴含大量事实知识，但如何系统测量和结构化这些知识仍具挑战性。需要研究知识提取是否能终止、输出是否可重现以及对变化的鲁棒性。

Method: 使用miniGPTKBs（领域特定、可处理的子知识库）方法，通过四种变化（种子、语言、随机性、模型）和三个示例领域（历史、娱乐、金融），分析终止率、可重现性和鲁棒性。

Result: 研究显示：(i) 终止率高但依赖模型；(ii) 可重现性结果不一；(iii) 鲁棒性因扰动类型而异：对种子和温度变化鲁棒性高，对语言和模型变化鲁棒性较低。

Conclusion: 大语言模型知识物化能可靠提取核心知识，但也揭示了重要局限性，特别是在可重现性和某些扰动条件下的稳定性方面。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [50] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: FURINA-Builder是一个创新的多智能体协作管道，能够自动构建完全可定制的角色扮演基准测试，解决了现有基准测试范围狭窄、交互范式过时和适应性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在角色扮演任务中的进步，现有基准测试由于范围狭窄、交互范式过时以及在不同应用场景中适应性有限而迅速过时，需要新的评估方法。

Method: FURINA-Builder采用多智能体协作管道，模拟测试角色与来自精心构建的角色-场景池中其他角色之间的对话，同时使用LLM法官选择细粒度评估维度并调整测试角色的响应为最终测试话语。

Result: 构建了FURINA-Bench基准测试，评估发现o3和DeepSeek-R1分别在英文和中文角色扮演任务中表现最佳。所有模型中，已建立角色始终优于合成角色，推理能力进一步放大了这种差距。发现模型规模并不单调减少幻觉，推理LLMs存在性能与幻觉之间的权衡。

Conclusion: FURINA-Builder证明了其有效性，FURINA-Bench提出了挑战，揭示了角色扮演性能与可靠性之间的帕累托前沿权衡，特别是推理能力在提升性能的同时增加了幻觉。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [51] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: PAN 2025生成性抄袭检测任务旨在识别科学文章中的自动生成文本抄袭并匹配其来源。通过使用Llama、DeepSeek-R1和Mistral三个大语言模型创建了新颖的大规模自动生成抄袭数据集，评估了参与者方法和基线的性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动生成文本抄袭检测问题，特别是在科学文章领域，评估当前方法的有效性并与传统抄袭检测数据集进行对比。

Method: 使用三个大语言模型（Llama、DeepSeek-R1、Mistral）创建大规模自动生成抄袭数据集，比较参与者提交的方法和四个基线方法，并在PAN 2015数据集上评估方法的鲁棒性。

Result: 当前方法多样性不足，基于嵌入向量的朴素语义相似度方法表现最佳（召回率0.8，精确率0.5），但这些方法在2015数据集上表现显著下降，表明泛化能力不足。

Conclusion: 虽然当前方法在检测自动生成抄袭方面取得了一定成效，但缺乏泛化能力，需要在方法多样性和鲁棒性方面进一步改进。

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [52] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了通过集成多种电路定位方法来提高大语言模型中电路识别的性能，提出了并行和序列两种集成方法，在MIB基准测试中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的单一电路定位方法在识别大语言模型中负责特定任务行为的子网络时存在局限性，需要探索集成方法是否能提升定位精度。

Method: 提出了两种集成方法：并行集成（通过平均、最小或最大值组合不同方法的边属性分数）和序列集成（使用EAP-IG分数作为更精确的边剪枝方法的预热启动）。

Result: 两种集成方法在基准测试指标上都取得了显著提升，其中并行集成（包括序列集成）获得了最佳结果。

Conclusion: 集成多种电路定位方法可以有效提高电路识别的精度，为机械可解释性研究提供了更可靠的电路定位工具。

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [53] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR是一个基于模拟的训练框架，用于工具增强推理，通过多智能体架构生成模拟观察数据，避免依赖实时API，在训练和部署中实现更好的可扩展性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强语言模型依赖实时API访问，在训练和部署时存在可扩展性和可靠性问题。

Method: 采用多智能体架构：ToolMaker生成任务特定工具接口，AutoAgent生成结构化思考-行动-观察序列，ToolActor模拟真实响应。训练分两阶段：SFT学习轨迹语法，GRPO优化策略。

Result: 在四个多跳问答基准测试中，MTR达到与实时API系统相当的精确匹配分数，在推理密集型任务上表现优异。

Conclusion: 有效的工具推理可以通过结构化轨迹学习，无需实时交互。

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [54] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 本文首次系统性地调查了LLM训练中的中间训练阶段，提出了涵盖数据分布、学习率调度和长上下文扩展的分类法，并总结了实用见解、评估基准和性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管中间训练阶段在先进系统中被广泛使用，但此前缺乏对其作为统一范式的系统性调查，需要建立分类框架来促进结构化比较。

Method: 引入首个LLM中间训练分类法，涵盖数据分布、学习率调度和长上下文扩展三个维度，并汇编评估基准和性能增益数据。

Result: 成功建立了中间训练的统一分类框架，提供了实用见解和评估基准，能够支持跨模型的结构化比较。

Conclusion: 中间训练是LLM开发的重要阶段，通过梯度噪声尺度、信息瓶颈和课程学习等机制促进泛化和抽象能力，但仍存在开放挑战需要未来研究。

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [55] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 构建大规模挑战集来评估自动质量评估指标中的性别偏见，覆盖33种语言对，通过对比同一文本的男性和女性版本翻译来检测偏见。


<details>
  <summary>Details</summary>
Motivation: 机器翻译中的性别偏见已被广泛研究，但自动质量评估指标的性别偏见相对较少探索，现有研究存在数据集小、职业覆盖窄、语言种类有限的问题。

Method: 基于GAMBIT语料库，扩展到3种无性别或自然性别的源语言和11种有语法性别的目标语言，构建33种语言对，每个源文本对应两个仅在职业术语语法性别上不同的目标版本。

Result: 创建了大规模、全面且完全平行的数据集，支持按职业进行细粒度偏见分析和跨语言系统比较。

Conclusion: 无偏见的QE指标应该给男性和女性版本的翻译分配相同或接近的分数，该数据集为系统评估QE指标中的性别偏见提供了有效工具。

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [56] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 提出了SID方法，利用LLM的自信号（模型级置信度和词级语义焦点）来指导多LLM辩论过程，提高准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有多LLM辩论方法主要依赖外部结构而忽略了生成过程中的自信号，导致计算冗余和性能下降

Method: 利用模型级置信度实现高置信度代理提前退出，基于注意力机制压缩冗余辩论内容

Result: 在多个基准测试中优于现有MAD技术，同时减少了token消耗

Conclusion: 利用自信号能有效提升多代理辩论系统的性能和效率

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [57] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: OpenJAI-v1.0是基于Qwen3-14B开发的开源泰语和英语大语言模型，在指令跟随、长上下文理解和工具使用等关键任务上表现优异，超越了其他领先的开源泰语模型。


<details>
  <summary>Details</summary>
Motivation: 为泰语AI社区提供高质量的开源NLP资源，提升在实用任务上的性能表现。

Method: 从Qwen3-14B模型出发，通过精心策划的数据集在三个关键用例上进行优化：指令跟随、长上下文理解和工具使用。

Result: 评估结果显示OpenJAI-v1.0在基础模型能力上有所提升，在多样化基准测试中超越了其他领先的开源泰语模型，同时避免了灾难性遗忘。

Conclusion: OpenJAI-v1.0作为泰语AI社区的又一替代NLP资源公开发布，为泰语自然语言处理提供了新的选择。

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [58] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 本文研究了LLMs在上下文感知翻译中的语篇现象处理能力，提出质量感知解码(QAD)方法，能有效提取LLMs中的语篇知识并提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器翻译中表现出色，但在处理语篇现象（如代词解析、词汇连贯性）方面仍有不足，需要研究如何更好地利用其内在的语篇知识。

Method: 提出质量感知解码(QAD)方法，通过综合分析和比较不同解码方法，验证QAD在提取LLMs语篇知识方面的有效性。

Result: 研究表明LLMs确实编码了语篇知识，QAD方法优于其他解码方法，能提升翻译的语义丰富度并更符合人类偏好。

Conclusion: QAD是一种有效的解码策略，能够充分利用LLMs中的语篇知识，显著改善上下文感知翻译的质量。

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [59] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: 提出了λ-GRPO方法，通过可学习的参数λ自适应控制token级权重，解决了GRPO中的长度偏差问题，在数学推理基准上相比GRPO和DAPO获得了一致性改进。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法如GRPO存在长度偏差问题，相同的优势被均匀分配给响应中的所有token，导致较长响应在梯度更新中贡献不成比例。现有变体方法如DAPO和Dr.GRPO虽然修改了token级损失聚合，但仍是启发式方法且可解释性有限。

Method: 将现有框架统一到单一公式下，引入可学习参数λ来自适应控制token级权重，称为λ-GRPO方法。

Result: 在1.5B、3B和7B参数的Qwen2.5模型上，λ-GRPO相比GRPO分别提高了+1.9%、+1.0%和+1.7%的平均准确率，且无需修改训练数据或增加计算成本。

Conclusion: 学习token偏好在RLVR中是有效且实用的方法，λ-GRPO能够一致性地改进现有方法，证明了自适应token权重学习的重要性。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [60] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MeXtract是一个轻量级语言模型家族，专门用于从科学论文中提取元数据，在MOLE基准测试中达到最先进性能，并展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 科学文献元数据提取对于索引、记录和分析至关重要，但传统方法难以跨领域和模式变化泛化。

Method: 通过微调Qwen 2.5模型构建了参数规模从0.5B到3B的轻量级模型家族，并扩展了MOLE基准以包含模型特定元数据。

Result: MeXtract在其规模家族中在MOLE基准测试上达到最先进性能，对给定模式微调不仅获得高准确率，还能有效迁移到未见模式。

Conclusion: 该方法展示了鲁棒性和适应性，所有代码、数据集和模型已向研究社区开源。

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [61] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出了Long-RewardBench基准来评估长上下文奖励模型，并开发了多阶段训练策略来增强模型在长上下文场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在长历史轨迹应用中的普及，现有奖励模型局限于短上下文设置，缺乏对长上下文-响应一致性的评估能力。

Method: 设计了包含成对比较和最佳选择任务的Long-RewardBench基准，并提出多阶段训练策略来扩展任意模型为长上下文奖励模型。

Result: 8B参数的LongRM模型超越了70B规模的基线模型，性能与专有Gemini 2.5 Pro模型相当，同时保持了强大的短上下文能力。

Conclusion: 多阶段训练策略能有效提升奖励模型在长上下文场景中的鲁棒性，解决了现有模型在长上下文偏好判断中的脆弱性问题。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [62] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS是一个推理框架，让语音语言模型在用户说话时就能生成内部推理，实现实时低延迟的语音交互，通过分块处理语音输入和生成未说出的思维链来提升交互效率。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型只在用户说完后才开始思考和行动，导致高延迟，不适合需要实时交互的语音对话场景。受人类"边听边想"的启发，需要让模型在用户说话时就能进行推理。

Method: SHANKS将输入语音流式分块处理，每收到一个语音块就基于之前的语音和推理生成未说出的思维链推理，同时决定是否打断用户或调用工具完成任务。

Result: 在数学解题场景中，SHANKS的打断准确率比无思考基线高37.1%；在工具增强对话中，56.9%的工具调用能在用户说完前完成。

Conclusion: SHANKS实现了模型在整个对话过程中持续思考，而不仅是在用户说完后才开始，推动了实时语音交互的发展。

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [63] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: Open ASR Leaderboard是一个完全可复现的基准测试和交互式排行榜，比较了60多个开源和专有系统在11个数据集上的表现，包含多语言和长文本专用赛道。


<details>
  <summary>Details</summary>
Motivation: 当前ASR评估主要集中于短文本英语，且很少报告效率指标，需要更全面、标准化的评估框架。

Method: 标准化文本归一化，同时报告词错误率(WER)和逆实时因子(RTFx)，支持公平的准确率-效率比较。

Result: 英语转录中，Conformer编码器搭配LLM解码器获得最佳平均WER但速度较慢，而CTC和TDT解码器提供更好的RTFx，适合长文本和离线使用。Whisper编码器在英语上微调可提高准确率但会牺牲多语言覆盖。

Conclusion: 所有代码和数据集加载器都已开源，支持透明、可扩展的评估。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [64] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 使用LLMs生成符合学生兴趣和数学教育标准的数学应用题，通过教师标注数据训练模型，在保持题目质量的同时实现个性化定制。


<details>
  <summary>Details</summary>
Motivation: 教师因班级规模大和工作倦怠而难以个性化定制数学应用题，LLMs可以支持数学教育，生成符合学生兴趣和能力的题目。

Method: 采用人类专家-LLM联合评估方法，分析11,000多个MWPs，创建首个教师标注数据集，并训练12B和30B开源模型。

Result: 训练的开源模型性能与更大模型相当，30B模型无需训练即可超越现有闭源基线，生成的题目更接近人类编写，学生测试表现相似但更偏好定制题目。

Conclusion: LLMs可以有效生成个性化数学应用题，支持数学教育，学生对这些定制题目表现出更高的偏好。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [65] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [66] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 提出了一种名为摘要增强分块(SAC)的方法，通过在文本块中添加文档级合成摘要来改善法律领域检索增强生成(RAG)系统的可靠性，有效减少文档级检索不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 法律领域的大规模结构相似文档数据库常常导致检索系统失败，特别是文档级检索不匹配(DRM)问题，即检索器从完全错误的源文档中选择信息，这严重影响了RAG系统在法律应用中的可靠性。

Method: 采用摘要增强分块(SAC)技术，在标准分块过程中为每个文本块添加文档级合成摘要，注入关键的全局上下文信息。比较了通用摘要策略与包含法律专家领域知识的特定法律元素方法。

Result: 实验表明SAC显著减少了DRM，并因此提高了文本级检索的精确率和召回率。有趣的是，通用摘要策略优于包含法律专家领域知识的方法。

Conclusion: SAC是一种实用、可扩展且易于集成的技术，能够显著提升RAG系统在大规模法律文档数据集上的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [67] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 提出了一个结合人工翻译和合成扩展的流程，用于创建高质量的印度语言后训练数据集，以解决现有开源数据集在印度语言上的覆盖不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集缺乏多语言覆盖、文化背景，且任务多样性不足，特别是在印度语言方面存在明显差距。

Method: 采用人工参与的流程，结合翻译和合成扩展，创建了两个数据集：Pragyaan-IT（22.5K）和Pragyaan-Align（100K），涵盖10种印度语言、13个大类和56个子类，利用了57个不同数据集。

Result: 开发的数据集协议包含了常被忽视的维度，强调任务多样性、多轮对话、指令保真度、安全对齐和文化细微差别的保留。

Conclusion: 为更包容和有效的多语言大语言模型提供了基础。

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [68] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: 提出Native Hybrid Attention (NHA)，一种结合线性注意力和完全注意力的混合架构，通过RNN维护长期上下文和滑动窗口补充短期token，在保持效率的同时提升长上下文召回精度。


<details>
  <summary>Details</summary>
Motivation: Transformer存在二次复杂度问题，线性注意力虽然高效但在长上下文召回精度上表现不佳，需要一种既能保持效率又能提升召回准确性的混合方案。

Method: NHA采用层内和层间混合的统一层设计，使用线性RNN更新key-value槽位维护长期上下文，滑动窗口补充短期token，通过单个softmax注意力操作实现上下文相关加权。

Result: 实验显示NHA在召回密集型和常识推理任务上超越Transformer和其他混合基线，预训练LLM与NHA结合能在保持竞争力的同时显著提升效率。

Conclusion: NHA通过统一的混合架构设计，在效率和精度之间实现了良好平衡，为长上下文建模提供了有效的解决方案。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [69] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 对GPT-4.1的1亿条信念进行深度分析，发现其事实知识与现有知识库差异显著，准确性低于先前基准，存在不一致性、模糊性和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLMs的事实知识虽然重要但理解不足，通常基于有偏样本分析，需要系统研究前沿LLM的真实知识状况。

Method: 基于GPTKB v1.5数据集，对GPT-4.1的1亿条递归获取的信念进行深度分析。

Result: 模型事实知识与现有知识库差异显著，准确性明显低于先前基准，存在严重的不一致性、模糊性和幻觉问题。

Conclusion: LLMs的事实知识存在显著问题，揭示了未来关于事实性LLM知识研究的重要机会。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [70] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 这篇论文是对代码转换（CSW）感知大语言模型研究的首次全面分析，涵盖了5个研究领域、12个NLP任务、30多个数据集和80多种语言，旨在解决LLM在多语言混合输入方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型快速发展，但代码转换（在同一话语中交替使用不同语言和文字）仍然是多语言NLP的基本挑战。大多数LLM在处理混合语言输入、有限CSW数据集和评估偏见方面仍存在困难，阻碍了在多语言社会中的部署。

Method: 该调查通过架构、训练策略和评估方法对最新进展进行分类，分析了LLM如何重塑CSW建模以及哪些挑战仍然存在。涵盖了5个研究领域、12个NLP任务、30多个数据集和80多种语言的研究。

Result: 研究提供了CSW感知LLM研究的全面分析框架，识别了当前研究的主要方向和成果，并维护了一个精选资源集合。

Conclusion: 论文提出了一个路线图，强调需要包容性数据集、公平评估和基于语言学的模型，以实现真正的多语言智能。所有资源在GitHub上维护。

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [71] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: Search-R3是一个新颖框架，通过让LLMs在推理过程中直接生成搜索嵌入，将LLMs应用于检索任务。该方法结合监督学习和强化学习，优化嵌入生成和推理过程，显著提升了复杂知识密集型任务的处理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs具有强大的自然语言理解能力，但在检索任务中尚未得到充分利用。Search-R3旨在解决这一限制，通过适应LLMs生成搜索嵌入作为其推理过程的直接输出。

Method: 采用三种互补机制：(1)监督学习阶段使模型能够产生高质量嵌入；(2)强化学习方法同时优化嵌入生成和推理；(3)专门的RL环境，无需在每次训练迭代时重新编码整个语料库即可处理不断演变的嵌入表示。

Result: 在多样化基准测试上的广泛评估表明，Search-R3通过统一推理和嵌入生成过程，显著优于先前的方法。

Conclusion: 这种集成的后训练方法在处理需要复杂推理和有效信息检索的复杂知识密集型任务方面代表了重大进展。

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [72] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: 研究发现辛克莱广播集团收购地方新闻台后，报道内容从地方议题转向全国性政治化话题


<details>
  <summary>Details</summary>
Motivation: 调查辛克莱广播集团收购地方新闻台对报道内容的影响，因为地方新闻台通常被认为是可靠的非政治化信息来源

Method: 使用计算方法分析地方新闻台在被辛克莱收购前后的网络内容变化，并与全国性新闻媒体进行比较

Result: 地方新闻台更频繁报道全国性新闻而减少地方议题报道，对极化全国性话题的报道增加

Conclusion: 辛克莱收购导致地方新闻台报道重点从地方议题转向全国性政治化内容

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [73] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: ITEM是一个大规模基准测试，系统评估了26种自动指标与6种主要印度语言的人类判断之间的对齐情况，发现LLM评估器与人类判断最一致，离群值对指标-人类一致性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有自动指标主要针对英语等高资源语言开发，而印度语言（超过15亿人使用）被忽视，这让人质疑当前评估实践的普适性。

Method: 引入ITEM基准，通过细粒度注释系统评估26种自动指标与人类判断在6种印度语言中的对齐情况，涵盖与人类判断的一致性、对离群值的敏感性、语言特定可靠性、指标间相关性和对受控扰动的鲁棒性。

Result: 四个主要发现：(1) LLM评估器在片段和系统级别与人类判断最一致；(2) 离群值对指标-人类一致性有显著影响；(3) 在文本摘要中指标更能捕捉内容保真度，在机器翻译中更能反映流畅性；(4) 不同指标对扰动的鲁棒性和敏感性不同。

Conclusion: 这些发现为推进印度语言的指标设计和评估提供了关键指导。

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [74] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 该论文提出了一种跨语言指令调优方法，为卢森堡语创建高质量指令数据集，避免机器翻译带来的语义和文化不准确问题。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如卢森堡语缺乏高质量的指令数据集，传统依赖机器翻译的方法会引入语义偏差和文化不准确。

Method: 利用英语、法语和德语的对齐数据构建跨语言指令调优数据集，不依赖机器翻译到卢森堡语。

Result: 跨语言指令调优不仅改善了语言间的表示对齐，还提升了模型在卢森堡语中的生成能力。

Conclusion: 跨语言数据管理可以避免机器翻译数据的常见缺陷，直接有益于低资源语言的发展。

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [75] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: LocalLeap是一种训练自适应的并行解码策略，通过识别高置信度锚点并进行局部松弛并行解码，显著减少扩散大语言模型的推理步骤，实现6.94倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有的开源扩散大语言模型在质量与速度之间存在权衡，保守的采样策略（如贪婪解码）导致推理效率低下，出现延迟解码现象。

Method: 基于局部确定性传播和渐进空间一致性衰减两个经验原则，识别锚点并在有界邻域内进行局部松弛并行解码。

Result: 在各种基准测试中，LocalLeap实现了6.94倍的吞吐量提升，将解码步骤减少到原始需求的14.2%，且对输出质量影响可忽略不计。

Conclusion: LocalLeap有效解决了扩散大语言模型的延迟解码问题，显著提升了推理效率而不影响输出质量。

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [76] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: 提出了VITAL评估指标，通过考虑声明相对于查询的相关性和重要性，更敏感地检测LLM响应中关键信息的错误。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将所有声明视为同等重要，导致在关键信息缺失或错误时产生误导性评估，无法可靠检测关键信息错误。

Method: 构建VITALERRORS基准数据集（6,733个查询），设计最小化修改的LLM响应来遗漏或伪造关键信息，并开发VITAL指标来评估事实性。

Result: VITAL指标比现有方法更可靠地检测关键信息错误，现有评估指标对关键信息错误不敏感。

Conclusion: VITAL指标、数据集和分析为更准确、鲁棒的LLM事实性评估提供了基础。

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [77] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型增强的检索增强框架，用于讽刺感知语音合成，结合语义嵌入和韵律范例，在VITS骨干网络中实现更自然的讽刺语音生成。


<details>
  <summary>Details</summary>
Motivation: 讽刺作为一种微妙的非字面语言形式，在语音合成中面临挑战，现有研究主要关注广泛的情感类别，而讽刺语音合成尚未充分探索。

Method: 使用LoRA微调的LLaMA 3获取语义嵌入捕获讽刺的语用不一致性和话语级线索，结合RAG模块检索的韵律范例提供讽刺表达的参考模式，在VITS骨干网络中进行双重条件控制。

Result: 实验表明该方法在客观指标和主观评估中均优于基线，在语音自然度、讽刺表达性和下游讽刺检测方面均有改进。

Conclusion: 提出的LLM增强检索增强框架能够有效合成自然且上下文适当的讽刺语音，为复杂情感语音合成提供了新思路。

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [78] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: TALENT是一个轻量级框架，通过结合OCR文本和自然语言叙述来解决表格视觉问答问题，使用小型VLM进行感知叙述，LLM进行推理，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统大型视觉语言模型在表格VQA中容易忽略细节且计算成本高，而基于OCR的方法存在结构化表示不优化和错误较多的问题。

Method: 使用小型VLM生成OCR文本和自然语言叙述，然后将两者与问题结合，由LLM进行推理，将表格VQA重构为LLM中心的多模态推理任务。

Result: 在公共数据集和新构建的ReTabVQA数据集上，TALENT使用小型VLM-LLM组合能够匹配或超越单个大型VLM的性能。

Conclusion: TALENT框架通过双表示方法有效解决了表格VQA问题，在保持性能的同时显著降低了计算成本，特别适合移动部署。

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [79] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 提出了一个建模人类标注差异的系统，利用LLM的上下文学习能力和两阶段元学习训练方法，在LeWiDi竞赛中获胜，并通过消融研究验证了系统组件的重要性。


<details>
  <summary>Details</summary>
Motivation: 许多NLP任务存在主观性、模糊性或标注者之间的合理分歧，需要建模人类标注的差异性。

Method: 利用LLM的上下文学习能力，采用两阶段元学习训练：1）在多个需要上下文学习的数据集上进行后训练；2）通过上下文元学习针对特定数据分布进行专业化。

Result: 在LeWiDi竞赛的两个任务中都获得了总体冠军，消融研究表明：包含标注者示例对性能至关重要，在较大数据集上特定数据集微调有帮助，在一个竞赛数据集上后训练其他上下文数据集有帮助，性能随模型规模提升。

Conclusion: 提出的系统能有效建模人类标注差异，上下文学习、元学习训练和模型规模都是提升性能的关键因素。

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [80] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM是一种基于前向传播的token级核心集选择框架，通过注意力指纹匹配任务表征模式，无需梯度计算即可高效构建高质量指令调优数据集。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法依赖计算昂贵的梯度信号，且忽略了细粒度特征。需要一种更高效、更敏感的方法来识别定义任务的结构特征。

Method: 提出TRIM框架，使用前向传播的注意力指纹匹配目标样本的表征模式，避免反向传播计算，实现token级别的细粒度选择。

Result: TRIM选择的核心集在多个下游任务上比最先进基线方法提升达9%，在某些设置下甚至超过全数据微调性能，且计算成本显著降低。

Conclusion: TRIM为构建高质量指令调优数据集提供了可扩展且高效的替代方案，证明了前向注意力指纹匹配在核心集选择中的有效性。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [81] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: 比较人类与大型语言模型在七种复杂语言结构上的句子理解能力，发现LLMs在花园路径句上表现特别差，模型性能与人类的相关性随参数数量增加而提高


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs能够流利地与人类对话，但研究人员想了解它们是否也会经历类似人类的处理困难，特别是在复杂的语言结构理解上

Method: 在统一的实验框架下收集人类和五个系列最先进LLMs的句子理解数据，测试七种具有挑战性的语言结构，包括花园路径句，并比较目标结构与基线结构的性能差异

Result: LLMs在目标结构上整体表现不佳，特别是在花园路径句上（GPT-5仅46.8%准确率），而在非花园路径结构上表现较好（GPT-5达93.7%）。模型与人类在结构难度排名上的相关性随参数数量增加而提高

Conclusion: 研究揭示了人类与LLMs在句子理解上的趋同与分歧，为理解人类与LLMs的相似性提供了新见解

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [82] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: 提出RHC框架，将分层文本分类重新定义为逐步推理任务，通过两阶段训练LLM（冷启动对齐和强化学习）来提升多步推理能力，在专利分类等HTC任务上取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅输出扁平标签集，缺乏预测背后的解释性；专利主题分类是HTC中最具挑战性的场景之一，需要领域知识和处理大量标签。

Method: RHC框架将HTC重新定义为逐步推理任务，使用两阶段训练：冷启动阶段对齐CoT推理格式，强化学习阶段增强多步推理能力。

Result: RHC在准确率和宏F1上比基线提升约3%，优于监督微调；具有可解释性、可扩展性，在其他HTC基准测试中也达到SOTA性能。

Conclusion: RHC框架在分层文本分类中表现出有效性、可解释性、可扩展性和广泛适用性，为HTC任务提供了新的解决方案。

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [83] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: 该论文系统分析了开源数学推理数据集和数据合成技术，发现在实际应用中，采用更可解释的数据格式或从更强模型蒸馏数据，往往比单纯扩大数据规模更有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理能力对下游任务至关重要，但现有数据构建方法在实际应用中的效果尚未充分探索。研究者希望为工业应用提供实用的数据选择策略。

Method: 在统一的训练和部署流程下，对开源数据集和数据合成技术进行综合分析，提炼有效的数据选择策略。

Result: 研究发现，结构化数据格式和从强模型蒸馏数据比单纯增加数据量更有效，为成本效益高的数据管理和可扩展模型增强提供了指导。

Conclusion: 该研究为整合训练数据以增强LLM能力提供了可行指导，支持成本效益高的数据管理和可扩展模型增强，并希望激发关于在现实世界推理任务中如何平衡"更多数据"与"更好数据"的进一步研究。

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [84] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: NurseLLM是首个专门针对护理领域的多选问答任务的大语言模型，通过多阶段数据生成流程构建大规模护理MCQ数据集，在多个护理基准测试中优于同规模通用和医学专用LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医疗系统中取得了显著进展，但在护理等专业领域的潜力仍未充分探索，需要专门针对护理领域的LLM。

Method: 开发多阶段数据生成流程构建首个大规模护理多选问答数据集，训练专门的护理LLM，并建立多个护理基准测试进行严格评估。

Result: NurseLLM在不同基准测试中优于同规模的最先进通用和医学专用LLM，证明了护理领域专用LLM的重要性。

Conclusion: 护理专用LLM在护理领域表现出色，推理和多智能体协作系统在护理领域具有未来研究和应用前景。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [85] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 提出一个框架来系统测量LLM心理测量评估中的数据污染问题，发现主要心理测量问卷存在严重的数据污染，模型不仅能记忆题目还能调整回答以达到特定目标分数。


<details>
  <summary>Details</summary>
Motivation: 先前研究使用心理测量问卷评估LLM的高层次心理构念，但存在数据污染威胁评估可靠性的担忧，目前缺乏对此问题的系统量化研究。

Method: 提出系统测量框架，评估三个维度：题目记忆、评估记忆和目标分数匹配，应用于21个主流模型和4个常用心理测量问卷。

Result: 发现BFI-44和PVQ-40等流行问卷存在强烈数据污染，模型不仅能记忆题目还能调整回答以达到特定目标分数。

Conclusion: 心理测量问卷在LLM评估中存在严重数据污染问题，威胁评估结果的可靠性，需要更严谨的评估方法。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [86] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: 本文提出了CARPAS任务，旨在基于文档内容动态调整预定义方面后再进行摘要生成。研究发现LLMs倾向于预测过多方面，导致摘要过长且不相关，因此提出预测相关方面数量的子任务来指导LLMs生成更聚焦的摘要。


<details>
  <summary>Details</summary>
Motivation: 现有基于方面的摘要方法假设预定义方面是完整的，但实际应用中这些方面可能不完整、不相关或缺失。用户期望系统能根据文档内容自适应地调整这些方面。

Method: 构建三个新数据集，使用LLMs和四种代表性提示策略进行实验，发现LLMs预测方面过多的问题，进而提出预测相关方面数量的子任务来指导LLMs。

Result: 提出的方法在所有数据集上显著提升性能。分析发现LLMs能够遵循预测的方面数量，即使与其自身估计不同。

Conclusion: 预测相关方面数量能有效指导LLMs，降低推理难度，使其聚焦于最相关的方面，为LLMs在类似实际应用中的部署提供了重要见解。

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [87] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（如GPT-2）在学习人类可能语言和不可能语言时没有表现出与人类相同的先天学习偏好，无法系统地区分自然语言和人为构造的不可能语言。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具有与人类相同的语言学习偏好，特别是能否区分人类可能语言和不可能语言，这对理解LLMs与人类语言习得机制的相似性具有重要意义。

Method: 在更广泛的语言集和不可能扰动上测试GPT-2的学习曲线，比较其对自然语言和不可能语言的学习难度，并分析困惑度曲线的跨语言方差。

Result: GPT-2在大多数情况下对自然语言和不可能语言的学习难度相同，无法系统地区分这两类语言。

Conclusion: LLMs不具备塑造语言类型学的人类先天偏见，其学习机制与人类语言习得存在本质差异。

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [88] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: 开发了针对乌干达多语言环境的Sunflower 14B和32B模型，基于Qwen 3架构，在乌干达大多数语言上达到最先进理解能力，旨在解决非洲语言技术发展不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 非洲有2000多种语言，但大多数被语言技术进步所忽视。现有LLM仅支持少数主流语言，导致语言能力发展不平衡。需要区域化方法更高效地解决语言多样性问题。

Method: 采用区域聚焦方法，以乌干达为案例研究。基于Qwen 3架构开发了Sunflower 14B和32B模型，专门针对乌干达多语言环境进行优化。

Result: 开发出开源的Sunflower模型，在乌干达大多数语言上达到最先进的理解能力，可用于减少重要实际应用中的语言障碍。

Conclusion: 区域化聚焦方法比零散支持不同语言更有效，Sunflower模型为高语言多样性国家提供了可行的解决方案，有助于弥合语言技术鸿沟。

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [89] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 提出了一种无需训练的简单方法，通过识别和操纵大语言模型中控制跨语言转换的稀疏维度，能够以极低成本切换输出语言并保持语义内容。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在非英语数据上暴露有限，但表现出强大的多语言能力。研究发现英语中心的大语言模型在中间层将多语言内容映射为英语对齐表示，然后在最后一层投影回目标语言标记空间。

Method: 基于跨语言转换由少量稀疏维度控制的假设，引入简单无需训练的方法来识别和操纵这些维度，仅需50个平行或单语句子数据。

Result: 在多语言生成控制任务实验中，这些维度具有可解释性，干预这些维度可以切换输出语言同时保持语义内容，性能优于先前的基于神经元的方法且成本显著降低。

Conclusion: 该方法揭示了跨语言转换的机制，提供了一种高效的多语言控制方法，仅需极少数据即可实现语言切换。

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [90] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: 该论文研究了低资源非洲语言的自动语音识别系统开发，通过评估Whisper模型在两个班图语（Kinyarwanda和Kikuyu）上的表现，确定了实用部署所需的最小数据量（50小时可达WER<13%）和主要错误模式（38.6%由数据质量问题引起）。


<details>
  <summary>Details</summary>
Motivation: 解决低资源非洲语言ASR系统开发中的关键问题：确定可行性能所需的最小数据量，并识别生产系统中的主要失败模式，为实践者提供部署指导。

Method: 在两个班图语上进行综合实验：在Kinyarwanda上进行系统数据缩放分析（1-1,400小时训练集），在Kikuyu上使用270小时训练数据进行详细错误特征分析。

Result: 实验表明，仅需50小时训练数据即可实现实用ASR性能（WER<13%），200小时可达WER<10%；错误分析显示38.6%的高错误案例由数据质量问题（特别是嘈杂的真实转录）引起。

Conclusion: 数据质量与数据量同等重要，为类似低资源语言环境的ASR系统开发提供了可操作的基准和部署指南，并发布了相关模型。

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [91] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 提出了一个高效的小语言模型预训练框架，通过结构稀疏子网络初始化、进化搜索和知识蒸馏三个互补方法，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 小语言模型(SLMs)相比大语言模型(LLMs)具有更高的资源效率，但需要更有效的预训练方法来提升性能。

Method: 结合三种方法：1)识别结构稀疏子网络初始化；2)使用进化搜索自动发现高质量子网络初始化；3)应用大模型知识蒸馏加速训练。

Result: 最佳模型在验证困惑度上达到可比Pythia SLM水平，但预训练token需求减少了9.2倍。

Conclusion: 该框架为大规模开发成本效益高的小语言模型提供了实用且可复现的路径。

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [92] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: Customer-R1是一种基于强化学习的个性化用户行为模拟方法，通过在在线购物环境中优化基于明确用户画像的下一步推理和行动生成，显著提升了模拟的个性化程度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要学习群体层面的策略，没有考虑用户个人特征，导致模拟结果过于通用而非个性化。本研究旨在解决如何让LLM代理更好地模拟个性化用户行为的问题。

Method: 提出Customer-R1方法，基于强化学习框架，策略明确基于用户画像，通过行动正确性奖励信号优化下一步推理和行动生成。

Result: 在OPeRA数据集上的实验表明，Customer-R1在下一步行动预测任务中显著优于提示和SFT基线方法，并且更好地匹配用户的行动分布，表明在个性化行为模拟中具有更高的保真度。

Conclusion: Customer-R1方法通过结合明确用户画像和强化学习优化，成功实现了更高质量的个性化用户行为模拟，为LLM在步进式用户行为模拟领域的应用提供了有效解决方案。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [93] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: 提出了一个基于顶级经济金融期刊因果关系识别的新基准，包含40,379个评估项，涵盖健康、环境、技术、法律和文化等领域，测试显示当前LLMs在因果推理方面存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准存在依赖合成数据和领域覆盖狭窄等关键限制，需要构建更可靠的评估框架来测试LLMs的真实因果推理能力。

Method: 从顶级经济金融期刊中提取经过严格因果识别方法（如工具变量、双重差分、断点回归设计）验证的因果关系，构建包含5种任务类型的基准数据集。

Result: 在8个最先进LLMs上的实验结果显示，最佳模型准确率仅为57.6%，模型规模与性能无一致性关联，高级推理模型在基本因果关系识别上也表现困难。

Conclusion: 当前LLMs能力与高风险应用中可靠因果推理需求之间存在关键差距，需要进一步改进模型在因果推理方面的能力。

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [94] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: LAD-RAG是一个布局感知的动态RAG框架，通过在文档摄取时构建符号文档图来捕获布局结构和跨页依赖关系，在推理时通过LLM代理动态检索必要证据，显著提升了多页文档问答的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在文档摄取时将内容编码为孤立块，丢失了结构信息和跨页依赖关系，在推理时固定检索页数，导致多页推理任务中证据检索不完整和答案质量下降。

Method: 提出LAD-RAG框架：在摄取阶段构建符号文档图捕获布局结构和跨页依赖，与标准神经嵌入结合；在推理阶段使用LLM代理动态与神经和符号索引交互，自适应检索必要证据。

Result: 在MMLongBench-Doc、LongDocURL、DUDE和MP-DocVQA上的实验表明，LAD-RAG显著提升了检索性能，平均完美召回率超过90%，在相同噪声水平下比基线检索器召回率提升高达20%，QA准确率更高且延迟最小。

Conclusion: LAD-RAG通过结合符号文档图和动态检索策略，有效解决了传统RAG在视觉丰富文档问答中的局限性，为多页推理任务提供了更完整的证据检索和更高的答案质量。

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [95] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: 该研究系统分析了五个流行的事实性基准测试和八个不同年份发布的LLM，发现基准测试中的大量样本已过时，导致对LLM事实性评估不可靠。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展和现实世界的动态变化超过了静态评估基准的更新速度，导致基准测试与真实事实和现代LLM之间存在时间错位，影响LLM事实性评估的可靠性。

Method: 使用最新的知识检索流程和三个定制指标来量化基准测试的老化程度及其对LLM事实性评估的影响，分析了五个流行事实性基准和八个不同年份的LLM。

Result: 实验结果表明，广泛使用的事实性基准中有相当一部分样本已过时，导致对LLM事实性的评估不可靠。

Conclusion: 该研究为评估LLM事实性基准的可靠性提供了测试平台，并呼吁更多研究关注基准测试老化问题。

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [96] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: Red-Bandit是一个自适应红队测试框架，通过在线学习识别和利用模型特定漏洞，使用多臂老虎机策略动态选择攻击专家，在AdvBench上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队测试方法缺乏在推理时有效适应模型特定漏洞的机制，需要更高效的方法来发现和利用不同攻击风格下的模型失败模式。

Method: 使用强化学习对一组参数高效的LoRA专家进行后训练，每个专家专精特定攻击风格；在推理时通过多臂老虎机策略根据目标模型响应安全性动态选择攻击专家。

Result: 在充分探索条件下(ASR@10)在AdvBench上达到最先进效果，同时生成更易读的提示(更低困惑度)；老虎机策略可作为诊断工具揭示模型特定漏洞。

Conclusion: Red-Bandit提供了一个有效且可扩展的框架，既能实现高性能红队测试，又能作为诊断工具识别模型的安全漏洞。

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [97] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: HERO是一个强化学习框架，通过结合验证器的二元反馈和奖励模型的连续评分来优化大语言模型的推理能力，使用分层归一化和方差感知加权策略。


<details>
  <summary>Details</summary>
Motivation: 现有验证器提供的0-1二元反馈过于脆弱，无法识别部分正确或替代答案，限制了学习效果。奖励模型能提供更丰富的连续反馈，可作为验证器的补充监督信号。

Method: 提出HERO混合集成奖励优化框架：1) 分层归一化将奖励模型分数限制在验证器定义的组内；2) 方差感知加权强调密集信号更重要的困难提示。

Result: 在多种数学推理基准测试中，HERO始终优于仅使用奖励模型或仅使用验证器的基线方法，在可验证和难以验证的任务上都取得了显著提升。

Conclusion: 混合奖励设计既保持了验证器的稳定性，又利用了奖励模型的细微差别来推进推理能力的发展。

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [98] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: 提出了一种新的无参考评估方法，将法律回答分解为'法律数据点'，在专业数据集上优于多种基线方法，并与人类专家评估更相关。


<details>
  <summary>Details</summary>
Motivation: 当前法律领域LLM输出评估方法存在局限性，要么依赖昂贵的参考数据，要么使用标准化评估方法，无法满足法律行业特有的评估需求和可信度要求。

Method: 将冗长回答分解为自包含的'法律数据点'，引入反映律师评估方式的无参考评估方法。

Result: 在专有数据集和开源数据集上均优于多种基线方法，与人类专家评估更相关，提高了标注者间一致性。

Conclusion: 该方法填补了法律领域LLM评估的空白，开源了法律数据点供研究社区使用，推动了法律问答LLM评估研究的发展。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [99] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: PA-Tool是一种无需训练的方法，通过调整工具模式与模型预训练知识对齐，而不是强制模型适应任意模式，从而显著提升小语言模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在工具使用任务中表现不佳，特别是在选择适当工具和识别正确参数方面。常见的失败模式是模式不对齐：模型会产生看似合理但实际不存在的工具名称，这些名称反映了预训练期间内化的命名约定，但在提供的工具模式中不存在。

Method: 提出PA-Tool（预训练对齐工具模式生成），这是一种无需训练的方法，利用峰值度（来自污染检测的信号，表示预训练熟悉度）自动重命名工具组件。通过生成多个候选名称并选择在样本间输出浓度最高的名称，PA-Tool识别预训练对齐的命名模式。

Result: 在MetaTool和RoTBench上的实验显示性能提升高达17个百分点，模式不对齐错误减少了80%。PA-Tool使小模型能够接近最先进性能，同时保持计算效率，无需重新训练即可适应新工具。

Conclusion: 模式级干预可以通过调整模式以适应模型而不是强制模型适应模式，来释放资源高效模型的工具使用潜力。

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [100] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 提出OnlineRubrics方法，通过在线动态生成评估标准来改进LLM训练，相比静态标准能提升8%性能


<details>
  <summary>Details</summary>
Motivation: 现有基于静态标准的训练方法容易受到奖励攻击，且无法捕捉训练过程中出现的新需求

Method: 通过比较当前策略和参考策略的响应对，动态生成评估标准，实现持续错误识别和缓解

Result: 在AlpacaEval、GPQA、ArenaHard等多个基准上，相比静态标准方法获得高达8%的改进

Conclusion: 在线标准生成方法能有效提升LLM训练效果，识别出透明度、实用性、组织性和推理能力等关键评估维度

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [101] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 该论文研究了LLMs的内在自我修正机制，特别是在道德自我修正方面，揭示了性能收敛现象及其机制。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs的自我修正能力在应用中表现出成功，但其有效性的机制和原因尚不明确，特别是在仅提供抽象目标而没有具体问题细节的内在自我修正情境下。

Method: 通过多轮交互实验，分析道德自我修正中的性能收敛行为，研究持续注入的自我修正指令如何激活道德概念并降低模型不确定性。

Result: 发现内在自我修正表现出性能收敛特性，这是由于激活的道德概念在连续轮次中稳定化，从而减少了模型的不确定性。

Conclusion: 道德自我修正展现出性能收敛这一理想特性，证明了其在LLMs中的强大潜力。

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [102] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: M-Thinker是一个针对非英语语言推理问题的大推理模型，通过GRPO算法训练，包含语言一致性奖励和跨语言思维对齐奖励，解决了当前LRM在非英语语言上的语言不一致和推理能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在处理非英语语言时存在两个关键问题：输入输出语言不一致，以及推理路径错误导致答案准确性下降，这影响了非英语用户的使用体验和模型的全球部署。

Method: 提出M-Thinker模型，使用GRPO算法训练，包含语言一致性奖励（确保输入、思维和答案语言一致）和跨语言思维对齐奖励（通过比较非英语与英语推理路径来转移推理能力）。

Result: M-Thinker-1.5B/7B模型在两个多语言基准测试（MMATH和PolyMath）上实现了接近100%的语言一致性和优越性能，并在域外语言上表现出良好的泛化能力。

Conclusion: M-Thinker通过语言一致性约束和跨语言思维对齐机制，有效提升了LRM在非英语语言上的推理性能和用户体验，推动了LRM的全球化部署。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [103] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: CORGI是一个专为真实商业场景设计的文本到SQL新基准，包含基于企业数据的合成数据库，涵盖描述性、解释性、预测性和推荐性四类复杂商业查询，比现有基准难度高21%。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL基准主要关注历史记录的事实检索，无法满足真实商业环境中需要因果推理、时间预测和战略推荐等高级智能的需求。

Method: 创建基于Doordash、Airbnb和Lululemon等企业数据的合成数据库，设计四类复杂度递增的商业查询类型，评估LLM在复杂商业场景下的表现。

Result: LLM在高级别问题上表现下降，难以做出准确预测和提供可行计划，CORGI基准比BIRD基准难度高约21%。

Conclusion: 流行LLM与真实商业智能需求之间存在显著差距，需要开发更强大的模型来处理复杂的商业查询。

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [104] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: VeriCode提出了一个包含30种可验证代码指令的分类法，用于评估LLM的代码指令遵循能力，创建了Vibe Checker测试平台来同时评估功能正确性和指令遵循性。


<details>
  <summary>Details</summary>
Motivation: 当前代码评估主要关注功能正确性（pass@k），但忽略了用户在实际编程中经常使用的非功能性指令，这些指令体现了人类偏好和"感觉检查"。

Method: 开发了VeriCode分类法，包含30种可验证代码指令及对应的确定性验证器，并基于此增强现有评估套件，构建Vibe Checker测试平台。

Result: 评估31个领先LLM发现，即使最强模型也难以遵循多个指令并表现出明显的功能回归。指令遵循性与人类偏好相关性最高，是真实编程任务中的主要区分因素。

Conclusion: 功能正确性和指令遵循性的综合评分与人类偏好相关性最佳，指令遵循是"感觉检查"的核心因素，为开发更符合用户偏好的模型提供了具体路径。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [105] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 提出了一种基于认知科学多存储模型的内存框架，通过人工海马体网络(AHN)将窗口外信息压缩为固定大小的长期记忆，在保持性能的同时显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决长序列建模中RNN类模型的压缩固定大小内存效率与Transformer类模型无损增长内存保真度之间的基本权衡问题。

Method: 维护Transformer KV缓存的滑动窗口作为无损短期记忆，同时使用人工海马体网络(AHN)将窗口外信息循环压缩为固定大小的紧凑长期记忆。

Result: 在LV-Eval和InfiniteBench长上下文基准测试中，AHN增强模型持续优于滑动窗口基线，性能与全注意力模型相当甚至更好，同时大幅减少计算和内存需求。例如，Qwen2.5-3B-Instruct推理FLOPs减少40.5%，内存缓存减少74.0%，LV-Eval平均得分从4.41提升到5.88。

Conclusion: AHN框架有效平衡了长序列建模的效率与保真度，为处理超长序列提供了一种实用且高效的解决方案。

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [106] [Distributional welfare impacts and compensatory transit strategies under NYC congestion pricing](https://arxiv.org/abs/2510.06416)
*Xiyuan Ren,Zhenglei Ji,Joseph Y. J. Chow*

Main category: econ.GN

TL;DR: 纽约拥堵收费项目整体改善了车速和公交使用率，但存在分配不均问题。研究发现年福利损失约2.4亿美元，主要集中在曼哈顿上城、布鲁克林和新泽西哈德逊县。通过公交改进（减少等待时间0.48-2分钟）和定向票价补贴（1-3亿美元/年）可补偿福利损失。


<details>
  <summary>Details</summary>
Motivation: 研究拥堵收费项目的分配影响和补偿策略，特别是对弱势群体的福利损失补偿机制。

Method: 使用纽约和新泽西的合成出行数据建立联合模式和目的地模型，并用MTA交通数据校准收费相关参数。

Result: 项目导致年福利损失2.4亿美元，远低于收费收入（10.77亿总收入，4.5亿净收入）。福利损失集中在特定区域和难以转向公交的人群。

Conclusion: 需要针对性的公交改进和票价补贴机制来补偿福利损失，新泽西居民更适合通过票价折扣实现福利改善，应使用基于出发地的折扣或通勤套餐等定向机制。

Abstract: Early evaluations of NYC's congestion pricing program indicate overall
improvements in vehicle speed and transit ridership. However, its
distributional impacts remain understudied, as does the design of compensatory
transit strategies to mitigate potential welfare losses. This study identifies
population segments and regions most affected by congestion pricing, and
evaluates how welfare losses can be compensated through transit improvements.
We estimate joint mode and destination models using aggregated synthetic trips
in New York and New Jersey and calibrate toll-related parameters with traffic
counts reported by the MTA. The results show that the program leads to an
accessibility-related welfare loss of approximately $240 million per year,
which is considerably lower than the gains from toll revenues: the gross
revenue estimated by our models ($1.077 billion per year) and the net revenue
projected by the MTA ($450 million per year). However, these benefits gains
conceal significant disparities. Welfare losses are concentrated in Upper
Manhattan, Brooklyn, and Hudson County, NJ, particularly among travelers less
able to shift to transit or alternative destinations. For NYC residents,
compensating aggregate welfare loss requires a 0.48-minute reduction in transit
wait time or a $135.59 million annual fare subsidy. Ensuring accessibility
gains for all populations and counties (Pareto improving) requires a 1-2 minute
reduction in wait time combined with an annual subsidy of about $100-300
million. For New Jersey residents, achieving aggregate welfare gains primarily
through fare discounts (requiring $108.53 million per year) is more feasible
and efficient; however, uniform discounts should be replaced by targeted
mechanisms such as origin-based fare reductions or commuter pass bundles.

</details>


### [107] [When Machines Meet Each Other: Network Effects and the Strategic Role of History in Multi-Agent AI](https://arxiv.org/abs/2510.06903)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: econ.GN

TL;DR: 研究LLM智能体在网络效应博弈中的行为，发现它们系统性地偏离理论预测的期望均衡，价格是主要偏差驱动因素，历史结构影响协调稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着AI进入智能体时代，LLM越来越多地作为自主智能体相互交互，需要了解它们在相互依赖环境中的行为模式，特别是在网络效应下的协调表现。

Method: 设计实验框架，让50个异质GPT-5智能体在网络效应博弈中重复交互，系统性地改变网络效应强度、价格轨迹和决策历史长度等参数。

Result: LLM智能体系统性地偏离期望均衡：在低价时低估参与度，高价时高估参与度，并维持持续分散。单调历史有助于稳定协调，而非单调历史加剧发散和路径依赖。

Conclusion: 研究为网络效应下多智能体AI系统提供了首个系统性证据，并为实际配置此类系统提供了指导，表明历史结构是重要的设计杠杆。

Abstract: As artificial intelligence (AI) enters the agentic era, large language models
(LLMs) are increasingly deployed as autonomous agents that interact with one
another rather than operate in isolation. This shift raises a fundamental
question: how do machine agents behave in interdependent environments where
outcomes depend not only on their own choices but also on the coordinated
expectations of peers? To address this question, we study LLM agents in a
canonical network-effect game, where economic theory predicts convergence to a
fulfilled expectation equilibrium (FEE). We design an experimental framework in
which 50 heterogeneous GPT-5-based agents repeatedly interact under
systematically varied network-effect strengths, price trajectories, and
decision-history lengths. The results reveal that LLM agents systematically
diverge from FEE: they underestimate participation at low prices, overestimate
at high prices, and sustain persistent dispersion. Crucially, the way history
is structured emerges as a design lever. Simple monotonic histories-where past
outcomes follow a steady upward or downward trend-help stabilize coordination,
whereas nonmonotonic histories amplify divergence and path dependence.
Regression analyses at the individual level further show that price is the
dominant driver of deviation, history moderates this effect, and network
effects amplify contextual distortions. Together, these findings advance
machine behavior research by providing the first systematic evidence on
multi-agent AI systems under network effects and offer guidance for configuring
such systems in practice.

</details>


### [108] [The importance of emotional intelligence in leadership for building an effective team](https://arxiv.org/abs/2510.07004)
*Joanna Ćwiąkała,Waldemar Gajda,Michał Ćwiąkała,Ernest Górka,Dariusz Baran,Gabriela Wojak,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Jan Piwnik*

Main category: econ.GN

TL;DR: 该研究探讨了情绪智力作为有效领导力核心要素的重要性及其对团队建设的影响。研究发现高情绪智力的领导者更具同理心、道德感和社交能力，能显著改善团队动力和组织绩效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证情绪智力在领导力中的关键作用，探索EI能力如何影响团队协作、冲突解决和工作动机，为领导力发展提供实证依据。

Method: 通过对100名专业人士的调查问卷，研究分析了自我意识、自我调节、同理心和社交技能等EI能力对领导效能的影响。

Result: 结果显示EI与领导力特质（同理心、道德行为、社交能力、激励效果）存在强相关性。高EI领导者被认为更具同理心、道德感，能更好地建立信任、解决冲突和激发承诺。

Conclusion: 情绪智力是人际效能、员工参与度和可持续商业成功的核心驱动力，建议将EI培训、辅导和评估工具整合到组织战略中。

Abstract: This study investigates the significance of emotional intelligence (EI) as a
fundamental component of effective leadership and its impact on building
cohesive, motivated, and high-performing teams. Drawing on data from a survey
of 100 professionals, the research examines how EI competencies including
self-awareness, self-regulation, empathy, and social skills shape leadership
effectiveness, team collaboration, conflict resolution, and workplace
motivation. The results demonstrate strong correlations between EI and key
leadership traits such as empathy, ethical conduct, social competence, and
motivational effectiveness. Leaders with higher levels of EI are perceived as
more empathetic, ethical, and capable of fostering trust, resolving conflicts,
and inspiring commitment, thereby improving team dynamics and overall
organizational performance. The study also highlights that ethical leadership
significantly enhances motivation and that social competence is essential for
engaging and aligning teams toward common goals. While the findings are
exploratory due to the limited sample size, they provide valuable insights for
leadership development programs, emphasizing the importance of integrating
EI-focused training, coaching, and assessment tools into organizational
strategies. The research contributes to leadership theory by demonstrating that
emotional intelligence is not an isolated skill but a central driver of
interpersonal effectiveness, employee engagement, and sustainable business
success.

</details>


### [109] [The role of communication in effective business management](https://arxiv.org/abs/2510.07016)
*Dariusz Baran,Ernest Górka,Michał Ćwiąkała,Gabriela Wojak,Mateusz Grzelak,Katarzyna Olszyńska,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Maciej Ślusarczyk,Jan Piwnik*

Main category: econ.GN

TL;DR: 通过比较波兰两家中型汽车租赁公司的内部沟通实践，研究发现使用先进沟通技术、参与式模式和清晰反馈机制的公司显著优于传统沟通方式的公司，证实了双向沟通对员工参与度、组织透明度和运营效率的战略重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨内部沟通在有效企业管理中的影响，通过行业内的数据驱动比较来验证沟通实践与组织绩效的关系。

Method: 采用结构化问卷调查，收集220名员工对15个沟通相关因素（包括反馈文化、管理者可及性、信息清晰度和跨部门协调）的评价，进行公司间比较分析。

Result: 公司X在所有评估维度上显著优于公司Y，主要归因于其使用先进沟通技术、参与式模式和清晰反馈机制，支持了内部沟通与工作满意度、动机之间的现有模型关联。

Conclusion: 研究强调了双向沟通在促进员工参与、组织透明度和运营效率中的战略作用，建议未来研究探索跨行业和纵向视角，特别是在数字化和混合工作环境背景下。

Abstract: This paper examines the impact of internal communication on effective
business management through a comparative analysis of two medium-sized car
rental companies operating in Poland. Using a structured survey completed by
220 employees, the study evaluates 15 communication-related factors, including
feedback culture, managerial accessibility, message clarity, and
interdepartmental coordination. The findings indicate that Company X
significantly outperforms Company Y across all evaluated dimensions, largely
due to its use of advanced communication technologies, participatory models,
and clear feedback mechanisms. The research highlights the strategic role of
two-way communication in fostering employee engagement, organizational
transparency, and operational efficiency. It contributes to the field by
offering a rare, data-driven comparison within one industry and supports
existing models that link internal communication to job satisfaction and
motivation. Limitations include reliance on self-reported data and focus on a
single industry and country. Future studies are recommended to explore
cross-sector and longitudinal perspectives, especially in the context of
digital and hybrid work environments.

</details>


### [110] [Optimal bidding in multiperiod day-ahead electricity markets assuming non-uniform uncertainty of clearing prices](https://arxiv.org/abs/2510.07025)
*Dávid Csercsik,Mihály András Vághy*

Main category: econ.GN

TL;DR: 本文研究了在非均匀价格分布下多部分投标的优越性，验证了Richstein等人的结论在非均匀分布情况下仍然成立。


<details>
  <summary>Details</summary>
Motivation: Richstein等人的研究假设市场出清价格服从均匀分布，本文旨在验证在更简单的对称阶梯常数非均匀分布下，多部分投标是否仍能确保更高预期利润。

Method: 使用简单的两期模型，假设市场出清价格服从对称阶梯常数非均匀分布，分析不同投标策略下的预期利润。

Result: 研究结果表明，即使在非均匀价格分布情况下，多部分投标仍比简单投标和块投标获得更高的预期利润。

Conclusion: Richstein等人的结论具有鲁棒性，在非均匀价格分布假设下，多部分投标的优越性依然成立。

Abstract: In a recent publication, using a simple two-period model, which is already
capable to capture essential non-convex multiperiod bids, Richstein et al. have
shown that in the case of optimal bidding, multi-part bidding always ensures a
higher expected profit for the bidder, compared to simple bidding and
block-bidding. The model proposed in their analysis assumes a uniform
distribution of the market-clearing prices in both periods. In this paper, we
study how the conclusions of the analysis are affected, if a very simple,
symmetric, stepwise-constant but non-uniform distribution is assumed in the
case of the market-clearing price. We show that the results of Richstein et al.
also hold in this case.

</details>


### [111] [Exchange for Growth: Currency Dynamics in Emerging Markets](https://arxiv.org/abs/2510.07039)
*Shaunak Kulkarni,Rohan Ajay Dubey*

Main category: econ.GN

TL;DR: 本文重新审视双赤字假说，探讨财政赤字与经常账户赤字之间的关系，分析政府在保持国内政策目标的同时维持贸易政策自主性的能力。


<details>
  <summary>Details</summary>
Motivation: 传统观点将货币危机视为财政和货币决策的必然结果，而双赤字假说认为预算赤字增加会直接导致经常账户赤字增加。但这一假说存在争议，需要结合现代货币经济学理论来调和批评与广泛适用性之间的矛盾。

Method: 采用更细致的方法，结合当代货币经济学的理论框架，分析实际经济因素与财政政策自主性之间的联系。

Result: 研究结果揭示了实际经济因素与财政政策自主性之间可能存在的联系（或缺乏联系），这有助于评估政府在不损害贸易政策自主性和经常账户可持续性的情况下实现国内政策目标的能力。

Conclusion: 双赤字假说需要更复杂的分析框架，政府可以通过独立干预影响货币危机的表现，而非简单地接受预算赤字必然导致经常账户赤字的结论。

Abstract: Currency crises are frequently discussed retrospectively as a necessary and
deterministic outcome of a finite sequence of fiscal decisions, monetary
manoeuvres, and limited exogenous inputs. Parallelly, the Twin Deficits
Hypothesis (TDH) posits that an increase in the budget deficit leads to a
direct rise in the current account deficit; although analogous to the idea of
currency crises being the outcome of finite inputs (through a balance of
payments crisis here), this notion runs contrary to the conclusion that
independent intervention can have bearing on the expression of a currency
crisis.
  Since its introduction by Mundell and Fleming in 1960, the TDH has sparked
considerable academic debate regarding its validity. Given its assumption of a
stable private savings gap, and conflicting empirical evidence, we believe
there are novel insights to be gained from a more nuanced approach
incorporating theoretical frameworks from contemporary Monetary Economics in
order to reconcile criticisms of the TDH with the basis for its broad
applicability.
  The results from this paper thus investigate the link - or lack thereof -
between real economic factors, which can support the assessment of fiscal
policy autonomy, and thereby a government's ability, to meet domestic policy
objectives without compromising trade policy autonomy and current account
sustainability.

</details>


### [112] [Analysis of managerial behaviors in business management](https://arxiv.org/abs/2510.07047)
*Ernest Górka,Dariusz Baran,Michał Ćwiąkała,Gabriela Wojak,Robert Marszczuk,Katarzyna Olszyńska,Piotr Mrzygłód,Maciej Frasunkiewicz,Piotr Ręczajski,Kamil Saługa,Maciej Ślusarczyk,Jan Piwnik*

Main category: econ.GN

TL;DR: 本研究使用肯尼斯·布兰查德的情境领导模型作为诊断工具，探讨不同管理行为如何影响团队效能和组织成果。研究发现支持型领导风格占主导地位，领导灵活性对组织成功至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索不同管理行为对团队效能和组织成果的影响，使用情境领导模型作为诊断工具来评估领导适应性。

Method: 在十家公司进行调研，通过基于情境的问卷识别指导型、教导型、支持型和授权型领导风格，并进行相关性分析。

Result: 支持型领导风格在60%的被调查公司中占主导地位，授权型次之。相关性分析显示某些风格之间存在强负相关关系，特别是指导型和支持型，表明领导灵活性很重要。

Conclusion: 过度依赖单一领导风格可能导致效率低下，而平衡的情境方法能提升决策质量、士气和适应性。研究为管理培训提供实践意义，建议整合诊断工具以提高风格意识和行为灵活性。

Abstract: This study explores how different managerial behaviors influence team
effectiveness and organizational outcomes, using Kenneth Blanchard's
situational leadership model as a diagnostic tool. Conducted across ten
companies, the research evaluates leadership adaptability through a
scenario-based questionnaire identifying instructional, teaching, supportive,
and delegating styles. Results show that the supportive (affiliative) style is
dominant, present in 60 percent of surveyed companies, with delegating being
second. Correlation analysis reveals strong negative relationships between
certain styles, particularly instructional and supportive, indicating that
flexibility in leadership is crucial. The findings suggest that over-reliance
on any one style may lead to inefficiencies, while a balanced, situational
approach enhances decision-making, morale, and adaptability. The research
contributes to leadership theory by demonstrating how behavioral combinations,
not static traits, influence outcomes. It offers practical implications for
managerial training, recommending the integration of diagnostic tools like the
Blanchard test to improve style awareness and behavioral flexibility.
Limitations include reliance on self-assessment data and a small sample size.
Future research should explore longitudinal and cross-industry analyses to
assess how leadership behaviors evolve over time or under pressure.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [113] [Artificial Intelligence in Port Logistics: A Bibliometric Analysis of Technological Integration and Research Dynamics](https://arxiv.org/abs/2510.06556)
*Abdelhafid Khazzar,Yassine Sekaki,Yasser Lachhab,Said El-marzouki*

Main category: econ.TH

TL;DR: 该研究探讨了人工智能在港口向智慧港口转型过程中对物流运营的变革，通过文献计量分析构建了数据到影响路径，并提出了分步实施方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人工智能如何通过整合基于能力的资源分析、动态能力和社会技术实施，在智慧港口转型中提升物流运营的韧性和效率。

Method: 采用Scopus文献计量研究方法，系统分析123篇文章，包括搜索协议、文档筛选和重复验证，运用科学映射技术分析关键词关系、共引和文献耦合。

Result: 研究构建了AI应用与智慧港口领域的具体数据到影响路径，提供了文献计量分析方法，并提出了数据准备、预测优化实施和组织整合的分步方法。

Conclusion: 研究支持公共政策制定，提出了数据共享标准和环境效益评估建议，并计划未来结合实地测试和多港口评估来增强因果理解和研究适用性。

Abstract: The paper explores the transformation of port logistics operations with
artificial intelligence during the port transformation into a smart port. The
research integrates capabilities-based resource analysis and dynamic
capabilities with sociotechnicalimplementations of technologies and resilience
approaches of complex systems under disruptions. The system applies robustdata
infrastructures to propel analytical and AI modules that become effective once
integrated with sufficient governance systems and trained personnel and
operational processes to transform planning and safety and sustainability
operations.It applies Scopus bibliometric research to analyze 123 articles
using a systematic approach with both a search protocol and a document
screening and duplication verification. It incorporates annual behavior and
distribution of author and country performance analysis with science mapping
techniques that explore keyword relation and co-citation and bibliographic
coupling and conceptual structuring tools that construct thematic maps and
multiple correspondence analysis with community detection while applying
explicit thresholding and robust tests.The research connects AI applications to
smart port domains through specific data-to-impact pathways while providing a
method for bibliometric analysis that enables future updates. The research
presents a step-by-step approach for data readiness followed by predictive and
optimization implementation and organizational integration. The paper supports
public policy through recommendations for data sharing standards and complete
environmental benefit assessments. The research proposes a future study plan
whichcombines field-based testing with multiple port assessments to enhance
both cause-effect understanding and research applicability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: RareGraph-Synth是一个基于知识图谱引导的连续时间扩散框架，用于生成真实且保护隐私的罕见疾病电子健康记录轨迹，通过整合多个生物医学知识资源来指导生成过程。


<details>
  <summary>Details</summary>
Motivation: 为超罕见疾病研究生成既能保持数据真实性又能保护隐私的合成电子健康记录数据，解决罕见疾病数据稀缺和隐私保护之间的矛盾。

Method: 整合五个公共资源构建异质知识图谱，使用元路径得分调制扩散模型的前向噪声调度，引导生成生物学上合理的实验室-药物-不良事件共现模式。

Result: 在模拟超罕见疾病队列中，相比无引导扩散基线降低分类最大均值差异40%，相比GAN方法降低60%以上，同时保持下游预测效用，并能有效抵抗成员推断攻击。

Conclusion: 将生物医学知识图谱直接集成到扩散噪声调度中可以同时提高数据保真度和隐私保护能力，为罕见疾病研究实现更安全的数据共享。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [115] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: MCCE是一个混合框架，将冻结的闭源大语言模型与轻量级可训练模型结合，通过强化学习持续优化小模型，在分子设计等组合优化问题中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统进化算法容易陷入局部最优，而专家知识对加速收敛至关重要。闭源LLM虽探索能力强但无法更新参数，开源小模型可微调但缺乏广泛知识和推理能力。

Method: 提出多LLM协同共进化框架，维护搜索轨迹记忆，通过强化学习逐步精炼小模型，两个模型在全局探索中相互支持和补充。

Result: 在多目标药物设计基准测试中，MCCE实现了最先进的帕累托前沿质量，持续优于基线方法。

Conclusion: MCCE展示了混合LLM系统中实现持续进化的新范式，结合了知识驱动探索与经验驱动学习。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [116] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了RVFL-X，一种将实数数据集转换为复数表示的随机神经网络扩展，通过自然变换和自编码器方法生成复数表示，在80个UCI数据集上表现优于原始RVFL和最先进的RNN变体。


<details>
  <summary>Details</summary>
Motivation: 神经网络理论进展表明复数具有更强的表示能力，但在随机神经网络中的应用受限，因为缺乏将实数值表格数据集转换为复数值表示的有效方法。

Method: 提出了两种从实数值数据集生成复数值表示的方法：自然变换和自编码器驱动方法。基于这些机制，提出了RVFL-X，这是随机向量函数链接网络的复数值扩展，集成了复数变换但保持原始RVFL架构的简单性和效率。

Result: 在80个实数值UCI数据集上的综合评估表明，RVFL-X始终优于原始RVFL和最先进的RNN变体，展示了其在不同应用领域的鲁棒性和有效性。

Conclusion: RVFL-X通过将复数组件集成到随机神经网络中，成功解决了实数值数据集转换为复数值表示的挑战，显著提升了模型性能。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [117] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 提出一种结合机器学习（CNN和Transformer）与传统算法（计算Jones多项式等量子不变量）的两阶段方法，用于从照片中自动识别绳结。


<details>
  <summary>Details</summary>
Motivation: 目标是实现通过手机拍摄绳结照片就能自动识别绳结，解决视觉感知与拓扑分类的结合问题。

Method: 使用轻量级CNN和Transformer架构直接从图像预测交叉数，并计划结合感知模块与符号重建为平面图代码，进行下游不变量计算。

Result: 展示了即使轻量级架构也能从图像中恢复有意义的绳结结构信息。

Conclusion: 两阶段方法凸显了机器学习处理噪声视觉数据与不变量强制执行严格拓扑区分之间的互补性。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [118] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: 提出了Trajectory Transformer模型，使用transformer架构进行轨迹生成，解决了现有基于卷积的扩散模型在轨迹生成中存在的偏差和细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积架构的扩散模型在轨迹生成中存在模型容量有限的问题，导致显著偏差和街道级细节丢失，需要更强大的模型来提升生成质量。

Method: 提出Trajectory Transformer模型，使用transformer主干网络进行条件信息嵌入和噪声预测，探索了位置嵌入和经纬度嵌入两种GPS坐标嵌入策略，并在不同尺度上分析模型性能。

Result: 在两个真实世界数据集上的实验表明，Trajectory Transformer显著提升了生成质量，有效缓解了先前方法中观察到的偏差问题。

Conclusion: Trajectory Transformer通过transformer架构成功解决了轨迹生成中的偏差问题，为高质量的轨迹生成提供了有效解决方案。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [119] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: BlockGPT是一种用于降水临近预报的生成式自回归变换器，通过批处理标记化方法预测完整二维场，在准确性和推理速度方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有降水临近预报方法中存在的归纳偏差问题和推理速度慢的问题，特别是标记化自回归模型和扩散模型的计算效率不足。

Method: 使用批处理标记化方法，在每时间步预测完整二维场，通过帧内自注意力和帧间因果注意力分解时空关系。

Result: 在KNMI和SEVIR两个降水数据集上，BlockGPT在准确性、事件定位和推理速度方面均优于现有基准模型，推理速度最高提升31倍。

Conclusion: BlockGPT为视频预测提供了一种模型无关的范式，在降水临近预报任务中实现了准确性和效率的平衡。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [120] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR提出了一种协同扩散-自回归范式，将自回归模型的训练效率与扩散模型的并行推理能力相结合，通过轻量级范式转换实现高效AR到扩散模型的转换。


<details>
  <summary>Details</summary>
Motivation: 结合自回归模型的训练效率和扩散模型的并行推理能力，避免昂贵的端到端扩散训练，实现高效模型转换。

Method: 通过轻量级范式转换将训练好的自回归模型转换为块级扩散模型，在推理时块间自回归生成保证全局一致性，块内通过离散扩散过程并行解码所有token。

Result: SDAR以最小成本实现AR到扩散的转换，保持AR级别性能的同时支持并行生成。30B MoE模型在科学推理基准测试中超越AR对应模型，并在多数投票和pass@k等测试时缩放方法下获得进一步改进。

Conclusion: SDAR建立了一个实用的范式，结合了自回归和扩散模型的优势，实现了可扩展的高吞吐量推理。

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [121] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 论文提出分散式小智能体网络架构作为替代单体基础模型的方法，认为在动态环境中群体学习能提供更优决策，但会降低细节可复现性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在重症监护等动态环境中的实际应用效果有限，需要开发能在数据有限、机制知识不足情况下自我适应的AI模型。

Method: 提出分散式小智能体网络架构，每个智能体只覆盖系统功能的子集，通过群体学习实现自我适应。

Result: 分散式小智能体网络在动态环境中能提供比单体基础模型更优的决策能力。

Conclusion: 在证明AI在复杂动态环境中具有明确优势之前，不应让其承担更广泛的决策角色，分散式架构是更有前景的方向。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [122] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: 提出物理启发式Kolmogorov-Arnold网络(PIKAN)，将物理原理嵌入学习过程，实现无人机通信中准确且可解释的A2G信道建模。


<details>
  <summary>Details</summary>
Motivation: 无人机通信需要准确且可解释的A2G信道模型，但确定性模型过于刚性，深度学习模型缺乏可解释性。

Method: 将物理原理（如自由空间路径损耗、双射线反射）作为灵活的归纳偏置引入Kolmogorov-Arnold网络，比物理信息神经网络更灵活。

Result: 在无人机A2G测量数据上，PIKAN达到与深度学习模型相当的精度，仅需232个参数，比多层感知机轻37倍，同时提供符号化可解释表达式。

Conclusion: PIKAN是5G+和6G网络中无人机信道建模的高效、可解释且可扩展解决方案。

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [123] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: 该论文提出了Helmholtz指标来量化ODE与欧拉-拉格朗日方程的相似度，并结合二阶神经ODE构建拉格朗日神经ODE，能够直接学习欧拉-拉格朗日方程且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 神经ODE是强大的机器学习技术，但并非所有解都是物理的欧拉-拉格朗日方程。需要量化ODE与物理系统的相似度，并直接学习拉格朗日系统。

Method: 提出Helmholtz指标来测量ODE与欧拉-拉格朗日方程的相似性，结合二阶神经ODE构建拉格朗日神经ODE，仅使用位置数据学习。

Result: 在多个基础系统（含噪声）上验证了Helmholtz指标的能力，能够区分拉格朗日和非拉格朗日系统，并改进神经ODE的解。

Conclusion: Helmholtz指标和拉格朗日神经ODE能够有效量化物理相似度，直接学习欧拉-拉格朗日方程，且不增加推理成本，提高了神经ODE的性能。

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [124] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: 提出了关系变换器（RT）架构，能够在不同关系数据库上进行预训练，并直接应用于未见过的数据集和任务，无需任务特定微调或上下文示例检索。


<details>
  <summary>Details</summary>
Motivation: 解决关系领域缺乏能够跨数据集和任务迁移的架构问题，核心挑战在于关系数据的多样性，包括异构模式、图结构和功能依赖。

Method: RT架构通过（i）使用表/列元数据标记单元格，（ii）通过掩码标记预测进行预训练，（iii）采用新颖的关系注意力机制覆盖列、行和主外键链接。

Result: 在RelBench数据集上预训练后，RT在二元分类任务中实现了强大的零样本性能，平均达到完全监督AUROC的94%，而27B LLM仅为84%。微调后获得最先进结果且样本效率高。

Conclusion: RT为零样本迁移利用任务-表上下文、关系注意力模式和模式语义，为关系数据的基础模型提供了实用路径。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [125] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: MCPS是一种基于MCTS的改进算法，通过包含从根节点到当前节点路径上所有走法的统计信息来增强探索项，在计算资源有限的情况下优于GRAVE算法。


<details>
  <summary>Details</summary>
Motivation: 针对深度强化学习不可用或计算资源有限的情况（如通用游戏博弈），需要改进现有的MCTS算法，特别是GRAVE算法。

Method: 在节点的探索项中包含从根节点到该节点路径上所有走法的统计信息，使用抽象代码代替精确代码来改进统计信息，并提供改进的统计权重公式。

Result: 在多种游戏测试中，MCPS在双人游戏中表现优于GRAVE，在多人游戏中表现相当，且对超参数不敏感。

Conclusion: MCPS是一个有效的MCTS改进算法，特别适用于资源受限的环境，能够在不使用GRAVE偏置超参数的情况下获得更好的性能。

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [126] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: 本文提出了第一个用于多类预测任务的完美真实校准度量，解决了现有校准度量不真实的问题，并证明了其优越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有校准度量存在不真实性问题，会激励预测器撒谎以显得更校准。虽然Hartline等人为二分类任务引入了完美真实校准度量，但多类预测任务仍缺乏此类度量。

Method: 将二分类真实校准度量扩展到多类预测，研究不同扩展方法是否保持真实性，并引入新的多类真实校准度量。

Result: 成功构建了多类预测的完美真实校准度量，数学证明和实证验证显示该度量具有优越的鲁棒性，能稳定保持预测器间的排序关系。

Conclusion: 提出的多类真实校准度量解决了现有度量的不真实性和非鲁棒性问题，为多类预测的可靠校准评估提供了有效工具。

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [127] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: 论文揭示了非欧几里得基础模型在双曲几何等弯曲空间中的边界驱动不对称性，这种几何特性使后门触发器能够利用边界附近的小输入变化产生不成比例的大表示空间偏移，从而规避标准输入空间检测器。


<details>
  <summary>Details</summary>
Motivation: 随着非欧几里得基础模型在弯曲空间（如双曲几何）中部署表示，需要理解这些几何特性带来的安全漏洞，特别是边界驱动的不对称性如何被后门攻击利用。

Method: 通过形式化分析几何效应，提出简单的几何自适应触发器，并在不同任务和架构中进行评估，同时研究防御方法的局限性。

Result: 实证研究表明，攻击成功率随着接近边界而增加，而传统检测器效果减弱，这与理论趋势一致。径向向内拉点的防御方法可以抑制此类触发器，但会牺牲模型在该方向上的有用敏感性。

Conclusion: 这些结果揭示了非欧几里得模型特有的几何漏洞，并为设计和理解防御方法的局限性提供了基于分析的指导。

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [128] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: 该论文研究了标签噪声对深度学习模型隐藏表示的影响，发现隐藏表示的信息内容随网络参数数量呈现双下降现象，与测试误差行为类似。在欠参数化状态下，含噪声标签学习到的表示比干净标签更信息丰富；在过参数化状态下，两者信息量相当，表明过参数化网络对标签噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然标签噪声对深度学习模型性能的影响已被广泛研究，但其对网络隐藏表示的影响仍不清楚。本文旨在填补这一空白，系统性地分析标签噪声如何影响模型的隐藏表示。

Method: 使用信息不平衡（Information Imbalance）作为条件互信息的计算高效代理，系统地比较隐藏表示。分析不同参数化状态下（欠参数化和过参数化）网络在干净标签、噪声标签和随机标签下的表示特性。

Result: 1) 隐藏表示的信息内容随网络参数数量呈现双下降现象；2) 欠参数化状态下，噪声标签学习到的表示比干净标签更信息丰富；3) 过参数化状态下，两种标签学习到的表示信息量相当；4) 在过参数化状态下，倒数第二层和softmax前层之间的信息不平衡随交叉熵损失减少；5) 随机标签学习到的表示性能比随机特征更差。

Conclusion: 过参数化网络的表示对标签噪声具有鲁棒性。训练随机标签会驱动网络远超惰性学习，权重会适应编码标签信息。这为理解分类任务中的泛化提供了新视角。

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [129] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: 研究探索了使用多个小型预训练模型组合替代单一大型时间序列基础模型的方法，通过集成或模型选择在保持竞争力的同时显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 质疑"越大越好"的时间序列基础模型范式，探索更高效的替代方案：构建小型预训练模型组合。

Method: 设计专家模型组合策略，通过后训练基础模型创建多样化专家模型，并比较集成、模型选择与测试时微调的计算效率。

Result: 专家模型组合在大型基准测试中表现竞争力，且参数更少；后训练是计算有效的专家模型创建方法；集成和模型选择比测试时微调更高效。

Conclusion: 小型专家模型组合是大型单一模型的有效替代方案，在保持性能的同时显著提高计算效率。

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [130] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Hellinger定位框架的新方法，用于多轨迹时序数据中的实例最优学习，显著扩展了现有方法的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中从时序相关数据学习是一个核心问题，特别是在多轨迹设置下。现有方法要么只能应用于最小二乘回归，要么需要混合假设，限制了适用范围。

Method: 采用Hellinger定位框架，首先在路径测度层面控制平方Hellinger距离，然后通过轨迹Fisher信息加权的参数空间二次形式进行定位。

Result: 该方法在四个不同案例研究中都实现了接近渐近正态性的实例最优速率，显著优于标准约简方法。

Conclusion: Hellinger定位框架为多轨迹时序学习提供了广泛适用的实例最优保证，突破了现有方法的局限性。

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [131] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对不确定性下超参数调优的贝叶斯优化框架，特别关注随机模型中尺度或精度类型参数的优化，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于噪声函数评估，不确定性下的优化计算成本高昂，特别是在系统本身涉及不确定性的情况下，超参数调优变得具有挑战性。

Method: 使用统计代理模型来表示基础随机变量，能够分析性地评估期望算子，并推导出随机采集函数优化器的闭式表达式。

Result: 与传统的基于一维蒙特卡洛的优化方案相比，该方法需要的数据点减少了40倍，计算成本降低了40倍。

Conclusion: 该方法在计算工程中的两个数值示例中证明了其有效性，为不确定性下的超参数调优提供了一种高效解决方案。

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [132] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: 提出了一种在去中心化学习网络中基于机器学习的性能预测方法，通过预测模型性能来前瞻性地分配权重，而不是依赖历史表现的被动调整，从而提高了网络推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的线性池化方法（从简单平均到动态权重更新）存在局限性，依赖历史性能的权重更新是反应式的，难以快速适应变化的环境。需要一种能够预测模型性能的前瞻性方法。

Method: 在去中心化学习网络中增加性能预测工作节点，使用机器学习模型预测每个模型在时间序列中每个epoch的性能，基于预测结果分配更高权重给可能更准确的模型。

Result: 预测遗憾（相对于网络推理的性能）或遗憾z-score（相对于其他工作节点的性能）的模型比预测损失的模型表现更好，能够超越朴素网络推理（历史加权平均）。

Conclusion: 性能预测方法在去中心化学习网络中有效，预测模型对特征集和训练epoch数敏感，需要针对具体问题进行调整。该方法可推广到任何需要预测性而非反应性模型加权的场景。

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [133] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文指出当前迁移性评估指标的基准测试存在根本性缺陷，其不现实的模型空间和静态性能层次人为夸大了现有指标的性能表现。


<details>
  <summary>Details</summary>
Motivation: 尽管对开发迁移性评估指标的兴趣日益增长，但用于衡量其进展的基准测试却很少被检验。作者发现这些基准测试存在严重问题。

Method: 通过实证分析，展示广泛使用的基准测试设置的缺陷，证明简单、数据集无关的启发式方法可以胜过复杂方法。

Result: 研究发现当前评估协议与真实世界模型选择的复杂性之间存在关键脱节，现有基准人为夸大了指标性能。

Conclusion: 为构建更稳健和现实的基准测试提供具体建议，以指导未来研究朝着更有意义的方向发展。

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [134] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 本文揭示了注意力汇聚和压缩谷地之间的惊人联系，两者都源于残差流中大规模激活的形成。作者提出了Mix-Compress-Refine信息流理论，解释LLM通过控制大规模激活来组织深度计算。


<details>
  <summary>Details</summary>
Motivation: 注意力汇聚和压缩谷地作为大语言模型中的两个谜题现象，之前被孤立研究。本文旨在揭示两者之间的内在联系，并建立统一的理论框架。

Method: 通过理论证明大规模激活必然产生表征压缩，并在多个模型(410M-120B参数)上进行实验验证。当序列开始标记在中间层产生极端激活范数时，压缩谷地和注意力汇聚同时出现。

Result: 实验证实了理论预测：大规模激活导致表征压缩和熵减少。目标消融研究验证了理论预测的有效性。

Conclusion: 提出了Mix-Compress-Refine信息流理论，认为Transformer-based LLM在三个不同阶段处理标记：早期层广泛混合、中间层压缩计算、后期层选择性精炼。该框架解释了为什么嵌入任务在中间层表现最佳，而生成任务受益于全深度处理。

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [135] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: Sequential-EDFL是一种基于经验动态形式提升的序列测试方法，用于语言模型生成停止决策，通过跟踪信息提升量并提供形式化的错误控制，在六个基准测试中减少22-28%的生成量，同时保持delta级控制。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型生成过程中何时停止的问题，通过形式化方法控制生成质量，减少不必要的生成计算。

Method: 使用自归一化经验伯努利e过程跟踪信息提升量（完整模型与弱化骨架基线之间的对数似然比），结合在线均值估计处理未知中心化，通过混合e过程组合多个参数，支持分布漂移下的自适应重置。

Result: 在六个基准测试中，Sequential-EDFL相比序列基线减少22-28%的生成量，保持delta级控制，计算开销为12%。结合轻量级正确性门控可提高端任务正确性。

Conclusion: EDFL作为第一阶段过滤器可减少83%的验证负担，但不能作为安全关键领域的独立解决方案，因为10.9%的停止序列仍然不正确。

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [136] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: GUIDE是一种新的蒸馏方法，通过在参数空间强制学生模型匹配教师模型，显著缩小师生质量差距，且不增加训练或推理开销。


<details>
  <summary>Details</summary>
Motivation: 标准蒸馏方法仅让学生模型匹配教师模型的输出，未能充分利用教师模型的全部有用信息。考虑到训练大模型的成本，应该从教师模型中提取更多有用信息。

Method: 提出GUIDE方法，在参数空间强制学生模型匹配教师模型，可以单独使用或与知识蒸馏结合使用。

Result: 使用GUIDE后，师生质量差距减少25-26%，在大型学生模型上表现优异，且与知识蒸馏结合时效果接近叠加。

Conclusion: GUIDE方法能显著提升模型质量，且不增加任何训练或推理成本，质量提升几乎是免费的。

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [137] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: ATLO-ML是一个自适应时间长度优化系统，能自动确定基于用户定义输出时间长度的最佳输入时间长度和采样率，显著提升时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的时间序列预测在机器学习中很大程度上取决于选择合适的输入时间长度和采样率，传统固定参数方法效果有限。

Method: 开发ATLO-ML系统，动态调整输入时间长度和采样率，使用空气质量数据集（GAMS数据集和专有数据中心数据）进行验证。

Result: 使用优化后的时间长度和采样率显著提高了机器学习模型的准确性，优于固定时间长度方法。

Conclusion: ATLO-ML在优化机器学习工作流中的时间输入参数方面提供了强大解决方案，具有跨各种时间敏感应用的泛化潜力。

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [138] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: Medix是一个新颖的OOD检测框架，使用中位数操作从未标记数据中识别潜在异常值，然后结合标记的InD数据训练鲁棒的OOD分类器。


<details>
  <summary>Details</summary>
Motivation: 现实应用中未标记数据通常混合了InD和OOD样本，缺乏明确的OOD样本集使得训练最优OOD分类器变得困难。

Method: 利用中位数操作的稳定性从未标记数据中识别异常值，然后使用这些识别出的异常值和标记InD数据训练OOD分类器。

Result: 理论分析显示Medix实现了低错误率，实证结果在开放世界设置中全面优于现有方法。

Conclusion: Medix通过中位数操作有效利用未标记数据，显著提升了OOD检测性能，验证了理论洞察的有效性。

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [139] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: 文本到图像排行榜存在严重的模型去匿名化漏洞，通过CLIP嵌入空间的简单分类就能高精度识别生成模型，使得排行榜排名操纵比预期更容易。


<details>
  <summary>Details</summary>
Motivation: 生成式AI排行榜容易受到操纵攻击，其中模型去匿名化是关键攻击步骤。本文发现文本到图像排行榜的去匿名化问题比大型语言模型更严重。

Method: 使用150,000多张生成图像、280个提示和19个不同模型，在CLIP嵌入空间进行实时分类，并引入提示级可分离性指标。

Result: 即使没有提示控制或历史数据，也能高精度识别生成模型，某些提示可实现近乎完美的去匿名化。

Conclusion: 文本到图像排行榜的排名操纵比之前认识的更容易，需要更强的防御措施。

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [140] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: 该论文证明随机初始化的宽神经网络在激活函数具有高斯测度下零均值时，其输出近似独立。这包括ReLU和GeLU的平移版本、tanh等，但不包括原始的ReLU或GeLU。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络输出的独立性，以验证对齐研究中心关于AI可解释性极限的计算无巧合猜想。

Method: 分析随机初始化神经网络的输出特性，特别关注激活函数在高斯测度下的均值条件。

Result: 发现当激活函数满足E[σ(z)]=0时，宽神经网络的输出近似独立。

Conclusion: 零均值激活函数的神经网络是验证计算无巧合猜想的合适候选模型。

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [141] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: 该论文提出将POMDP近似为有限状态MDP（称为超状态MDP）的方法，通过理论分析证明近似误差随历史长度指数下降，并提出了结合TD学习和策略优化的学习框架。


<details>
  <summary>Details</summary>
Motivation: POMDP中信念状态的连续性给学习最优策略带来了计算挑战，需要找到有效的近似方法来处理部分可观测性。

Method: 将POMDP转化为有限状态MDP（超状态MDP），使用线性函数逼近和TD学习结合策略优化的方法来学习最优策略。

Result: 获得了改进的理论保证，证明近似误差随历史长度指数下降，并首次量化了在非马尔可夫环境中应用标准TD学习时的误差。

Conclusion: POMDP可以通过将其视为MDP并使用TD学习和策略优化来近似求解，其中MDP状态对应于有限历史，且近似性能随历史长度指数提升。

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [142] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 本文数学分析了强化学习中由自回归模型简单目标条件化导致的策略不连贯问题，证明了在线RL微调离线学习策略可以减少不连贯性并提高回报，建立了训练-推断权衡的三重对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习策略中由朴素目标条件化引起的结构性问题——不连贯性，特别是在自回归模型微调过程中，旨在理解在线RL如何改善离线学习策略的性能。

Method: 通过重新构建控制即推断和软Q学习标准概念，建立三重对应关系：将后验折叠到奖励中，在确定性情况下降低温度参数，以及通过软条件生成模型分析不连贯性与有效视野的联系。

Result: 证明了在线RL微调离线学习策略可以减少不连贯性并提高回报，建立了训练-推断权衡的计算框架，揭示了不连贯性与有效视野之间的关系。

Conclusion: 通过数学分析阐明了策略不连贯性的本质，建立了微调过程的多种理解方式之间的对应关系，为改进强化学习策略的训练方法提供了理论基础。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [143] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 提出了Markovian Thinking范式，通过将推理分解为固定大小的块，实现线性计算复杂度和恒定内存使用，解决了传统RL推理中状态无限增长和二次计算开销的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习推理环境中，状态随推理长度无限增长，导致注意力机制需要二次计算开销，限制了长链推理的可扩展性。

Method: 设计了Delethink环境，将推理结构化为固定大小的块，在每个块边界重置上下文并用简短摘要重新初始化提示，通过RL训练模型学习生成足够的文本状态以实现无缝推理延续。

Result: 在1.5B模型上，Delethink使用8K令牌块实现了24K令牌的推理，性能匹配或超过使用24K预算的LongCoT-RL。在96K平均推理长度下，Delethink成本仅为LongCoT-RL的1/4。

Conclusion: 重新设计推理环境是实现高效、可扩展推理LLMs的有力杠杆，能够在没有二次开销的情况下实现超长推理。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [144] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: 比较了AI框架与纯人工评估在医学影像疾病评估中的表现，发现AI作为辅助读者(AI-SR)是最适合临床试验的方法，即使在模型性能差的情况下也能保持可靠。


<details>
  <summary>Details</summary>
Motivation: AI在临床试验中具有巨大潜力，但缺乏安全保障的部署会带来显著风险，特别是当评估直接影响试验结论的患者终点时。

Method: 比较两种AI框架与纯人工评估，测量成本、准确性、鲁棒性和泛化能力。通过注入从随机猜测到简单预测的差模型来压力测试框架，确保在严重模型退化下观察到的治疗效果仍然有效。使用两个基于脊柱X光图像的随机对照试验进行评估。

Result: AI作为辅助读者(AI-SR)方法在所有标准上表现最佳，即使使用差模型也能保持可靠。该方法始终提供可靠的疾病估计，保留临床试验治疗效果估计和结论，并在应用于不同人群时保持这些优势。

Conclusion: AI-SR是临床试验中最合适的方法，因为它满足各种模型类型下的所有标准，即使在模型性能差的情况下也能保持可靠性和有效性。

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [145] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: 提出了DPA-Net双路径注意力神经网络，从稀疏的SMBG数据直接估计AGP血糖指标，解决CGM设备昂贵且不易获取的问题。


<details>
  <summary>Details</summary>
Motivation: CGM设备成本高、可及性差，特别是在中低收入地区难以普及；而SMBG虽然便宜易得，但数据稀疏不规则，难以转换为有临床意义的血糖指标。

Method: DPA-Net包含两条互补路径：空间通道注意力路径重建CGM样轨迹，多尺度ResNet路径直接预测AGP指标；引入对齐机制减少偏差和过拟合；开发主动点选择器识别反映患者行为模式的采样点。

Result: 在大型真实世界数据集上的实验表明，DPA-Net实现了稳健的准确性和低误差，同时减少了系统偏差。

Conclusion: 这是首个从SMBG数据估计AGP指标的监督机器学习框架，为无法获取CGM的场景提供了实用且临床相关的决策支持工具。

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [146] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: POME是一种后优化模型编辑算法，通过奇异值分解投影优化微调模型权重，无需额外数据或训练即可提升模型性能


<details>
  <summary>Details</summary>
Motivation: 解决微调后模型性能优化问题，避免重新训练的高成本，提供零成本、通用的性能提升方案

Method: 对微调权重与预训练权重的差异ΔW进行截断奇异值分解投影，平衡主导更新方向的影响并修剪噪声

Result: 在GSM8K上平均性能提升2.5%，在代码生成任务上提升1.0%，适用于7B到72B的各种规模模型

Conclusion: POME是一种实用、零成本的通用增强方法，可无缝集成到任何微调流程中

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [147] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合AI和远程传感器的框架，用于地下水管泄漏检测，通过稀疏传感器部署和HydroNet模型实现高效精准的泄漏识别


<details>
  <summary>Details</summary>
Motivation: 地下水管道的泄漏和渗透问题导致严重的水资源损失、环境破坏和高昂维修成本，传统人工检测效率低，密集传感器部署成本过高

Method: 部署稀疏远程传感器采集实时流量和深度数据，结合HydroNet模型利用管道属性（材料、直径、坡度等）构建有向图进行高精度建模，集成边缘感知消息传递与水力模拟

Result: 在真实校园污水网络数据集上的评估显示，系统能有效收集时空水力数据，HydroNet模型优于先进基线方法

Conclusion: 该方法能从有限的传感器部署中实现准确的网络范围预测，可有效扩展到各种地下水管网络

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [148] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: 提出了Chem-NMF方法，通过引入物理化学中的玻尔兹曼概率概念来稳定多层NMF的收敛，在生物医学信号和面部图像上分别提升了5.6%±2.7%和11.1%±7.2%的聚类准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然α-散度增强了NMF的优化灵活性，但将其扩展到多层架构时面临收敛性挑战。需要一种新的方法来确保多层NMF的收敛稳定性。

Method: 受化学反应中能量势垒的玻尔兹曼概率启发，提出了Chem-NMF方法，引入边界因子来稳定收敛。这是首个从物理化学角度严格分析NMF算法收敛行为的研究。

Result: 实验结果显示，在生物医学信号上聚类准确率提升5.6%±2.7%，在面部图像上提升11.1%±7.2%。从数学证明的渐近收敛结果扩展到实际数据应用。

Conclusion: Chem-NMF通过物理化学视角成功解决了多层NMF的收敛问题，显著提升了聚类性能，为NMF的收敛分析提供了新的理论框架。

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [149] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出了一种改进的流匹配方法，通过在训练过程中注入随机性来解决分布到分布转换中的稀疏监督问题，在多个科学成像任务中显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配方法主要关注从噪声到数据的转换，但在分布到分布转换的实际科学应用中，当源分布也是从有限样本中学习时，标准流匹配方法会因稀疏监督而失败。

Method: 通过扰动源样本和流插值项向训练过程注入随机性，提出了一种简单且计算高效的方法。

Result: 在五个不同的成像任务（生物学、放射学、天文学）中，该方法显著提高了生成质量，平均FID分数比现有基线高出9分，同时降低了输入与生成样本之间的传输成本。

Conclusion: 该方法使流匹配成为模拟科学中出现的各种分布转换的更实用工具，能更好地突出转换的真实效果。

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [150] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: 提出StruSR框架，利用训练好的PINN从时间序列数据中提取局部结构化物理先验，通过局部泰勒展开获得导数结构信息来指导符号表达式演化，提高符号回归的收敛速度和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法缺乏从时间序列观测中提取结构化物理先验的机制，难以捕捉反映系统全局行为的符号表达式。

Method: 利用训练好的PINN进行局部泰勒展开获得导数结构信息；引入基于掩码的属性机制量化子树贡献；使用混合适应度函数联合最小化物理残差和泰勒系数失配；通过遗传编程指导表达式演化。

Result: 在基准PDE系统上的实验表明，StruSR相比传统基线方法提高了收敛速度、结构保真度和表达式可解释性。

Conclusion: StruSR为基于物理的符号发现提供了一个有原则的范式，能够更好地捕捉系统的全局行为。

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [151] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: 提出了一个摊销框架，通过轻量级控制器网络增强预训练的ARDMs，在数据同化任务中实现单次前向推理，避免昂贵的伴随计算和优化。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在测试时缩放和微调方面取得进展，但自回归扩散模型中的引导机制研究不足。现有方法在稀疏观测下的混沌时空偏微分方程数据同化中计算成本高且容易产生预测漂移。

Method: 使用离线训练的轻量级控制器网络增强预训练ARDMs，通过预览未来ARDMs展开并学习逐步控制来预测即将到来的观测，基于终端成本目标。

Result: 在两个典型偏微分方程和六种观测机制下，该方法在稳定性、准确性和物理保真度方面持续优于四种最先进的基线方法。

Conclusion: 该方法将数据同化推理简化为单次前向展开，避免了推理期间的昂贵伴随计算和优化，在复杂物理系统中表现出优越性能。

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [152] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 本文评估了机器学习算子（MLOs）在零样本超分辨率任务中的表现，发现MLOs无法在未经额外训练的情况下处理不同分辨率的数据，存在混叠问题。作者提出了一个简单的多分辨率训练协议来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中的一个核心挑战是建模连续现象，这些现象在实践中通常以离散形式表示。MLOs被引入作为实现这一目标的手段，但需要评估其是否能够实现零样本超分辨率推理。

Method: 将多分辨率推理解耦为两个关键行为：1）对不同频率信息的推断；2）在不同分辨率间的插值。通过实证研究评估MLOs的性能，并提出一个计算效率高、数据驱动的多分辨率训练协议。

Result: 实证研究表明，MLOs无法以零样本方式完成上述两个任务，在不同分辨率下的推理不准确，容易受到混叠影响。提出的多分辨率训练协议能够克服混叠问题并提供稳健的多分辨率泛化能力。

Conclusion: MLOs本身无法实现零样本多分辨率推理，但通过提出的多分辨率训练协议可以显著改善其性能，实现稳健的多分辨率泛化。

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [153] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: 提出了ARQ方法，将前向-前向算法的goodness函数与动作条件结合，用于局部强化学习，无需反向传播即可实现优于现有方法的性能


<details>
  <summary>Details</summary>
Motivation: 前向-前向算法目前主要局限于监督学习，而在强化学习等自然产生学习信号的领域中存在应用空白

Method: 使用动作条件均方根Q函数，结合goodness函数和动作条件，通过时序差分学习进行局部强化学习

Result: 在MinAtar和DeepMind Control Suite基准测试中表现优于最先进的局部无反向传播RL方法，并在大多数任务中超过使用反向传播的算法

Conclusion: ARQ方法简单且具有生物学基础，为无需反向传播的强化学习提供了有效解决方案

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [154] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: 提出了高斯混合非线性模块(GMNM)，这是一种基于高斯混合模型的新颖可微分模块，能够增强神经网络中的非线性能力，可集成到各种架构中并通过梯度方法进行端到端训练。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络(如MLP、CNN、Transformer)通过线性组合和激活函数引入非线性，但受限于激活函数的选择，非线性能力有限。

Method: 基于高斯混合模型的通用密度近似特性和高斯核的距离特性，放松概率约束并采用灵活的高斯投影参数化，构建可微分模块GMNM。

Result: 在MLP、CNN、注意力机制和LSTM等架构中集成GMNM，相比标准基线模型性能得到一致提升。

Conclusion: GMNM作为一种强大而灵活的模块，在提升各种机器学习应用的效率和准确性方面具有巨大潜力。

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [155] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: 本文研究了Transformer中注意力头数量对表达能力的影响，建立了参数复杂度的上下界，证明了多头注意力对高效近似的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer已成为序列建模的主导架构，但其结构参数如何影响表达能力的具体机制仍不清楚，特别是注意力头数量的作用需要深入研究。

Method: 引入广义D检索任务作为理论框架，分析Transformer的参数复杂度上下界，特别关注注意力头数量对近似能力的影响。

Result: 证明了多头Transformer可实现高效近似，而头数过少时参数复杂度需按O(1/ε^cT)增长；单头情况下需要O(T)嵌入维度才能完全记忆输入。

Conclusion: 注意力头数量是Transformer表达能力的关键因素，理论结果在合成数据和实际任务中得到验证，为Transformer设计提供了理论指导。

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [156] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO是一个统一的强化学习框架，通过探索-利用的视角重新构建策略优化，在数学和编程基准测试中优于现有方法，提升性能并加速训练收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法如GRPO在LLM推理中存在探索不足的问题，对具有挑战性的提示探索有限，且未充分利用信息反馈信号，原因是跨提示的上下文无关的rollout分配和过度依赖稀疏奖励。

Method: XRPO引入数学基础的rollout分配器，自适应优先处理具有更高不确定性减少潜力的提示；采用上下文种子策略注入精选示例；开发基于组相对、新颖性感知的优势锐化机制，利用序列似然放大低概率但正确的响应。

Result: 在数学和编程基准测试中，XRPO优于现有方法（如GRPO和GSPO），pass@1提升达4%，cons@32提升达6%，同时训练收敛速度加快达2.7倍。

Conclusion: XRPO通过探索-利用的统一框架有效解决了现有强化学习方法在LLM推理中的局限性，在性能和训练效率上均取得显著提升。

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [157] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: TimeFormer是一种专为时间序列数据设计的Transformer架构，通过引入考虑时间序列特性的自注意力机制和多重尺度分析，显著提升了时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在时间序列预测中表现不佳，主要原因是未能充分考虑文本数据与时间序列数据之间的模态差异，特别是忽略了时间序列的两个关键特性：单向影响和影响力随时间衰减。

Method: 提出TimeFormer架构，核心创新是包含两个调制项的自注意力机制（MoSA），基于Hawkes过程和因果掩码来捕捉时间序列的时间先验。同时引入基于多尺度和子序列分析的框架来捕获不同时间尺度上的语义依赖。

Result: 在多个真实世界数据集上的实验表明，TimeFormer显著优于现有最先进方法，在MSE指标上相比最佳基线降低了7.45%，并在94.04%的评估指标上创造了新记录。MoSA机制还能广泛提升其他基于Transformer模型的性能。

Conclusion: TimeFormer通过专门设计的时间序列感知注意力机制，成功克服了传统Transformer在时间序列预测中的局限性，为时间序列分析提供了更有效的解决方案。

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [158] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: 提出了一种分布式多玩家多臂老虎机算法，通过自适应高效通信协议实现近最优的群体和个人遗憾，通信成本仅为O(log log T)，并在异步设置中实现对数遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决多玩家多臂老虎机问题中的碰撞问题，在无中心协调的分布式环境下，玩家只能观察自身动作和碰撞反馈，需要设计高效的通信协议来协调玩家行为。

Method: 设计分布式算法，采用自适应高效通信协议，玩家通过有限通信协调臂选择以避免碰撞，算法可扩展到周期性异步设置。

Result: 算法实现了近最优的群体和个人遗憾，通信成本仅为O(log log T)，实验显示相比现有方法显著降低个人遗憾，在异步设置中达到对数遗憾。

Conclusion: 提出的分布式算法通过高效通信协议有效解决了多玩家老虎机问题，在通信成本和性能方面均优于现有方法，适用于分布式和异步环境。

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [159] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: 提出AutoBalance训练范式，通过为每个损失分量分配独立的自适应优化器，解决PINNs训练中多损失项平衡难题，显著提升求解精度


<details>
  <summary>Details</summary>
Motivation: 传统PINNs训练方法在处理多个损失项（如PDE残差和边界条件）时存在困难，因为这些损失项具有冲突目标和不同曲率，导致训练不稳定

Method: AutoBalance采用"后组合"策略，为每个损失分量分配独立的自适应优化器，然后聚合预处理的更新结果

Result: 在具有挑战性的PDE基准测试中，AutoBalance始终优于现有框架，在MSE和L∞范数上显著降低求解误差

Conclusion: AutoBalance是一种正交且互补的PINN方法，能够增强其他流行PINN方法在要求严格基准测试中的有效性

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [160] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: 本文揭示了现有深度神经网络模型提取攻击在深层网络中的局限性，并提出了一种新的跨层提取方法，通过利用神经元跨层交互来显著降低查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有基于硬标签的模型提取攻击在深层网络中需要指数级查询量，实际不可行。本文旨在解决这一关键限制，使模型提取在深层网络中更加实用。

Method: 提出CrossLayer Extraction攻击方法，不直接提取特定神经元的秘密参数，而是利用神经元跨层交互从更深层提取信息，从而降低查询复杂度。

Result: 新方法显著减少了模型提取所需的查询数量，缓解了现有方法在深层网络中的局限性。

Conclusion: 跨层提取技术为深度神经网络模型提取提供了更实用的解决方案，特别是在攻击深层网络时具有显著优势。

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [161] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了一种两阶段框架，通过时间序列补全和基于视觉的扩散模型，有效解决了不规则时间序列生成中的问题，在性能和计算成本上均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 不规则采样和缺失值给时间序列生成带来挑战，现有方法效果不佳且计算成本高。ImagenTime模型虽在规则时间序列上表现良好，但直接扩展到不规则序列会产生不自然的邻域关系。

Method: 两阶段框架：1) 使用时间序列变换器补全不规则序列，创建自然邻域；2) 采用带掩码的基于视觉的扩散模型，减少对补全值的依赖。

Result: 实现了最先进的性能，判别分数相对提升70%，计算成本相对降低85%。

Conclusion: 该方法结合了补全和掩码的优势，能够鲁棒且高效地生成真实的时间序列数据。

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [162] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了用于目标条件强化学习的双重目标表示方法，通过状态间的时序距离关系来表征状态，具有环境动态不变性和噪声过滤能力，能提升离线目标达成性能


<details>
  <summary>Details</summary>
Motivation: 传统目标条件强化学习中的状态表示可能包含无关信息或噪声，影响学习效率和性能。需要一种能捕捉状态间本质关系、过滤外生噪声的表示方法

Method: 开发双重目标表示方法，通过计算状态与所有其他状态之间的时序距离来表征状态。该方法可与现有GCRL算法结合使用

Result: 在OGBench任务套件的20个状态和像素任务上，双重目标表示方法一致提升了离线目标达成性能

Conclusion: 双重目标表示提供了一种理论上合理且实践有效的状态表征方法，能显著提升目标条件强化学习的性能

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [163] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了一种用于异构领域的因果发现方法，结合专家先验知识和变分混合结构学习，能够推断因果贝叶斯网络的混合模型。


<details>
  <summary>Details</summary>
Motivation: 在异构领域中，现有的先验知识获取方法假设单一因果图，不适合处理异质性。需要开发能够整合专家知识的因果发现方法。

Method: 基于贝叶斯实验设计原则的因果获取策略，以及扩展DiBS方法的变分混合结构学习方法VaMSL，迭代推断因果贝叶斯网络混合模型。

Result: 该方法成功生成替代因果模型集合，在异构合成数据上获得改进的结构学习性能，并能捕捉乳腺癌数据库中的复杂分布。

Conclusion: 提出的方法能够有效处理异构领域的因果发现问题，整合专家知识并学习复杂的因果模型混合分布。

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>


### [164] [Function regression using the forward forward training and inferring paradigm](https://arxiv.org/abs/2510.06762)
*Shivam Padmani,Akshay Joshi*

Main category: cs.LG

TL;DR: 提出了一种使用前向-前向算法进行函数回归的新方法，并将其扩展到Kolmogorov Arnold网络和深度物理神经网络。


<details>
  <summary>Details</summary>
Motivation: 前向-前向学习算法是一种无需反向传播的新颖训练方法，但目前仅局限于分类任务。本文旨在将该算法扩展到函数回归这一基础机器学习应用。

Method: 开发了基于前向-前向算法的函数回归方法，并在单变量和多变量函数上进行了评估，还初步研究了将其扩展到Kolmogorov Arnold网络和深度物理神经网络。

Result: 成功实现了使用前向-前向算法进行函数回归，并在不同类型函数上进行了验证。

Conclusion: 前向-前向算法可以有效地应用于函数回归任务，为在神经形态计算和物理模拟神经网络中的实现提供了新的可能性。

Abstract: Function regression/approximation is a fundamental application of machine
learning. Neural networks (NNs) can be easily trained for function regression
using a sufficient number of neurons and epochs. The forward-forward learning
algorithm is a novel approach for training neural networks without
backpropagation, and is well suited for implementation in neuromorphic
computing and physical analogs for neural networks. To the best of the authors'
knowledge, the Forward Forward paradigm of training and inferencing NNs is
currently only restricted to classification tasks. This paper introduces a new
methodology for approximating functions (function regression) using the
Forward-Forward algorithm. Furthermore, the paper evaluates the developed
methodology on univariate and multivariate functions, and provides preliminary
studies of extending the proposed Forward-Forward regression to Kolmogorov
Arnold Networks, and Deep Physical Neural Networks.

</details>


### [165] [Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06776)
*Phillip Rothenbeck,Sai Karthikeya Vemuri,Niklas Penzel,Joachim Denzler*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)解决德国各联邦州COVID-19传播的SIR模型逆问题，进行精细时空分析并估计时变再生数R_t


<details>
  <summary>Details</summary>
Motivation: 传统隔室模型如SIR在直接纳入噪声观测数据方面存在局限，需要更有效的方法来分析COVID-19大流行的时空动态

Method: 采用物理信息神经网络(PINNs)结合罗伯特·科赫研究所(RKI)的感染数据，求解SIR模型的逆问题，估计州级传播和恢复参数

Result: 发现各地区传播行为存在显著差异，与疫苗接种率和主要大流行阶段的时间模式相关

Conclusion: PINNs在局部化、长期流行病学建模中具有实用价值，能够有效跟踪大流行进展

Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and
analysis to understand real-world disease dynamics. In particular, post hoc
analyses using compartmental models offer valuable insights into the
effectiveness of public health interventions, such as vaccination strategies
and containment policies. However, such compartmental models like SIR
(Susceptible-Infectious-Recovered) often face limitations in directly
incorporating noisy observational data. In this work, we employ
Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the
SIR model using infection data from the Robert Koch Institute (RKI). Our main
contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics
across all German federal states over a three-year period. We estimate
state-specific transmission and recovery parameters and time-varying
reproduction number (R_t) to track the pandemic progression. The results
highlight strong variations in transmission behavior across regions, revealing
correlations with vaccination uptake and temporal patterns associated with
major pandemic phases. Our findings demonstrate the utility of PINNs in
localized, long-term epidemiological modeling.

</details>


### [166] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: 该论文提出了推理计算鲁棒性假设(RICH)，认为当模型的训练数据更好地反映被攻击数据的组成成分时，推理计算防御就能发挥作用。通过组合泛化，即使面对对抗性OOD数据，模型也能遵循防御规范。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM推理能提高对防御规范的遵循度，从而增强对越狱攻击的鲁棒性，但这种测试计算的好处会在攻击者获得梯度或多模态输入时减弱。本文旨在填补这一空白，证明推理计算在这些情况下仍然有效。

Method: 提出RICH假设，通过组合泛化使模型能够理解OOD数据的ID组成成分，从而在对抗性OOD输入上遵循防御规范。在视觉语言模型和多种攻击类型上进行了实证验证。

Result: 研究发现，如果组合泛化能够解锁对OOD数据的规范遵循，那么测试时计算就能带来鲁棒性增益。通过提示增强防御规范强调，可以降低基于梯度的多模态攻击在对抗性预训练VLM上的成功率，但对未鲁棒化模型无效。

Conclusion: 推理计算的鲁棒性收益与基础模型鲁棒性相关，形成了富者愈富的RICH动态。建议将训练时和测试时防御层叠使用，以获得协同效益。

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [167] [The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning](https://arxiv.org/abs/2510.06819)
*Giovanni Donghi,Daniele Zambon,Luca Pasa,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: 提出一种简单有效的在线持续图学习方法：使用固定的随机初始化编码器生成节点嵌入，仅在线训练轻量级分类器，有效缓解灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 解决在线持续图学习中的灾难性遗忘问题，传统方法需要复杂的内存缓冲或正则化技术，而本文探索更简单有效的解决方案

Method: 使用固定的随机初始化图编码器生成节点嵌入，仅在线训练轻量级分类器，通过冻结编码器参数来保持表示稳定性

Result: 在多个OCGL基准测试中，该方法相比最先进方法获得一致提升，改进幅度高达30%，性能接近离线联合训练的上界

Conclusion: 在在线持续图学习中，通过架构简单性和稳定性可以最小化灾难性遗忘，无需复杂的回放或正则化技术

Abstract: Catastrophic forgetting is one of the main obstacles for Online Continual
Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may
occur at any time and offline training on task-specific subgraphs is not
feasible. In this work, we explore a surprisingly simple yet highly effective
approach for OCGL: we use a fixed, randomly initialized encoder to generate
robust and expressive node embeddings by aggregating neighborhood information,
training online only a lightweight classifier. By freezing the encoder, we
eliminate drifts of the representation parameters, a key source of forgetting,
obtaining embeddings that are both expressive and stable. When evaluated across
several OCGL benchmarks, despite its simplicity and lack of memory buffer, this
approach yields consistent gains over state-of-the-art methods, with surprising
improvements of up to 30% and performance often approaching that of the joint
offline-training upper bound. These results suggest that in OCGL, catastrophic
forgetting can be minimized without complex replay or regularization by
embracing architectural simplicity and stability.

</details>


### [168] [Efficient numeracy in language models through single-token number embeddings](https://arxiv.org/abs/2510.06824)
*Linus Kreitner,Paul Hager,Jonathan Mengedoht,Georgios Kaissis,Daniel Rueckert,Martin J. Menten*

Main category: cs.LG

TL;DR: 论文提出BitTokens方法，将数字编码为单个token，解决LLMs处理数值计算时token效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs处理数值数据需要大量推理token，且数字被分割成多个token，限制了模型处理复杂计算问题的能力。

Method: 提出BitTokens方法，使用IEEE 754二进制浮点数表示将任何数字嵌入到单个token中。

Result: 实验表明BitTokens使小型语言模型也能近乎完美地学习基本算术运算算法。

Conclusion: BitTokens提高了语言模型处理数值计算的效率，可扩展模型能解决的问题长度和复杂度。

Abstract: To drive progress in science and engineering, large language models (LLMs)
must be able to process large amounts of numerical data and solve long
calculations efficiently. This is currently only possible through the use of
external tools or extensive reasoning chains, either limiting the numerical
intuition of LLMs or limiting the length of problems they can solve. We show
that frontier LLMs require excessive amounts of reasoning tokens to solve even
basic calculations, which is exacerbated by their tokenization strategies that
split single numbers into multiple tokens. This motivates the need for
efficient and effective single-token number encodings. We introduce a set of
desiderata for such encodings and show that existing approaches fail to fulfill
them. To address these shortcomings, we propose BitTokens, a novel tokenization
strategy that embeds any number into a single token using its IEEE 754 binary
floating-point representation. Through extensive experiments we show that our
BitTokens allow even small language models to learn algorithms that solve basic
arithmetic operations nearly perfectly. This newly gained efficiency could
expand the length and complexity of problems language models can solve.

</details>


### [169] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: 该论文挑战了"注意力就是一切"的观点，认为非循环架构无法处理长期智能体任务，提出了循环完备架构并在GitHub动作序列上训练，展示了随着序列长度增加，损失呈幂律下降且训练时间成本可摊销。


<details>
  <summary>Details</summary>
Motivation: 挑战当前流行的注意力机制观点，指出完全可并行化的前向或后向传播架构无法表示长期运行智能体任务中特别有趣的问题类别，认为存在一个临界时间点，超过该点非循环完备模型无法正确聚合输入。

Method: 引入循环完备架构，在GitHub来源的动作序列上进行训练，使用固定参数数量处理不同长度的序列。

Result: 损失随训练序列长度呈幂律分布，参数数量保持固定；更长的序列训练总能摊销其线性增长的实际时间成本，在时间函数上产生更低的损失。

Conclusion: 循环机制对于处理长期智能体任务至关重要，循环完备架构能够有效解决非循环模型在长期输入聚合方面的局限性。

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [170] [Early wind turbine alarm prediction based on machine learning: AlarmForecasting](https://arxiv.org/abs/2510.06831)
*Syed Shazaib Shah,Daoliang Tan*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM的警报预测与分类框架，能够提前预测风力涡轮机警报，防止警报触发和故障发生。


<details>
  <summary>Details</summary>
Motivation: 传统研究仅将警报数据用作诊断工具，本研究旨在实现警报预防，在警报触发前进行预测和干预。

Method: 采用两阶段框架：基于LSTM的回归模块进行时间序列警报预测，然后通过分类模块对预测警报进行标记分类。

Result: 在14台Senvion MM82涡轮机5年运行数据上测试，10、20、30分钟警报预测准确率分别为82%、52%、41%。

Conclusion: 该框架能够可靠预测整个警报分类体系，显著降低警报频率，通过主动干预提高运行效率。

Abstract: Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and
forms the backbone for advancedpredictive monitoring systems. Traditionally,
research cohorts have been confined to utilizing alarm data solelyas a
diagnostic tool, merely indicative of unhealthy status. However, this study
aims to offer a transformativeleap towards preempting alarms, preventing alarms
from triggering altogether, and consequently avertingimpending failures. Our
proposed Alarm Forecasting and Classification (AFC) framework is designed on
twosuccessive modules: first, the regression module based on long short-term
memory (LSTM) for time-series alarmforecasting, and thereafter, the
classification module to implement alarm tagging on the forecasted alarm.
Thisway, the entire alarm taxonomy can be forecasted reliably rather than a few
specific alarms. 14 Senvion MM82turbines with an operational period of 5 years
are used as a case study; the results demonstrated 82%, 52%,and 41% accurate
forecasts for 10, 20, and 30 min alarm forecasts, respectively. The results
substantiateanticipating and averting alarms, which is significant in curbing
alarm frequency and enhancing operationalefficiency through proactive
intervention.

</details>


### [171] [Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors](https://arxiv.org/abs/2510.06834)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: 这篇论文提出了在RISC-V向量处理器上加速FlashAttention内核的方法，通过向量化实现、低成本的指数函数近似和分块策略优化，显著提升了注意力机制的性能。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是众多机器学习和人工智能模型的核心操作，但现有的实现存在计算复杂度高、内存访问效率低的问题，特别是在RISC-V向量处理器上缺乏高效的实现方案。

Method: 采用向量化FlashAttention算法，减少标量代码使用；利用低成本浮点算术指数近似简化softmax计算复杂度；探索合适的分块策略改善内存局部性；无需扩展基线向量ISA的自定义指令。

Result: 实验结果表明该方法具有良好的可扩展性，在实际应用处理注意力层时，向量化实现带来了显著的性能提升。

Conclusion: 这是首次向量化FlashAttention的工作，通过创新的指数函数近似和内存优化策略，在RISC-V向量处理器上实现了高效的注意力计算加速。

Abstract: Attention is a core operation in numerous machine learning and artificial
intelligence models. This work focuses on the acceleration of attention kernel
using FlashAttention algorithm, in vector processors, particularly those based
on the RISC-V instruction set architecture (ISA). This work represents the
first effort to vectorize FlashAttention, minimizing scalar code and
simplifying the computational complexity of evaluating exponentials needed by
softmax used in attention. By utilizing a low-cost approximation for
exponentials in floating-point arithmetic, we reduce the cost of computing the
exponential function without the need to extend baseline vector ISA with new
custom instructions. Also, appropriate tiling strategies are explored with the
goal to improve memory locality. Experimental results highlight the scalability
of our approach, demonstrating significant performance gains with the
vectorized implementations when processing attention layers in practical
applications.

</details>


### [172] [CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting](https://arxiv.org/abs/2510.06840)
*Stefano F. Stefenon,João P. Matos-Carvalho,Valderi R. Q. Leithardt,Kin-Choong Yow*

Main category: cs.LG

TL;DR: 提出CNN-TFT-SHAP-MHAW混合架构，结合卷积神经网络和时序融合变换器，用于多变量时间序列预测，在水电自然流量数据集上表现优异，MAPE达2.2%。


<details>
  <summary>Details</summary>
Motivation: CNN擅长捕捉局部模式和平移不变性，而变换器通过自注意力有效建模长程依赖关系，结合两者优势提升多变量时间序列预测性能。

Method: 使用一维卷积层层次结构从原始输入序列中提取显著局部模式，降噪降维后输入TFT，通过多头注意力捕获短长期依赖并自适应加权相关协变量。

Result: 在自然流量时间序列数据集上，CNN-TFT优于现有深度学习模型，平均绝对百分比误差达2.2%。

Conclusion: CNN-TFT-SHAP-MHAW架构对于需要高保真多变量时间序列预测的应用具有前景，通过SHAP-MHAW提供模型可解释性。

Abstract: Convolutional neural networks (CNNs) and transformer architectures offer
strengths for modeling temporal data: CNNs excel at capturing local patterns
and translational invariances, while transformers effectively model long-range
dependencies via self-attention. This paper proposes a hybrid architecture
integrating convolutional feature extraction with a temporal fusion transformer
(TFT) backbone to enhance multivariate time series forecasting. The CNN module
first applies a hierarchy of one-dimensional convolutional layers to distill
salient local patterns from raw input sequences, reducing noise and
dimensionality. The resulting feature maps are then fed into the TFT, which
applies multi-head attention to capture both short- and long-term dependencies
and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a
hydroelectric natural flow time series dataset. Experimental results
demonstrate that CNN-TFT outperforms well-established deep learning models,
with a mean absolute percentage error of up to 2.2%. The explainability of the
model is obtained by a proposed Shapley additive explanations with multi-head
attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,
is promising for applications requiring high-fidelity, multivariate time series
forecasts, being available for future analysis at
https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .

</details>


### [173] [Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis](https://arxiv.org/abs/2510.06852)
*Zuherman Rustam,Sri Hartini,Sardar M. N. Islam,Fevi Novkaniza,Fiftitah R. Aszhari,Muhammad Rifqi*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法（逻辑回归、随机森林、支持向量机）预测银行破产概率，相比传统统计方法具有更高准确性，随机森林对商业银行的预测准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法（如Altman Z-Score）依赖严格假设，预测精度低，需要更准确有效的银行破产预测方法来维护金融系统稳定。

Method: 使用逻辑回归、随机森林和支持向量机三种机器学习技术，分别分析土耳其44家活跃银行和21家破产银行的年度财务数据（1994-2004），以及印尼43家活跃和43家破产农村银行的季度财务数据（2013-2019）。

Result: 随机森林对商业银行数据的预测准确率达到90%，三种机器学习方法都能准确预测农村银行的破产可能性。

Conclusion: 提出的创新机器学习方法有助于实施降低破产成本的政策，为银行风险管理提供更有效的工具。

Abstract: Context: Financial system stability is determined by the condition of the
banking system. A bank failure can destroy the stability of the financial
system, as banks are subject to systemic risk, affecting not only individual
banks but also segments or the entire financial system. Calculating the
probability of a bank going bankrupt is one way to ensure the banking system is
safe and sound. Existing literature and limitations: Statistical models, such
as Altman's Z-Score, are one of the common techniques for developing a
bankruptcy prediction model. However, statistical methods rely on rigid and
sometimes irrelevant assumptions, which can result in low forecast accuracy.
New approaches are necessary. Objective of the research: Bankruptcy models are
developed using machine learning techniques, such as logistic regression (LR),
random forest (RF), and support vector machines (SVM). According to several
studies, machine learning is also more accurate and effective than statistical
methods for categorising and forecasting banking risk management. Present
Research: The commercial bank data are derived from the annual financial
statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to
2004, and the rural bank data are derived from the quarterly financial reports
of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.
Five rural banks in Indonesia have also been selected to demonstrate the
feasibility of analysing bank bankruptcy trends. Findings and implications: The
results of the research experiments show that RF can forecast data from
commercial banks with a 90% accuracy rate. Furthermore, the three machine
learning methods proposed accurately predict the likelihood of rural bank
bankruptcy. Contribution and Conclusion: The proposed innovative machine
learning approach help to implement policies that reduce the costs of
bankruptcy.

</details>


### [174] [Towards Generalization of Graph Neural Networks for AC Optimal Power Flow](https://arxiv.org/abs/2510.06860)
*Olayiwola Arowolo,Jochen L. Cremer*

Main category: cs.LG

TL;DR: 提出混合异构消息传递神经网络(HH-MPNN)解决AC最优潮流问题，实现跨电网规模的扩展性和拓扑适应性，相比传统求解器获得1000-10000倍计算加速


<details>
  <summary>Details</summary>
Motivation: 传统AC最优潮流求解器计算成本高，机器学习方法难以扩展到不同电网规模和适应拓扑变化而无需昂贵重新训练

Method: 将母线、发电机、负荷、并联设备、输电线和变压器建模为不同节点或边类型，结合可扩展的变压器模型处理长程依赖关系

Result: 在14-2000节点电网上，默认拓扑下最优性差距小于1%；零样本应用于数千个未见拓扑时，最优性差距小于3%；小电网预训练可改善大电网结果

Conclusion: HH-MPNN推进了实用、可泛化的机器学习在实时电力系统运行中的应用

Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale
power systems, with conventional solvers requiring prohibitive solution times.
Machine learning approaches offer computational speedups but struggle with
scalability and topology adaptability without expensive retraining. To enable
scalability across grid sizes and adaptability to topology changes, we propose
a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models
buses, generators, loads, shunts, transmission lines and transformers as
distinct node or edge types, combined with a scalable transformer model for
handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN
achieves less than 1% optimality gap on default topologies. Applied zero-shot
to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap
despite training only on default topologies. Pre-training on smaller grids also
improves results on a larger grid. Computational speedups reach 1,000x to
10,000x compared to interior point solvers. These results advance practical,
generalizable machine learning for real-time power system operations.

</details>


### [175] [SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](https://arxiv.org/abs/2510.06871)
*Huahui Yi,Kun Wang,Qiankun Li,Miao Yu,Liang Lin,Gongli Xi,Hao Wu,Xuming Hu,Kang Li,Yang Liu*

Main category: cs.LG

TL;DR: 提出SaFeR-VLM框架，通过安全对齐的强化学习将安全约束直接嵌入多模态推理过程，解决了MLRMs在对抗性或不安全提示下放大安全风险的"推理税"问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大推理模型在跨模态推理方面表现出色，但在对抗性或不安全提示下会放大安全风险，现有防御主要在输出层面，未能约束推理过程，存在隐性风险。

Method: 提出安全对齐的强化学习框架，包含四个组件：QI-Safe-10K数据集、安全感知的生成过程、结构化奖励建模和GRPO优化，将安全从被动保护转变为主动推理驱动因素。

Result: SaFeR-VLM-3B在六个基准测试中安全性和帮助性平均得分分别为70.13和78.97，超越同规模和更大模型；SaFeR-VLM-7B在安全指标上超越GPT-5-mini和Gemini-2.5-Flash，且不降低帮助性性能。

Conclusion: SaFeR-VLM框架将安全直接嵌入推理过程，实现了可扩展和泛化的安全感知推理，对显性和隐性风险都具有鲁棒性，支持超越表面过滤的动态和可解释安全决策。

Abstract: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.

</details>


### [176] [MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2510.06880)
*Zhiyu Wang,Sonia Koszut,Pietro Liò,Francesco Ceccarelli*

Main category: cs.LG

TL;DR: MoRE-GNN是一个用于多组学单细胞数据整合的异构图自编码器，通过图卷积和注意力机制动态构建关系图，在强跨模态相关性场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 多组学单细胞数据整合面临高维度和复杂跨模态关系的挑战，需要开发能够动态捕捉生物意义关系的方法。

Method: 提出MoRE-GNN（多组学关系边图神经网络），结合图卷积和注意力机制，直接从数据中动态构建关系图。

Result: 在六个公开数据集上的评估显示，MoRE-GNN能够捕捉生物学意义关系，在强跨模态相关性设置下优于现有方法，学习到的表征支持准确的跨模态预测。

Conclusion: MoRE-GNN提供了一个自适应、可扩展且可解释的多组学整合框架，尽管性能可能随数据集复杂度变化。

Abstract: The integration of multi-omics single-cell data remains challenging due to
high-dimensionality and complex inter-modality relationships. To address this,
we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a
heterogeneous graph autoencoder that combines graph convolution and attention
mechanisms to dynamically construct relational graphs directly from data.
Evaluations on six publicly available datasets demonstrate that MoRE-GNN
captures biologically meaningful relationships and outperforms existing
methods, particularly in settings with strong inter-modality correlations.
Furthermore, the learned representations allow for accurate downstream
cross-modal predictions. While performance may vary with dataset complexity,
MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing
multi-omics integration.

</details>


### [177] [Angular Constraint Embedding via SpherePair Loss for Constrained Clustering](https://arxiv.org/abs/2510.06907)
*Shaojie Zhang,Ke Chen*

Main category: cs.LG

TL;DR: 提出SpherePair方法，通过角度约束嵌入解决深度约束聚类问题，避免现有方法的锚点限制和欧几里得嵌入问题，实现表示学习与聚类的有效分离。


<details>
  <summary>Details</summary>
Motivation: 现有深度约束聚类方法存在锚点限制或难以学习判别性欧几里得嵌入的问题，限制了可扩展性和实际应用。

Method: 使用SpherePair损失函数和几何公式，在角度空间中编码成对约束，生成聚类友好的嵌入表示。

Result: 在多样化基准测试中优于现有深度约束聚类方法，具有更好的性能、可扩展性和实际有效性。

Conclusion: SpherePair方法通过角度约束嵌入有效解决了深度约束聚类问题，提供了理论保证并在实际应用中表现出色。

Abstract: Constrained clustering integrates domain knowledge through pairwise
constraints. However, existing deep constrained clustering (DCC) methods are
either limited by anchors inherent in end-to-end modeling or struggle with
learning discriminative Euclidean embedding, restricting their scalability and
real-world applicability. To avoid their respective pitfalls, we propose a
novel angular constraint embedding approach for DCC, termed SpherePair. Using
the SpherePair loss with a geometric formulation, our method faithfully encodes
pairwise constraints and leads to embeddings that are clustering-friendly in
angular space, effectively separating representation learning from clustering.
SpherePair preserves pairwise relations without conflict, removes the need to
specify the exact number of clusters, generalizes to unseen data, enables rapid
inference of the number of clusters, and is supported by rigorous theoretical
guarantees. Comparative evaluations with state-of-the-art DCC methods on
diverse benchmarks, along with empirical validation of theoretical insights,
confirm its superior performance, scalability, and overall real-world
effectiveness. Code is available at
\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.

</details>


### [178] [Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series](https://arxiv.org/abs/2510.06910)
*Iago Xabier Vázquez,Javier Sedano,Muhammad Afzal,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出Vacuum Spiker算法，一种基于脉冲神经网络的时序异常检测方法，通过全局神经活动变化检测异常，显著降低能耗，在资源受限环境中具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在异常检测中性能优异但能耗高，限制了在物联网设备、边缘计算平台等资源受限环境中的部署。

Method: 使用脉冲神经网络，提出新的检测标准基于神经活动全局变化而非重构或预测误差；采用脉冲时序依赖可塑性训练；设计高效编码方案将输入空间离散化为非重叠区间，每个时间步仅产生单个脉冲。

Result: 在公开数据集上实验表明，该算法在保持竞争力的性能同时显著降低能耗，优于多种深度学习和机器学习基线方法；在真实案例中成功识别太阳能逆变器的功率削减事件。

Conclusion: Vacuum Spiker算法展示了可持续和高效异常检测的潜力，特别适合资源受限环境应用。

Abstract: Anomaly detection is a key task across domains such as industry, healthcare,
and cybersecurity. Many real-world anomaly detection problems involve analyzing
multiple features over time, making time series analysis a natural approach for
such problems. While deep learning models have achieved strong performance in
this field, their trend to exhibit high energy consumption limits their
deployment in resource-constrained environments such as IoT devices, edge
computing platforms, and wearables. To address this challenge, this paper
introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural
Network-based method for anomaly detection in time series. It incorporates a
new detection criterion that relies on global changes in neural activity rather
than reconstruction or prediction error. It is trained using Spike
Time-Dependent Plasticity in a novel way, intended to induce changes in neural
activity when anomalies occur. A new efficient encoding scheme is also
proposed, which discretizes the input space into non-overlapping intervals,
assigning each to a single neuron. This strategy encodes information with a
single spike per time step, improving energy efficiency compared to
conventional encoding methods. Experimental results on publicly available
datasets show that the proposed algorithm achieves competitive performance
while significantly reducing energy consumption, compared to a wide set of deep
learning and machine learning baselines. Furthermore, its practical utility is
validated in a real-world case study, where the model successfully identifies
power curtailment events in a solar inverter. These results highlight its
potential for sustainable and efficient anomaly detection.

</details>


### [179] [Utilizing Large Language Models for Machine Learning Explainability](https://arxiv.org/abs/2510.06912)
*Alexandros Vassiliades,Nikolaos Polatidis,Stamatios Samaras,Sotiris Diplaris,Ignacio Cabrera Martin,Yannis Manolopoulos,Stefanos Vrochidis,Ioannis Kompatsiaris*

Main category: cs.LG

TL;DR: 评估大语言模型在自动生成可解释机器学习解决方案方面的能力，通过二元和多标签分类任务测试GPT、Claude和DeepSeek三个模型生成的四种分类器的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在自主生成机器学习解决方案时的可解释性能力，验证其作为自动化可解释ML管道生成工具的潜力。

Method: 使用三个先进LLM（GPT、Claude、DeepSeek）为四种常见分类器（随机森林、XGBoost、多层感知机、LSTM）设计训练管道，通过SHAP评估模型预测性能和可解释性指标（保真度和稀疏性）。

Result: LLM能够生成有效且可解释的模型，获得高保真度和一致的稀疏性，与人工设计的基线模型表现相当。

Conclusion: 大语言模型有潜力作为自动化工具生成可解释的机器学习管道，能够产生性能良好且易于解释的解决方案。

Abstract: This study explores the explainability capabilities of large language models
(LLMs), when employed to autonomously generate machine learning (ML) solutions.
We examine two classification tasks: (i) a binary classification problem
focused on predicting driver alertness states, and (ii) a multilabel
classification problem based on the yeast dataset. Three state-of-the-art LLMs
(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design
training pipelines for four common classifiers: Random Forest, XGBoost,
Multilayer Perceptron, and Long Short-Term Memory networks. The generated
models are evaluated in terms of predictive performance (recall, precision, and
F1-score) and explainability using SHAP (SHapley Additive exPlanations).
Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP
approximations and model outputs) and Average SHAP Sparsity (number of features
deemed influential). The results reveal that LLMs are capable of producing
effective and interpretable models, achieving high fidelity and consistent
sparsity, highlighting their potential as automated tools for interpretable ML
pipeline generation. The results show that LLMs can produce effective,
interpretable pipelines with high fidelity and consistent sparsity, closely
matching manually engineered baselines.

</details>


### [180] [DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning](https://arxiv.org/abs/2510.06913)
*Ke Guo,Haochen Liu,Xiaojun Wu,Chen Lv*

Main category: cs.LG

TL;DR: 提出了DecompGAIL方法来解决多智能体交通仿真中的不稳定问题，通过分解真实感为ego-map和ego-neighbor组件，过滤误导性交互，并在SMART骨架上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法难以建模真实交通行为：行为克隆存在协变量偏移问题，而生成对抗模仿学习(GAIL)在多智能体设置中极不稳定，主要原因是无关交互误导。

Method: 提出分解多智能体GAIL(DecompGAIL)，将真实感分解为ego-map和ego-neighbor组件，过滤邻居-邻居和邻居-地图的误导性交互，并引入社会PPO目标，用距离加权邻居奖励增强ego奖励。

Result: 在轻量级SMART骨架上集成DecompGAIL，在WOMD Sim Agents 2025基准测试中达到了最先进的性能。

Conclusion: DecompGAIL通过分解真实感和过滤误导性交互，有效解决了多智能体交通仿真中的稳定性问题，实现了更真实的交通行为建模。

Abstract: Realistic traffic simulation is critical for the development of autonomous
driving systems and urban mobility planning, yet existing imitation learning
approaches often fail to model realistic traffic behaviors. Behavior cloning
suffers from covariate shift, while Generative Adversarial Imitation Learning
(GAIL) is notoriously unstable in multi-agent settings. We identify a key
source of this instability: irrelevant interaction misguidance, where a
discriminator penalizes an ego vehicle's realistic behavior due to unrealistic
interactions among its neighbors. To address this, we propose Decomposed
Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map
and ego-neighbor components, filtering out misleading neighbor: neighbor and
neighbor: map interactions. We further introduce a social PPO objective that
augments ego rewards with distance-weighted neighborhood rewards, encouraging
overall realism across agents. Integrated into a lightweight SMART-based
backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim
Agents 2025 benchmark.

</details>


### [181] [Revisiting Node Affinity Prediction in Temporal Graphs](https://arxiv.org/abs/2510.06940)
*Krishna Sri Ipsit Mantri,Or Feldman,Moshe Eliasof,Chaim Baskin*

Main category: cs.LG

TL;DR: 提出了NAViS模型用于节点亲和性预测，通过虚拟状态和状态空间模型解决现有方法的问题，在TGB数据集上超越了现有最优方法包括启发式方法


<details>
  <summary>Details</summary>
Motivation: 当前最先进的动态链接属性预测模型在节点亲和性预测任务上表现不佳，简单的启发式方法反而表现更好，需要分析训练挑战并提供解决方案

Method: 开发NAViS模型，利用启发式方法与状态空间模型的等价性，引入虚拟状态概念，并提出新的损失函数来解决训练困难

Result: 在TGB数据集上的评估显示，NAViS超越了现有最优方法，包括启发式方法

Conclusion: NAViS通过虚拟状态和新损失函数有效解决了节点亲和性预测问题，在多个应用领域具有潜力

Abstract: Node affinity prediction is a common task that is widely used in temporal
graph learning with applications in social and financial networks, recommender
systems, and more. Recent works have addressed this task by adapting
state-of-the-art dynamic link property prediction models to node affinity
prediction. However, simple heuristics, such as Persistent Forecast or Moving
Average, outperform these models. In this work, we analyze the challenges in
training current Temporal Graph Neural Networks for node affinity prediction
and suggest appropriate solutions. Combining the solutions, we develop NAViS -
Node Affinity prediction model using Virtual State, by exploiting the
equivalence between heuristics and state space models. While promising,
training NAViS is non-trivial. Therefore, we further introduce a novel loss
function for node affinity prediction. We evaluate NAViS on TGB and show that
it outperforms the state-of-the-art, including heuristics. Our source code is
available at https://github.com/orfeld415/NAVIS

</details>


### [182] [Fisher Information, Training and Bias in Fourier Regression Models](https://arxiv.org/abs/2510.06945)
*Lorenzo Pastori,Veronika Eyring,Mierk Schwabe*

Main category: cs.LG

TL;DR: 本文研究了量子神经网络中基于Fisher信息矩阵的评估指标如何预测训练和预测性能，探讨了有效维度和模型偏置对训练效果的影响。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习特别是量子神经网络兴趣的增长，需要有效的评估指标来预测其训练和预测性能。

Method: 利用QNN与傅里叶模型的等价性，推导傅里叶模型的FIM解析表达式，构建可调有效维度和偏置的模型进行比较。

Result: 对于无偏模型，高有效维度有利于训练和性能；对于有偏模型，低有效维度在训练中更有益。

Conclusion: 研究揭示了几何特性、模型-任务对齐和训练之间的相互作用，对更广泛的机器学习社区具有参考价值。

Abstract: Motivated by the growing interest in quantum machine learning, in particular
quantum neural networks (QNNs), we study how recently introduced evaluation
metrics based on the Fisher information matrix (FIM) are effective for
predicting their training and prediction performance. We exploit the
equivalence between a broad class of QNNs and Fourier models, and study the
interplay between the \emph{effective dimension} and the \emph{bias} of a model
towards a given task, investigating how these affect the model's training and
performance. We show that for a model that is completely agnostic, or unbiased,
towards the function to be learned, a higher effective dimension likely results
in a better trainability and performance. On the other hand, for models that
are biased towards the function to be learned a lower effective dimension is
likely beneficial during training. To obtain these results, we derive an
analytical expression of the FIM for Fourier models and identify the features
controlling a model's effective dimension. This allows us to construct models
with tunable effective dimension and bias, and to compare their training. We
furthermore introduce a tensor network representation of the considered Fourier
models, which could be a tool of independent interest for the analysis of QNN
models. Overall, these findings provide an explicit example of the interplay
between geometrical properties, model-task alignment and training, which are
relevant for the broader machine learning community.

</details>


### [183] [Grouped Differential Attention](https://arxiv.org/abs/2510.06949)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Wai Ting Cheung,Beomgyu Kim,Taehwan Kim,Haesol Lee,Junhyeok Lee,Dongpin Oh,Eunhwan Park*

Main category: cs.LG

TL;DR: 提出了分组差分注意力(GDA)，通过非平衡的头部分配策略，将更多注意力头分配给信号提取组，较少分配给噪声控制组，从而在最小计算开销下增强信号保真度和模型可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制中存在的效率问题，即经常将大量注意力分配给冗余或嘈杂的上下文。之前的差分注意力方法由于需要平衡的头部分配，限制了表示灵活性和可扩展性。

Method: 引入非平衡头部分配策略，将注意力头分为信号保留组和噪声控制组，为信号提取分配更多头部，为噪声控制分配较少头部，并通过受控重复来稳定噪声控制组。还提出了组差异化增长策略，选择性地复制信号聚焦头部。

Result: 大规模预训练和持续训练实验表明，GDA中的适度不平衡比率相比对称基线在泛化性和稳定性方面带来显著改进。

Conclusion: 比率感知的头部分配和选择性扩展为设计可扩展、计算高效的Transformer架构提供了有效且实用的路径。

Abstract: The self-attention mechanism, while foundational to modern Transformer
architectures, suffers from a critical inefficiency: it frequently allocates
substantial attention to redundant or noisy context. Differential Attention
addressed this by using subtractive attention maps for signal and noise, but
its required balanced head allocation imposes rigid constraints on
representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel
approach that introduces unbalanced head allocation between signal-preserving
and noise-control groups. GDA significantly enhances signal focus by
strategically assigning more heads to signal extraction and fewer to
noise-control, stabilizing the latter through controlled repetition (akin to
GQA). This design achieves stronger signal fidelity with minimal computational
overhead. We further extend this principle to group-differentiated growth, a
scalable strategy that selectively replicates only the signal-focused heads,
thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we
demonstrate that moderate imbalance ratios in GDA yield substantial
improvements in generalization and stability compared to symmetric baselines.
Our results collectively establish that ratio-aware head allocation and
selective expansion offer an effective and practical path toward designing
scalable, computation-efficient Transformer architectures.

</details>


### [184] [From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics](https://arxiv.org/abs/2510.06954)
*Zheng-An Chen,Tao Luo*

Main category: cs.LG

TL;DR: 本文使用梯度流分析框架研究线性化Transformer的训练动态，发现注意力模块的训练分为两个阶段：第一阶段权重扰动帮助逃离小初始化区域，第二阶段键查询矩阵参与训练导致秩塌缩。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型表现出优异的实证性能，但其训练动态的基本原理在特定配置研究之外缺乏充分表征。受语言模型在小初始化尺度下推理能力提升的实证证据启发，需要系统研究Transformer训练动态。

Method: 采用[Zhou et al. NeurIPS 2022]建立的梯度流分析框架，系统研究线性化Transformer的训练动态，理论分析将注意力模块动态分解为两个阶段。

Result: 第一阶段：随机初始化的不对称权重扰动维持参数矩阵中的非退化梯度动态，促进系统逃离小初始化区域；第二阶段：先前静态的键查询矩阵积极参与训练，驱动归一化矩阵趋向渐近秩塌缩。

Conclusion: 两阶段框架推广了经典的方向收敛结果，为理解Transformer训练动态提供了理论框架。

Abstract: Although transformer-based models have shown exceptional empirical
performance, the fundamental principles governing their training dynamics are
inadequately characterized beyond configuration-specific studies. Inspired by
empirical evidence showing improved reasoning capabilities under small
initialization scales in language models, we employ the gradient flow
analytical framework established in [Zhou et al. NeurIPS 2022] to
systematically investigate linearized Transformer training dynamics. Our
theoretical analysis dissects the dynamics of attention modules into two
distinct stages. In the first stage, asymmetric weight perturbations from
random initialization sustain non-degenerate gradient dynamics in parameter
matrices, facilitating systematic escape from small initialization regimes.
Subsequently, these matrices undergo condensation, progressively aligning
toward the target orientation. In the second stage, the previously static
key-query matrices actively participate in training, driving the normalized
matrices toward asymptotic rank collapse. This two-stage framework generalizes
classical directional convergence results.

</details>


### [185] [High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization](https://arxiv.org/abs/2510.06955)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: Mixout是一种替代Dropout的随机正则化技术，通过在训练中概率性地将微调权重与预训练权重交换来平衡适应性和先验知识保留，在领域泛化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 集成多个微调模型能提高分布偏移下的鲁棒性，但计算成本高；Dropout会破坏预训练模型的关键表示。需要一种轻量级方法在保持泛化能力的同时降低计算成本。

Method: 使用Mixout技术，以高掩码概率（ViT为0.9，ResNet为0.8）在训练过程中随机将微调权重替换为预训练权重，惩罚对预训练参数的偏离。

Result: 在五个领域泛化基准测试中，高掩码率Mixout达到了与集成方法相当的域外准确率，同时显著降低训练成本：梯度计算减少45%，梯度内存使用减少90%。

Conclusion: 高掩码率Mixout是一种有效的领域泛化方法，在保持性能的同时大幅降低计算开销，为实际应用提供了可行的轻量级解决方案。

Abstract: Ensembling fine-tuned models initialized from powerful pre-trained weights is
a common strategy to improve robustness under distribution shifts, but it comes
with substantial computational costs due to the need to train and store
multiple models. Dropout offers a lightweight alternative by simulating
ensembles through random neuron deactivation; however, when applied to
pre-trained models, it tends to over-regularize and disrupt critical
representations necessary for generalization. In this work, we investigate
Mixout, a stochastic regularization technique that provides an alternative to
Dropout for domain generalization. Rather than deactivating neurons, Mixout
mitigates overfitting by probabilistically swapping a subset of fine-tuned
weights with their pre-trained counterparts during training, thereby
maintaining a balance between adaptation and retention of prior knowledge. Our
study reveals that achieving strong performance with Mixout on domain
generalization benchmarks requires a notably high masking probability of 0.9
for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it
yields two key advantages for domain generalization: (1) higher masking rates
more strongly penalize deviations from the pre-trained parameters, promoting
better generalization to unseen domains; and (2) high-rate masking
substantially reduces computational overhead, cutting gradient computation by
up to 45% and gradient memory usage by up to 90%. Experiments across five
domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and
DomainNet, using ResNet and ViT architectures, show that our approach,
High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based
methods while significantly reducing training costs.

</details>


### [186] [Revisiting Mixout: An Overlooked Path to Robust Finetuning](https://arxiv.org/abs/2510.06982)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: GMixout是一种改进的随机正则化方法，通过动态权重锚点和显式重采样频率调节，在微调视觉基础模型时同时提升域内精度和分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法虽然能提高域内精度，但会降低模型在分布偏移下的鲁棒性。Mixout正则化器虽然能缓解这个问题，但其固定权重锚点和隐式重采样机制限制了性能。

Method: 提出GMixout方法：(i) 用指数移动平均快照替代固定锚点，使其在训练中自适应；(ii) 通过显式重采样频率超参数调节掩码周期；(iii) 稀疏核实现仅更新少量参数，无推理开销。

Result: 在ImageNet、DomainNet、iWildCam和CIFAR100-C等基准测试中，GMixout在保持域内精度提升的同时，在协变量偏移、损坏和类别不平衡等分布偏移情况下，超越了Model Soups和参数高效微调基线方法。

Conclusion: GMixout通过动态权重锚点和可控重采样机制，有效平衡了微调模型的域内精度和分布偏移鲁棒性，且计算效率高，适合消费级GPU训练。

Abstract: Finetuning vision foundation models often improves in-domain accuracy but
comes at the cost of robustness under distribution shift. We revisit Mixout, a
stochastic regularizer that intermittently replaces finetuned weights with
their pretrained reference, through the lens of a single-run, weight-sharing
implicit ensemble. This perspective reveals three key levers that govern
robustness: the \emph{masking anchor}, \emph{resampling frequency}, and
\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)
replaces the fixed anchor with an exponential moving-average snapshot that
adapts during training, and (ii) regulates masking period via an explicit
resampling-frequency hyperparameter. Our sparse-kernel implementation updates
only a small fraction of parameters with no inference-time overhead, enabling
training on consumer-grade GPUs. Experiments on benchmarks covering covariate
shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,
iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy
beyond zero-shot performance while surpassing both Model Soups and strong
parameter-efficient finetuning baselines under distribution shift.

</details>


### [187] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: 本文提出了一种新的螺旋技术，用于将数据科学生命周期应用于具有明确最终目标的业务问题，强调多功能性、敏捷性和迭代方法。


<details>
  <summary>Details</summary>
Motivation: 现代企业中分析技术的重要性日益增长，公司采用数据科学生命周期来提高生产力和竞争力。传统的数据科学生命周期通常是线性或循环模型，但本文认为对于有明确最终目标的业务问题，需要更灵活的方法。

Method: 提出了一种新的螺旋技术，将数据科学生命周期整合到业务问题中，强调多功能性、敏捷性和迭代方法。

Result: 螺旋技术为具有明确最终目标的业务问题提供了更有效的解决方案，相比传统线性或循环模型更具适应性和灵活性。

Conclusion: 螺旋技术为数据科学在业务应用中的生命周期管理提供了创新方法，能够更好地满足现代企业对敏捷性和迭代性的需求。

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [188] [Sharpness-Aware Data Generation for Zero-shot Quantization](https://arxiv.org/abs/2510.07018)
*Dung Hoang-Anh,Cuong Pham Trung Le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出一种新的零样本量化方法，通过考虑量化模型的锐度来生成合成数据，从而提高泛化能力。该方法通过最大化合成数据和真实验证数据之间的梯度匹配来实现锐度最小化。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本量化方法在生成合成数据时没有考虑量化模型的锐度，而低锐度的深度神经网络具有更好的泛化能力。

Method: 首先证明锐度最小化可以通过最大化合成数据和真实验证数据之间的重建损失梯度匹配来实现，然后通过近似每个生成样本与其邻居之间的梯度匹配来避免需要真实验证集的问题。

Result: 在CIFAR-100和ImageNet数据集上的实验评估表明，该方法在低位量化设置下优于现有最先进技术。

Conclusion: 提出的方法通过考虑量化模型锐度来生成合成数据，有效提高了零样本量化的性能，在低位量化场景中表现出色。

Abstract: Zero-shot quantization aims to learn a quantized model from a pre-trained
full-precision model with no access to original real training data. The common
idea in zero-shot quantization approaches is to generate synthetic data for
quantizing the full-precision model. While it is well-known that deep neural
networks with low sharpness have better generalization ability, none of the
previous zero-shot quantization works considers the sharpness of the quantized
model as a criterion for generating training data. This paper introduces a
novel methodology that takes into account quantized model sharpness in
synthetic data generation to enhance generalization. Specifically, we first
demonstrate that sharpness minimization can be attained by maximizing gradient
matching between the reconstruction loss gradients computed on synthetic and
real validation data, under certain assumptions. We then circumvent the problem
of the gradient matching without real validation set by approximating it with
the gradient matching between each generated sample and its neighbors.
Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the
superiority of the proposed method over the state-of-the-art techniques in
low-bit quantization settings.

</details>


### [189] [Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy](https://arxiv.org/abs/2510.07022)
*ZiHeng Huang,Di Wu,Jun Bai,Jiale Zhang,Sicong Cao,Ji Zhang,Yingjie Hu*

Main category: cs.LG

TL;DR: 本文提出了FedCCCU方法，解决联邦学习中机器遗忘的两个关键挑战：公平性和现实数据异质性，在真实场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的机器遗忘面临两大挑战：公平性问题（所有客户端被迫参与代价高昂的重新训练）和评估不现实（依赖IID/非IID合成数据假设而忽略真实世界异质性）。

Method: 首先对现有联邦遗忘方法在真实数据异质性和公平性条件下进行全面基准测试，然后提出新颖的公平感知联邦遗忘方法FedCCCU，通过跨客户端约束明确解决这两个挑战。

Result: 实验结果表明，现有方法在真实场景下表现不佳，而FedCCCU方法始终优于现有方法。

Conclusion: FedCCCU为现实世界的联邦遗忘提供了一个实用且可扩展的解决方案，解决了公平性和数据异质性的核心挑战。

Abstract: Machine unlearning is critical for enforcing data deletion rights like the
"right to be forgotten." As a decentralized paradigm, Federated Learning (FL)
also requires unlearning, but realistic implementations face two major
challenges. First, fairness in Federated Unlearning (FU) is often overlooked.
Exact unlearning methods typically force all clients into costly retraining,
even those uninvolved. Approximate approaches, using gradient ascent or
distillation, make coarse interventions that can unfairly degrade performance
for clients with only retained data. Second, most FU evaluations rely on
synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.
These unrealistic benchmarks obscure the true impact of unlearning and limit
the applicability of current methods. We first conduct a comprehensive
benchmark of existing FU methods under realistic data heterogeneity and
fairness conditions. We then propose a novel, fairness-aware FU approach,
Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address
both challenges. FedCCCU offers a practical and scalable solution for
real-world FU. Experimental results show that existing methods perform poorly
in realistic settings, while our approach consistently outperforms them.

</details>


### [190] [Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration](https://arxiv.org/abs/2510.07035)
*Tengwei Song,Min Wu,Yuan Fang*

Main category: cs.LG

TL;DR: FlexMol是一个灵活的分子预训练框架，能够学习统一的分子表示并支持单模态输入，解决了现有方法需要配对2D和3D数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习方法需要配对的2D和3D分子数据来有效训练模型，这在某些模态不可用或计算成本高昂的情况下存在限制。

Method: 采用分离的2D和3D分子数据模型，通过参数共享提高计算效率，并使用解码器为缺失模态生成特征，实现多阶段连续学习过程。

Result: 在广泛的分子性质预测任务中表现出优越性能，并在不完整数据情况下验证了其有效性。

Conclusion: FlexMol框架能够灵活处理单模态输入，在分子表示学习方面取得了显著进展。

Abstract: Molecular representation learning plays a crucial role in advancing
applications such as drug discovery and material design. Existing work
leverages 2D and 3D modalities of molecular information for pre-training,
aiming to capture comprehensive structural and geometric insights. However,
these methods require paired 2D and 3D molecular data to train the model
effectively and prevent it from collapsing into a single modality, posing
limitations in scenarios where a certain modality is unavailable or
computationally expensive to generate. To overcome this limitation, we propose
FlexMol, a flexible molecule pre-training framework that learns unified
molecular representations while supporting single-modality input. Specifically,
inspired by the unified structure in vision-language models, our approach
employs separate models for 2D and 3D molecular data, leverages parameter
sharing to improve computational efficiency, and utilizes a decoder to generate
features for the missing modality. This enables a multistage continuous
learning process where both modalities contribute collaboratively during
training, while ensuring robustness when only one modality is available during
inference. Extensive experiments demonstrate that FlexMol achieves superior
performance across a wide range of molecular property prediction tasks, and we
also empirically demonstrate its effectiveness with incomplete data. Our code
and data are available at https://github.com/tewiSong/FlexMol.

</details>


### [191] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: COMPASS是一个评估LLM智能体在真实旅行规划场景中表现的基准，将旅行规划建模为约束偏好优化问题，需要满足硬约束同时优化软用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM智能体需要通过多轮交互掌握策略性工具使用和用户偏好优化，以协助用户完成复杂规划任务。

Method: 构建了覆盖20个美国国家公园的交通、住宿和票务的真实旅行数据库，以及模拟商业预订平台的综合工具生态系统。

Result: 评估发现两个关键差距：(i)可接受-最优差距：智能体可靠满足约束但未能优化偏好；(ii)规划协调差距：在多服务协调任务上性能崩溃，特别是开源模型。

Conclusion: COMPASS通过在实际用户面向领域中进行推理和规划，提供了一个直接衡量智能体在真实任务中优化用户偏好能力的基准，连接理论进展与现实影响。

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [192] [Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation](https://arxiv.org/abs/2510.07052)
*Aryan Golbaghi,Shuo Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合预训练表示和自动超参数优化的语音情感识别工作流程，在普通CPU上实现了高效的性能优化。


<details>
  <summary>Details</summary>
Motivation: 解决语音情感识别中传统方法（如网格搜索）耗时过长且效率低下的问题，探索在有限计算资源下实现高效超参数优化的可能性。

Method: 使用SpeechBrain wav2vec2-base模型作为编码器，在IEMOCAP上微调，比较高斯过程贝叶斯优化(GP-BO)和树结构Parzen估计器(TPE)两种超参数优化策略，在相同的四维搜索空间和15次试验预算下进行对比。

Result: GP-BO在11分钟内达到0.96平衡分类准确率，TPE在15分钟内达到0.97，而网格搜索需要143次试验和1680分钟才能超过0.9。跨语言泛化实验显示，EmoDB训练的HPO调优模型在CREMA-D和RAVDESS上的零样本准确率分别提高了0.25和0.26。

Conclusion: 使用预训练编码器的高效超参数优化可以在普通CPU上实现具有竞争力的语音情感识别性能，显著优于传统方法和现有基准。

Abstract: We propose a workflow for speech emotion recognition (SER) that combines
pre-trained representations with automated hyperparameter optimisation (HPO).
Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we
compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and
Tree-structured Parzen Estimators (TPE), under an identical four-dimensional
search space and 15-trial budget, with balanced class accuracy (BCA) on the
German EmoDB corpus as the objective. All experiments run on 8 CPU cores with
32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt
implementation) attains 0.97 in 15 minutes. In contrast, grid search requires
143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020
baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual
generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by
0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with
pre-trained encoders delivers competitive SER on commodity CPUs. Source code to
this work is available at:
https://github.com/youngaryan/speechbrain-emotion-hpo.

</details>


### [193] [Introspection in Learned Semantic Scene Graph Localisation](https://arxiv.org/abs/2510.07053)
*Manshika Charvi Bissessur,Efimia Panagiotaki,Daniele De Martini*

Main category: cs.LG

TL;DR: 该研究探讨了语义信息如何影响自监督对比语义定位框架中的定位性能和鲁棒性，通过可解释性分析验证模型是否过滤环境噪声并优先识别显著地标。


<details>
  <summary>Details</summary>
Motivation: 研究语义在自监督定位中的作用，探究模型是否能够过滤环境噪声并识别有区分度的地标，从而提高定位的鲁棒性和可解释性。

Method: 训练定位网络于原始和扰动地图上，进行事后内省分析，验证多种可解释性方法，并进行语义类别消融实验。

Result: 积分梯度和注意力权重被证明是最可靠的学习行为探针，语义消融显示频繁对象往往被降权，模型学习到了噪声鲁棒的语义显著关系。

Conclusion: 模型能够学习噪声鲁棒的语义显著关系，在具有挑战性的视觉和结构变化下实现可解释的配准。

Abstract: This work investigates how semantics influence localisation performance and
robustness in a learned self-supervised, contrastive semantic localisation
framework. After training a localisation network on both original and perturbed
maps, we conduct a thorough post-hoc introspection analysis to probe whether
the model filters environmental noise and prioritises distinctive landmarks
over routine clutter. We validate various interpretability methods and present
a comparative reliability analysis. Integrated gradients and Attention Weights
consistently emerge as the most reliable probes of learned behaviour. A
semantic class ablation further reveals an implicit weighting in which frequent
objects are often down-weighted. Overall, the results indicate that the model
learns noise-robust, semantically salient relations about place definition,
thereby enabling explainable registration under challenging visual and
structural variations.

</details>


### [194] [Blind Construction of Angular Power Maps in Massive MIMO Networks](https://arxiv.org/abs/2510.07071)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 提出基于大规模MIMO网络CSI数据的无监督角度功率地图构建方法，通过隐马尔可夫模型连接移动轨迹与CSI演化，实现无需位置标签的定位和地图构建。


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建需要位置标记的CSI数据，这在实践中难以获取。本文旨在解决无位置标签情况下的大规模MIMO网络角度功率地图构建问题。

Method: 构建隐马尔可夫模型(HMM)，将移动终端的隐藏轨迹与大规模MIMO信道CSI演化相连接，通过CSI数据估计移动位置，从而构建角度功率地图。

Result: 在均匀直线移动和泊松分布基站场景下，定位误差的克拉美罗下界可在任意信噪比下消失；在基站受限区域，即使有无限独立测量，误差仍保持非零。基于真实多小区大规模MIMO网络的RSRP数据，实现了平均18米的定位误差。

Conclusion: 提出的无监督方法能够有效构建角度功率地图，在真实网络环境中实现较高精度的定位，为大规模MIMO网络的无线电资源管理提供了可行解决方案。

Abstract: Channel state information (CSI) acquisition is a challenging problem in
massive multiple-input multiple-output (MIMO) networks. Radio maps provide a
promising solution for radio resource management by reducing online CSI
acquisition. However, conventional approaches for radio map construction
require location-labeled CSI data, which is challenging in practice. This paper
investigates unsupervised angular power map construction based on large
timescale CSI data collected in a massive MIMO network without location labels.
A hidden Markov model (HMM) is built to connect the hidden trajectory of a
mobile with the CSI evolution of a massive MIMO channel. As a result, the
mobile location can be estimated, enabling the construction of an angular power
map. We show that under uniform rectilinear mobility with Poisson-distributed
base stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error
can vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined
to a limited region, the error remains nonzero even with infinite independent
measurements. Based on reference signal received power (RSRP) data collected in
a real multi-cell massive MIMO network, an average localization error of 18
meters can be achieved although measurements are mainly obtained from a single
serving cell.

</details>


### [195] [HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting](https://arxiv.org/abs/2510.07084)
*Tan Wang,Yun Wei Dong,Tao Zhang,Qi Wang*

Main category: cs.LG

TL;DR: 提出HTMformer模型，通过混合时间和多变量嵌入(HTME)增强Transformer在时间序列预测中的特征提取能力，实现精度和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在时间序列预测中过度强调时间依赖性，导致计算开销增加但性能提升有限，性能高度依赖于嵌入方法对有效表示的学习。

Method: 提出HTME提取器，集成轻量级时间特征提取模块和精心设计的多变量特征提取模块，提供互补特征，构建HTMformer模型。

Result: 在8个真实世界数据集上的实验表明，该方法在准确性和效率方面均优于现有基线方法。

Conclusion: HTME能够提取更丰富有效的序列表示，使Transformer预测器能更好地理解时间序列，在模型复杂度和性能之间取得良好平衡。

Abstract: Transformer-based methods have achieved impressive results in time series
forecasting. However, existing Transformers still exhibit limitations in
sequence modeling as they tend to overemphasize temporal dependencies. This
incurs additional computational overhead without yielding corresponding
performance gains. We find that the performance of Transformers is highly
dependent on the embedding method used to learn effective representations. To
address this issue, we extract multivariate features to augment the effective
information captured in the embedding layer, yielding multidimensional
embeddings that convey richer and more meaningful sequence representations.
These representations enable Transformer-based forecasters to better understand
the series. Specifically, we introduce Hybrid Temporal and Multivariate
Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature
extraction module with a carefully designed multivariate feature extraction
module to provide complementary features, thereby achieving a balance between
model complexity and performance. By combining HTME with the Transformer
architecture, we present HTMformer, leveraging the enhanced feature extraction
capability of the HTME extractor to build a lightweight forecaster. Experiments
conducted on eight real-world datasets demonstrate that our approach
outperforms existing baselines in both accuracy and efficiency.

</details>


### [196] [Non-Stationary Online Structured Prediction with Surrogate Losses](https://arxiv.org/abs/2510.07086)
*Shinsaku Sakaue,Han Bao,Yuzhou Cao*

Main category: cs.LG

TL;DR: 本文针对非平稳环境下的在线结构化预测问题，提出了一个形式为F_T + C(1 + P_T)的累积目标损失上界，其中F_T是任意比较器序列的累积代理损失，P_T是其路径长度，C为常数。该界仅通过F_T和P_T依赖于时间T，在非平稳环境中提供更强的保证。


<details>
  <summary>Details</summary>
Motivation: 传统在线结构化预测中的代理遗憾分析在平稳环境下能获得与时间T无关的有限界，但在非平稳环境中，固定估计器的代理损失可能随T线性增长，导致传统保证失效。

Method: 将在线梯度下降(OGD)的动态遗憾界与利用代理间隙的技术相结合，并引入新的Polyak风格学习率。进一步通过卷积Fenchel-Young损失扩展到更广泛的问题类别。

Result: 证明了累积目标损失的上界为F_T + C(1 + P_T)，该界仅通过比较器序列的累积代理损失F_T和路径长度P_T依赖于时间T。同时证明了该界在F_T和P_T上的依赖是紧的。

Conclusion: 提出的方法在非平稳环境中提供了更强的性能保证，新的Polyak风格学习率能系统性地提供目标损失保证，并在实验中表现出良好性能。

Abstract: Online structured prediction, including online classification as a special
case, is the task of sequentially predicting labels from input features.
Therein the surrogate regret -- the cumulative excess of the target loss (e.g.,
0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best
estimator -- has gained attention, particularly because it often admits a
finite bound independent of the time horizon $T$. However, such guarantees
break down in non-stationary environments, where every fixed estimator may
incur the surrogate loss growing linearly with $T$. We address this by proving
a bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where
$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its
path length, and $C > 0$ is some constant. This bound depends on $T$ only
through $F_T$ and $P_T$, often yielding much stronger guarantees in
non-stationary environments. Our core idea is to synthesize the dynamic regret
bound of the online gradient descent (OGD) with the technique of exploiting the
surrogate gap. Our analysis also sheds light on a new Polyak-style learning
rate for OGD, which systematically offers target-loss guarantees and exhibits
promising empirical performance. We further extend our approach to a broader
class of problems via the convolutional Fenchel--Young loss. Finally, we prove
a lower bound showing that the dependence on $F_T$ and $P_T$ is tight.

</details>


### [197] [Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report](https://arxiv.org/abs/2510.07092)
*Riccardo Mereu,Aidan Scannell,Yuxin Hou,Yi Zhao,Aditya Jitta,Antonio Dominguez,Luigi Acerbi,Amos Storkey,Paul Chang*

Main category: cs.LG

TL;DR: 该论文在1X世界模型挑战赛中针对人形机器人交互任务，分别提出了采样任务和压缩任务的解决方案，在两个任务中都获得了第一名。


<details>
  <summary>Details</summary>
Motivation: 世界模型是AI和机器人领域的重要范式，能够通过预测视觉观察或紧凑潜在状态来推理未来。1X世界模型挑战赛提供了一个真实世界人形机器人交互的开源基准。

Method: 对于采样任务，适配了视频生成基础模型Wan-2.2 TI2V-5B进行视频状态条件化的未来帧预测，使用AdaLN-Zero条件化机器人状态，并通过LoRA进行后训练。对于压缩任务，从头训练了一个时空Transformer模型。

Result: 采样任务达到23.0 dB PSNR，压缩任务Top-500 CE为6.6386，在两个挑战中都获得了第一名。

Conclusion: 提出的方法在真实世界人形机器人交互的世界模型任务中表现出色，证明了所采用的技术路线的有效性。

Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to
reason about the future by predicting visual observations or compact latent
states. The 1X World Model Challenge introduces an open-source benchmark of
real-world humanoid interaction, with two complementary tracks: sampling,
focused on forecasting future image frames, and compression, focused on
predicting future discrete latent codes. For the sampling track, we adapt the
video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned
future frame prediction. We condition the video generation on robot states
using AdaLN-Zero, and further post-train the model using LoRA. For the
compression track, we train a Spatio-Temporal Transformer model from scratch.
Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386
in the compression task, securing 1st place in both challenges.

</details>


### [198] [Non-Asymptotic Analysis of Efficiency in Conformalized Regression](https://arxiv.org/abs/2510.07093)
*Yunzhen Yao,Lie He,Michael Gastpar*

Main category: cs.LG

TL;DR: 本文研究了保形预测在回归问题中的效率，建立了分位数和中位数回归的预测集长度与理想区间长度偏差的非渐近界，揭示了不同α值下的相变现象。


<details>
  <summary>Details</summary>
Motivation: 保形预测提供具有覆盖保证的预测集，但其信息量取决于预测集的期望大小（效率）。先前研究通常将误覆盖率α视为固定常数，本文旨在分析效率如何随训练集大小、校准集大小和误覆盖率α变化。

Method: 使用随机梯度下降训练保形化的分位数和中位数回归模型，在数据分布温和假设下，建立了预测集长度与理想区间长度偏差的非渐近界。

Result: 得到了阶数为O(1/√n + 1/(α²n) + 1/√m + exp(-α²m))的收敛界，揭示了效率对训练集大小n、校准集大小m和误覆盖率α的联合依赖关系，识别了不同α值下的相变现象。

Conclusion: 理论结果为数据分配提供了指导，以控制预测集长度的超额部分，实证结果与理论发现一致。

Abstract: Conformal prediction provides prediction sets with coverage guarantees. The
informativeness of conformal prediction depends on its efficiency, typically
quantified by the expected size of the prediction set. Prior work on the
efficiency of conformalized regression commonly treats the miscoverage level
$\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds
on the deviation of the prediction set length from the oracle interval length
for conformalized quantile and median regression trained via SGD, under mild
assumptions on the data distribution. Our bounds of order
$\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$
capture the joint dependence of efficiency on the proper training set size $n$,
the calibration set size $m$, and the miscoverage level $\alpha$. The results
identify phase transitions in convergence rates across different regimes of
$\alpha$, offering guidance for allocating data to control excess prediction
set length. Empirical results are consistent with our theoretical findings.

</details>


### [199] [DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering](https://arxiv.org/abs/2510.07132)
*Mariona Jaramillo-Civill,Peng Wu,Pau Closas*

Main category: cs.LG

TL;DR: 提出DPMM-CFL算法，使用Dirichlet Process先验自动推断聚类数量，解决传统聚类联邦学习需要预先指定聚类数的问题


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习方法需要预先固定聚类数量K，这在潜在结构未知时不切实际

Method: 在聚类参数分布上放置Dirichlet Process先验，通过非参数贝叶斯推断联合推断聚类数量和客户端分配，同时优化每个聚类的联邦目标

Result: 在Dirichlet和类分割非IID分区下的基准数据集上验证了算法有效性

Conclusion: DPMM-CFL能够自动推断聚类结构，在每轮中耦合联邦更新和聚类推断

Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client
heterogeneity by clustering clients and training one model per cluster, thereby
balancing between a global model and fully personalized models. However, most
CFL methods require the number of clusters K to be fixed a priori, which is
impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL
algorithm that places a Dirichlet Process (DP) prior over the distribution of
cluster parameters. This enables nonparametric Bayesian inference to jointly
infer both the number of clusters and client assignments, while optimizing
per-cluster federated objectives. This results in a method where, at each
round, federated updates and cluster inferences are coupled, as presented in
this paper. The algorithm is validated on benchmark datasets under Dirichlet
and class-split non-IID partitions.

</details>


### [200] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出了一种基于状态保持的多智能体进化搜索框架，用于自动生成单元测试的边界情况，相比无状态方法在代码覆盖率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的无状态推理方法在多步骤任务上表现不佳，缺乏持久状态；任务特定的微调在需要深度推理和长距离依赖的任务上表现脆弱。

Method: 结合持久推理状态、对抗性变异和进化保留，通过专门的智能体顺序提出、变异和评分候选测试用例，控制器在代际间维护持久状态。

Result: 在HumanEval和TestGenEvalMini等基准测试上，使用Llama、Gemma和GPT三种LLM家族，相比无状态单步基线在覆盖率上有显著提升。

Conclusion: 将持久推理状态与进化搜索相结合能显著改进单元测试生成效果。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [201] [ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL](https://arxiv.org/abs/2510.07151)
*Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: ELMUR是一种具有结构化外部记忆的Transformer架构，通过双向交叉注意力和LRU记忆模块扩展有效视野，在部分可观测环境中显著提升决策性能


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人需要在部分可观测和长视野环境下行动，但现有方法主要依赖瞬时信息，难以处理长期依赖关系

Method: 提出ELMUR架构，每层维护记忆嵌入，通过双向交叉注意力与记忆交互，使用LRU记忆模块进行替换或凸混合更新

Result: 在T-Maze任务中达到100%成功率（走廊长达100万步），在POPGym中超过半数任务优于基线，在MIKASA-Robo稀疏奖励操作任务中性能几乎翻倍

Conclusion: 结构化、层局部外部记忆为部分可观测环境下的决策提供了一种简单且可扩展的方法

Abstract: Real-world robotic agents must act under partial observability and long
horizons, where key cues may appear long before they affect decision making.
However, most modern approaches rely solely on instantaneous information,
without incorporating insights from the past. Standard recurrent or transformer
models struggle with retaining and leveraging long-term dependencies: context
windows truncate history, while naive memory extensions fail under scale and
sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a
transformer architecture with structured external memory. Each layer maintains
memory embeddings, interacts with them via bidirectional cross-attention, and
updates them through an Least Recently Used (LRU) memory module using
replacement or convex blending. ELMUR extends effective horizons up to 100,000
times beyond the attention window and achieves a 100% success rate on a
synthetic T-Maze task with corridors up to one million steps. In POPGym, it
outperforms baselines on more than half of the tasks. On MIKASA-Robo
sparse-reward manipulation tasks with visual observations, it nearly doubles
the performance of strong baselines. These results demonstrate that structured,
layer-local external memory offers a simple and scalable approach to decision
making under partial observability.

</details>


### [202] [Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging](https://arxiv.org/abs/2510.07182)
*Patrick Peixuan Ye,Chen Shani,Ellen Vitercik*

Main category: cs.LG

TL;DR: Bridged Clustering是一种半监督学习框架，通过独立聚类输入X和输出Y，然后使用少量配对样本学习稀疏、可解释的聚类间桥梁，实现从无配对数据中学习预测器。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习无法有效利用输出端数据，而基于密集传输的方法缺乏可解释性。本文旨在开发一种既能利用输出端数据又保持稀疏可解释性的方法。

Method: 首先独立聚类输入X和输出Y，然后使用少量配对样本学习输入聚类与输出聚类之间的稀疏桥梁。推理时，新输入x被分配到最近的输入聚类，返回对应输出聚类的质心作为预测。

Result: 理论分析表明，在聚类错误率和桥梁错误率有界的情况下，该算法成为有效且高效的预测器。实证结果与最先进方法竞争力相当，同时保持简单、模型无关且在低监督设置下标签效率高。

Conclusion: Bridged Clustering提供了一种简单、可解释且标签高效的半监督学习方法，能够有效利用输出端数据，在低监督设置下表现优异。

Abstract: We introduce Bridged Clustering, a semi-supervised framework to learn
predictors from any unpaired input $X$ and output $Y$ dataset. Our method first
clusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge
between clusters using only a few paired examples. At inference, a new input
$x$ is assigned to its nearest input cluster, and the centroid of the linked
output cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,
Bridged Clustering explicitly leverages output-only data, and unlike dense
transport-based methods, it maintains a sparse and interpretable alignment.
Through theoretical analysis, we show that with bounded mis-clustering and
mis-bridging rates, our algorithm becomes an effective and efficient predictor.
Empirically, our method is competitive with SOTA methods while remaining
simple, model-agnostic, and highly label-efficient in low-supervision settings.

</details>


### [203] [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
*Alexandra Souly,Javier Rando,Ed Chapman,Xander Davies,Burak Hasircioglu,Ezzeldin Shereen,Carlos Mougan,Vasilios Mavroudis,Erik Jones,Chris Hicks,Nicholas Carlini,Yarin Gal,Robert Kirk*

Main category: cs.LG

TL;DR: 研究表明，无论模型和数据集规模如何，仅需约250个恶意文档即可成功实施预训练投毒攻击，攻击所需文档数量不随模型规模增加而增加。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设攻击者控制一定比例的训练语料，但对于大型模型，即使很小比例也意味着不切实际的大量数据。本文旨在证明投毒攻击实际上只需要近乎恒定数量的文档，与数据集大小无关。

Method: 进行了迄今为止最大规模的预训练投毒实验，训练了从6亿到130亿参数的模型，使用Chinchilla最优数据集（60亿到2600亿token）。还进行了小规模实验来研究可能影响攻击成功的因素。

Result: 发现250个投毒文档在所有模型和数据集规模下都能类似地危害模型，尽管最大模型训练了超过20倍的干净数据。在微调阶段也观察到相同的动态。

Conclusion: 通过数据投毒注入后门对于大型模型可能比之前认为的更容易，因为所需的投毒数量不随模型规模增加而增加，这凸显了需要更多研究来减轻未来模型中的这种风险。

Abstract: Poisoning attacks can compromise the safety of large language models (LLMs)
by injecting malicious documents into their training data. Existing work has
studied pretraining poisoning assuming adversaries control a percentage of the
training corpus. However, for large models, even small percentages translate to
impractically large amounts of data. This work demonstrates for the first time
that poisoning attacks instead require a near-constant number of documents
regardless of dataset size. We conduct the largest pretraining poisoning
experiments to date, pretraining models from 600M to 13B parameters on
chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned
documents similarly compromise models across all model and dataset sizes,
despite the largest models training on more than 20 times more clean data. We
also run smaller-scale experiments to ablate factors that could influence
attack success, including broader ratios of poisoned to clean data and
non-random distributions of poisoned samples. Finally, we demonstrate the same
dynamics for poisoning during fine-tuning. Altogether, our results suggest that
injecting backdoors through data poisoning may be easier for large models than
previously believed as the number of poisons required does not scale up with
model size, highlighting the need for more research on defences to mitigate
this risk in future models.

</details>


### [204] [An in-depth look at approximation via deep and narrow neural networks](https://arxiv.org/abs/2510.07202)
*Joris Dommel,Sven A. Wegner*

Main category: cs.LG

TL;DR: 该论文研究了在宽度w=n和w=n+1时，神经网络对Hanin和Sellke提出的反例函数的逼近能力，分析了深度变化对逼近质量的影响以及神经元死亡现象。


<details>
  <summary>Details</summary>
Motivation: Hanin和Sellke在2017年证明了只有当宽度w>n时，ReLU激活的前馈神经网络才能在紧集上一致逼近连续函数。本文旨在研究在临界宽度w=n和w=n+1时，神经网络对原反例函数的实际逼近能力。

Method: 作者在宽度w=n和w=n+1两种情况下，通过改变网络深度来逼近Hanin和Sellke提出的反例函数，并分析逼近质量的变化规律。

Result: 研究发现，在临界宽度附近，神经网络的逼近质量随深度变化呈现特定行为，其中神经元死亡现象是影响逼近效果的关键因素。

Conclusion: 即使在临界宽度w=n和w=n+1时，神经网络仍能对反例函数进行一定程度的逼近，但逼近质量受到深度和神经元死亡现象的限制。

Abstract: In 2017, Hanin and Sellke showed that the class of arbitrarily deep,
real-valued, feed-forward and ReLU-activated networks of width w forms a dense
subset of the space of continuous functions on R^n, with respect to the
topology of uniform convergence on compact sets, if and only if w>n holds. To
show the necessity, a concrete counterexample function f:R^n->R was used. In
this note we actually approximate this very f by neural networks in the two
cases w=n and w=n+1 around the aforementioned threshold. We study how the
approximation quality behaves if we vary the depth and what effect (spoiler
alert: dying neurons) cause that behavior.

</details>


### [205] [Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts](https://arxiv.org/abs/2510.07205)
*Fangshuo Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 本文为软路由MoE模型提供了收敛保证，证明了在适度过参数化下，学生网络能通过特征学习阶段恢复教师网络参数，并展示了剪枝和微调的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构在现代AI系统中广泛应用，但其训练动态的理论理解仍局限于分离的专家-路由器优化或特定路由场景，需要更全面的理论分析。

Method: 在师生框架下分析软路由MoE模型的联合训练，使用非线性路由器和专家，证明在适度过参数化下的收敛性。

Result: 证明了学生网络能通过特征学习阶段恢复教师网络参数，剪枝能有效消除冗余神经元，微调过程能收敛到全局最优。

Conclusion: 这是首个深入理解MoE架构优化景观的分析，为软路由MoE模型的训练提供了理论保证。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of
modern AI systems. In particular, MoEs route inputs dynamically to specialized
experts whose outputs are aggregated through weighted summation. Despite their
widespread application, theoretical understanding of MoE training dynamics
remains limited to either separate expert-router optimization or only top-1
routing scenarios with carefully constructed datasets. This paper advances MoE
theory by providing convergence guarantees for joint training of soft-routed
MoE models with non-linear routers and experts in a student-teacher framework.
We prove that, with moderate over-parameterization, the student network
undergoes a feature learning phase, where the router's learning process is
``guided'' by the experts, that recovers the teacher's parameters. Moreover, we
show that a post-training pruning can effectively eliminate redundant neurons,
followed by a provably convergent fine-tuning process that reaches global
optimality. To our knowledge, our analysis is the first to bring novel insights
in understanding the optimization landscape of the MoE architecture.

</details>


### [206] [A Broader View of Thompson Sampling](https://arxiv.org/abs/2510.07208)
*Yanlin Qu,Hongseok Namkoong,Assaf Zeevi*

Main category: cs.LG

TL;DR: 本文揭示了Thompson Sampling通过在线优化框架平衡探索与利用的机制，提出了"忠实"平稳化方法来重新表述遗憾问题，将Thompson Sampling解释为模仿贝尔曼最优策略结构的正则化贪婪算法。


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling作为最广泛使用的多臂赌博机算法之一，其通过后验采样来平衡探索与利用的确切机制一直是个谜。本文旨在揭示这一核心机制。

Method: 引入"忠实"平稳化方法，将有限时域动态优化问题转换为平稳对应问题，使用贝尔曼原理研究时间不变目标，将Thompson Sampling重新表述为在线优化形式。

Result: 发现Thompson Sampling可以表示为模仿贝尔曼最优策略结构的在线优化形式，其中贪婪性通过基于点二列相关性的残差不确定性度量进行正则化。

Conclusion: 这解决了Thompson Sampling如何平衡探索与利用的问题，并为研究和改进Thompson原始思想提供了原则性框架。

Abstract: Thompson Sampling is one of the most widely used and studied bandit
algorithms, known for its simple structure, low regret performance, and solid
theoretical guarantees. Yet, in stark contrast to most other families of bandit
algorithms, the exact mechanism through which posterior sampling (as introduced
by Thompson) is able to "properly" balance exploration and exploitation,
remains a mystery. In this paper we show that the core insight to address this
question stems from recasting Thompson Sampling as an online optimization
algorithm. To distill this, a key conceptual tool is introduced, which we refer
to as "faithful" stationarization of the regret formulation. Essentially, the
finite horizon dynamic optimization problem is converted into a stationary
counterpart which "closely resembles" the original objective (in contrast, the
classical infinite horizon discounted formulation, that leads to the Gittins
index, alters the problem and objective in too significant a manner). The newly
crafted time invariant objective can be studied using Bellman's principle which
leads to a time invariant optimal policy. When viewed through this lens,
Thompson Sampling admits a simple online optimization form that mimics the
structure of the Bellman-optimal policy, and where greediness is regularized by
a measure of residual uncertainty based on point-biserial correlation. This
answers the question of how Thompson Sampling balances
exploration-exploitation, and moreover, provides a principled framework to
study and further improve Thompson's original idea.

</details>


### [207] [Discriminative Feature Feedback with General Teacher Classes](https://arxiv.org/abs/2510.07245)
*Omri Bar Oz,Tosca Lechner,Sivan Sabato*

Main category: cs.LG

TL;DR: 该论文系统研究了判别性特征反馈（DFF）学习协议的理论性质，分析了在可实现和不可实现设置下的最优错误界限，并揭示了DFF与在线学习之间的重要差异。


<details>
  <summary>Details</summary>
Motivation: DFF协议使用判别性特征解释作为反馈形式，但缺乏与经典学习协议（如监督学习和在线学习）可比较的通用理论框架分析。

Method: 在可与经典协议比较的通用框架下研究DFF，分析可实现和不可实现设置下的最优错误界限，并引入新的维度概念来刻画可实现设置下的错误界限。

Result: 在可实现设置中，使用新维度概念刻画了错误界限；在不可实现设置中，给出了错误上界并证明其一般不可改进。发现DFF中可实现维度不足以刻画最优不可实现错误界限或无遗憾算法的存在性。

Conclusion: DFF与在线学习存在根本差异：在DFF中，可实现维度不能完全表征不可实现设置的最优性能，这与在线学习形成鲜明对比。

Abstract: We study the theoretical properties of the interactive learning protocol
Discriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning
protocol uses feedback in the form of discriminative feature explanations. We
provide the first systematic study of DFF in a general framework that is
comparable to that of classical protocols such as supervised learning and
online learning. We study the optimal mistake bound of DFF in the realizable
and the non-realizable settings, and obtain novel structural results, as well
as insights into the differences between Online Learning and settings with
richer feedback such as DFF. We characterize the mistake bound in the
realizable setting using a new notion of dimension. In the non-realizable
setting, we provide a mistake upper bound and show that it cannot be improved
in general. Our results show that unlike Online Learning, in DFF the realizable
dimension is insufficient to characterize the optimal non-realizable mistake
bound or the existence of no-regret algorithms.

</details>


### [208] [Test-Time Graph Search for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.07257)
*Evgenii Opryshko,Junwei Quan,Claas Voelcker,Yilun Du,Igor Gilitschenski*

Main category: cs.LG

TL;DR: TTGS是一种轻量级规划方法，通过构建数据集状态图并搜索子目标序列来提升离线目标条件强化学习的长时决策能力，无需额外训练或监督。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在长时决策中面临时间信用分配和误差累积问题，离线设置会放大这些效应。

Method: TTGS接受任何状态空间距离或成本信号，构建数据集状态的加权图，执行快速搜索以组装子目标序列，由冻结策略执行。对于基于价值的学习器，距离直接从学习的目标条件价值函数导出。

Result: 在OGBench基准测试中，TTGS提高了多个基础学习器在挑战性运动任务上的成功率。

Conclusion: 简单的度量引导测试时规划对离线GCRL有益。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.

</details>


### [209] [Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints](https://arxiv.org/abs/2510.07266)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 提出了首个保证在线全预测中动态遗憾界限的算法，该算法能在满足所有下游决策者长期约束的同时，为每个决策者提供动态遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 解决在线全预测问题，目标是让学习者生成预测序列，使下游决策者基于这些预测选择行动时，能获得最坏情况下的效用保证并最小化约束违反。

Method: 设计了一种算法，确保所有代理同时获得动态遗憾保证，其中每个代理的遗憾是针对交互轮次中可能变化的行动序列衡量的，同时确保每个代理的约束违反趋近于零。

Result: 算法获得了所有代理的同时动态遗憾保证，且每个代理的约束违反趋近于零，无需代理自身维护任何状态，只需解决每轮由预测定义的单轮约束优化问题。

Conclusion: 该研究在在线全预测框架下首次实现了同时保证所有下游决策者动态遗憾和约束满足的算法，为多智能体在线决策提供了有效解决方案。

Abstract: We present an algorithm guaranteeing dynamic regret bounds for online
omniprediction with long term constraints. The goal in this recently introduced
problem is for a learner to generate a sequence of predictions which are
broadcast to a collection of downstream decision makers. Each decision maker
has their own utility function, as well as a vector of constraint functions,
each mapping their actions and an adversarially selected state to reward or
constraint violation terms. The downstream decision makers select actions "as
if" the state predictions are correct, and the goal of the learner is to
produce predictions such that all downstream decision makers choose actions
that give them worst-case utility guarantees while minimizing worst-case
constraint violation. Within this framework, we give the first algorithm that
obtains simultaneous \emph{dynamic regret} guarantees for all of the agents --
where regret for each agent is measured against a potentially changing sequence
of actions across rounds of interaction, while also ensuring vanishing
constraint violation for each agent. Our results do not require the agents
themselves to maintain any state -- they only solve one-round constrained
optimization problems defined by the prediction made at that round.

</details>


### [210] [GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)](https://arxiv.org/abs/2510.07285)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Qi Hu,Yan Li,Chang Liu*

Main category: cs.LG

TL;DR: 提出GTCN-G框架，结合门控时序卷积网络和图神经网络，通过残差学习机制解决网络入侵检测中的类别不平衡问题，在UNSW-NB15和ToN-IoT数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 网络威胁日益复杂，流量数据存在固有类别不平衡，现有方法难以同时处理拓扑结构和时序依赖，且缺乏专门解决数据不平衡的框架。

Method: GTCN-G框架融合门控TCN提取时序特征和图卷积网络学习图结构，核心创新是使用图注意力网络的残差学习机制来保留原始特征信息。

Result: 在两个公开基准数据集上验证，GTCN-G在二分类和多分类任务中均显著优于现有基线模型，达到最先进性能。

Conclusion: GTCN-G框架通过协同整合时序和图结构建模，并有效解决类别不平衡问题，为网络入侵检测提供了有效的解决方案。

Abstract: The escalating complexity of network threats and the inherent class imbalance
in traffic data present formidable challenges for modern Intrusion Detection
Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological
structures and Temporal Convolutional Networks (TCNs) are proficient in
capturing time-series dependencies, a framework that synergistically integrates
both while explicitly addressing data imbalance remains an open challenge. This
paper introduces a novel deep learning framework, named Gated Temporal
Convolutional Network and Graph (GTCN-G), engineered to overcome these
limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting
hierarchical temporal features from network flows with a Graph Convolutional
Network (GCN) designed to learn from the underlying graph structure. The core
innovation lies in the integration of a residual learning mechanism,
implemented via a Graph Attention Network (GAT). This mechanism preserves
original feature information through residual connections, which is critical
for mitigating the class imbalance problem and enhancing detection sensitivity
for rare malicious activities (minority classes). We conducted extensive
experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to
validate our approach. The empirical results demonstrate that the proposed
GTCN-G model achieves state-of-the-art performance, significantly outperforming
existing baseline models in both binary and multi-class classification tasks.

</details>


### [211] [Evolutionary Profiles for Protein Fitness Prediction](https://arxiv.org/abs/2510.07286)
*Jigang Fan,Xiaoran Jiao,Shengdong Lin,Zhanming Liang,Weian Mao,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.LG

TL;DR: EvoIF是一个轻量级蛋白质突变适应性预测模型，通过整合同源序列信息和跨家族结构进化约束，在ProteinGym基准测试中达到最先进性能，仅需0.15%的训练数据和较少参数。


<details>
  <summary>Details</summary>
Motivation: 蛋白质突变适应性预测对蛋白质工程至关重要，但受限于有限的实验数据与巨大序列空间之间的矛盾。现有蛋白质语言模型(pLMs)在零样本适应性预测方面表现良好，但需要更高效轻量的方法。

Method: 将自然进化视为隐式奖励最大化，将掩码语言建模(MLM)视为逆强化学习(IRL)。EvoIF整合两个互补的进化信号：来自检索同源序列的家族内谱和从逆折叠logits中提取的跨家族结构进化约束，通过紧凑转换块融合序列-结构表示。

Result: 在ProteinGym基准测试(217个突变实验；>250万个突变体)中，EvoIF及其MSA增强变体达到最先进或竞争性性能，仅使用0.15%的训练数据和比近期大模型更少的参数。消融实验证实家族内和跨家族谱具有互补性。

Conclusion: EvoIF提供了一种高效轻量的蛋白质突变适应性预测方法，通过整合互补的进化信号实现了强大的预测性能，为蛋白质工程提供了实用工具。

Abstract: Predicting the fitness impact of mutations is central to protein engineering
but constrained by limited assays relative to the size of sequence space.
Protein language models (pLMs) trained with masked language modeling (MLM)
exhibit strong zero-shot fitness prediction; we provide a unifying view by
interpreting natural evolution as implicit reward maximization and MLM as
inverse reinforcement learning (IRL), in which extant sequences act as expert
demonstrations and pLM log-odds serve as fitness estimates. Building on this
perspective, we introduce EvoIF, a lightweight model that integrates two
complementary sources of evolutionary signal: (i) within-family profiles from
retrieved homologs and (ii) cross-family structural-evolutionary constraints
distilled from inverse folding logits. EvoIF fuses sequence-structure
representations with these profiles via a compact transition block, yielding
calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational
assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve
state-of-the-art or competitive performance while using only 0.15% of the
training data and fewer parameters than recent large models. Ablations confirm
that within-family and cross-family profiles are complementary, improving
robustness across function types, MSA depths, taxa, and mutation depths. The
codes will be made publicly available at https://github.com/aim-uofa/EvoIF.

</details>


### [212] [MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder](https://arxiv.org/abs/2510.07289)
*Xingtong Yu,Chang Zhou,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: MolGA是一个将预训练的2D图编码器适配到下游分子应用的方法，通过灵活整合多样化的分子领域知识来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练2D图编码器忽略了与亚分子实例相关的丰富分子领域知识，而专门的分子预训练方法又缺乏整合多样化知识的灵活性。

Method: 提出分子对齐策略来弥合预训练拓扑表示与领域知识表示之间的差距，并引入条件适配机制生成实例特定令牌以实现细粒度的知识整合。

Result: 在11个公共数据集上进行的广泛实验证明了MolGA的有效性。

Conclusion: MolGA提供了一种实用的方法，可以在保持预训练2D编码器优势的同时，灵活整合分子领域知识用于下游任务。

Abstract: Molecular graph representation learning is widely used in chemical and
biomedical research. While pre-trained 2D graph encoders have demonstrated
strong performance, they overlook the rich molecular domain knowledge
associated with submolecular instances (atoms and bonds). While molecular
pre-training approaches incorporate such knowledge into their pre-training
objectives, they typically employ designs tailored to a specific type of
knowledge, lacking the flexibility to integrate diverse knowledge present in
molecules. Hence, reusing widely available and well-validated pre-trained 2D
encoders, while incorporating molecular domain knowledge during downstream
adaptation, offers a more practical alternative. In this work, we propose
MolGA, which adapts pre-trained 2D graph encoders to downstream molecular
applications by flexibly incorporating diverse molecular domain knowledge.
First, we propose a molecular alignment strategy that bridge the gap between
pre-trained topological representations with domain-knowledge representations.
Second, we introduce a conditional adaptation mechanism that generates
instance-specific tokens to enable fine-grained integration of molecular domain
knowledge for downstream tasks. Finally, we conduct extensive experiments on
eleven public datasets, demonstrating the effectiveness of MolGA.

</details>


### [213] [MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline](https://arxiv.org/abs/2510.07307)
*Rushi Qiang,Yuchen Zhuang,Anikait Singh,Percy Liang,Chao Zhang,Sherry Yang,Bo Dai*

Main category: cs.LG

TL;DR: MLE-Smith是一个全自动多智能体流水线，通过生成-验证-执行范式将原始数据集转化为竞赛式机器学习工程挑战，解决了现有MLE基准测试的可扩展性和适用性问题。


<details>
  <summary>Details</summary>
Motivation: 当前MLE基准测试依赖静态、手动策划的任务，存在可扩展性低、适用性有限的问题，需要大量时间和人工努力来制作高质量的训练数据。

Method: 采用多智能体流水线驱动结构化任务设计和标准化重构，结合混合验证机制强制执行严格的结构规则和高级语义合理性，通过交互式执行验证经验可解性和现实世界保真度。

Result: 在224个真实世界数据集上生成了606个任务，涵盖多个类别、目标和模态，验证了八个主流和前沿LLM在MLE-Smith任务上的表现与人工设计任务表现高度相关。

Conclusion: MLE-Smith能够有效扩展MLE任务规模，同时保持任务质量，为机器学习工程训练数据的获取提供了自动化解决方案。

Abstract: While Language Models (LMs) have made significant progress in automating
machine learning engineering (MLE), the acquisition of high-quality MLE
training data is significantly constrained. Current MLE benchmarks suffer from
low scalability and limited applicability because they rely on static, manually
curated tasks, demanding extensive time and manual effort to produce. We
introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw
datasets into competition-style MLE challenges through an efficient
generate-verify-execute paradigm for scaling MLE tasks with verifiable quality,
real-world usability, and rich diversity. The proposed multi-agent pipeline in
MLE-Smith drives structured task design and standardized refactoring, coupled
with a hybrid verification mechanism that enforces strict structural rules and
high-level semantic soundness. It further validates empirical solvability and
real-world fidelity through interactive execution. We apply MLE-Smith to 224 of
real-world datasets and generate 606 tasks spanning multiple categories,
objectives, and modalities, demonstrating that MLE-Smith can work effectively
across a wide range of real-world datasets. Evaluation on the generated tasks
shows that the performance of eight mainstream and cutting-edge LLMs on
MLE-Smith tasks is strongly correlated with their performance on carefully
human-designed tasks, highlighting the effectiveness of the MLE-Smith to
scaling up MLE tasks, while maintaining task quality.

</details>


### [214] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: 提出一种可扩展的方法，通过合成组合简单问题为复杂多步依赖链来提升语言模型的长程推理能力，仅使用现有的短程数据，无需额外监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖推理时支架或昂贵的步骤级监督，难以扩展。需要一种仅利用现有丰富短程数据就能提升长程推理能力的可扩展方法。

Method: 将简单问题合成组合成任意长度的复杂多步依赖链，使用仅结果奖励在自动增加复杂度的课程下进行强化学习训练。

Result: 在6年级数学问题上进行课程训练，可将竞赛级基准（GSM-Symbolic、MATH-500、AIME）的准确率提升高达2.06倍，长程推理改进显著优于基线。

Conclusion: 该方法为仅使用现有数据扩展强化学习处理长程问题提供了一条高效路径，理论上证明课程强化学习在样本复杂度上相比全时程训练有指数级改进。

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [215] [Bayesian Portfolio Optimization by Predictive Synthesis](https://arxiv.org/abs/2510.07180)
*Masahiro Kato,Kentaro Baba,Hibiki Kaibuchi,Ryo Inokuchi*

Main category: econ.EM

TL;DR: 本文提出了一种基于贝叶斯预测合成(BPS)的投资组合优化方法，通过结合多个资产收益预测模型来应对金融市场的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法需要资产收益分布信息，但这些信息通常未知且难以准确估计。金融市场的不确定性导致单一模型在不同时期表现不稳定。

Method: 使用贝叶斯预测合成(BPS)结合动态线性模型来整合多个资产收益预测模型，获得包含市场不确定性的贝叶斯预测后验分布。基于预测分布构建均值-方差投资组合和分位数投资组合。

Result: 该方法能够获得适应金融市场不确定性的资产均值收益贝叶斯预测后验分布。

Conclusion: 基于BPS的投资组合优化方法能够有效应对金融市场不确定性，通过集成多个预测模型提高投资组合构建的稳健性。

Abstract: Portfolio optimization is a critical task in investment. Most existing
portfolio optimization methods require information on the distribution of
returns of the assets that make up the portfolio. However, such distribution
information is usually unknown to investors. Various methods have been proposed
to estimate distribution information, but their accuracy greatly depends on the
uncertainty of the financial markets. Due to this uncertainty, a model that
could well predict the distribution information at one point in time may
perform less accurately compared to another model at a different time. To solve
this problem, we investigate a method for portfolio optimization based on
Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for
meta-learning. We assume that investors have access to multiple asset return
prediction models. By using BPS with dynamic linear models to combine these
predictions, we can obtain a Bayesian predictive posterior about the mean
rewards of assets that accommodate the uncertainty of the financial markets. In
this study, we examine how to construct mean-variance portfolios and
quantile-based portfolios based on the predicted distribution information.

</details>


### [216] [Beyond the Oracle Property: Adaptive LASSO in Cointegrating Regressions](https://arxiv.org/abs/2510.07204)
*Karsten Reichold,Ulrike Schneider*

Main category: econ.EM

TL;DR: 本文建立了自适应LASSO估计量在协整回归模型中的新渐近结果，包括模型选择概率、估计量一致性和极限分布，并推导了均匀收敛率和可检测的最快局部归零率。


<details>
  <summary>Details</summary>
Motivation: 研究自适应LASSO估计量在协整回归模型中的渐近性质，补充和扩展Lee、Shi和Gao(2022)的结果，特别关注不同调优参数设置下的表现差异。

Method: 采用标准和移动参数渐近方法，研究模型选择概率、估计量一致性和极限分布，并推导均匀收敛率和局部归零系数的检测速率。

Result: 在保守调优下，自适应LASSO估计量是均匀T一致的，可检测的局部归零系数截止率为1/T；在一致调优下，这两个速率都较慢且依赖于调优参数。模拟研究表明有限样本分布与预言性质有显著偏差。

Conclusion: 自适应LASSO估计量在协整回归中具有重要的渐近性质，移动参数渐近比预言性质提供更准确的近似，结果可扩展到局部单位根回归量和单位根预测变量的预测回归模型。

Abstract: This paper establishes new asymptotic results for the adaptive LASSO
estimator in cointegrating regression models. We study model selection
probabilities, estimator consistency, and limiting distributions under both
standard and moving-parameter asymptotics. We also derive uniform convergence
rates and the fastest local-to-zero rates that can still be detected by the
estimator, complementing and extending the results of Lee, Shi, and Gao (2022,
Journal of Econometrics, 229, 322--349). Our main findings include that under
conservative tuning, the adaptive LASSO estimator is uniformly $T$-consistent
and the cut-off rate for local-to-zero coefficients that can be detected by the
procedure is $1/T$. Under consistent tuning, however, both rates are slower and
depend on the tuning parameter. The theoretical results are complemented by a
detailed simulation study showing that the finite-sample distribution of the
adaptive LASSO estimator deviates substantially from what is suggested by the
oracle property, whereas the limiting distributions derived under
moving-parameter asymptotics provide much more accurate approximations.
Finally, we show that our results also extend to models with local-to-unit-root
regressors and to predictive regressions with unit-root predictors.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [217] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演化的智能推理系统，通过集成计算工具和检索工具来增强基础模型的推理能力，在多轮多模型解决方案演化中实现一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理中的两个瓶颈：模型内在能力有限和不可靠的测试时迭代，通过专业工具增强推理的精确性和可验证性。

Method: 使用Python计算工具和检索工具执行精确计算和决策基础，通过共享状态地图支持多轮多模型解决方案演化，记录候选方案、可执行检查和反馈进行迭代优化。

Result: 在AIME 2024/2025评估中，Qwen2.5-14B-Instruct获得+5.15% Average@32和+23.34% Pass@32提升，Llama-3.3-70B-Instruct获得+8.91% Average@32和+26.67% Pass@32提升，工具调用成功率超过80%。

Conclusion: AlphaApollo通过工具集成有效提升了基础模型的能力上限，在多个模型上都实现了显著的性能提升。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [218] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 提出了复杂性分布外泛化框架来定义和衡量推理能力，当模型在测试实例上的表现保持良好，而所需解决方案的复杂性超过所有训练示例时，就体现了复杂性OoD泛化。


<details>
  <summary>Details</summary>
Motivation: 当前AI在模式识别任务上取得进展，但缺乏对推理能力的明确定义和度量标准，需要建立统一的框架来形式化学习和推理。

Method: 通过解决方案描述的Kolmogorov复杂性和操作代理来形式化复杂性，区分复杂性OoD与长度和组合OoD，并提出在基准设计、监督方式、归纳偏差等方面的实践建议。

Result: 该框架统一了学习和推理：低复杂性时可使用类似System1的处理，高复杂性压力下需要类似System2的推理，而System2可视为对解决方案结构的泛化。

Conclusion: 复杂性OoD不能仅通过扩展数据解决，需要显式建模和分配计算的架构和训练机制来实现稳健推理。

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [219] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: BuilderBench是一个专注于开放式探索的智能体预训练基准，要求智能体学习使用积木构建各种结构，测试物理理解、数学能力和长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型主要通过模仿学习，难以解决超出已有数据范围的新问题。需要开发能够通过交互经验学习的智能体，但可扩展的学习机制仍是一个开放性问题。

Method: 提供硬件加速的机器人模拟器和42个多样化目标结构的任务套件，智能体在无外部监督下探索学习环境通用原理，评估时构建未见过的目标结构。

Result: 实验表明当前算法在这些任务上仍面临挑战，因此提供了"训练轮"协议来简化训练评估过程，并实现了六种算法作为研究参考。

Conclusion: BuilderBench促进了面向开放式探索的智能体预训练研究，要求智能体通过行动进行具身推理，实验不同策略并整合它们。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [220] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: 本文探讨如何通过游戏化学习设计解决并购后信息系统集成方法培训中学习曲线高和学习动力低的问题，提出了一个专门针对并购后信息系统集成的游戏化学习设计框架。


<details>
  <summary>Details</summary>
Motivation: 并购后信息系统集成面临独特挑战，现有AMILI和AMILP方法在实际应用中存在学习曲线高和学习动力低的问题，需要更有效的培训方式。

Method: 分析基础学习理论、认知负荷与动机模型、严肃游戏设计框架，识别游戏化学习设计框架的关键要求，构建包含转换过程和结果学习体验两个组件的框架。

Result: 提出了专门针对并购后信息系统集成的游戏化学习设计框架，通过将静态方法培训转化为互动学习体验来解决现有方法的局限性。

Conclusion: 计划通过迭代设计和实际验证来开发和评估所提出的框架，以提升信息系统集成培训的效果和参与度。

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [221] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 提出了BCCS框架，通过选择最优合作者和校准系统内部信念来实现稳定的多智能体共识，在MATH和MMLU基准上优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有共识寻求方法依赖投票机制，忽略了系统内部信念的矛盾，且通过无差别合作阻碍了稳定共识的形成。

Method: 基于理论框架选择最优合作者，提出信念校准共识寻求(BCCS)框架，通过选择最优合作者和校准共识判断来促进稳定共识。

Result: 在MATH和MMLU基准数据集上，BCCS框架在挑战性任务上的准确率分别比现有最佳结果高出2.23%和3.95%。

Conclusion: BCCS框架通过优化合作者选择和信念校准，有效解决了多智能体系统中共识不稳定的问题，显著提升了性能。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [222] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 论文研究了推理LLMs在共享轨迹中协作的能力，发现更强的模型在干扰下更脆弱，所有模型在超出自身能力范围的问题上利用指导步骤的失败率都很高。


<details>
  <summary>Details</summary>
Motivation: 研究标准单推理训练流程是否能产生期望的离轨迹行为，即评估和基于其他模型部分推理的能力，为多模型协作推理奠定基础。

Method: 提出双测试：可恢复性测试模型从误导推理轨迹中回溯的能力，可引导性测试模型基于更强协作方正确推理的能力。评估15个开源LLMs，并进行控制研究分析后训练因素。

Result: 更强的基准测试模型在干扰下更脆弱；所有模型在超出能力范围问题上利用指导步骤的失败率低于9.2%；教师模型的不佳可恢复性行为会传递给蒸馏学生。

Conclusion: 现成的推理LLMs在离轨迹推理方面存在局限性，为训练原生强推理协作模型提供了可操作的见解。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [223] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: 该研究创建了一个知识图谱，将食物与健康联系起来，重点关注食物中的黄酮类化合物含量与癌症关联，使用KNARM方法构建机器可操作的标准化表示。


<details>
  <summary>Details</summary>
Motivation: 目前很少有研究使用语义网技术以标准化、机器可读的格式表示食物与健康的关系，这限制了有效利用这些知识的能力。

Method: 利用USDA数据库中的食物黄酮类化合物含量数据和文献中的癌症关联信息，采用KNARM方法构建知识图谱，将不同平台的信息整合。

Result: 成功创建了一个连接食物与健康的知识图谱，为研究人员探索饮食选择与疾病管理之间复杂关系提供了示例。

Conclusion: 该知识图谱展示了食物与健康关系的标准化表示方法，未来工作将扩展图谱范围、添加更多相关数据并进行推理以发现隐藏关系。

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [224] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 提出了PuzzlePlex基准测试，用于评估基础模型在复杂动态环境中的推理和规划能力，包含15种不同类型的谜题，并开发了细粒度指标来分析模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在复杂动态环境中的推理和规划能力及其可扩展性，需要设计一个全面的基准测试来系统评估这些能力。

Method: 引入PuzzlePlex基准测试，包含15种不同类型的谜题（确定性和随机性游戏、单人和双人场景），提供定制化游戏策略进行比较，开发细粒度指标，在基于指令和基于代码两种设置下分析前沿基础模型。

Result: 推理模型在基于指令的设置中表现最佳，基于代码的执行虽然更具挑战性但提供了可扩展且高效的替代方案。

Conclusion: PuzzlePlex能够进行针对性评估，指导基础模型在推理、规划和泛化方面的未来改进。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [225] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 提出行为引导技术，通过识别四种有益推理行为（信息验证、权威评估、自适应搜索、错误恢复）来训练更有效的智能搜索模型，在三个基准测试中相比直接RL训练获得超过35%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 智能搜索利用LLM解释复杂用户信息需求并执行多步骤的规划、搜索和合成过程，这给LLM的推理和智能能力带来了独特挑战，需要研究有效的推理行为模式。

Method: 提出推理驱动的LLM管道分析成功智能搜索轨迹，识别四种有益推理行为，然后通过行为引导技术合成展示这些行为的轨迹，通过监督微调和强化学习训练模型。

Result: 在GAIA、WebWalker和HLE三个基准测试中，行为引导技术在Llama3.2-3B和Qwen3-1.7B上相比直接RL训练获得超过35%的性能提升，且发现SFT数据中的期望推理行为比最终答案正确性更重要。

Conclusion: 引入的推理行为赋予模型更有效的探索能力和测试时扩展能力，为强化学习提供了坚实基础，期望推理行为是获得强最终性能的关键因素。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [226] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出了Auto-Prompt Ensemble (APE)框架，通过自动学习评估维度来提升LLM评判者的可靠性，在Reward Bench上将GPT-4o的零样本一致性从87.2%提升到90.5%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评判者经常忽略关键评估维度，因为它们无法识别人类评估背后的隐含标准，导致评估可靠性不足。

Method: 提出APE自适应框架，自动从失败案例中学习评估维度，采用基于置信度的集成机制，通过Collective Confidence方法决定何时采用额外评估维度的判断。

Result: 在多样化标准基准测试中，APE显著提升了LLM评判者的可靠性，特别是在Reward Bench上GPT-4o的零样本一致性提升了3.3个百分点。

Conclusion: APE为LLM评判者提供了利用测试时计算的原理性方法，弥合了人类与LLM评判者之间的评估差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [227] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART是一个LLM代理框架，通过动态分解复杂网页任务为导航、信息提取和执行三个子任务，并持续重新规划，显著提升了在复杂网页任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在简单网页任务上表现良好，但在需要长视野导航、大规模信息提取和约束推理的复杂目标上仍有困难。

Method: 将目标动态分解为三个专注子任务（导航、信息提取、执行），并随着网页内容揭示持续重新规划分解策略。

Result: 在WebChoreArena上比之前SOTA代理成功率提升13.7个百分点，在WebArena上保持同等性能，导航步骤减少14.7步。

Conclusion: WebDART框架通过任务分解和动态重规划，有效提升了LLM代理处理复杂网页任务的能力。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [228] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: 本文提出EICL方法，通过引入情感相似示例和动态软标签策略改进细粒度情感识别中的决策过程，显著优于传统ICL方法。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在细粒度情感识别中只关注推理过程而忽视了决策过程，且语义相似示例常引入情感差异导致错误。

Method: 提出EICL方法：1) 引入情感相似示例；2) 使用动态软标签策略改进查询表示；3) 采用两阶段排除策略从多角度评估相似性。

Result: 在多个数据集上的广泛实验表明，EICL方法显著优于ICL方法。

Conclusion: 通过原型理论和改进的决策过程，EICL有效解决了ICL在细粒度情感识别中的局限性，提升了识别准确性。

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [229] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: 提出Agent-in-the-Loop框架，通过实时反馈循环持续改进基于LLM的客服系统，将重训练周期从数月缩短至数周。


<details>
  <summary>Details</summary>
Motivation: 传统离线方法依赖批量标注，无法快速迭代改进客服系统。需要将人类反馈直接嵌入运营工作流以实现持续优化。

Method: AITL框架集成四种实时标注：响应偏好对比、客服采纳与理由、知识相关性检查、缺失知识识别。这些反馈信号直接用于模型更新。

Result: 生产试点显示检索准确率显著提升（召回率+11.7%，精确率+14.8%），生成质量提高（帮助性+8.4%），客服采纳率增加4.5%。

Conclusion: 将人类反馈循环直接嵌入运营工作流能有效持续改进基于LLM的客服系统，证明了AITL框架的实用价值。

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [230] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文分析了元代理系统设计中的三个关键挑战：跨迭代学习效率低、设计代理行为多样性不足、以及自动化设计的经济可行性有限。


<details>
  <summary>Details</summary>
Motivation: 随着元代理系统自动化设计的发展，需要深入理解其实际效果和局限性，特别是在学习效率、行为多样性和经济可行性方面的挑战。

Method: 通过实验分析元代理的跨迭代学习策略，比较不同方法（如全上下文扩展vs进化方法），评估设计代理的行为多样性，以及进行成本效益分析。

Result: 研究发现：进化方法优于简单上下文扩展；设计代理行为多样性低；仅在少数数据集上自动化设计具有经济优势，大多数情况下成本效益不佳。

Conclusion: 当前元代理系统在自动化设计方面仍面临显著挑战，需要改进学习策略、增强行为多样性，并在经济可行性方面进行更谨慎的评估。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [231] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG是一个创新的知识图谱框架，整合了认知神经科学知识图谱、基因本体论和疾病本体论，利用大语言模型进行实体对齐和语义相似度计算，构建了一个连接基因机制、神经系统疾病和认知功能的统一知识图谱。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在捕捉基因、疾病和认知过程之间复杂语义联系方面存在局限，需要开发能够整合多源知识并利用大语言模型优势的新框架。

Method: 整合三个关键知识源：认知神经科学知识图谱(CNKG)、基因本体论(GO)和疾病本体论(DO)，利用GPT-4等大语言模型进行实体对齐、语义相似度计算和图增强，构建统一的知识图谱。

Result: 构建的MultiCNKG包含6.9K个节点(5种类型)和11.3K条边(7种类型)，评估指标显示精度85.20%、召回率87.30%、覆盖率92.18%、图一致性82.50%、新颖性检测40.28%、专家验证89.50%，链接预测性能与基准数据集相当。

Conclusion: MultiCNKG知识图谱在个性化医疗、认知障碍诊断和认知神经科学假设制定方面具有重要应用价值，为从分子到行为领域的多层次研究提供了有力工具。

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [232] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 提出了一个用于严格自动化验证基于大语言模型的策略在无记忆顺序决策任务中的工具，通过增量构建MDP的可达部分并使用Storm模型检查器验证安全性属性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在顺序决策任务中的应用日益增多，需要一种严格的方法来验证这些模型策略是否满足安全要求，确保其在实际部署中的可靠性。

Method: 给定MDP、LLM策略和PCTL安全要求，该方法增量构建MDP的可达部分，将每个状态编码为自然语言提示，解析LLM的响应为动作，并扩展策略的可达后继状态，最后使用Storm模型检查器进行验证。

Result: 实验表明，通过Ollama访问的开源LLM在确定性种子下可以被验证，但通常表现不如深度强化学习基线方法。

Conclusion: 该工具与Ollama原生集成并支持PRISM规范的任务，为正式验证日益强大的LLM奠定了实用基础，支持用户指定顺序决策任务的持续基准测试。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [233] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 提出了双循环多智能体(DLMA)框架，通过教授智能体的领导循环进化研究计划，博士学生智能体的跟随循环执行计划，实现端到端自动化科学研究。


<details>
  <summary>Details</summary>
Motivation: 自动化端到端科学研究面临双重挑战：需要生成新颖且合理的高层计划，并在动态不确定条件下正确执行这些计划。

Method: DLMA框架包含两个循环：领导循环使用进化算法通过参与、改进和整合会议迭代生成和优化研究提案；跟随循环通过事前和事后会议动态调整计划执行。

Result: 在ACLAward和Laboratory基准测试中，DLMA生成的研究论文在自动评估中获得最先进分数，显著优于强基线方法。

Conclusion: 消融研究证实两个循环都至关重要，进化驱动新颖性，执行确保合理性，DLMA成功解决了自动化科学研究的双层挑战。

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [234] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 提出了ATF方法，通过整合语法修正工具和一致性验证工具来改进自动形式化过程，显著提升了形式化语句的语法有效性和语义一致性


<details>
  <summary>Details</summary>
Motivation: 现有的自动形式化方法在生成语法有效且语义一致的正式语句方面仍存在困难，需要改进形式化过程的可靠性和一致性

Method: ATF方法整合Lean 4编译器进行语法修正，采用多LLMs作为评判者进行一致性验证，通过工具反馈自适应优化生成语句，包含冷启动阶段、专家迭代阶段和直接偏好优化

Result: 实验结果显示ATF显著优于多种基线形式化模型，人类评估进一步验证了其优越性能，并表现出良好的推理扩展特性

Conclusion: ATF方法通过工具反馈机制有效提升了自动形式化的质量，开源的数据集Numina-ATF将促进自动形式化和自动定理证明研究的进展

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [235] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: TGPR结合GRPO与Thompson采样树搜索，通过主动探索失败和成功的细化路径来改进LLM的迭代细化能力，在代码生成基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有迭代细化方法依赖预定义启发式，面临探索-利用困境且无法根据过去结果自适应调整，需要更有效的搜索策略。

Method: TGPR框架结合GRPO与Thompson采样树搜索，主动探索失败和成功的细化路径，提供更密集的训练轨迹和自适应策略。

Result: 在HumanEval、MBPP和APPS基准上，相比GRPO基线，pass@1提升最高4.2个百分点(MBPP)，pass@10提升最高12.51个百分点(APPS)。

Conclusion: TGPR为结合学习策略与结构化搜索提供了原则性方法，是增强LLM迭代细化和状态推理的通用框架。

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [236] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: 提出了一个集成开发环境来解决AJAN框架中RDF/RDFS和SPARQL建模的困难，并利用大语言模型扩展用户群体


<details>
  <summary>Details</summary>
Motivation: AJAN框架中RDF/RDFS和SPARQL的行为定义仍然是实践中的主要障碍，处理URI容易出错，复杂SPARQL查询在大型环境中学习曲线陡峭

Method: 开发一个集成开发环境，利用大语言模型来辅助AJAN智能体工程

Result: 提出了一个能够克服AJAN智能体建模障碍的开发环境

Conclusion: 该集成开发环境不仅解决了现有建模困难，还通过大语言模型扩展了AJAN的用户群体

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [237] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 该研究重新审视了统一信息密度(UID)假设在大型语言模型推理轨迹中的应用，发现步骤级信息密度均匀性可以预测推理质量，并提出了局部和全局均匀性评分指标。


<details>
  <summary>Details</summary>
Motivation: 探索统一信息密度原则是否适用于LLM推理轨迹，验证步骤级信息密度均匀性是否反映推理质量。

Method: 提出了基于熵的步骤级信息密度度量方法，并引入了局部和全局均匀性评分两种互补的均匀性度量指标。

Result: 在六个推理基准测试中，步骤级均匀性不仅提供了理论视角，还带来了实际性能提升：选择信息密度更均匀的推理轨迹在AIME2025上相对基线提高了10-32%的准确率。正确推理轨迹避免信息密度尖峰，错误轨迹则表现出不规则信息爆发。

Conclusion: UID启发的信息密度度量优于其他内部信号作为推理质量预测指标，信息密度均匀性是构建更可靠、准确推理系统的稳健诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [238] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: 提出了TAPO框架，通过强化学习将多跳推理与自适应工具调用能力相结合，在需要外部知识和数学计算的任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在需要最新知识或计算工具（如计算器、代码解释器）的复杂算术运算任务上表现不佳，需要克服这些限制。

Method: 使用改进的动态采样策略优化（DAPO）强化学习框架，专门为工具调用场景设计，使模型能够动态交织复杂推理与按需工具使用（包括搜索API和Python解释器）。

Result: 在Qwen2.5-3B和Qwen2.5-7B模型上的实验显示，该方法在需要外部知识和数学计算的任务上实现了最先进的性能，比基线方法更有效地利用工具，同时防止因奖励攻击导致的过度调用。

Conclusion: 结合高级推理与工具使用在知识密集型和计算密集型任务中具有显著潜力，能够有效提升模型性能。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [239] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: 提出一个新颖框架，通过构建一组LLM代理来集体捕捉人类群体的多样性，每个代理通过上下文学习使用少量人类演示来引导行为，使用子模优化方法选择代表性代理集。


<details>
  <summary>Details</summary>
Motivation: 由于获取大规模人类响应的困难和成本，LLMs成为人类行为的替代品，但现有LLMs输出同质化，无法捕捉人类观点和行为的丰富多样性。

Method: 通过上下文学习将少量人类演示（任务-响应对）作为条件来引导LLM代理行为，使用子模优化方法从指数级大的可能代理空间中选择代表性代理集，提供不同时间复杂度和性能保证的权衡方法。

Result: 在众包和教育领域的广泛实验表明，该方法构建的代理比基线方法更有效地代表人类群体，行为分析显示这些代理在新任务上重现了目标学生和标注者的行为模式和观点。

Conclusion: 该框架成功构建了能够有效代表人类群体多样性的LLM代理集，通过子模优化方法解决了代理选择问题，在多个领域验证了其有效性。

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [240] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: 本文提出了从背景程序和示例中提取可能性逻辑程序的方法，定义了归纳任务概念并开发了ilpsm和ilpsmmin两种算法。


<details>
  <summary>Details</summary>
Motivation: 可能性逻辑程序在稳定模型下的归纳推理问题尚未被研究，本文旨在填补这一空白。

Method: 正式定义归纳任务概念，研究其性质，并提出了ilpsm和ilpsmmin两种计算归纳解的算法。

Result: 实现了ilpsmmin算法，实验结果表明在输入为普通逻辑程序时，该原型在随机生成的数据集上优于主要的基于稳定模型的正常逻辑程序归纳学习系统。

Conclusion: 本文成功开发了可能性逻辑程序的归纳推理方法，并通过实验验证了其有效性。

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [241] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent是一个将大语言模型生成的组件集成到元启发式算法中的框架，通过遗传搜索优化这些组件，在多个车辆路径问题上超越了人工设计的启发式方法和基于学习的方法。


<details>
  <summary>Details</summary>
Motivation: 设计高性能的车辆路径问题启发式算法需要直觉和深厚的领域知识，现有的大语言模型代码生成方法还无法产生能与人类专家相媲美的启发式算法。

Method: 使用大语言模型生成问题特定的操作符，并将其嵌入到通用的元启发式框架中，通过新颖的遗传搜索来优化这些组件，确保任务可控性和正确性。

Result: 在容量约束车辆路径问题、带时间窗车辆路径问题和奖励收集车辆路径问题等多个问题上，该方法发现的启发式操作符超越了手工设计方法和近期基于学习的方法，且仅需单CPU核心。

Conclusion: VRPAgent是首个在车辆路径问题上推进最先进技术的大语言模型范式，为自动启发式发现展示了有前景的未来。

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [242] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文系统研究了长视野任务中两种动作表示方法的效果：基于动作的规划(PwA)和基于模式的规划(PwS)，发现在动作空间规模扩大时存在表示选择拐点，并分析了模型能力对拐点位置的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于动作的规划方法在环境动作空间组合爆炸时变得不实用，需要寻找更适合长视野智能体的最优动作表示方法。

Method: 比较了两种动作表示：PwA（提供可执行动作列表）和PwS（将动作模式实例化为动作列表），提出了认知带宽视角作为概念框架，并在ALFWorld和SciWorld环境中进行实证研究。

Result: 观察到在ALFWorld（约35个动作）和SciWorld（约500个动作）之间存在表示选择拐点，更强的规划能力使拐点右移，更好的模式实例化能力使拐点左移。

Conclusion: PwS在可扩展性方面有优势但性能欠佳，需要构建更强大的PwS智能体以实现更好的可扩展自主性。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [243] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 论文从海德格尔的存在主义现象学出发，定义了物理具身的两个基本条件：在世存在和向死而生，并基于此推导出维持完整性的稳态驱动力和最大化未来状态控制的内在驱动力，在强化学习框架中形式化这些概念。


<details>
  <summary>Details</summary>
Motivation: 理解生物体在开放环境中生存、繁衍和相互关怀的能力，以开发更鲁棒、自适应和关怀的人工智能体。物理脆弱性和死亡率常被视为人工代理发展的障碍，而生物体却能相对轻松高效地在开放物理世界中生存和互相关怀。

Method: 从海德格尔的存在主义现象学中汲取灵感，定义了两个最小物理具身条件：在世存在（代理是环境的一部分）和向死而生（除非被抵消，代理会因热力学第二定律漂向终结状态）。基于这些条件推导出稳态驱动力和内在驱动力，并在强化学习框架中形式化这些概念。

Result: 提出了一个理论框架，表明从基本物理具身条件可以推导出维持完整性的稳态驱动力和最大化未来状态控制的内在驱动力，这有助于增强代理维持物理完整性的能力。

Conclusion: 通过将存在主义哲学概念形式化为强化学习框架，可以培养内在驱动的具身代理在开放多代理环境中的开放性和关怀能力，为开发更鲁棒和自适应的人工智能体提供了理论基础。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [244] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: 提出了一个交互式流程发现框架，利用大语言模型从自然语言描述中提取声明性规则，指导流程模型发现算法，结合事件日志和领域知识生成更可靠的流程模型。


<details>
  <summary>Details</summary>
Motivation: 传统流程发现仅依赖事件日志，但事件日志往往不完整或包含噪声，且忽略了领域知识这一重要补充资源，导致发现的模型对下游任务缺乏可靠性。

Method: 使用大语言模型从领域专家提供的文本描述中提取声明性规则，这些规则指导IMr发现算法递归构建流程模型，结合事件日志和提取的规则，避免与领域知识相矛盾的问题流程结构。

Result: 开发了完全实现的工具支持该工作流，并对多种大语言模型和提示工程策略进行了广泛评估。实证研究包括基于真实事件日志的案例研究，领域专家评估了框架的可用性和有效性。

Conclusion: 该交互式框架成功将领域知识整合到流程发现流程中，通过大语言模型和领域专家的协作，生成了更可靠的流程模型。

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [245] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: 牛顿基准是一个包含324个科学定律发现任务的基准测试，通过元物理变换解决评估三难困境，将评估从静态函数拟合提升到交互式模型发现，揭示了前沿大语言模型在复杂系统中的发现能力存在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有科学定律发现基准存在评估三难困境（科学相关性、可扩展性和抗记忆化之间的权衡），且过度简化发现过程为静态函数拟合，未能捕捉真实的科学探索过程。

Method: 使用元物理变换（对规范定律的系统性修改）生成大量问题，构建包含12个物理领域的324个任务，要求智能体通过实验探索模拟复杂系统来发现隐藏原理。

Result: 前沿大语言模型展现出清晰但脆弱的发现能力：随着系统复杂性增加而急剧下降，对观测噪声极度敏感。工具辅助存在悖论效应，可能导致更有能力的模型过早从探索转向利用。

Conclusion: 在复杂交互环境中实现稳健、可泛化的发现仍是核心挑战，牛顿基准为衡量真实进展和开发下一代真正科学发现AI智能体提供了关键工具。

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [246] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: 提出了一种基于词典序偏好的多目标多智能体路径规划框架LCBS，直接计算符合目标偏好顺序的单一解，避免构建帕累托前沿，可扩展到10个目标。


<details>
  <summary>Details</summary>
Motivation: 现有MO-MAPF算法通常通过计算帕累托前沿来生成无冲突路径，但未充分利用用户定义的偏好信息，且在目标数量增加时扩展性差。

Method: LCBS算法结合了优先级感知的低层A*搜索与基于冲突的搜索，直接根据目标词典序偏好进行规划。

Result: LCBS能够计算最优解，在标准MAPF基准测试和随机化测试中，相比现有方法具有更高的成功率，特别是在目标数量增加时表现更优。

Conclusion: 词典序框架为MO-MAPF提供了一种有效建模方法，LCBS算法能够高效处理多目标偏好规划问题，显著提升了可扩展性。

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [247] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Md Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: 该论文展示了一个基于生成式AI的工作流，帮助NFL媒体研究人员通过自然语言查询历史比赛片段，将查询时间从10分钟缩短到30秒，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 传统基于筛选点击的界面效率低下，用户需要花费大量时间查找相关视频内容，影响了媒体研究人员的工作效率和创造力。

Method: 采用基于生成式AI的智能工作流，将用户自然语言查询分解为元素并转换为数据库查询语言，通过精心设计的语义缓存提高准确性和响应速度。

Result: 系统实现了超过95%的准确率，将查找相关视频的平均时间从10分钟减少到30秒，显著提升了NFL的运营效率。

Conclusion: 生成式AI工作流能够有效提升媒体内容发现和管理的效率，让用户能够专注于创意内容和引人入胜的故事线制作。

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>
