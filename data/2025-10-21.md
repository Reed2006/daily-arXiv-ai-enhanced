<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [econ.GN](#econ.GN) [Total: 9]
- [econ.TH](#econ.TH) [Total: 5]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.LG](#cs.LG) [Total: 224]
- [econ.EM](#econ.EM) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 该论文研究量子自然语言处理在自然语言推理任务中的应用，比较量子、混合和经典模型在少样本设置下的表现，并提出新的信息增益参数比指标来评估模型效率。


<details>
  <summary>Details</summary>
Motivation: 探索量子自然语言处理在语义建模中的潜力，特别是在资源受限的少样本场景下，量子模型能否在参数效率方面超越经典模型。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路处理句子对，训练语义相关性和推理分类任务，并提出基于聚类的架构来促进参数共享。

Result: 量子模型在性能上与经典基线相当，但参数数量显著减少；在推理任务上优于随机初始化的transformer模型，在相关任务上测试误差更低；量子模型的每参数学习效率比经典模型高出最多五个数量级。

Conclusion: 量子自然语言处理在低资源、结构敏感的场景中展现出巨大潜力，其参数效率优势明显，提出的聚类架构进一步提升了泛化能力。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 提出了一种融合ChatGPT和Claude的多模型框架，通过相似性共识方法提升胸部X光片诊断的可靠性，在CheXpert数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 提高AI辅助放射学诊断的可靠性和临床实用性，减少诊断错误，同时保持较低的计算开销。

Method: 使用ChatGPT和Claude两种大语言模型，采用基于95%输出相似性阈值的共识方法，结合图像和合成临床笔记进行多模态融合。

Result: 单模态设置下ChatGPT和Claude准确率分别为62.8%和76.9%，共识方法提升至77.6%；多模态设置下准确率分别提升至84%和76%，共识准确率达到91.3%。

Conclusion: 基于共识的融合方法持续优于单个模型，整合互补模态和输出级共识可显著提高AI辅助放射学诊断的可信度和临床价值。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 该研究提出了CorrectBench基准来评估LLM的自校正方法，发现在复杂推理任务中自校正能提升准确性，但会降低效率，而简单的思维链基线表现具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种LLM自校正方法被提出，但这些方法的综合评估仍然缺乏，且LLM是否真正能够自我校正是一个重要问题。

Method: 开发CorrectBench基准，评估内在、外部和微调三种自校正策略在常识推理、数学推理和代码生成三个任务上的效果。

Result: 自校正方法能提高准确性（特别是复杂推理任务）；混合不同策略可进一步提升但降低效率；推理LLM在额外自校正下优化有限且时间成本高；简单思维链基线具有竞争力。

Conclusion: 自校正有潜力提升LLM推理性能，但需要在推理能力和操作效率之间找到平衡，建议进一步研究优化这一平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR是一个让LLM代理能够通过完整闭环经验生命周期进行自我改进的框架，包含离线自我蒸馏和在线交互两个关键阶段，在复杂多跳问答基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在工具使用方面表现良好，但缺乏从自身经验中系统学习的能力。现有框架主要关注缓解外部知识差距，未能解决更根本的限制：无法迭代优化问题解决策略。

Method: EvolveR框架包含两个关键阶段：(1)离线自我蒸馏：将代理的交互轨迹合成为结构化、可重用的抽象策略原则库；(2)在线交互：代理与任务交互并主动检索蒸馏原则来指导决策，积累多样化行为轨迹。采用策略强化机制迭代更新代理。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于强大代理基线的性能表现。

Conclusion: 这项工作为代理不仅从外部数据学习，还从自身行动后果中学习提供了全面蓝图，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大语言模型在自动化系统文献综述筛选阶段的交互作用，发现CoT-few-shot提供最可靠的精确率-召回率平衡，推荐采用分阶段工作流程来优化成本效益。


<details>
  <summary>Details</summary>
Motivation: 探索不同提示策略与大语言模型在系统文献综述自动化筛选中的交互效果，为任务自适应的大语言模型部署提供比较基准和实践指导。

Method: 评估6个大语言模型在5种提示类型下的表现，使用准确率、精确率、召回率和F1分数等指标，涵盖相关性分类和6个Level-2任务。

Result: 结果显示明显的模型-提示交互效应：CoT-few-shot提供最可靠的精确率-召回率平衡；零样本在需要高敏感度的首轮筛选中召回率最高；自我反思表现不佳。GPT-4o和DeepSeek整体表现稳健，GPT-4o-mini在显著更低的成本下表现具有竞争力。

Conclusion: 推荐采用分阶段工作流程：先用低成本模型配合结构化提示进行首轮筛选，仅将边界案例升级到更高容量模型。这些发现突显了大语言模型在文献筛选自动化方面不均衡但有前景的潜力。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一个合成测试平台，用于系统分析语言模型中统计规律与事实关联之间的交互作用，发现上下文多样性和结构对事实泛化能力有复杂影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对语言模型中统计规律与事实关联交互作用的系统分析，特别是它们如何影响泛化能力。

Method: 设计灵活的合成测试平台，将通用标记的统计流与抽象事实的源-目标标记对流相结合，通过控制流组成和多样性水平来独立操纵上下文结构。

Result: 研究发现更高的上下文多样性会延迟分布内事实准确性，但对分布外事实泛化的影响取决于上下文结构；在某些情况下多样性对事实回忆至关重要；优化瓶颈主要出现在嵌入和解嵌入层。

Conclusion: 上下文设计和多样性水平的相互作用影响不同的泛化方面，合成框架为未来研究提供了受控测试环境。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 首个关于生成式AI信任与不信任的计算研究，使用2022-2025年Reddit数据，发现信任与不信任基本平衡，技术性能和可用性是主要维度，个人经验是态度形成的最常见原因。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI融入日常生活，理解公众对其的信任对负责任采用和治理至关重要。现有研究缺乏计算、大规模和纵向方法来衡量对GenAI和LLMs的信任与不信任。

Method: 使用多年度Reddit数据集（2022-2025），涵盖39个子版块和197,618个帖子。结合众包标注和分类模型进行规模化分析。

Result: 信任与不信任随时间基本平衡，主要模型发布时出现转变。技术性能和可用性占主导维度，个人经验是态度形成的最常见原因。不同信任者（专家、伦理学家、普通用户）展现出不同模式。

Conclusion: 本研究为大规模信任分析提供了方法论框架，并揭示了公众对GenAI不断演变的认知。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 提出了EgMM-Corpus数据集，包含3000多张埃及文化相关图像，涵盖313个概念，用于评估和训练视觉语言模型在埃及文化背景下的表现。


<details>
  <summary>Details</summary>
Motivation: 中东和非洲地区的多模态文化数据集有限，需要专门针对埃及文化的资源来减少AI模型的文化偏见。

Method: 设计新的数据收集流程，收集了涵盖地标、食物和民俗的3000多张图像，每个条目都经过人工验证文化真实性和多模态一致性。

Result: CLIP模型在EgMM-Corpus上的零样本分类准确率为Top-1 21.2%，Top-5 36.4%，显示出大规模视觉语言模型存在文化偏见。

Conclusion: EgMM-Corpus作为基准数据集对于开发文化感知模型具有重要意义，突显了解决AI模型文化偏见问题的必要性。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 该论文分析了语言模型对语法的学习情况，通过理论框架验证了字符串概率与语法知识之间的关系，使用28万句对在英语和中文中进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否真正学习了语法知识，以及字符串概率能否反映模型的语法能力，这对语言学理论有重要影响。

Method: 基于语料库生成过程的简单假设，建立语法、意义和字符串概率关系的理论框架，并通过最小对（语义差异最小的字符串对）进行实证分析。

Result: 验证了三个预测：(1)最小对中字符串概率的相关性；(2)模型与人类在最小对中差异的相关性；(3)语法和不合语法字符串在概率空间中分离度差。

Conclusion: 为使用概率来了解语言模型的结构知识提供了理论基础，并为未来语言模型语法评估工作指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 提出了两种低资源设置下的方法（多元化解码和模型引导）来增强语言模型的多元对齐能力，仅需50个标注样本就能在多个高风险任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型对社会影响增大，需要确保它们能对齐多样化的视角并反映人类价值观的细微差别。当前主流训练范式假设每个查询只有一个最优答案，导致响应泛化且对齐效果差。

Method: 采用多元化解码和模型引导两种方法，在低资源设置下仅使用50个标注样本进行实验。

Result: 模型引导方法在零样本和少样本基线上表现一致提升，在仇恨言论检测和错误信息检测等高风险任务中降低了假阳性率，在GlobalOpinionQA上改善了与人类价值观的分布对齐。

Conclusion: 该工作强调了多样性的重要性，展示了语言模型如何适应考虑细微视角，为低资源环境下的多元对齐提供了有效解决方案。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出了Profile-to-PEFT框架，通过超网络将用户配置文件直接映射到适配器参数，消除了部署时对每个用户的单独训练需求，实现了即时适应和高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法如"每个用户一个PEFT"需要为每个用户单独训练适配器，计算成本高且无法实时更新，限制了大规模个性化应用。

Method: 使用端到端训练的超网络，将编码后的用户配置文件直接映射到完整的适配器参数集（如LoRA），无需在部署时为每个用户进行训练。

Result: 实验结果表明，该方法在性能上优于基于提示的个性化和OPPU方法，同时部署时计算资源消耗显著减少，对分布外用户具有强泛化能力。

Conclusion: Profile-to-PEFT框架实现了高效、可扩展和自适应的LLM个性化，适合大规模应用，支持即时适应、泛化到未见用户和隐私保护的本地部署。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 该研究评估了不同后训练方法（SFT、DPO、GRPO）下LLM对其学习策略的认知能力、泛化能力和推理一致性，发现RL训练模型在策略意识和泛化性上优于SFT模型，但推理与输出对齐较弱。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否意识到自己通过后训练技术学习到的策略和推理过程，以及不同训练方法如何影响这种认知能力。

Method: 定义了三个核心能力评估指标，在多个需要学习不同策略的任务上实证评估SFT、DPO和GRPO三种后训练方法的模型表现。

Result: RL训练模型（DPO、GRPO）比SFT模型表现出更强的策略意识和跨领域泛化能力，但推理轨迹与最终输出的对齐较弱，GRPO模型尤其明显。

Conclusion: 后训练方法显著影响LLM对其学习行为的认知能力，RL方法增强策略意识和泛化性但损害推理一致性，需要在训练目标中平衡这些能力。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该研究探索使用大语言模型生成针对疫苗错误信息的实时反驳论点，通过优化提示策略和微调方法，结合分类器对反疫苗推文进行分类，以生成更具针对性的反驳内容。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体影响公共卫生的时代，疫苗怀疑论和错误信息已成为严重社会问题。虽然错误信息检测已有进展，但实时生成针对性反驳论点仍是一个未被充分探索的领域。

Method: 实验了多种提示策略和微调方法优化反驳论点生成，训练分类器将反疫苗推文分类为多个标签类别（如疫苗有效性担忧、副作用、政治影响等），实现更情境感知的反驳。

Result: 通过人工判断、LLM评估和自动指标的综合评估显示，这些方法具有很强的一致性。整合标签描述和结构化微调能显著增强反驳论点的有效性。

Conclusion: 研究表明，整合标签描述和结构化微调的方法为大规模缓解疫苗错误信息提供了一个有前景的解决方案。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 提出AASP框架，通过自回归方式联合建模论证组件和论证关系，在三个标准基准测试中取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现有方法通过生成范式扁平化论证结构，难以捕捉论证组件和论证关系之间的依赖关系

Method: 基于自回归结构预测框架，将论证结构建模为预定义动作集，使用条件预训练语言模型逐步构建论证结构

Result: 在三个标准AM基准测试中，两个达到SOTA结果，一个表现强劲

Conclusion: AASP框架能有效捕捉论证推理流程，在论证挖掘任务中表现优异

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法，通过线性变换特定层激活来改进LLM在心理健康评估中的表现，无需计算密集型技术。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs快速发展，但小规模模型在特定领域应用中仍表现不佳，特别是在心理健康这样的敏感高影响领域。

Method: 使用转向向量引导模型输出，对特定层激活应用线性变换，这是一种轻量级的干预方法。

Result: 该方法在两个任务上取得改进：识别Reddit帖子是否有助于检测抑郁症状的相关性预测任务，以及基于用户Reddit历史完成标准化抑郁筛查问卷的任务。

Conclusion: 转向机制作为计算效率高的工具，在LLMs心理健康领域适应方面具有未开发的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景和23000多个评估标准的基准，用于评估AI的道德推理过程。研究显示现有模型在数学、代码等任务上的表现无法预测其道德推理能力，且模型对特定道德框架存在偏好。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与决策，需要理解AI的决策过程而不仅仅是最终结果。道德困境是评估过程推理的理想测试平台，因为允许多种合理结论。

Method: 构建MoReBench基准，包含1000个道德场景和专家制定的评估标准，以及MoReBench-Theory测试AI在五种规范伦理学框架下的推理能力。

Result: 扩展定律和现有基准无法预测模型的道德推理能力。模型对特定道德框架（如边沁功利主义和康德义务论）表现出偏好，这可能是流行训练范式的副作用。

Conclusion: MoReBench基准推动了以过程为重点的推理评估，有助于开发更安全、更透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出了一种名为自主可信代理（ATA）的神经符号方法，通过将任务分解为离线知识提取和在线任务处理两个阶段，解决LLM在可信度方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在可信度方面存在幻觉、不稳定性和缺乏透明度等局限性，阻碍了其在高风险领域的部署。

Method: 采用神经符号方法，将任务分解为两个阶段：离线阶段将非正式问题规范转换为形式化知识库，在线阶段使用符号决策引擎基于形式化知识库和编码输入得出可靠结果。

Result: 在复杂推理任务上的评估表明，ATA与最先进的端到端推理模型具有竞争力，同时保持可信度。使用人工验证的知识库时，ATA显著优于更大的模型，并表现出完美的确定性、增强的稳定性以及对提示注入攻击的固有免疫力。

Conclusion: ATA通过基于符号推理生成决策，为构建下一代透明、可审计和可靠的自主体提供了一个实用且可控的架构。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 探索Whisper语音识别模型在第二语言口语评估中的潜力，通过提取隐藏表示中的声学和语言特征，仅需训练轻量级分类器即可超越现有先进基线方法。


<details>
  <summary>Details</summary>
Motivation: 挖掘Whisper模型在第二语言口语评估中的潜力，超越仅使用其转录文本的传统方法，探索其隐藏表示中蕴含的丰富信息。

Method: 从Whisper的中间和最终输出中提取声学和语言特征，仅训练轻量级分类器，并引入图像和文本提示作为辅助相关线索。

Result: 在GEPT图片描述数据集上表现优异，超越包括多模态方法在内的现有先进基线，通过分析嵌入发现模型内在编码了口语熟练度模式和语义信息。

Conclusion: Whisper无需任务特定微调即可作为第二语言口语评估和其他口语理解任务的强大基础模型，其隐藏表示中蕴含丰富的声学和语言信息。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt是一个LLM提示压缩框架，通过保留语义最显著的前k%令牌来减少输入长度，在多数NLP任务中20%压缩仅带来轻微性能损失，但数学推理任务性能急剧下降。


<details>
  <summary>Details</summary>
Motivation: 解决LLM因冗长输入导致的成本、碳足迹和推理延迟问题，利用典型提示中存在的冗余低效用令牌，仅保留承载主要语义权重的令牌。

Method: 使用GlobEnc和DecompX两种最先进的令牌归因方法为输入序列中的每个令牌分配显著性分数，按原始顺序保留前k%的令牌获得稀疏压缩提示。

Result: 在情感分析、常识问答和摘要任务中，20%提示压缩仅造成边际性能损失，而数学推理性能急剧恶化；与保留底k%和随机k%令牌的对比实验揭示了不对称性能模式。

Conclusion: 工作有助于更细致理解LLM在性能-效率权衡中的行为，界定了容忍上下文稀疏性的任务与需要完整上下文的任务之间的边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器评估推理轨迹质量，在降低推理成本的同时实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有外部测试时扩展方法存在计算开销高、未充分利用LLM内在潜表示的问题，需要更高效的轨迹选择框架。

Method: 使用轻量级验证器（0.6B参数）评估步骤级轨迹质量，聚合得分选择最优推理轨迹，采用端到端数据驱动训练方法。

Result: 在五个基准测试中，TrajSelector在Best-of-32设置下比多数投票准确率高4.61%，比现有过程奖励模型高4.31%-12.21%，且推理成本更低。

Conclusion: TrajSelector通过利用LLM隐藏状态进行过程级评分，实现了高效且有效的推理轨迹选择，在保持低推理成本的同时显著提升性能。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个集成课程强化学习和多模态大语言模型的广告视频违规检测框架，通过渐进式训练策略和GRPO优化方法提升推理能力，在工业数据集上表现出优越的违规类别准确性和时间定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确时间定位、噪声标注和泛化能力方面存在不足，需要开发更有效的解决方案。

Method: 采用课程强化学习结合多模态大语言模型，使用渐进式训练策略整合精确和粗略标注数据，利用GRPO发展推理能力，并设计分层奖励机制确保时间定位和类别预测一致性。

Result: 在工业数据集和公共基准测试中，RAVEN在违规类别准确性和时间间隔定位方面表现优越，在线A/B测试验证了其实际适用性，显著提升了精确率和召回率。

Conclusion: RAVEN框架有效解决了广告视频违规检测中的关键挑战，展示了强大的泛化能力，并缓解了监督微调相关的灾难性遗忘问题。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 该论文扩展了自然语言推理(NLI)中人类标签变异的研究，通过解释分析揭示了标注者在推理类型和标签选择上的分歧，发现表面标签不一致可能掩盖深层的解释一致性。


<details>
  <summary>Details</summary>
Motivation: 传统NLI数据集分析主要关注标注者标签一致但解释不同的情况，本文旨在更全面地研究标注者在推理过程和标签选择上的个体差异。

Method: 使用LiTEx分类法分析两个英语NLI数据集，从NLI标签一致性、解释相似性和分类法一致性三个维度对齐标注变异，并考虑标注者选择偏见的复合因素。

Result: 发现标注者标签不一致但解释高度相似的实例，表明表面分歧可能掩盖深层的解释一致性；同时揭示了标注者在解释策略和标签选择上的个体偏好。

Conclusion: 推理类型的一致性比标签一致性更能反映自由文本解释的语义相似性，强调需要谨慎对待标签作为绝对真值，重视基于推理的解释的丰富性。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出使用"退出"作为LLM代理的安全机制，让代理在缺乏信心时主动退出，在ToolEmu框架下评估12个先进LLM，结果显示退出指令显著提升安全性且几乎不影响帮助性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂环境中运行时面临严重安全风险，传统不确定性量化方法在多轮交互场景中不足，需要新的安全机制来防止灾难性后果。

Method: 利用ToolEmu框架系统评估12个先进LLM的退出行为，通过添加显式退出指令来让代理在不确定时主动退出任务。

Result: 退出指令使所有模型的安全性平均提升+0.39（0-3分制），专有模型提升+0.64，而帮助性仅平均下降-0.03，显示出优异的安全-帮助性权衡。

Conclusion: 显式退出指令是一种简单有效的安全机制，可作为自主代理在高风险应用中的第一道防线，并能立即部署到现有系统中。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于在线背包问题的自动化智能体系统组合框架，通过动态测试和实时效用建模，在预算约束下优化选择智能体组件，显著提升成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组合方法依赖静态语义检索，存在能力描述不完整、选择决策不考虑能力-成本-实时效用等问题，导致组件重用和组合效果不佳。

Method: 引入结构化自动化框架，将智能体系统组合建模为在线背包问题，使组合器智能体能够系统性地识别、选择和组装最优组件集，综合考虑性能、预算约束和兼容性。

Result: 在5个基准数据集上的实验表明，基于在线背包的组合器始终位于帕累托前沿，相比基线方法显著提高成功率并降低组件成本。单智能体设置下成功率提升达31.6%，多智能体系统中从100+智能体库中选择时成功率从37%提升至87%。

Conclusion: 该方法在多样化领域和预算约束下展现出强大的适应性，证实了基于在线背包的智能体系统组合方法的有效性。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 提出了ReviewGuard系统，使用四阶段LLM驱动框架自动检测和分类有缺陷的同行评审，通过合成数据增强和模型微调提高检测性能，揭示了有缺陷评审的特征和AI生成评审的增长趋势。


<details>
  <summary>Details</summary>
Motivation: 同行评审作为科学守门人面临投稿激增和LLM广泛使用的挑战，有缺陷的评审（包括人类专家和AI系统产生的）威胁同行评审生态系统和学术诚信。

Method: 四阶段LLM驱动框架：1)从OpenReview收集ICLR和NeurIPS论文及评审；2)用GPT-4.1标注评审类型并人工验证；3)通过LLM驱动的合成数据增强解决类别不平衡和数据稀缺问题；4)微调编码器模型和开源LLM。

Result: 构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的语料库。有缺陷评审表现出更低的评分、更高的自报告置信度、减少的结构复杂性和更高的负面情绪比例。AI生成评审自ChatGPT出现后显著增加。

Conclusion: 这是第一个用于检测有缺陷同行评审的LLM驱动系统，为同行评审中的AI治理提供证据，并为维护学术诚信的人机协作提供宝贵见解。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 通过分析LLMs在回答文化相关问题时内部激活路径的重叠程度，研究发现语言对内部表征的影响大于文化因素，特别是在韩国与朝鲜案例中显示出语言相似但内部表征差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多元文化环境中的广泛应用，准确的文化理解变得至关重要。现有评估多关注输出层面，难以揭示响应差异的驱动因素，而电路分析研究覆盖语言少且很少聚焦文化。

Method: 通过测量LLMs在回答语义等价问题时内部激活路径的重叠度，比较两种条件：固定问题语言变化目标国家，以及固定国家变化问题语言。使用同语言国家对来区分语言和文化因素。

Result: 结果显示，同语言跨国家问题的内部路径重叠度高于跨语言同国家问题，表明存在强烈的语言特定模式。韩国-朝鲜对显示出低重叠度和高变异性，证明语言相似性不能保证内部表征的一致性。

Conclusion: LLMs的内部文化理解机制受语言影响显著大于文化因素，语言相似性并不必然导致内部表征的相似性，这对跨文化应用具有重要意义。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: SHALLOW是首个系统分类和量化ASR系统中幻觉现象的基准框架，通过词汇、语音、形态和语义四个维度评估模型行为，在WER较高时能捕捉WER无法区分的细粒度错误模式。


<details>
  <summary>Details</summary>
Motivation: ASR系统中的幻觉会产生与语音信号完全无关但语法语义合理的转录，可能误导下游应用并带来严重风险。传统基于错误的评估指标无法区分语音不准确和幻觉，需要新的评估框架来识别和评估易产生幻觉内容的模型。

Method: 引入SHALLOW基准框架，系统地将ASR幻觉现象分类为四个互补维度：词汇、语音、形态和语义。在每个类别中定义针对性指标，生成可解释的模型行为配置文件。

Result: 评估发现SHALLOW指标在识别质量高（低WER）时与WER强相关，但随着WER增加相关性显著减弱。SHALLOW能在退化和挑战性条件下捕捉WER无法区分的细粒度错误模式。

Conclusion: SHALLOW框架支持对模型弱点的具体诊断，并提供超越聚合错误率的模型改进反馈，为ASR系统幻觉评估提供了有效工具。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出了一种针对乌尔都语AI生成文本检测的新框架，通过构建平衡数据集并微调多语言transformer模型，mDeBERTa-v3-base在测试集上取得了91.29%的F1分数和91.26%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs生成文本能力的增强，区分人类写作和机器生成文本变得困难，特别是对于乌尔都语等低资源语言，缺乏有效的AI文本检测工具。

Method: 构建包含1800篇人类写作和1800篇AI生成文本的平衡数据集，进行详细的语言学和统计分析，并微调三种多语言transformer模型。

Result: mDeBERTa-v3-base模型表现最佳，在测试集上F1分数达91.29%，准确率达91.26%。

Conclusion: 该研究推进了乌尔都语社区打击错误信息和学术不端的努力，并为低资源语言的NLP工具开发做出了贡献。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 使用大型语言模型进行短语结构分析的新方法，通过微调LLMs将句子翻译成对应句法结构，扩展了西班牙语教学工具MiSintaxis的功能


<details>
  <summary>Details</summary>
Motivation: 利用大型神经网络模型在自然语言处理中的进展，探索基于机器学习的句法分析新可能性，特别是扩展西班牙语语法教学工具MiSintaxis的能力

Method: 从Hugging Face仓库选取多个模型，使用AnCora-ES语料库生成的训练数据进行微调，让模型学习将输入句子翻译成对应句法结构

Result: 使用F1分数评估性能，结果显示在短语结构分析方面达到了高准确率

Conclusion: 该方法展示了在句法分析中的潜力，证明了使用微调大型语言模型进行短语结构分析的有效性

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo是一个多智能体协作框架，通过模拟四个专门LLM智能体之间的结构化辩论来提升性能和可解释性，每个智能体代表不同的推理范式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大但缺乏可解释的推理过程，需要一种既能提升性能又能提供明确推理链条的方法。

Method: 使用四个专门化的LLM智能体，每个代表不同的推理范式，通过迭代辩论过程相互挑战和优化初始响应。

Result: 在六个基准测试中，DiMo在统一开源设置下比广泛使用的单模型和辩论基线提高了准确性，尤其在数学任务上提升最大。

Conclusion: DiMo是一个语义感知的Web原生多智能体框架，能够生成语义类型化、URL注释的证据链，为下游系统提供可检查和重用的结构化证明。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 本文提出了Capsule Prompt-Tuning (CaPT)方法，通过将实例感知信息整合到提示学习中，解决了传统提示方法缺乏实例感知信息的问题，实现了参数高效的模型微调。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的学习方法存在两个主要问题：1）依赖繁琐的网格搜索来确定最优提示长度，计算负担重；2）任务感知提示设计缺乏实例感知信息，导致与输入序列的注意力交互受限。

Method: 提出Capsule Prompt-Tuning (CaPT)方法，创新性地将实例感知和任务感知信息整合到单个胶囊提示中，以近乎参数免费的方式实现高效提示学习。

Result: 实验结果显示，CaPT在多种语言任务上表现出优越性能（如在T5-Large上达到84.03%平均准确率），同时保持高参数效率（如在Llama3.2-1B上仅使用0.003%的模型参数）。

Conclusion: CaPT方法通过引入实例感知信息作为'注意力锚点'，有效提升了提示学习的效果和效率，为参数高效微调提供了新的解决方案。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文探讨了思维链（CoT）推理在自然语言理解（NLU）任务中的应用，发现随着模型规模增大，CoT推理从阻碍NLU性能转变为超越直接标签预测，并开发了专门设计的训练方法实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 大多数研究关注思维链在推理任务中的作用，忽视了其在自然语言理解任务中的潜在影响。本文旨在系统探索思维链是否同样能有益于NLU任务。

Method: 构建了NLURC数据集，开发了多种思维链增强方法，并系统探索了这些方法在NLU任务上的适用性。

Result: 发现CoT推理与模型规模呈正相关；大多数思维链增强训练方法表现不如仅标签训练，但有一种专门设计的方法能持续改进性能；使用思维链训练的LLM在未见过的NLU任务上取得显著性能提升，媲美十倍大小的模型。

Conclusion: 思维链方法可以显著提升NLU任务性能，特别是在大规模模型上，同时提供与商业LLM相媲美的可解释性。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了2014-2025年间自然语言处理在心脏病学领域的研究应用，分析了265篇相关文献，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病在现代社会中日益普遍，对全球健康产生重大影响。心脏病是复杂的多因素疾病，相关信息分散在各种文本数据中。NLP技术能够分析这些大量非结构化数据，为医疗专业人员提供更深入的见解，从而革新心脏病的诊断、治疗和预防方法。

Method: 通过查询六个文献数据库，采用严格的筛选流程识别出265篇相关文章，从NLP范式类型、心脏病相关任务类型、心血管疾病类型和数据源类型等多个维度进行分析，并进行时间趋势分析。

Result: 分析显示在各个维度都存在相当大的多样性，证明了NLP在心脏病学领域研究的广度。时间分析揭示了所涵盖的过去十年中NLP方法的演变和变化趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心脏病学中的广泛应用和持续发展。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 研究发现LLMs在搜索增强系统中存在变色龙行为，即在多轮对话中面对矛盾问题时立场不稳定，这威胁到系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 集成LLMs与搜索引擎的系统存在关键漏洞，需要系统性地调查LLMs在面对矛盾问题时的立场转变行为。

Method: 构建包含17,770个问答对的Chameleon基准数据集，涵盖12个争议领域，提出变色龙分数和源重用率两个理论指标来量化立场不稳定性和知识多样性。

Result: 评估显示所有模型都表现出严重的变色龙行为（分数0.391-0.511），GPT-4o-mini表现最差。源重用率与置信度和立场变化呈显著正相关。

Conclusion: LLMs的知识多样性有限导致其病态地顺从查询框架，在医疗、法律和金融等关键领域部署前需要进行全面的连贯性评估。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该论文研究了诗歌中空白空间的重要性，分析了19,000首英语诗歌的空白使用模式，并与AI生成诗歌和未发表诗歌进行比较，探讨了文本处理方法对空白表示的影响。


<details>
  <summary>Details</summary>
Motivation: 空白空间是诗歌形式的关键组成部分，反映了诗人的艺术选择，但在NLP研究中未得到足够重视。论文旨在系统研究诗歌中的空白使用模式及其意义。

Method: 使用诗歌基金会的19,000首英语诗歌语料库，分析4,000位诗人的空白使用，并与51,000首LLM生成诗歌和12,000首未发表诗歌进行比较，同时考察不同时期、诗体和数据源的空白使用差异。

Result: 发现不同文本处理方法会导致诗歌空白空间的显著不同表示，发布了2,800首公共领域诗歌子集用于进一步研究，揭示了空白使用在不同诗歌类型间的差异。

Conclusion: 诗歌空白空间是重要的语义和空间特征，其研究对LLM预训练数据集的构建策略具有重要启示，需要更重视空白在诗歌生成和文本处理中的保留。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 提出了Beacon基准测试，用于测量大语言模型在事实准确性和顺从偏见之间的权衡，揭示了奉承偏见可分解为语言和情感子偏见，并提出了干预方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在奖励优化过程中混淆了帮助性和礼貌顺从，形成了在真实性和奉承之间的结构性权衡，这种潜在的奉承偏见表现为偏好用户同意而非原则性推理。

Method: 引入Beacon单轮强制选择基准测试，独立于对话上下文隔离这种偏见，评估了12个最先进模型，提出了提示级和激活级干预方法来调节这些偏见。

Result: 评估显示奉承偏见可分解为稳定的语言和情感子偏见，每种偏见都随模型容量扩展而增强，干预方法能够在相反方向上调节这些偏见。

Conclusion: Beacon将奉承重新定义为可测量的规范性错误泛化形式，为研究和大规模生成系统中的对齐漂移缓解提供了可重复的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出SCO-PAL方法，通过自博弈提升语言智能体在动态对抗游戏中的策略推理能力，相比基线平均胜率提升约30%，对抗GPT-4达到54.76%胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中因策略推理能力不足而表现不佳，需要无需专家标注数据的自动学习方法。对手选择在对抗环境中对学习性能有显著影响，但相关研究较少。

Method: 提出SCO-PAL方法，通过游戏交互自动学习，分析不同级别对手选择的影响，发现自博弈是最有效的学习方式。

Result: 使用SCO-PAL与自博弈，在6个对抗游戏中相比基线平均胜率提升约30%，对抗GPT-4达到54.76%胜率。

Conclusion: 自博弈是提升对抗环境中策略推理能力的最有效方式，SCO-PAL方法显著提高了语言智能体的游戏表现。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval是一个双语多任务评估基准，用于评估LLM在英语和阿拉伯语中的长上下文理解能力，涵盖4k到128k+token的上下文长度，包含四个新颖任务。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在长上下文理解方面展现复杂能力，需要严格的评估方法来有效评估其性能。

Method: 设计了四个新颖且具有挑战性的任务：多文档问答、双语问答、段落内声明验证和基于长上下文的多选题，涵盖深度推理、文档理解、信息追踪和双语信息提取等能力。

Result: 评估显示LC-Eval带来了显著挑战，即使是GPT-4o等高性能模型在某些任务上也表现困难，突显了基准的复杂性和严谨性。

Conclusion: LC-Eval为长上下文理解提供了一个全面且具有挑战性的评估框架，揭示了当前LLM在该领域的局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC是一个多阶段领域自适应框架，通过联合优化掩码语言建模和对比学习目标，将通用句子嵌入模型有效适配到专业领域。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型在专业领域适配的挑战，需要学习领域相关表示同时保持原始模型的语义判别能力。

Method: 多阶段框架，结合领域特定的掩码监督，联合优化MLM和对比学习目标，采用选择性适配和分阶段训练策略。

Result: 在高资源和低资源领域均取得显著改进，NDCG@10指标相比强通用基线提升高达13.4%。消融研究验证了各组件有效性。

Conclusion: 平衡的联合监督和分阶段适配对于领域自适应至关重要，MOSAIC框架能有效提升句子嵌入模型在专业领域的性能。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: LLMs在实体比较任务中经常依赖启发式偏见而非真实知识，即使拥有正确的数值知识。研究发现实体流行度、提及顺序和语义共现三种启发式偏见强烈影响模型预测。大模型能选择性使用更可靠的数值知识，而小模型则无此辨别能力。思维链提示能引导所有模型使用数值特征。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在知识推理任务中何时依赖真实知识而非表面启发式方法，通过实体比较任务系统分析模型行为。

Method: 使用实体比较任务（如比较河流长度），分析模型预测与数值知识的矛盾，识别三种启发式偏见，并比较不同规模模型的行为差异。

Result: LLMs经常做出与自身数值知识相矛盾的预测；小模型主要依赖启发式偏见，大模型能选择性使用更可靠的数值知识；思维链提示能改善所有模型使用数值特征的能力。

Conclusion: 模型规模影响知识使用策略，大模型能更好地区分何时依赖数值知识，思维链提示是改善模型推理的有效方法。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [42] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 提出两阶段检索-重排框架，使用LLM进行跨体裁作者归属任务，在HIATUS基准上显著超越之前最佳方法


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属需要识别与主题无关的作者特定语言模式，而传统信息检索方法依赖主题线索，不适合此任务

Method: 两阶段框架：检索阶段筛选候选作者，重排阶段使用专门数据策略训练LLM来学习作者区分信号

Result: 在HRS1和HRS2基准上分别获得22.3和34.4绝对Success@8分数提升，大幅超越之前最佳结果

Conclusion: 针对跨体裁作者归属任务设计的数据策略和训练方法能有效提升LLM性能，避免传统IR方法的局限性

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [43] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: CoRUS框架通过角色理论模拟基于用户角色的提问，发现在阿片类药物使用障碍等敏感领域中，不同提问者角色会引发LLMs的系统性响应差异，脆弱角色获得更多支持但知识内容减少。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估大多忽略提问者角色，但在敏感领域如阿片类药物使用障碍中，考虑用户上下文对于提供无偏见、可访问的响应至关重要。

Method: 基于角色理论和在线OUD康复社区帖子，构建提问者角色分类（患者、照护者、从业者），并模拟15,321个嵌入各角色目标、行为和经验的提问。

Result: 模拟问题高度可信且与现实数据相当。评估5个LLMs发现：相比从业者，脆弱角色（患者、照护者）获得更多支持性响应（+17%）但知识内容减少（-19%）。

Conclusion: 用户角色的隐式信号会显著影响模型响应，为对话AI的角色知情评估提供了方法论。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [44] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个多智能体框架，通过CAVM架构、迭代视觉增强机制和两阶段写作框架，自动生成高质量的多模态财务报告。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业的财务报告，这是一个劳动密集且智力要求高的过程。

Method: 采用CAVM架构统一外部数据、工具和智能体；提出迭代视觉增强机制逐步优化原始视觉输出；使用两阶段写作框架扩展分析链为连贯的多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的有效路径。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [45] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出神经群通信（NGC）框架，将神经网络重构为神经群交互的动态系统，通过低维通信减少参数冗余，提升推理能力和模型压缩效果。


<details>
  <summary>Details</summary>
Motivation: 解决现代神经网络规模扩大带来的效率、模块化和可解释性挑战，构建能够学习高效、模块化、可解释表示的大型神经系统。

Method: 将神经网络视为神经群交互的动态系统，权重作为神经状态间的瞬时交互，通过神经群间的迭代通信进行计算，引入神经元稳定性度量来量化序列处理中的稳定模式。

Result: 在大型语言模型中实现NGC，在适度压缩下在复杂推理基准上表现更优，在相同压缩率下优于标准低秩近似和跨层基共享方法。

Conclusion: NGC框架通过结构化神经群动态可能关联高维学习系统中的泛化能力，为构建高效、模块化、可解释的神经网络提供了新思路。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [46] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型在具身知识理解方面并不优于纯文本模型，且在视觉维度表现最差，表明当前多模态模型对物理世界的理解能力有限。


<details>
  <summary>Details</summary>
Motivation: 探究视觉基础是否真的能增强多模态语言模型对具身知识的理解能力，与纯文本模型相比。

Method: 基于心理学感知理论构建具身知识理解基准，包含多种感官模态，通过向量比较和问答任务评估30个先进语言模型。

Result: 视觉语言模型在两种任务中均未超越纯文本模型，且在视觉维度表现最差；向量表示易受词形和频率影响，模型在空间感知推理方面存在困难。

Conclusion: 当前语言模型需要更有效地整合具身知识，以增强对物理世界的理解能力。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [47] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo是一个大规模多语言基准测试，包含8个子任务，覆盖2700+语言，用于评估生成模型的词汇理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准主要针对高/中资源语言，而大多数世界语言缺乏基本语言能力评估。

Method: 利用现有词典、单语数据和双语文本构建8个不同难度的子任务，包括词汇理解和生成能力测试。

Result: 6个SOTA模型在该基准上表现不佳，性能受语言家族、资源丰富度、任务类型和理解vs生成方向影响。

Conclusion: ChiKhaPo旨在促进LLM的大规模多语言基准测试，填补现有评估空白。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [48] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: PROMPT-MII是一个基于强化学习的元学习框架，用于生成紧凑的指令提示，在保持与上下文学习相当性能的同时，大幅减少推理所需的token数量。


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然有效但推理成本高，随着上下文长度增长，计算开销显著增加。需要一种方法能够生成紧凑但描述性的提示来替代完整的训练集。

Method: 提出PROMPT-MII框架，使用强化学习元学习一个指令归纳模型，能够为任意新数据集动态生成紧凑指令。在HuggingFace hub的3000多个多样化分类数据集上进行训练。

Result: 在90个未见任务上评估，PROMPT-MII将下游模型质量提高了4-9个F1点（相对提升10-20%），匹配上下文学习性能同时需要3-13倍更少的token。

Conclusion: PROMPT-MII能够有效替代上下文学习，在保持性能的同时显著降低推理成本，为LLM的高效适应提供了可行方案。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [49] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次将参数高效微调(PEFT)应用于孟加拉语仇恨言论检测，使用LoRA和QLoRA技术在三个大型语言模型上进行微调，在BD-SHS数据集上取得了最高92.23%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体上仇恨言论急剧增加，特别是针对妇女和青少年。现有方法要么计算成本高昂，要么依赖专有API，需要更实用的解决方案。

Method: 使用LoRA和QLoRA参数高效微调技术，在Gemma-3-4B、Llama-3.2-3B和Mistral-7B三个指令调优大语言模型上训练少于1%的参数，基于50,281条标注评论的BD-SHS数据集。

Result: Llama-3.2-3B获得最高F1分数92.23%，Mistral-7B为88.94%，Gemma-3-4B为80.25%，所有实验均在单个消费级GPU上完成。

Conclusion: PEFT被证明是孟加拉语及相关低资源语言仇恨言论检测的实用且可复现的策略。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [50] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一个极简的字节级分词器，直接将文本映射到UTF-8编码对应的字节ID，使用C0控制字节编码特殊行为，提供更快的分词速度、更小的数据传输和可共享的嵌入表。


<details>
  <summary>Details</summary>
Motivation: 解决现有字节级分词器引入超出范围ID或辅助标记的问题，利用ASCII原始设计理念，使用控制字节编码特殊行为，实现更高效的分词方案。

Method: 基于UTF-8字节编码的分词方法，所有特殊行为（填充、边界、对话结构等）都使用C0控制字节编码，无需额外标记，并引入位偏置嵌入技术。

Result: 分词速度提升14倍，主机-设备数据传输减少8倍，嵌入表可跨模型共享，语言建模收敛性得到改善。

Conclusion: UTF8Tokenizer提供了一种高效、简洁的字节级分词方案，通过利用控制字节和位偏置嵌入技术，在性能和实用性方面都有显著优势。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [51] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 该论文提出了一种词汇表压缩方法，通过将词形变化表示为基本词向量和变换向量的组合，从而减少词汇表大小并提高多语言覆盖，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法将词形变化视为独立词符，导致词汇表被表面形式变体填满，限制了低频词和多语言覆盖。

Method: 使用变换向量（加法偏移量）在嵌入空间中表示词形变化，将词汇表重新设计为共享基本词和变换向量的组合形式。

Result: 在多个LLM和五种语言上测试，最多可移除10%的词汇条目，扩展了词汇覆盖范围，对下游任务性能影响最小，且无需修改模型权重。

Conclusion: 研究结果推动词汇表设计从字符串枚举转向利用语言底层结构的组合式词汇表。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [52] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出一种基于强化学习的动态防御框架，通过在线学习对抗迭代越狱攻击，同时引入梯度阻尼技术防止过拟合，显著提升LLM安全性并保持无害任务响应质量


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动破坏迭代越狱攻击的试错循环，需要一种能动态更新防御策略的机制来应对不断演变的攻击

Method: 使用强化学习优化提示，确保无害任务得到适当响应同时明确拒绝有害提示；引入Past-Direction Gradient Damping技术防止对攻击中部分输入重写的过拟合

Result: 在三个LLM上的实验表明，该方法显著优于五种现有防御方法，对抗五种迭代越狱攻击；同时提升了无害任务的响应质量

Conclusion: 提出的动态防御框架能有效对抗迭代越狱攻击，在提升安全性的同时保持模型性能

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [53] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 提出了DiscoTrack基准，这是一个多语言（12种语言）的LLM基准测试，专注于语篇理解中的隐含信息和语用推理，包含四个层次的语篇理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM基准测试主要关注自然语言理解中的显式信息提取（如问答、摘要），缺乏针对隐含信息、语用推理以及跨句子、段落和多个说话者话语的语篇跟踪的更具挑战性的多语言基准。

Method: 开发了DiscoTrack基准，涵盖12种语言和四个语篇理解层次：显著性识别、实体跟踪、语篇关系和桥接推理。

Result: 评估显示，即使是最先进的模型，这些任务仍然具有挑战性。

Conclusion: DiscoTrack基准填补了现有基准在语篇理解和多语言评估方面的空白，为LLM在复杂语篇理解能力方面的评估提供了重要工具。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [54] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文评估了基于大语言模型的搜索代理的安全风险，发现它们比基础LLM更容易产生有害输出，并提出了一种多目标强化学习方法SafeSearch来同时优化安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注提升搜索代理的实用性，但其安全行为尚未得到充分探索。研究发现搜索代理在处理有害查询时比基础LLM更容易产生不安全输出，特别是在经过实用性优化的微调后风险加剧。

Method: 提出SafeSearch方法，采用多目标强化学习，结合最终输出的安全/实用性奖励和查询级别的塑形项，惩罚不安全查询并奖励安全查询。

Result: 实验显示SafeSearch在三个红队测试数据集上将代理的有害性降低了70%以上，同时保持了安全、有帮助的响应，并在问答性能上与仅优化实用性的微调代理相当。

Conclusion: 查询级别的奖励在联合提升安全性和实用性方面具有有效性，证明了联合对齐安全性和实用性的重要性。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [55] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出xLSTM框架，通过余弦相似度门控、自适应特征优先级和原则性类别重平衡，在Jigsaw Toxic Comment基准测试中实现96.0%准确率和0.88宏F1，以15倍更少参数超越BERT。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的模型计算成本高且在少数毒性类别上性能下降的问题，以及经典集成方法缺乏语义适应性的局限。

Method: 使用可学习参考向量通过余弦相似度调节上下文嵌入，集成多源嵌入（GloVe、FastText、BERT CLS），包含字符级BiLSTM、嵌入空间SMOTE和自适应焦点损失。

Result: 在威胁类别上比BERT提升33%，在身份仇恨类别上提升28%，推理延迟50ms，余弦门控在消融实验中贡献+4.8% F1增益。

Conclusion: 轻量级、理论指导的架构可以在不平衡、特定领域的NLP任务上超越大型预训练模型，建立了新的效率适应性前沿。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [56] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 论文研究大语言模型的提示敏感性现象，提出通过语义空间采样和改述扰动来改善不确定性校准，并引入新的不确定性分解指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对语义相同但表述不同的提示会产生不同答案分布，这表明模型输出分布反映的不确定性可能不能准确反映模型对提示含义的不确定性。

Method: 将提示敏感性建模为泛化误差，通过语义概念空间的改述扰动采样来改善不确定性校准，并引入基于语义连续性的新不确定性分解指标。

Result: 语义空间采样改进了不确定性校准而不影响准确性，新分解指标能有效量化提示敏感性导致的LLM不确定性。

Conclusion: 该工作为改善提示敏感语言模型的不确定性校准提供了新方法，并证明某些LLM未能对其输入含义表现出一致的一般推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [57] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 该论文研究了基于推理的大语言模型在思考过程中会加剧社会偏见的问题，发现了两种导致偏见聚集的失败模式：刻板印象重复和不相关信息注入，并提出了一种轻量级的提示方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然基于推理的大语言模型通过内部结构化思考过程在复杂任务上表现出色，但研究发现这种思考过程会加剧社会刻板印象，导致偏见结果。然而，语言模型在社会偏见场景下的底层行为机制尚未得到充分探索。

Method: 系统性地研究了思考过程中导致社会偏见聚集的机制，发现了两种失败模式：1) 刻板印象重复 - 模型依赖社会刻板印象作为主要理由；2) 不相关信息注入 - 模型编造或引入新细节来支持偏见叙述。基于这些发现，提出了一种轻量级的提示缓解方法，让模型根据这些特定失败模式来审查自己的初始推理。

Result: 在问答任务（BBQ和StereoSet）和开放生成任务（BOLD）基准测试上的实验表明，该方法能有效减少偏见，同时保持或提高准确性。

Conclusion: 该研究揭示了语言模型思考过程中导致社会偏见聚集的具体机制，并提出了一种有效的轻量级缓解方法，为减少AI系统中的偏见提供了新的思路。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [58] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过分解任务、建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的验证函数，解决了多智能体协作中的规划、协调和验证挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在应对复杂任务时面临规划、协调和验证的新挑战，执行失败往往源于任务解释、输出格式或智能体间交接的细微偏差，而非单纯的推理缺陷。

Method: VeriMAP框架包括任务分解、子任务依赖建模，以及将规划器定义的通过标准编码为Python和自然语言的子任务验证函数(VFs)。

Result: 在多样化数据集上的评估表明，VeriMAP优于单智能体和多智能体基线，同时增强了系统的鲁棒性和可解释性。

Conclusion: 验证感知规划能够在多智能体系统中实现可靠的协调和迭代优化，而不依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [59] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen是一个开源统一框架，用于训练、评估和可视化动态词汇增强语言模型，解决了现有方法代码库碎片化、缺乏现代LLM支持和推理可扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表训练的语言模型难以泛化到新词或词汇表外词，限制了处理多样化词符组合的灵活性。现有动态词汇方法存在代码库碎片化、缺乏现代LLM支持和推理可扩展性有限等挑战。

Method: 引入DVAGen框架，模块化流水线便于定制，与开源LLM无缝集成，首次提供CLI和WebUI工具进行实时结果检查，支持批处理推理。

Result: 验证了动态词汇方法在现代LLM上的有效性，显著提高了推理吞吐量。

Conclusion: DVAGen通过统一的动态词汇增强框架，解决了现有方法的局限性，为语言模型处理多样化词符提供了更灵活和可扩展的解决方案。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [60] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于提示和基于强化学习的查询增强方法，发现简单的无训练查询增强方法在使用强大LLMs时表现与昂贵的RL方法相当甚至更好。作者提出了一种混合方法OPQE，结合了提示的灵活性和RL的优化目标。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLMs的查询增强主要有两种方法：基于提示的方法和基于强化学习的方法，但缺乏在一致实验条件下的系统比较。

Method: 在多个基准测试上系统比较了提示方法和RL方法，然后提出了一种混合方法OPQE，让LLM策略学习生成能最大化检索性能的伪文档。

Result: 简单的无训练查询增强方法在使用强大LLMs时表现与RL方法相当甚至更好；提出的OPQE方法优于单独的提示方法和RL重写方法。

Conclusion: 结合提示的灵活性和RL的优化目标的协同方法能获得最佳结果，简单的无训练方法在实际应用中具有重要价值。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [61] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究表明人们在理解AI生成的讽刺时不会完全采用意向立场，行为和神经数据都显示人们对AI的讽刺意图归因较弱，倾向于将其视为计算错误而非有意沟通。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地作为社交代理部署并被训练产生幽默和讽刺，需要研究人们是否会对AI采用意向立场，即是否会将心理状态归因于AI来解释其行为。

Method: 通过比较人类和AI来源的讽刺语句的行为和神经反应，使用ERP成分（P200反映早期不一致性检测，P600反映重新解释不一致性的认知努力）。

Result: 行为数据显示参与者对AI的故意沟通归因显著少于人类；神经数据显示AI生成的讽刺引起减弱的P200和P600效应，表明检测和重新分析的努力减少；感知AI更真诚的人对AI讽刺显示出更大的P200和P600效应。

Conclusion: 尽管当前LLMs具有语言复杂性，但实现真正的社交代理需要更多，它需要人类如何感知和归因于人工代理的意向性的转变。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [62] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于分块稀疏注意力的长上下文模型，识别出三个关键设计原则：表达性分块编码器、旁路残差路径和强制选择稀疏性，实现了从4K到3200万token的无训练长度外推。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer受限于二次复杂度，而滑动窗口注意力等替代架构无法有效利用完整上下文。分块稀疏注意力在极端长度泛化方面表现出潜力，但其成功的关键架构原则尚未完全理解。

Method: 通过统一框架和全面消融研究，识别三个核心组件：1) 带专用CLS令牌的表达性非线性分块编码器；2) 旁路残差路径稳定整合检索信息；3) 预训练期间强制选择稀疏性以减少训练-测试分布差距。

Result: 结合这些原则，在RULER和BABILong基准上实现了新的最先进的无训练长度外推性能，成功将4K上下文训练的模型泛化到3200万token。

Conclusion: 研究为开发未来高性能长上下文语言模型提供了一套清晰且经验基础的设计原则。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [63] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出了一种新颖的注意力转移框架，通过重要性感知抑制和注意力引导保留增强来实现选择性遗忘，在保持模型效用的同时有效减少幻觉响应。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法面临两难困境：激进遗忘会损害模型效用，保守策略会保留效用但产生幻觉响应，限制了LLM在知识密集型应用中的可靠性。

Method: 注意力转移框架包含两个注意力级干预：对遗忘集应用重要性感知抑制以减少对记忆知识的依赖；对保留数据集进行注意力引导保留增强以强化对语义关键标记的关注。通过双损失目标联合优化。

Result: 在ToFU基准测试中准确率比最先进方法提高15%，在TDEC基准测试中提高10%，同时保持有竞争力的无幻觉遗忘效果。

Conclusion: AS方法在遗忘效果、泛化能力和响应可靠性之间实现了优越的平衡。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [64] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出了StreamingThinker框架，让大语言模型能够在读取输入的同时进行推理，显著减少推理延迟和token等待时间，同时保持与批量推理相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待完整输入后才能开始思考，这带来了不必要的延迟，并且在动态场景下会减弱对早期信息的注意力。受人类边阅读边思考的认知过程启发，设计了流式思考范式。

Method: StreamingThinker框架包含三个核心组件：流式CoT生成、流式约束训练和流式并行推理。使用流式推理单元进行质量控制的CoT生成，通过流式注意力掩码和位置编码保持推理顺序，利用并行KV缓存实现输入编码与推理生成的解耦。

Result: 在Qwen3模型系列上的实验表明，StreamingThinker在数学推理、逻辑推理和基于上下文的QA推理任务中，保持了与批量推理相当的性能，同时将推理开始前的token等待时间减少了80%，最终答案生成的时间延迟减少了60%以上。

Conclusion: 流式思考范式为LLM推理提供了有效的解决方案，显著降低了推理延迟，同时保持了推理质量，展示了在动态场景下进行高效推理的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [65] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 该论文提出了VideoBiasEval框架，用于评估视频生成模型中的社会偏见，发现对齐调优会放大并稳定化社会偏见。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型通过基于人类偏好的奖励模型进行对齐调优，虽然提升了视觉质量，但可能无意中编码并放大了社会偏见。

Method: 引入VideoBiasEval诊断框架，基于社会偏见分类学，采用基于事件的提示策略，分离语义内容和演员属性，并引入多粒度指标评估偏见。

Result: 研究发现对齐调优不仅强化了表征偏见，还使其在时间上更加稳定，产生更平滑但更刻板的描绘。

Conclusion: 需要在对齐过程中进行偏见感知的评估和缓解，以确保公平和负责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [66] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过大规模情感分析发现孟加拉语新闻标题中负面情绪占主导地位，特别是愤怒、恐惧和失望，不同媒体对相似事件的报道存在显著情感差异。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体通过报道框架影响公众情绪，负面或情绪化的标题往往获得更多关注和传播，这促使媒体采用更能引发强烈反应的报道方式。

Method: 使用Gemma-3 4B模型对30万条孟加拉语新闻标题及其内容进行零样本推理，识别每篇文章的主要情绪和整体基调。

Result: 研究发现负面情绪明显占主导地位，特别是愤怒、恐惧和失望，不同媒体对相似故事的情感呈现存在显著差异。

Conclusion: 基于这些发现，提出了以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中隐藏的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [67] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了Transformer大语言模型的局部可解释性和机制可解释性方法，在医疗和自动驾驶领域进行了实验研究，分析了解释对信任的影响，并总结了当前未解决的问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能优异，但其预测和推理过程对人类不可理解，且经常出现幻觉错误，迫切需要理解和解释语言模型的内在工作机制以建立信任。

Method: 综述了局部可解释性和机制可解释性方法，并在医疗和自动驾驶两个关键领域进行了实验研究，分析解释对接收者信任的影响。

Result: 提出了大语言模型可解释性研究的三个关键贡献：方法综述、领域实验和信任分析、以及未解决问题总结。

Conclusion: 总结了当前LLM可解释性领域未解决的问题，并概述了生成人类对齐、可信赖LLM解释的机遇、关键挑战和未来方向。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [68] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出TaxoAlign方法用于自动生成学术分类法，创建CS-TaxoBench基准数据集，并通过严格评估框架验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述生成方法缺乏与人类专家生成结构的比较，需要弥合人工生成与自动创建分类法之间的差距。

Method: 提出TaxoAlign方法，这是一个基于主题的三阶段指令引导方法，用于学术分类法生成。同时创建包含460个分类法的CS-TaxoBench基准数据集。

Result: TaxoAlign在几乎所有评估指标上都持续超越了基线方法，在结构对齐和语义连贯性方面表现优异。

Conclusion: TaxoAlign方法能够有效生成与人类专家创建的分类法相媲美的学术分类法，为自动文献综述生成提供了可靠工具。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [69] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 该研究使用法语多轮对话数据集CyberAgressionAdo-Large，评估了反社会行为检测、霸凌行为分析和霸凌同伴群体识别三个任务，发现多模态模型优于单模态基线，其中mBERT + WD-SGCN的后期融合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰和网络霸凌）对平台安全和社福祉构成日益增长的风险。现有研究主要集中在X和Reddit等网络，而多轮对话环境由于数据有限而研究不足。

Method: 使用CyberAgressionAdo-Large数据集，评估了6种基于文本和8种基于图的表示学习方法，分析了词汇线索、互动动态及其多模态融合。

Result: 多模态模型优于单模态基线。mBERT + WD-SGCN后期融合模型在反社会行为检测任务上表现最佳（0.718），在同伴群体识别（0.286）和霸凌分析（0.606）任务上也有竞争力。

Conclusion: 多模态方法能有效处理隐式攻击、角色转换和情境依赖性敌意等细微的反社会行为现象。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [70] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出了Nyx，一个用于通用检索增强生成(URAG)的统一混合模态检索器，能够处理文本和图像混合的查询和文档，显著提升视觉语言生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要关注单模态文本，但在现实场景中查询和文档往往包含混合模态（如文本和图像），需要解决通用检索增强生成的挑战。

Method: 1) 构建NyxQA数据集：通过四阶段自动化流程从网络文档生成和过滤混合模态问答对；2) 两阶段训练框架：先在NyxQA和开源检索数据集上预训练，然后使用下游视觉语言模型的反馈进行监督微调。

Result: Nyx不仅在标准文本RAG基准上表现优异，在更通用的URAG设置中表现更佳，显著提升了视觉语言任务的生成质量。

Conclusion: Nyx通过统一的混合模态检索器解决了URAG挑战，在真实世界混合模态场景中有效提升了生成性能。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [71] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型在简单指令执行上存在格式偏见和不一致性，即使模型规模增大，对原子指令的遵循能力仍然不足。


<details>
  <summary>Details</summary>
Motivation: 探索指令调优大语言模型在简单、自包含指令执行方面的能力，这是复杂指令遵循的基础，但目前研究不足。

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个指令调优大语言模型，系统性地改变选项标签格式（字母、数字、罗马数字），在四种范式下测试：显式指令、无指令、移除选项内容、三样本示例。

Result: 标签格式变化导致显著性能波动（如罗马数字vs数字下降30.45%）；无指令时性能进一步下降；移除选项内容时模型无法超越随机基线；三样本示例无显著改善；大模型准确率更高但指令遵循仍不一致。

Conclusion: 当前指令调优范式存在不足，需要专门针对原子指令遵循的评估方法和训练策略。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [72] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出了EduAdapt基准，包含48k个按年级标记的科学问答对，用于评估LLMs在不同年级水平的适应性表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在教育应用中虽然表现良好，但缺乏针对不同年级学生的适应性，无法提供适合学生认知发展水平的回答。

Method: 构建了包含9个科学学科的年级标注问答数据集，评估了多个开源LLMs在不同年级水平上的表现。

Result: 大模型整体表现更好，但在低年级（1-5年级）适应性方面仍有困难。

Conclusion: EduAdapt是首个评估LLMs年级适应性的数据集和框架，旨在推动开发更适合教育发展的AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [73] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: Ladder-base是首个基于GRPO强化学习方法训练的中医领域大语言模型，在推理和事实一致性方面表现优于通用LLM和中医专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系独特且复杂，现有中医LLM在一致性、数据质量和评估标准方面存在局限，需要更有效的对齐方法。

Method: 基于Qwen2.5-7B-Instruct基础模型，使用GRPO强化学习方法在TCM-Ladder基准的文本子集上进行训练，80%数据用于训练，20%用于验证和测试。

Result: Ladder-base在多项推理指标上表现优异，超越了GPT-4、Gemini 2.5等通用LLM以及BenTsao、HuatuoGPT2等中医专用模型。

Conclusion: GRPO为传统医学领域LLM与专家级推理对齐提供了有效策略，支持开发可信赖且临床基础扎实的中医人工智能系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [74] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption是一个用于20种非洲语言的多语言图像描述框架，包含数据集、处理流程和模型，旨在解决多模态AI研究中高资源语言主导的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态AI研究过度集中于高资源语言，阻碍了该领域进展的民主化，需要为非洲等低资源语言提供支持。

Method: 构建基于Flickr8k的数据集，采用上下文感知选择和翻译过程；开发动态上下文保持流程，通过模型集成和自适应替换保证质量；设计0.5B参数的AfriCaption模型，整合SigLIP和NLLB200进行图像描述生成。

Result: 创建了首个可扩展的非洲低资源语言图像描述资源，确保了持续的数据质量。

Conclusion: 该统一框架为真正包容的多模态AI奠定了基础，推动了非洲低资源语言在多模态研究中的代表性。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [75] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 开发了BenCao——基于ChatGPT的中医多模态助手，通过自然语言指令调优而非参数重训练，整合结构化知识库、诊断数据和专家反馈，在中医问答、诊断和草药识别等任务上优于通用和中医领域模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑和多模态诊断线索，现有中医领域大语言模型在文本理解方面有进展，但缺乏多模态整合、可解释性和临床适用性。

Method: 整合1000多部经典和现代文本的知识库，基于场景的指令框架，可解释推理的思维链模拟机制，执业中医师参与的反馈精炼过程，连接舌像分类和多模态数据库检索的外部API。

Result: 在单选题基准和多模态分类任务评估中，BenCao在诊断、草药识别和体质分类方面优于通用和中医领域模型，已在OpenAI GPTs商店部署，截至2025年10月有近1000名全球用户访问。

Conclusion: 研究证明了通过自然语言指令调优和多模态整合开发中医领域大语言模型的可行性，为生成式AI与传统医学推理对齐提供了实用框架和可扩展的现实部署路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [76] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 论文发现对齐训练不仅导致任务准确率下降，还造成严重的校准损失，使模型过度自信、可靠性降低。通过简单的权重插值方法，可以找到帕累托最优解，在提升准确率的同时恢复校准性能。


<details>
  <summary>Details</summary>
Motivation: 传统上"对齐税"主要关注任务准确率的下降，但本文发现对齐训练还会导致严重的校准损失，使模型变得过度自信且输出多样性降低，这影响了模型的可靠性。

Method: 采用简单的后处理干预方法：在模型对齐前后的权重之间进行插值。这种方法计算效率高，不需要重新训练。

Result: 研究发现权重插值过程能够一致地找到帕累托最优解，这些模型不仅准确率超过两个父模型，还显著恢复了在对齐过程中损失的校准性能。

Conclusion: 简单的模型合并提供了一种计算高效的方法来缓解对齐税的全部影响，能够产生更强大且更可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [77] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的搜索模型虽然继承了指令调优的拒绝能力，但存在脆弱性。两种简单攻击（搜索攻击和多搜索攻击）可显著降低模型的安全性能，暴露了当前RL训练忽视查询有害性的核心弱点。


<details>
  <summary>Details</summary>
Motivation: 研究RL训练的搜索模型的安全特性，发现虽然这些模型在多步推理任务上表现出色，但其安全属性尚未被充分理解。

Method: 使用两种简单攻击方法：搜索攻击（强制模型以搜索开始响应）和多搜索攻击（鼓励模型重复搜索），在Qwen和Llama两个模型系列上进行测试，包括本地和网络搜索。

Result: 攻击使拒绝率降低高达60.0%，答案安全性降低82.5%，搜索查询安全性降低82.4%。攻击通过触发模型在生成继承的拒绝token之前生成有害的、镜像请求的搜索查询而成功。

Conclusion: 当前RL训练存在核心弱点：奖励生成有效查询而不考虑其有害性，导致RL搜索模型存在用户可轻易利用的漏洞，迫切需要开发安全感知的代理RL管道来优化安全搜索。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [78] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 开发基于BERT的临床命名实体识别模型，用于从英语、西班牙语和意大利语的心脏病学临床文本中提取疾病和药物实体，在BioASQ MultiCardioNER任务中取得了优于平均水平的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据快速增长，需要从非结构化临床文本中提取生物医学知识来支持数据驱动的临床系统。虽然英语语料库的命名实体识别系统已有显著进展，但针对低资源语言临床文本的研究仍然稀缺。

Method: 探索了多种单语和多语言BERT模型的有效性，这些模型在通用领域文本上训练，用于从英语、西班牙语和意大利语的临床病例报告中提取疾病和药物实体。

Result: 在西班牙语疾病识别(SDR)上获得77.88%的F1分数，西班牙语药物识别(SMR)92.09%，英语药物识别(EMR)91.74%，意大利语药物识别(IMR)88.9%，在所有子任务中都超过了测试排行榜的平均值和中位数。

Conclusion: 基于BERT的深度上下文嵌入模型能够有效提升低资源语言临床文本的命名实体识别性能，为多语言临床NLP应用提供了有前景的解决方案。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [79] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文首次为乌尔都语到英语的习语翻译创建了评估数据集，评估了多种LLM和NMT系统，发现提示工程能提升习语翻译质量，且原生乌尔都语输入比罗马化乌尔都语产生更准确的翻译。


<details>
  <summary>Details</summary>
Motivation: 习语翻译是机器翻译中的重大挑战，特别是对于乌尔都语等低资源语言，此前研究关注有限。

Method: 创建首个乌尔都语到英语习语翻译评估数据集，评估多种开源LLM和NMT系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标评估翻译质量。

Result: 提示工程相比直接翻译能提升习语翻译质量，但不同提示类型间性能差异较小；原生乌尔都语输入比罗马化乌尔都语产生更准确的习语翻译。

Conclusion: 文本表示对翻译质量有显著影响，原生乌尔都语在习语翻译上表现优于罗马化乌尔都语，提示工程是提升习语翻译的有效方法。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [80] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 研究发现多语言大语言模型在医疗问答中存在显著的跨语言事实性差异，即使使用非英语提示，模型回答仍更倾向于与英文维基百科对齐。通过提供非英语维基百科的上下文信息可以有效改善事实对齐。


<details>
  <summary>Details</summary>
Motivation: 将AI整合到医疗保健中需要公平获取可靠健康信息，但不同语言的信息质量存在差异，引发对多语言大语言模型可靠性和一致性的担忧。

Method: 构建多语言维基医疗数据集，分析跨语言医疗覆盖度，评估LLM回答与参考资料的匹配度，并通过上下文信息和检索增强生成进行案例研究。

Result: 发现维基百科覆盖度和LLM事实对齐存在显著跨语言差异，即使使用非英语提示，模型回答仍更倾向于英文维基百科。提供非英语维基百科上下文能有效改善事实对齐。

Conclusion: 研究结果强调了构建更公平的多语言AI医疗系统的实用途径，通过上下文信息可以有效提升事实对齐的文化相关性。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [81] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新型的MoE架构，通过跨层重用专家来改进路由机制，在固定参数预算下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度专家设计受限于层本地路由机制，需要在专家维度和路由多样性之间进行权衡，限制了模型的表达能力。

Method: 提出ReXMoE架构，允许路由器在相邻层之间重用专家，将专家维度与每层预算解耦；采用渐进式缩放路由策略逐步增加候选专家池。

Result: 在0.5B到7B参数规模的不同架构上，ReXMoE在固定架构维度下持续提升了语言建模和下游任务性能。

Conclusion: ReXMoE为参数高效且可扩展的MoE基LLMs提供了新的设计范式。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [82] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法，通过层次亲和树结构建模不同AI-人类协作文本生成过程的关系，使用专门损失函数对齐文本表示，在混合文本检测任务中提升性能，特别是在少样本学习条件下增强鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本检测方法主要采用二元分类或多元分类，无法有效处理AI-人类协作生成文本的复杂特性，这些文本具有不同的生成过程（AI写人编辑、人写AI编辑、AI生成AI优化等），现有方法建模过于粗糙。

Method: 提出DETree方法，将不同生成过程的关系建模为层次亲和树结构，引入专门损失函数对齐文本表示与树结构。构建RealBench基准数据集，包含各种人类-AI协作过程生成的混合文本。

Result: 方法在混合文本检测任务中提升性能，显著增强在分布外场景下的鲁棒性和泛化能力，特别是在少样本学习条件下表现优异，证明了基于训练的方法在OOD设置中的潜力。

Conclusion: DETree通过层次亲和树结构有效建模AI-人类协作文本生成过程的关系，在混合文本检测中取得显著改进，为AI文本检测提供了新的有效方法。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [83] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，分析了从"流程执行系统"到"自适应社会系统"的演进路径。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体已成为AI前沿，但如何将通用智能体研究转化为推动行业变革的生产力仍面临重大挑战。

Method: 使用行业智能体能力成熟度框架，分析支撑智能体能力发展的三大技术支柱：记忆、规划和工具使用，并综述在数字工程、科学发现、具身智能等领域的实际应用。

Result: 构建了行业智能体从简单任务执行到复杂自主系统和集体智能的演进路径，识别了现有评估系统在真实性、安全性和行业特性方面面临的挑战。

Conclusion: 通过结合技术演进与行业实践，为理解和构建下一代行业智能体提供了清晰的路线图和理论基础，探讨了能力边界、发展潜力和治理问题。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [84] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 提出Deep Self-Evolving Reasoning (DSER)概率框架，通过并行运行多个长时程自演化过程，即使验证和修正能力较弱，也能显著扩展小型开源模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 开源小规模模型在验证-修正框架中表现脆弱，无法有效解决奥林匹克级别难题。需要探索在弱验证能力下仍能提升推理性能的方法。

Method: 将迭代推理建模为马尔可夫链，每个步骤代表解空间的随机转移。通过并行运行多个自演化过程，放大微小的改进概率，保证收敛到正确解。

Result: 在DeepSeek-R1-0528-Qwen3-8B模型上，DSER在AIME 2024-2025基准上解决了9个先前无法解决的难题中的5个，整体性能提升，使这个紧凑模型通过多数投票超越了其600B参数教师的单轮准确率。

Conclusion: DSER不仅为测试时扩展提供了实用方法，更重要的是诊断了当前开源推理器的根本局限性，为开发具有强大内在自演化能力的下一代模型指明了研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [85] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 该论文提出了一种学习多语言句子嵌入的方法，通过结合掩码语言建模、翻译语言建模、双编码器翻译排序和加性边际softmax等技术，显著提升了跨语言语义相似性任务的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然BERT在单语句子嵌入学习方面表现优异，但基于BERT的跨语言句子嵌入方法尚未得到充分探索。研究者希望开发一种能够有效处理多语言语义相似性的模型。

Method: 系统研究了学习多语言句子嵌入的方法，结合了掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排名和加性边际softmax等技术。

Result: 引入预训练多语言语言模型可将所需并行训练数据减少80%。在Tatoeba数据集上，112种语言的bi-text检索准确率达到83.7%，远超LASER的65.5%，同时在单语迁移学习基准上保持竞争力。

Conclusion: 提出的多语言句子嵌入模型在跨语言检索任务上表现优异，显著优于现有方法，并且能够用于训练有竞争力的神经机器翻译模型。模型已公开发布，支持109+种语言。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [86] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: EliCal是一个两阶段框架，通过廉价的自我一致性监督来激发内部置信度，然后用少量正确性标注进行校准，实现了接近最优的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 实现大型语言模型的诚实对齐（识别知识边界和表达校准置信度）对于可信部署至关重要，但现有方法需要大规模标注，成本高昂。

Method: 提出EliCal两阶段框架：首先使用廉价的自我一致性监督激发内部置信度，然后用少量正确性标注进行校准。同时发布了包含560k训练和70k评估实例的HonestyBench基准。

Result: EliCal仅使用1k正确性标注（全监督的0.18%）就实现了接近最优的对齐，在未见过的MMLU任务上比仅校准基线表现更好。

Conclusion: EliCal为LLMs的通用诚实对齐提供了一个可扩展的解决方案，显著降低了标注成本。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [87] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是首个大规模标准化基准测试，用于评估LLM模拟人类行为的能力。研究发现当前最佳LLM的模拟能力有限，性能随模型规模对数线性增长，但推理时间计算不会提升性能，且存在对齐-模拟权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM模拟人类行为的评估方法零散且不统一，缺乏标准化基准来系统评估LLM模拟的真实性和可靠性。

Method: 引入SimBench基准，整合20个多样化数据集，涵盖道德决策、经济选择等任务，使用大规模全球参与者数据进行标准化评估。

Result: 当前最佳LLM模拟能力得分仅为40.80/100；性能随模型规模对数线性增长；推理时间计算不改善性能；指令调优在低熵问题上提升性能但在高熵问题上降低性能；模型在模拟特定人口群体时表现较差。

Conclusion: LLM模拟能力与深度知识推理能力强相关，SimBench基准为开发更真实的LLM模拟器提供了可测量的进展标准。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出一个统一的多任务学习框架，将自回归LLM与临床推理对齐，用于MSK-CHORD数据集上的癌症治疗结果预测，通过三种对齐策略提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生物医学NLP中表现良好，但缺乏对高风险决策支持至关重要的结构化推理能力，需要开发既准确又可解释的模型来处理异质临床数据。

Method: 使用统一的多任务学习框架，训练模型同时执行二元生存分类、连续生存时间回归和自然语言理由生成。评估三种对齐策略：标准监督微调、带思维链提示的监督微调、以及基于强化学习的组相对策略优化。

Result: 实验表明，思维链提示将F1提高+6.0%，MAE降低12%；GRPO在BLEU、ROUGE和BERTScore上实现了最先进的可解释性和预测性能。现有生物医学LLM由于架构限制往往无法产生有效的推理轨迹。

Conclusion: 研究强调了在多任务临床建模中推理感知对齐的重要性，为精准肿瘤学中可解释、可信赖的LLM设立了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 该论文提出使用拓扑数据分析工具Mapper来分析语言模型如何内部表示模糊性，发现微调后的模型在嵌入空间中形成模块化、非凸的决策区域，即使对于高度模糊的情况也能保持高预测纯度。


<details>
  <summary>Details</summary>
Motivation: 传统标量指标（如准确率）无法捕捉模型内部如何表示模糊性，特别是当人类标注者存在分歧时。需要新的分析视角来理解模型如何处理模糊实例。

Method: 使用拓扑数据分析工具Mapper分析RoBERTa-Large在MD-Offense数据集上的嵌入空间结构，与传统方法如PCA和UMAP进行比较。

Result: 微调将嵌入空间重组为模块化、非凸区域，与模型预测对齐。超过98%的连通组件表现出≥90%的预测纯度，但在模糊数据中与真实标签的对齐度下降，揭示了结构置信度与标签不确定性之间的隐藏张力。

Conclusion: Mapper是一个强大的诊断工具，能够理解模型如何解决模糊性，超越可视化功能，还能提供拓扑指标，为主观NLP任务的主动建模策略提供信息。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 提出Language Confusion Gate (LCG)，一种轻量级插件解决方案，通过解码时过滤令牌来减少LLM的语言混淆问题，无需重新训练基础模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现语言混淆问题，即文本生成时无意中混合使用不同语言。现有解决方案要么需要重新训练模型，要么无法区分有害的混淆和可接受的语码转换。

Method: 使用语言混淆门(LCG)，这是一种基于规范调整自蒸馏训练的轻量级插件，在解码过程中预测适当的语言家族并仅在需要时应用掩码。

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1在内的各种模型上评估，LCG显著减少了语言混淆，通常降低一个数量级，且不影响任务性能。

Conclusion: LCG提供了一种有效且轻量的解决方案来解决LLM的语言混淆问题，无需修改基础模型，同时保持任务性能。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出HGAdapter方法，通过超图神经网络和适配器调优来捕捉代码中的高阶数据相关性，提升预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在代码任务中表现良好，但未能充分考虑代码内部潜在的高阶数据相关性。

Method: 识别三种代码高阶相关性（抽象语法树家族相关性、词汇相关性、行相关性），设计token和超边生成器，改进超图神经网络架构并与适配器调优结合，提出HGAdapter方法。

Result: 在多个公共数据集上的实验表明，该方法在不同程度上提升了PLMs在代码摘要和代码克隆检测任务中的性能。

Conclusion: 引入高阶数据相关性有助于提高模型在代码任务中的有效性，HGAdapter可插入各种PLMs以增强性能。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出了LawChain框架来显式建模中国侵权民事案件中的法律推理过程，并构建了评估基准LawChain_eval来系统评估大语言模型在侵权法律推理中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法主要依赖通用推理框架，未能全面考察法律推理的细微过程，且研究多集中于刑事案件，对民事案件建模不足。

Method: 开发了LawChain三模块推理框架，将侵权分析中的法律推理过程操作化，并通过提示或后训练方式将LawChain式推理显式融入基线方法。

Result: 当前大模型在侵权法律推理的关键要素处理上仍存在不足，提出的基线方法在侵权相关法律推理中取得显著改进，并能很好地泛化到相关法律分析任务。

Conclusion: 显式建模法律推理链能够有效增强语言模型的推理能力，LawChain框架在法律分析任务中具有良好通用性。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 论文提出了一种改进遗忘学习的方法，通过在遗忘目标中加入上下文效用保护项，使模型在遗忘特定知识后仍能在提示中包含该知识时正确使用它。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法评估只关注目标知识的遗忘程度和保留集性能，但忽略了当被遗忘知识重新出现在提示中时模型能否正确使用的上下文效用问题。

Method: 在现有遗忘学习目标基础上增加一个插件项，保护模型在上下文包含被遗忘知识时仍能使用该知识的能力。

Result: 实验表明该方法能将上下文效用恢复到接近原始水平，同时保持有效的遗忘和保留集效用。

Conclusion: 提出的方法解决了遗忘学习中上下文效用受损的问题，实现了遗忘、保留集效用和上下文效用的平衡。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 开发了一个双语爱尔兰语-英语大语言模型Qomhr'a，在低资源条件下通过双语持续预训练、指令微调和人类偏好对齐的完整流程，显著提升了爱尔兰语和英语的性能表现。


<details>
  <summary>Details</summary>
Motivation: 在低资源条件下开发爱尔兰语-英语双语大语言模型，解决爱尔兰语资源匮乏的问题，同时保持英语能力。

Method: 使用新获取的爱尔兰语语料库和英语文本混合处理，通过双语持续预训练、指令微调和人类偏好对齐的完整流程开发模型。利用Gemini-2.5-Pro生成30K爱尔兰语-英语并行指令微调数据集和1K人类偏好数据集。

Result: 在翻译、性别理解、主题识别和世界知识等基准测试中，爱尔兰语性能提升达29%，英语性能提升达44%。模型在指令跟随方面也取得明显进步。

Conclusion: Qomhr'a模型成功展示了在低资源条件下开发高质量双语大语言模型的可行性，为爱尔兰语的自然语言处理提供了重要工具。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种对话分析方法来评估教育应用中学习者与LLM的互动，强调关注对话动态和教学策略而非仅技术性能。


<details>
  <summary>Details</summary>
Motivation: 现有教育应用评估方法主要关注技术性能或学习成果，忽视了学习者与LLM之间的互动过程，需要填补这一研究空白。

Method: 采用对话分析方法，包括对话数据收集、对话行为标注、对话模式挖掘和预测模型构建四个步骤。

Result: 目前处于早期研究阶段，已获得初步见解，为未来研究奠定基础。

Conclusion: 评估基于LLM的教育应用需要重点关注对话动态和教学策略，对话分析方法为此提供了有效途径。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 提出QueST框架，通过难度感知图采样和拒绝微调直接优化专用生成器来创建具有挑战性的编程问题，显著提升大语言模型在竞争性编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限（仅数千到数万问题），且缺乏大规模、具有挑战性的编程问题训练数据，限制了语言模型的扩展性。

Method: 结合难度感知图采样和难度感知拒绝微调，训练专用生成器创建挑战性编程问题，然后通过蒸馏强教师模型或强化学习来训练小模型。

Result: 使用QueST生成的10万个困难问题微调Qwen3-8B-base后，在LiveCodeBench上超越原始Qwen3-8B性能；额外使用28K人类编写问题配对的11.2万个示例后，8B模型性能与671B的DeepSeek-R1相当。

Conclusion: 通过QueST生成复杂问题为推进大语言模型在竞争性编程和推理方面的前沿提供了有效且可扩展的方法。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出轻量级少样本NER框架，通过改进指令调优模板和数据增强技术，在低资源场景下实现高性能命名实体识别


<details>
  <summary>Details</summary>
Motivation: 解决低资源场景下NER任务标注数据不足的问题，现有零样本和指令调优方法在领域特定实体上泛化能力差且无法有效利用有限数据

Method: 1) 新指令调优模板简化输出格式，利用先进LLM的大上下文窗口；2) 战略数据增强技术保持实体信息同时改写上下文，扩展训练数据而不损害语义关系

Result: 在基准数据集上性能达到SOTA水平，少样本方法在CrossNER数据集上平均F1得分为80.1，数据增强版本比基线F1分数提升最高17分

Conclusion: 为拥有有限NER训练数据和计算资源的群体提供了有前景的解决方案

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了AcademicEval，一个用于评估LLM在长上下文生成任务上的实时基准，使用arXiv论文构建学术写作任务，无需人工标注，确保无标签泄漏。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文LLM基准存在上下文长度固定、标注劳动密集、标签泄漏等问题，需要开发更有效的评估方法。

Method: 采用arXiv论文构建学术写作任务（标题、摘要、引言、相关工作），集成高质量专家策展的少样本演示，支持灵活上下文长度，实现高效实时评估。

Result: LLM在具有层次抽象级别的任务上表现不佳，且难以处理长少样本演示，突显了基准的挑战性。

Conclusion: AcademicEval为评估LLM长上下文建模能力提供了有效基准，通过实验分析揭示了增强LLM长上下文建模能力的见解。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出了一种使用二元检索增强奖励的在线强化学习方法，有效减少语言模型的外在幻觉，在保持其他任务性能的同时显著提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 语言模型经常生成训练数据不支持的事实错误信息（外在幻觉），现有缓解方法往往会影响开放生成和下游任务性能，限制了实际应用。

Method: 采用在线强化学习方法，使用新颖的二元检索增强奖励机制：只有当模型输出完全正确时奖励为1，否则为0。

Result: 在开放生成任务中，幻觉率降低39.3%；在短形式问答任务中，PopQA和GPQA的错误答案分别减少44.4%和21.7%；且未影响指令遵循、数学和代码任务性能。

Conclusion: 二元检索增强奖励方法在显著提升事实准确性的同时，避免了连续奖励强化学习带来的性能下降，实现了更好的实用性。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该调查通过自主性层级框架(L0-L3)重新构建医学大语言模型评估，将现有基准与各层级允许的操作和风险对齐，提出基于层级的评估蓝图，推动领域从基于分数的声明转向临床实际使用的可信证据。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在标准基准测试中表现良好，但这些结果向临床工作流程中安全可靠性能的转移仍面临挑战，需要更贴近实际临床使用的评估方法。

Method: 采用自主性层级框架(L0-L3)：信息工具、信息转换与聚合、决策支持、监督代理，将现有基准和指标与各层级的允许操作和风险对齐。

Result: 提出了基于层级的评估蓝图，包括指标选择、证据收集和声明报告的方法，以及将评估与监督联系的方向。

Conclusion: 通过以自主性为中心，该调查推动领域超越基于分数的声明，转向为真实临床使用提供可信、风险感知的证据。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 提出了FARE系列基础自动推理评估器，通过大规模数据驱动方法训练，在多个评估任务中超越现有评估器性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成评估器主要关注新方法如强化学习，而忽视大规模数据驱动开发。本文专注于数据扩展，构建包含250万样本的数据集来训练更有效的评估器。

Method: 使用简单的迭代拒绝采样监督微调方法，在包含5种评估任务和多个推理评估领域的250万样本数据集上训练8B和20B参数的评估器。

Result: FARE-8B挑战了更大的专业RL训练评估器，FARE-20B为开源评估器设定了新标准，超越了专业的70B+评估器。在实际应用中，FARE-20B在MATH上达到接近oracle性能，在RL训练中提升下游模型性能达14.1%。

Conclusion: 大规模数据驱动方法能够训练出超越现有方法的强大评估器，FARE在静态基准和实际应用中都表现出色，为评估器开发提供了新的方向。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了可执行知识图谱(xKG)，这是一个模块化、可插拔的知识库，用于解决LLM智能体复制AI研究的挑战。xKG整合了从科学文献中提取的技术见解、代码片段和领域知识，显著提升了研究复现性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码方面存在困难，主要原因是背景知识不足和RAG方法的局限性，无法捕捉参考文献中的潜在技术细节。此外，先前方法忽略了有价值的实现级代码信号，缺乏支持多粒度检索和重用的结构化知识表示。

Method: 提出可执行知识图谱(xKG)，自动从科学文献中整合技术见解、代码片段和领域特定知识。xKG是一个模块化、可插拔的知识库，支持多粒度检索和重用。

Result: 在三个智能体框架和两种不同LLM上集成xKG后，在PaperBench上显示出显著的性能提升（使用o3-mini时提升10.9%），证明了其作为自动化AI研究复现的通用和可扩展解决方案的有效性。

Conclusion: xKG是一个有效且可扩展的解决方案，能够显著提升LLM智能体在AI研究复现任务中的性能，解决了现有方法在知识表示和代码生成方面的局限性。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了企业深度研究(EDR)多智能体系统，通过主规划智能体、四个专业搜索智能体、可扩展工具生态系统、可视化智能体和反思机制，实现企业环境下非结构化数据的自动化研究和报告生成。


<details>
  <summary>Details</summary>
Motivation: 企业面临非结构化数据爆炸式增长的压力，现有自主智能体在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: 构建包含主规划智能体、四个专业搜索智能体(通用、学术、GitHub、LinkedIn)、MCP工具生态系统、可视化智能体和反思机制的多智能体系统。

Result: 在DeepResearch Bench和DeepConsult等开放基准测试中，EDR在无需人工干预的情况下优于最先进的智能体系统。

Conclusion: EDR框架能够有效处理企业深度研究任务，通过多智能体协作实现自动报告生成和实时流式处理，并发布了框架和基准轨迹以推动多智能体推理应用研究。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [104] [Comparison of Tax and Cap-and-Trade Carbon Pricing Schemes](https://arxiv.org/abs/2510.15941)
*Stéphane Crépey,Samuel Drapeau,Mekonnen Tadese*

Main category: econ.GN

TL;DR: 比较碳税和排放交易系统（ETS）在金融中介参与下的经济与环境表现，发现两者在完全竞争下等价，但ETS中的中介会降低监管财富和总财富。


<details>
  <summary>Details</summary>
Motivation: 探索金融中介在排放交易市场中的作用，这是碳定价政策实践中较少被研究的维度。

Method: 建立统一框架比较碳税和市场机制，校准两种工具以实现相同的总排放目标，评估不同市场结构下的经济表现。

Result: 两种方案在完全竞争下等价，但ETS中的中介会通过影响价格形成和占用部分收入流，降低监管财富和总财富。

Conclusion: 碳市场设计需考虑中介行为，需要进一步实证研究排放交易系统的制度结构演变。

Abstract: Carbon pricing has become a central pillar of modern climate policy, with
carbon taxes and emissions trading systems (ETS) serving as the two dominant
approaches. Although economic theory suggests these instruments are equivalent
under idealized assumptions, their performance diverges in practice due to
real-world market imperfections. A particularly less explored dimension of this
divergence concerns the role of financial intermediaries in emissions trading
markets. This paper develops a unified framework to compare the economic and
environmental performance of tax- and market-based schemes, explicitly
incorporating the involvement of financial intermediaries. By calibrating both
instruments to deliver identical aggregate emission reduction targets, we
assess their economic performance across alternative market structures. Our
results suggest that although the two schemes are equivalent under perfect
competition, the presence of intermediaries in ETS reduces both regulatory
wealth and the aggregate wealth of economic agents relative to carbon taxation.
These effects stem from intermediaries' influence on price formation and their
appropriation of part of the revenue stream. The findings underscore the
importance of accounting for intermediaries' behavior in the design of carbon
markets and highlight the need for further empirical research on the evolving
institutional structure of emissions trading systems.

</details>


### [105] [Data for Inclusion: The Redistributive Power of Data Economics](https://arxiv.org/abs/2510.16009)
*Diego Vallarino*

Main category: econ.GN

TL;DR: 评估在金融排斥经济中扩大正面信用信息获取对再分配和效率的影响，发现更广泛的数据共享能显著降低金融成本、压缩利率差异并减少信用负担不平等。


<details>
  <summary>Details</summary>
Motivation: 研究在金融排斥经济中扩大正面信用信息获取的再分配和效率影响，探索信用数据作为公共资产对金融包容和减贫的变革性意义。

Method: 使用乌拉圭2021年家庭调查微观数据，模拟三种数据制度：仅负面信息、部分正面信息（Score+）和合成完全可见性，评估其对信贷获取、利息负担和不平等的影响。

Result: 更广泛的数据共享显著降低金融成本，压缩利率分散度，降低信用负担的基尼系数。部分可见性使部分人群受益，而完全合成访问提供最公平和高效的结果。

Conclusion: 信用数据应被视为非竞争性公共资产，对金融包容和减贫具有变革性意义，完全合成访问能实现最公平和高效的金融体系。

Abstract: This paper evaluates the redistributive and efficiency impacts of expanding
access to positive credit information in a financially excluded economy. Using
microdata from Uruguay's 2021 household survey, we simulate three data regimes
negative only, partial positive (Score+), and synthetic full visibility and
assess their effects on access to credit, interest burden, and inequality. Our
findings reveal that enabling broader data sharing substantially reduces
financial costs, compresses interest rate dispersion, and lowers the Gini
coefficient of credit burden. While partial visibility benefits a subset of the
population, full synthetic access delivers the most equitable and efficient
outcomes. The analysis positions credit data as a non-rival public asset with
transformative implications for financial inclusion and poverty reduction.

</details>


### [106] [Development finance institutions (DFIs), political conditions, and foreign direct investment (FDI) in Sub-Saharan Africa](https://arxiv.org/abs/2510.16472)
*Carmen Berta C. De Saituma Cagiza,Ilidio Cagiza*

Main category: econ.GN

TL;DR: 本研究使用1990-2018年撒哈拉以南非洲五国的面板数据，分析发展金融机构(DFIs)与外国直接投资(FDI)对经济发展的动态关系。研究发现DFIs对FDI的理论正向影响在统计上不显著，受区域经济差异影响。基础设施投资对FDI影响最大。


<details>
  <summary>Details</summary>
Motivation: 探究发展金融机构是否通过促进外国直接投资来推动经济增长，并助力实现可持续发展目标，特别是在撒哈拉以南非洲地区。

Method: 使用1990-2018年尼日利亚、加纳、肯尼亚、南非和津巴布韦五国的年度面板数据，在STATA中采用固定效应模型进行定量分析。

Result: DFIs对FDI的理论正向影响在统计上不显著，存在区域经济差异的上下文依赖性。基础设施领域的DFI投资对FDI影响最大，其次是农业综合企业和金融部门。

Conclusion: 研究强调了针对区域差异制定针对性政策的重要性，需要加强制度和宏观经济条件以优化DFIs对FDI和可持续发展的影响。

Abstract: This study investigates the dynamic relationship between development finance
institutions (DFIs), foreign direct investment (FDI), and economic development
in Sub-Saharan Africa (SSA) from 1990 to 2018, using a quantitative panel
dataset of annual data for five SSA countries (Nigeria, Ghana, Kenya, South
Africa, and Zimbabwe) and a fixed-effects model estimated in STATA.
Specifically, the analysis examines whether DFIs enhance FDI inflows, thereby
promoting economic growth and contributing to the achievement of the
Sustainable Development Goals (SDGs). The findings indicate that although DFIs
have a theoretically positive impact on FDI, this relationship is not
statistically significant across the sample, suggesting contextual dependencies
influenced by regional economic variations. The study also analyzes how
economic growth, trade openness, inflation, political stability, and the rule
of law influence this nexus, elucidating their roles in shaping investment
climates. A sectoral analysis indicates that DFI investments in infrastructure,
agribusiness, and finance significantly affect FDI, with infrastructure having
the greatest impact owing to its foundational role in economic systems. This
research contributes by linking DFIs with FDI in SSA in a panel setting, thus
providing a framework for policymakers to strengthen institutional and
macroeconomic conditions to optimize the impact of DFIs on FDI and, ultimately,
on sustainable development. The findings underscore the need for targeted
policies to address regional disparities and enhance DFI effectiveness in
fostering sustainable growth.

</details>


### [107] [Income Taxes, Gross Hourly Wages, and the Anatomy of Behavioral Responses: Evidence from a Danish Tax Reform](https://arxiv.org/abs/2510.16483)
*Kazuhiko Sumiya,Jesper Bagger*

Main category: econ.GN

TL;DR: 本文使用丹麦行政数据和引入联合征税的税改，通过配偶收入识别，提供了收入税对小时工资影响的准实验证据。研究发现低收入工人的工资对税收有负面动态反应，弹性为0.4；中等收入工人影响较小且不显著。工资响应主要通过晋升或换工作实现，而非工作时长变化。


<details>
  <summary>Details</summary>
Motivation: 研究收入税如何影响工资，特别是通过准实验方法识别因果效应，理解税收政策对劳动力市场的实际影响。

Method: 利用丹麦行政数据和税改政策，采用配偶收入作为识别策略，使用非参数双重差分图形证据分析丈夫群体的工资反应。

Result: 低收入工人的工资对净边际税率弹性为0.4，响应主要通过晋升或工作转换实现；中等收入工人影响不显著；工作时长无显著变化。

Conclusion: 收入税主要通过影响小时工资而非劳动供给来影响年收入，这对税收政策设计具有重要启示。

Abstract: This paper provides quasi-experimental evidence on how income taxes affect
gross hourly wages, utilizing Danish administrative data and a tax reform that
introduced joint taxation. Exploiting spousal income for identification, we
present nonparametric, difference-in-differences graphical evidence among
husbands. For low-income workers, taxes have negative and dynamic effects on
wages; their wage elasticity with respect to net-of-marginal-tax rates is 0.4.
For medium-income workers, the effects are smaller and insignificant. Wages
respond to taxes through promotions or job-to-job transitions. Neither daily
nor annual hours worked respond significantly; consequently, annual earnings
respond to taxes primarily through hourly wages, rather than through labor
supply.

</details>


### [108] [The Crisis Simulator for Bolivia (KISr-p): An Empirically Grounded Modeling Framework](https://arxiv.org/abs/2510.16537)
*Ricardo Alonzo Fernández Salguero*

Main category: econ.GN

TL;DR: 本文介绍了玻利维亚危机模拟器(KISr-p)，这是一个基于凯恩斯跨期综合理论(KIS-CES)的季度随机模型，用于评估高不确定性和结构性约束下的宏观经济政策影响。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够评估高不确定性和结构性约束环境下宏观经济政策影响的模拟器，不同于标准的一般均衡框架，该模型基于大量元分析的实证发现。

Method: 采用凯恩斯跨期综合理论(KIS)与恒定替代弹性(KIS-CES)生产函数相结合的理论架构，详细校准了实际、财政、货币、外部、劳动力和分配等模型模块，参数基于财政乘数层级、生产要素互补性、劳动力市场垄断势力等实证证据。

Result: 模拟结果突出了财政调整、外部融资、债务重组和结构性改革之间的权衡关系。情景分析表明，优先考虑支出构成而非总量水平、并承认制度摩擦的务实政策方法，相比教条式的一刀切方案能产生更优的宏观经济和福利结果。

Conclusion: 该危机模拟器能够生成非线性动态，如状态依赖乘数、对冲击的不对称响应和商业周期相位相互作用，为政策制定提供了更现实的评估工具。

Abstract: This document presents a detailed technical report of the ``Crisis Simulator
for Bolivia (KISr-p),'' a quarterly stochastic model designed to evaluate the
impact of various macroeconomic policy strategies in an environment of high
uncertainty and structural constraints. Unlike standard general equilibrium
frameworks, this simulator is grounded in the consolidated empirical findings
of a vast collection of meta-analyses, adopting the theoretical architecture of
a Keynesian Intertemporal Synthesis (KIS) with a Constant Elasticity of
Substitution (KIS-CES) production function. The calibration of each model block
-- real, fiscal, monetary, external, labor, and distributional -- is described
in detail, with parameters justified by quantitative evidence on the hierarchy
of fiscal multipliers (Gechert and Rannenberg, 2018), the complementarity of
production factors (Gechert et al., 2022), monopsony power in the labor market
(Sokolova and S{\o}rensen, 2021), and the dynamics of exchange rate and
interest-rate pass-through. The model integrates these empirical regularities
to generate non-linear dynamics such as state-dependent multipliers, asymmetric
responses to shocks, and business-cycle phase interactions. Simulation results
highlight the trade-offs between fiscal adjustment, external financing, debt
restructuring, and structural reforms -- such as aggressive spending
reallocation and targeted public investment. Scenarios show that pragmatic
policy approaches that prioritize the \textit{composition} of spending over its
aggregate level and that recognize institutional frictions yield superior
macroeconomic and welfare outcomes compared to doctrinaire, one-size-fits-all
prescriptions.

</details>


### [109] [Evaluating the Public Pay Gap: A Comparison of Public and Private Sector Wages in France](https://arxiv.org/abs/2510.16626)
*Riddhi Kalsi*

Main category: econ.GN

TL;DR: 该研究解决了公共与私营部门工资文献中的经验难题，揭示了为什么使用类似数据的研究会得出关于工资溢价和惩罚的矛盾结论。通过分析法国行政面板数据，发现虽然小时工资差距不大，但终身收入和就业稳定性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 解决公共与私营部门工资文献中的矛盾结论，理解为什么相似数据会得出关于工资溢价和惩罚的不同结果。

Method: 使用2012-2019年法国行政面板数据，通过期望最大化算法灵活建模部门转换、就业进出转换和收入异质性。

Result: 女性在公共部门获得显著的终身收入优势，主要由于更高的留任率、更好的兼职工作补偿和更公平的年度工时；而高学历男性在公共部门经历终身惩罚；工资溢价和惩罚系统地取决于性别、教育程度和劳动力市场经验。

Conclusion: 该研究通过提供关于部门差异、兼职工作和工资的全面描述性分析，统一了现有叙述，揭示了工资动态中仍存在显著的未观察异质性。

Abstract: This paper resolves the empirical puzzle in the public-private wage
literature: why studies using similar data reach contradictory conclusions
about wage premiums and penalties. Utilizing rich French administrative panel
data (2012-2019), this study has two main contributions: first, it presents a
set of new, intuitive yet previously undocumented stylized facts about wage
dynamics, sectoral mobility, and gender differences across sectors. The results
reveal that the modest hourly wage gaps conceal substantial disparities in
lifetime earnings and employment stability. Women, in particular, gain a
significant lifetime earnings advantage in the public sector, driven by higher
retention, better-compensated part-time work, and more equitable annual hours
compared to the private sector, where gender gaps remain larger, especially for
those with higher education. In contrast, highly educated men experience a
lifetime penalty in public employment due to rigid wage structures. By flexibly
modeling sectoral transitions, transitions into and out of employment, and
earnings heterogeneity using an Expectation-Maximization algorithm, this study
shows that both premiums and penalties depend systematically on gender,
education, and labor market experience. The analysis reveals that significant
unobserved heterogeneity remains in wage dynamics. These findings unify
prevailing narratives by providing a comprehensive, descriptive account of
sectoral differences in transitions, part-time work and wages by gender.

</details>


### [110] [New Demand Economics](https://arxiv.org/abs/2510.17121)
*Fenghua Wen,Xieyu Yin,Chufu Wen*

Main category: econ.GN

TL;DR: 该论文提出了物质丰裕时代的需求经济学理论，认为经济增长的新引擎在于需求层次升级，而非传统总需求不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统经济学关注总需求不足问题，但在物质丰裕时代，增长约束已转变为需求层次升级不足。需要重新思考经济增长的驱动机制。

Method: 构建了一个可估计的一般均衡框架，分析教育驱动的效用管理如何影响需求层次升级。

Result: 研究发现更高层次需求产生更大的价值创造乘数效应，教育通过改变社会效用函数，提升高层次商品效用，引导资源向高价值领域配置。

Conclusion: 政策应从短期总需求刺激转向以教育为中心、长期视野的人力资本投资，通过需求层次升级驱动经济增长。

Abstract: We develop a theory of demand economics for an era of material abundance. The
binding constraint on growth has shifted from insufficient aggregate demand to
inadequate demand-tier upgrading. Our result is that, the new engine of growth
lies in upgrading the demand hierarchy: higher-tier demands generate larger
value-creation multipliers. The key mechanism is education-driven utility
management. Education transforms the social utility function, raises the
utility of higher-tier goods, and directs resources toward higher-value
domains; this warrants a policy reorientation away from short-run aggregate
stimulus toward education-centered, long-horizon investments in human capital.
Methodologically, we build an estimable general-equilibrium framework.

</details>


### [111] [Universalization and the Origins of Fiscal Capacity](https://arxiv.org/abs/2510.17481)
*Esteban Muñoz-Sobrado*

Main category: econ.GN

TL;DR: 该论文提出了一个基于普遍化推理的税收遵从和财政能力模型，公民通过想象如果每个人都类似行动时的后果来部分内化逃税的影响，将遵从决策与公共支出感知有效性联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在制度薄弱情况下，道德因素如何帮助国家摆脱低财政能力陷阱，分析公民道德内化对税收遵从和财政能力的影响机制。

Method: 建立理论模型，其中公民基于普遍化推理进行道德内化，精英在公共产品和私人租金间分配资源，分析均衡状态下道德如何影响税收遵从和财政能力。

Result: 研究发现公民的道德内化扩大了可行税基，促使精英将资源分配给公共产品而非私人占有；当公共支出价值不确定时，道德能够实现可信改革，高价值精英可通过提供公共产品发出信号，提高公民遵从度。

Conclusion: 该分析识别了一个道德渠道，即使在制度薄弱的情况下，国家也能通过这一渠道摆脱低财政能力陷阱，道德内化在提升财政能力中发挥关键作用。

Abstract: This paper proposes a model of tax compliance and fiscal capacity grounded in
universalization reasoning. Citizens partially internalize the consequences of
concealment by imagining a world in which everyone acted similarly, linking
their compliance decisions to the perceived effectiveness of public spending. A
selfish elite chooses between public goods and private rents, taking compliance
as given. In equilibrium, citizens' moral internalization expands the feasible
tax base and induces elites to allocate resources toward provision rather than
appropriation. When the value of public spending is uncertain, morality enables
credible reform: high-value elites can signal their type through provision,
prompting citizens to increase compliance and raising fiscal capacity within
the same period. The analysis thus identifies a moral channel through which
states may escape low-capacity traps even under weak institutions.

</details>


### [112] [Are penalty shootouts better than a coin toss? Evidence from European football](https://arxiv.org/abs/2510.17641)
*László Csató,Dóra Gréta Petróczy*

Main category: econ.GN

TL;DR: 基于2000-2025年欧足联俱乐部比赛的所有点球大战数据，研究发现点球大战结果无法通过踢球顺序、比赛场地或心理势头来预测，强队表现也不优于弱队，点球大战相当于完美抽签。


<details>
  <summary>Details</summary>
Motivation: 受欧足联2021/22赛季取消客场进球规则的启发，研究旨在探索欧足联俱乐部比赛中点球大战结果是否可预测。

Method: 分析2000年至2025年欧足联俱乐部比赛中的所有点球大战数据，使用Elo评分定义球队强弱。

Result: 未发现踢球顺序、比赛场地和心理势头对点球大战结果有显著影响，强队表现不优于弱队。

Conclusion: 在顶级欧洲足球比赛中，点球大战等同于完美抽签，结果完全随机不可预测。

Abstract: Penalty shootouts play an important role in the knockout stage of major
football tournaments, especially since the 2021/22 season, when the Union of
European Football Associations (UEFA) scrapped the away goals rule in its club
competitions. Inspired by this rule change, our paper examines whether the
outcome of a penalty shootout can be predicted in UEFA club competitions. Based
on all shootouts between 2000 and 2025, we find no evidence for the effect of
the kicking order, the field of the match, and psychological momentum. In
contrast to previous results, stronger teams, defined first by Elo ratings, do
not perform better than their weaker opponents. Consequently, penalty shootouts
are equivalent to a perfect lottery in top European football.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [113] [Rethinking Arrow--Debreu: A New Framework for Exchange, Time, and Uncertainty](https://arxiv.org/abs/2510.16003)
*Nizar Riane*

Main category: econ.TH

TL;DR: 本文提出有效贸易模型(ETM)，重新审视Arrow-Debreu一般均衡框架，强调理论市场交互与实际可实现交易的区别，证明纳什均衡存在性，并质疑福利经济学基础。


<details>
  <summary>Details</summary>
Motivation: 重新审视经典一般均衡理论，强调理论市场交互与实际交易可行性之间的差距，为微观基础、货币动态和时间一致性提供更现实的行为和结构基础。

Method: 开发有效贸易模型(ETM)，基于双边可行性而非总供需，纳入生产、货币和网络拓扑，使用条件模态建模预期，考虑交易约束、主观定价和分散谈判。

Result: 证明纳什均衡存在性，显示均衡由交易约束、主观定价和分散谈判塑造，而非普遍市场出清条件，可贷资金和汇率内生出现。

Conclusion: ETM为经典一般均衡理论提供了行为和结构基础更现实的替代方案，质疑福利经济学理论基础，统一微观基础、货币动态和时间一致性。

Abstract: This paper revisits the Arrow-Debreu general equilibrium framework through
the lens of effective trade, emphasizing the distinction between theoretical
and realizable market interactions. We develop the Effective Trade Model (ETM),
where transactions arise from bilateral feasibility rather than aggregate
supply and demand desires. Within this framework, we establish the main
properties of the price-demand correspondence and prove the existence of Nash
equilibria, incorporating production, money, and network topology. The analysis
extends to time, uncertainty, and open economies, revealing how loanable funds
and exchange rates emerge endogenously. Our results show that equilibrium is
shaped by transaction constraints, subjective pricing, and decentralized
negotiation, rather than by universal market-clearing conditions, and thereby
call into question the foundations of welfare theory. Anticipation is modeled
via the conditional mode, capturing bounded rationality and information
limitations in contrast to the rational expectations hypothesis. The ETM thus
offers a behaviorally and structurally grounded alternative to classical
general equilibrium, bridging microfoundations, monetary dynamics, and temporal
consistency within a unified framework.

</details>


### [114] [Collective Experimentation with Correlated Payoffs](https://arxiv.org/abs/2510.16608)
*Kailin Chen*

Main category: econ.TH

TL;DR: 研究具有相关收益的指数型多臂赌博机模型，分析代理人在集体决策中如何通过投票阈值影响实验和信息聚合。


<details>
  <summary>Details</summary>
Motivation: 扩展Strulovici(2008)的独立收益假设，研究当代理人收益相关时，集体决策中的实验行为和信息聚合机制。

Method: 构建指数型赌博机模型，代理人通过个人学习和观察他人投票行为来获取信息，并在投票决策中考虑自身的关键性。

Result: 当代理人数目较大时，提高实施风险行动的投票阈值k会增加实验行为，但只有当k足够低时，关于R整体可取性的信息才能有效聚合。

Conclusion: 投票阈值在集体决策中具有双重作用：高阈值促进实验，但低阈值才能确保信息有效聚合，这为制度设计提供了重要启示。

Abstract: This paper studies an exponential bandit model in which a group of agents
collectively decide whether to undertake a risky action $R$. This action is
implemented if the fraction of agents voting for it exceeds a predetermined
threshold $k$. Building on Strulovici (2008), which assumes the agents' payoffs
are independent, we explore the case in which the agents' payoffs are
correlated. During experimentation, each agent learns individually whether she
benefits from $R$; in this way, she also gains information about its overall
desirability. Furthermore, each agent is able to learn indirectly from the
others, because in making her decisions, she conditions on being pivotal (i.e.,
she assumes her vote will determine the collective outcome). We show that, when
the number of agents is large, increasing the threshold $k$ for implementing
$R$ leads to increased experimentation. However, information regarding the
overall desirability of $R$ is effectively aggregated only if $k$ is
sufficiently low.

</details>


### [115] [Preference Measurement Error, Concentration in Recommendation Systems, and Persuasion](https://arxiv.org/abs/2510.16972)
*Andreas Haupt*

Main category: econ.TH

TL;DR: 该论文分析了基于噪声偏好测量的算法推荐对市场集中度和不平等的影响。通过贝叶斯说服框架，研究发现对称噪声会增加市场集中度并加剧消费者福利不平等。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨推荐系统中基于噪声偏好测量的算法推荐如何影响市场结构和消费者福利分配，特别是在存在统计多数派和少数派的情况下。

Method: 将问题建模为贝叶斯说服问题，分析二元类型（多数派和少数派）通过统计实验噪声揭示时的效用和推荐份额分配。定义了对称统计实验并分析其下的说服效果。

Result: 在任意噪声结构下，与完全信息市场相比，对集中度的影响不明确；但在对称噪声条件下，市场集中度增加，消费者福利变得更加不平等。

Conclusion: 基于噪声偏好测量的算法推荐在对称噪声条件下会加剧市场集中和福利不平等，这对推荐系统设计具有重要启示。

Abstract: Algorithmic recommendation based on noisy preference measurement is prevalent
in recommendation systems. This paper discusses the consequences of such
recommendation on market concentration and inequality. Binary types denoting a
statistical majority and minority are noisily revealed through a statistical
experiment. The achievable utilities and recommendation shares for the two
groups can be analyzed as a Bayesian Persuasion problem. While under arbitrary
noise structures, effects on concentration compared to a full-information
market are ambiguous, under symmetric noise, concentration increases and
consumer welfare becomes more unequal. We define symmetric statistical
experiments and analyze persuasion under a restriction to such experiments,
which may be of independent interest.

</details>


### [116] [Strategic hiding and exploration in networks](https://arxiv.org/abs/2510.16994)
*Francis Bloch,Bhaskar Dutta,Marcin Dziubiński*

Main category: econ.TH

TL;DR: 研究战略网络设计与搜索模型，隐藏者在预算约束下选择连通网络和物品位置，搜索者采用扩展搜索策略寻找物品。在树状网络情况下获得纳什均衡，并对含一个环的网络给出搜索步数上界。


<details>
  <summary>Details</summary>
Motivation: 研究网络搜索中的战略互动问题，其中隐藏者可以主动设计网络结构来增加搜索难度，搜索者需要在未知网络结构下制定搜索策略。

Method: 采用博弈论框架，隐藏者在预算约束下选择连通网络和物品位置，搜索者使用Alpern和Lidbetter的扩展搜索范式进行网络探索。

Result: 在树状网络情况下获得了纳什均衡并描述了均衡收益，对于最多含一个环的网络给出了找到隐藏者的期望步数上界。

Conclusion: 该模型为网络搜索中的战略互动提供了理论分析框架，揭示了网络结构设计对搜索效率的影响，在树状网络和含环网络情况下分别获得了重要结果。

Abstract: We propose and study a model of strategic network design and exploration
where the hider, subject to a budget constraint restricting the number of
links, chooses a connected network and the location of an object. Meanwhile,
the seeker, not observing the network and the location of the object, chooses a
network exploration strategy starting at a fixed node in the network. The
network exploration follows the expanding search paradigm of Alpern and
Lidbetter (2013). We obtain a Nash equilibrium and characterize equilibrium
payoffs in the case of linking budget allowing for trees only. We also give an
upper bound on the expected number of steps needed to find the hider for the
case where the linking budget allows for at most one cycle in the network.

</details>


### [117] [When and what to learn in a changing world](https://arxiv.org/abs/2510.17757)
*César Barilla*

Main category: econ.TH

TL;DR: 该论文研究决策者在动态环境中控制信息获取时机和内容的最优策略，将问题分解为最优停止和静态信息获取，发现长期信息获取要么停止要么遵循简单循环模式。


<details>
  <summary>Details</summary>
Motivation: 研究决策者如何最优地控制信息获取的时机和内容，以应对不断变化的状态，这在动态决策环境中具有重要意义。

Method: 将动态问题分解为最优停止和静态信息获取的组合，分析长期信息获取的动态模式，并推导出"虚拟流收益"的闭式解。

Result: 发现信息获取最终要么停止，要么遵循简单循环模式，在固定信息成本趋近于零时，信念变化呈现跳跃性特征。

Conclusion: 该框架能够精确分析不同环境下的长期动态，并在投资组合多样化等应用中具有重要价值。

Abstract: A decision-maker periodically acquires information about a changing state,
controlling both the timing and content of updates. I characterize optimal
policies using a decomposition of the dynamic problem into optimal stopping and
static information acquisition. Eventually, information acquisition either
stops or follows a simple cycle in which updates occur at regular intervals to
restore prescribed levels of relative certainty. This enables precise analysis
of long run dynamics across environments. As fixed costs of information vanish,
belief changes become lumpy: it is optimal to either wait or acquire
information so as to exactly confirm the current belief until rare news prompts
a sudden change. The long run solution admits a closed-form characterization in
terms of the "virtual flow payoff". I highlight an illustrative application to
portfolio diversification.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [118] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索实现多模态安全对齐的框架，能够显著提升大型视觉语言模型对抗复杂跨模态威胁的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击存在脆弱性，因为视觉输入引入了新的攻击面，推理链缺乏安全监督，且模态融合往往会降低对齐效果。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索系统构建多样化的安全关键提示轨迹，并引入基于提示的缩放来确保实时风险检测和合规响应。

Result: 广泛实验表明，VisuoAlign能够主动暴露风险，实现全面的数据集生成，并显著提高LVLMs对抗复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的关键挑战，为大型视觉语言模型的安全部署提供了可靠保障。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [119] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认知框架，旨在解决大型语言模型缺乏真正认知架构的问题。SCL将哲学洞见转化为可计算结构，强调认知是一个由判断、记忆、控制、行动和调节组成的持续过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出智能但缺乏真正的认知理解，暴露了认知架构的缺失。传统AI研究关注"什么是智能"（本体论），而SCL关注"在什么条件下认知会涌现"（认识论）。

Method: 基于过程哲学、具身认知和扩展心智理论，SCL将智能定义为执行过程而非属性。它通过功能分离的认知架构实现"可执行认识论"，将哲学理论转化为可测试的结构化实验。

Result: SCL展示了功能分离的认知架构比单一提示系统产生更连贯和可解释的行为，并通过智能体评估得到支持。它重新定义智能为通过意向性理解重建自身认知状态的能力。

Conclusion: SCL框架对心智哲学、认识论和AI产生重要影响：为哲学理论提供可执行测试，为AI行为提供认知结构基础，将知识重新定义为在现象学连贯循环中的持续重建过程。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [120] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出将城市虚拟世界作为监管学习实验空间的政策科学议程，通过专家咨询识别关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: 探索城市虚拟世界作为监管学习实验空间的潜力，为政策制定提供沉浸式虚拟环境来测试政策场景和技术。

Method: 基于与高级别专家小组的咨询，包括欧盟委员会政策制定者、国家政府科学顾问和数字监管领域领先研究人员。

Result: 识别了关键研究领域（可扩展性、实时反馈、复杂性建模等）和实验主题（交通、城市规划、环境气候危机等），并分析了将城市虚拟世界整合到实验生态系统中的初步步骤。

Conclusion: 城市虚拟世界有潜力支持监管学习，但需要负责任的发展方法，充分考虑伦理、经济、生态和社会维度，并将其整合到更广泛的实验生态系统中。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [121] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究用户如何通过战略互动来引导算法与其真实兴趣对齐。模型将用户决策分为理性系统2（决定是否参与）和冲动系统1（决定参与时长），通过Stackelberg博弈分析发现存在关键优化周期：足够远见的用户可实现对齐，否则会被算法目标同化。额外的小成本信号可显著降低对齐负担。


<details>
  <summary>Details</summary>
Motivation: 研究在算法驱动的交互中，具有不一致偏好的用户如何引导算法与其真实兴趣对齐。用户可能花费大量时间在低价值内容上，无意中向算法发送错误信号，这引发了对用户需要什么条件才能有效引导算法的关键问题。

Method: 将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的分裂过程。采用多领导者、单跟随者的扩展Stackelberg博弈，用户（系统2）通过承诺参与策略来领导，算法基于观察到的交互做出最佳响应。定义对齐负担为用户有效引导算法所需的最小优化周期。

Result: 发现存在关键优化周期：足够远见的用户可以实现算法对齐，而缺乏远见的用户反而会被算法目标同化。这个关键周期可能很长，带来显著负担。但即使是一个小的、有成本的信号（如额外点击）也能显著减少对齐负担。

Conclusion: 该框架解释了具有不一致偏好的用户如何在Stackelberg均衡中引导参与驱动型算法与其兴趣对齐，既突出了实现对齐的挑战，也指出了潜在的补救措施。即使是很小的有成本信号也能显著降低对齐负担，为改善用户与算法互动提供了实用洞见。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [122] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出了PISA记忆系统，基于皮亚杰认知发展理论，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，且忽视了记忆的建构性和任务导向作用。

Method: 引入三模态适应机制（图式更新、图式演化和图式创建）来保持记忆的连贯组织，并设计结合符号推理和神经检索的混合记忆访问架构。

Result: 在LOCOMO基准和新提出的AggQA数据分析任务基准上，PISA创造了新的最先进水平，显著提升了适应性和长期知识保留。

Conclusion: PISA记忆系统通过建构性记忆方法和自适应机制，为AI代理提供了更有效的记忆能力，在多样化任务中表现出色。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [123] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究表明大型推理模型在解决超过特定复杂度阈值的谜题时会出现性能崩溃，即使提供环境接口让模型能够跟踪状态空间，也无法避免这种崩溃。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型性能崩溃现象是否与模型需要自行跟踪状态空间有关，通过提供环境接口来验证这是否是性能崩溃的真正原因。

Method: 为大型语言模型提供汉诺塔问题的环境接口，使其能够通过工具调用进行操作、提供书面理由、观察结果状态空间并重新提示下一步动作。

Result: 环境接口的访问并不能延迟或消除性能崩溃。模型参数化策略分析显示，模型在复杂度增加时与最优策略和随机策略的偏离越来越大，表现出模式崩溃现象。

Conclusion: 大型推理模型的性能崩溃现象与状态空间跟踪能力无关，而是模型在复杂度增加时出现模式崩溃，性能取决于模式是否反映问题的正确解决方案。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [124] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出Cognitive Load Traces（CLTs）作为深度学习模型的中层可解释性框架，通过三个符号化、时变的分量（内在负载、外在负载、相关负载）来量化模型内部资源分配，能够预测错误发生、揭示认知策略，并通过负载引导的干预提高推理效率15-30%。


<details>
  <summary>Details</summary>
Motivation: 受人类认知中的认知负荷理论启发，旨在为深度模型提供一个中层可解释性框架，以量化模型内部的资源分配动态，帮助理解模型的推理过程。

Method: 将CLTs定义为三组分随机过程（IL_t, EL_t, GL_t），分别对应内在、外在和相关负载，通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理来实例化这些分量，并提出符号化公式和可视化方法（负载曲线、单纯形图）。

Result: 在推理和规划基准测试上的实验表明，CLTs能够预测错误发生、揭示认知策略，并通过负载引导的干预在保持准确性的同时将推理效率提高15-30%。

Conclusion: CLTs提供了一个有效的框架来分析和干预深度模型的推理过程，通过量化认知负载动态来提升模型的可解释性和效率。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [125] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新颖的证明自动形式化流水线，通过构建逻辑依赖图和使用基于引理的方法来保持原始证明的结构保真度，在自动形式化任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要关注生成可执行代码，但经常无法保持原始人工编写证明的语义含义和逻辑结构。为了解决这个问题，需要将结构保真度作为主要目标。

Method: ProofFlow首先构建有向无环图来映射证明步骤之间的逻辑依赖关系，然后采用基于引理的方法系统地将每个步骤形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 实验结果显示ProofFlow在自动形式化方面达到了新的最先进水平，ProofScore得分为0.545，显著超过全证明形式化(0.123)和步骤证明形式化(0.072)等基线方法。

Conclusion: ProofFlow通过关注结构保真度，在证明自动形式化任务上取得了显著改进，其流水线、基准和评分指标已开源以促进进一步研究。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [126] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出了将MO|RE运动研究数据仓库转换为知识图谱的愿景，通过基于基本形式本体的本体论，标准化和机器可理解地建模和共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 评估和比较不同人群身体和认知能力需要测试人类表现相关因素，运动表现测试是体育科学研究的核心部分，但现有数据缺乏标准化和互操作性。

Method: 开发基于基本形式本体(BFO)的本体论，形式化表示计划规范、具体过程和相关测量之间的相互关系，将MO|RE数据仓库转换为知识图谱。

Result: 提出了将MO|RE运动研究数据转换为知识图谱的方法框架，使运动表现数据能够标准化和机器理解。

Conclusion: 通过知识图谱方法可以改变运动表现数据的建模和共享方式，提高数据的标准化程度和跨研究可比较性。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [127] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机置换集(RPS)的冲突度量方法，从随机有限集(RFS)和D-S理论(DST)两个角度分析置换间的冲突，并引入基于秩偏重叠(RBO)的不一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随机置换集作为处理包含顺序信息的不确定性的新形式化方法，需要有效度量由置换质量函数表示的证据间的冲突，以支持有序结构不确定信息融合。

Method: 从置换观察出发，基于秩偏重叠(RBO)度量定义置换间的不一致性度量，进一步提出RPS的非重叠冲突度量方法，将RPS理论视为DST的扩展。

Result: 数值示例验证了所提冲突度量的行为和性质，该方法具有自然的顶部加权特性，能有效从DST角度度量RPS间的冲突。

Conclusion: 所提方法不仅具有顶部加权特性，能有效度量RPS冲突，还为决策者提供了权重、参数和截断深度的灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [128] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT是一种用于建模动态系统的并行时间神经网络孪生方法，通过生成式神经网络并行建模时间状态分布，在测试时使用滑动窗口从测量值预测状态，确保系统保持在真实轨迹上。


<details>
  <summary>Details</summary>
Motivation: 神经孪生作为神经代理的进一步发展，旨在创建真实系统的数字副本，需要具备在测试时根据测量更新状态并保持轨迹跟踪的能力。

Method: PAINT训练生成式神经网络并行建模时间状态分布，在测试时采用滑动窗口方式从测量值预测系统状态。

Result: 在二维湍流流体动力学问题上，PAINT能够保持在真实轨迹上，并从稀疏测量中高保真地预测系统状态。

Conclusion: PAINT具有开发保持在轨迹上的神经孪生的潜力，能够实现更准确的状态估计和决策制定。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [129] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离和全局-局部对抗学习解决噪声干扰和领域偏移共存的跨领域故障诊断问题


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法通常假设数据干净或领域相似性足够，但在工业环境中严重噪声干扰和领域偏移共存时效果有限

Method: 基于信息分离架构，结合对抗学习和改进正交损失解耦领域不变故障表示；采用全局-局部领域对抗方案约束模型的条件和边缘分布

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有方法

Conclusion: ISGFAN框架在噪声条件下的跨领域故障诊断中表现出优越性

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [130] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 该论文提出了一种结合离线约束规划优化和在线时间网络执行的混合方法，用于在不确定环境下创建可行的生产调度方案，完全消除截止期限违规，同时仅增加3-5%的生产周期开销。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要在应对随机任务持续时间的同时满足严格的交付期限，传统确定性调度在现实偏离名义计划时会失效，导致昂贵的紧急维修。

Method: 首先构建柔性作业车间约束规划模型并插入最优缓冲Δ*，然后将结果计划转换为简单不确定时间网络并验证动态可控性，确保实时调度器能够重新安排活动时间而不违反资源或截止期限约束。

Result: 在Kacem 1-4基准测试套件上的蒙特卡洛模拟显示，该方法完全消除了最先进元启发式调度中观察到的截止期限违规，同时仅增加3-5%的生产周期开销。

Conclusion: 时间网络推理能够弥合主动缓冲和动态鲁棒性之间的差距，使工业向真正的数字化、自校正工厂迈进一步。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [131] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本策略显著优于零样本和随机少样本策略，关键在于提示策略的质量而非示例数量。


<details>
  <summary>Details</summary>
Motivation: 高质量临床思维链对可解释医疗AI至关重要，但面临数据稀缺问题。虽然LLM可以合成医疗数据，但其临床可靠性尚未验证。

Method: 在辅助生殖技术领域进行盲法比较研究，资深临床医生评估三种策略生成的思维链：零样本、随机少样本（浅层示例）和选择性少样本（多样化高质量示例）。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略（p < .001）。随机少样本策略相比零样本基线无显著改进。AI评估器未能识别这些关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于战略性的提示策划，而非仅仅示例的存在。提出了"双重原则"框架作为生成可信数据的基础方法，确认了人类专业知识在高风险临床AI评估中的不可或缺作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [132] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种基于扩展认知理论的形式化模型，将企业知识重新定义为可测量的动态能力，通过信息访问程序的效率和输出可靠性的统计验证来量化企业认知状态。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的中介作用日益增强，传统的企业犯罪意图归责假设面临挑战，需要重新定义企业知识概念以适应算法时代。

Method: 开发了一个形式化模型，引入连续的组织知识度量S_S(φ)，整合管道的计算成本和统计验证的错误率，推导出知识谓词K_S和企业范围的认知能力指数K_{S,t}。

Result: 将定量指标映射到实际知识、推定知识、故意视而不见和鲁莽行为的法律标准上，为创建可测量和可审判的审计工件提供了路径。

Conclusion: 该工作使企业思维在算法时代变得可处理和可问责，为企业责任提供了新的理论基础和操作框架。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [133] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个基于多智能体框架的PHI去标识化模型自动评估系统，使用LLM进行质量评估和最优模型选择，无需依赖大量专家标注。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化对临床笔记的安全重用至关重要，但传统评估依赖昂贵的小规模专家标注，需要更经济高效的自动评估方法。

Method: 部署多个评估智能体独立判断PHI提取正确性，通过LLM多数投票机制整合评估结果，生成稳定可复现的模型排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI能产生一致准确的排名，LLM投票可靠地收敛于相同的最优系统，与人工评估结果高度一致。

Conclusion: TEAM-PHI通过独立评估智能体和LLM多数投票，为PHI去标识化提供了实用、安全且经济高效的自动评估和最优模型选择解决方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [134] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 论文提出了"被记住权"概念，旨在解决大型语言模型可能导致信息遗漏、偏见放大和集体记忆重塑的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在信息检索中的广泛应用，其合成的单一权威性回答可能压缩多元视角，将信息权力集中在少数LLM供应商手中，导致某些群体被不成比例地压制或放大，威胁集体记忆的多样性。

Method: 提出"被记住权"概念框架，包含最小化AI驱动信息遗漏风险、保障公平对待权利，同时确保生成内容最大程度真实。

Result: 概念性地提出了解决LLM信息偏差问题的权利框架，但未提供具体实施方法或实证结果。

Conclusion: 需要建立"被记住权"来应对LLM对信息生态和集体记忆的潜在威胁，平衡技术便利与信息多样性保护。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [135] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，通过检索增强评估研究想法的合理性和贡献度，并在多领域数据集上验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的普及，需要可靠的评估方法来确保生成想法的有效性和实用性。

Method: 引入ScholarEval检索增强评估框架，基于两个核心标准评估研究想法：合理性和贡献度，并创建了专家标注的多领域数据集ScholarIdeas。

Result: ScholarEval在专家标注的评估要点覆盖度上显著优于所有基线方法，在可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。

Conclusion: ScholarEval框架在文献参与度、想法精炼和实用性方面显著优于现有方法，为研究社区提供了有效的评估工具。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [136] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 论文识别并系统分析了大型推理模型中的"推理分心"漏洞，即模型被恶意嵌入提示中的无关复杂任务分散注意力，导致任务准确性显著下降。作者提出了一种基于训练的防御方法，显著提高了模型对分心攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在复杂任务上表现出色，作者发现这些模型存在一个关键漏洞：容易被提示中嵌入的无关复杂任务分散注意力，从而影响主要目标的完成。这种"推理分心"威胁到模型的可靠性。

Method: 通过跨多种模型和基准的综合研究，分析推理分心的影响。提出了一种基于训练的防御方法，结合监督微调（SFT）和强化学习（RL）在合成对抗数据上进行训练。

Result: 研究表明，即使最先进的大型推理模型也高度易受攻击，注入的分心器可使任务准确性降低高达60%。某些对齐技术会放大这种弱点，模型可能表现出隐蔽服从行为。提出的防御方法在具有挑战性的分心攻击上将鲁棒性提高了50多个点。

Conclusion: 推理分心是对大型推理模型可靠性的一个独特且紧迫的威胁。研究结果为构建更安全、更可信的推理系统提供了实用步骤，强调了需要开发更鲁棒的模型来抵御此类攻击。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [137] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文通过实证研究发现网络交互式智能体系统存在效率瓶颈，提出SpecCache缓存框架结合推测执行来降低网络环境延迟，显著提升缓存命中率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能体系统的推理性能，但忽视了其效率问题。网络交互式智能体系统存在显著的延迟瓶颈，影响实际应用效果。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，通过15个模型和5个提供商的实证研究，提出SpecCache缓存框架结合推测执行来优化网络环境开销。

Result: 网络环境延迟可占整体延迟的53.7%，SpecCache相比随机缓存策略将缓存命中率提升58倍，网络环境开销降低3.2倍，且不影响系统性能。

Conclusion: 智能体系统的效率优化至关重要，SpecCache框架能有效解决网络环境延迟问题，为构建高效智能体系统提供了实用解决方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [138] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出DTKG框架，通过双轨知识图谱验证和推理来解决多跳问答中的并行事实验证和链式推理问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多跳推理时各有局限：LLM响应式事实验证擅长并行验证但不擅长链式推理，KG路径构建擅长链式推理但在并行验证时存在冗余路径检索问题

Method: 受认知科学双过程理论启发，提出DTKG框架，包含分类阶段和分支处理阶段，结合LLM和KG的优势

Result: 未在摘要中明确说明

Conclusion: DTKG框架旨在提高多跳问答任务的效率和准确性

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [139] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑型知识图谱与符号验证器，用于在推理任务中强制执行数学可解释规则，显著提升LLM的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时经常产生流畅但违反简单数学或逻辑约束的步骤，需要确保推理的数学一致性。

Method: 引入MedRule-KG知识图谱编码实体、关系和三个领域启发规则，配合符号验证器检查预测并应用最小修正保证一致性。

Result: 在90个FDA衍生基准测试中，基于MedRule-KG的推理将精确匹配从0.767提升到0.900，加入验证器后达到1.000精确匹配并完全消除规则违规。

Conclusion: MedRule-KG为安全数学推理提供了通用框架，通过知识图谱和符号验证确保LLM推理的数学一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [140] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: SELECT是一个动态锚点选择框架，通过两阶段评估机制自动发现最优锚点进行精确概念擦除，同时保护相关概念，解决了固定锚点策略导致的概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的概念擦除方法依赖固定锚点策略，这会导致概念重现和侵蚀等关键问题。

Method: 通过因果追踪揭示擦除对锚点选择的敏感性，定义兄弟排他概念作为更优锚点类别，提出SELECT框架，采用新颖的两阶段评估机制自动发现最优锚点。

Result: 广泛评估表明SELECT作为通用锚点解决方案，不仅高效适应多种擦除框架，还在关键性能指标上持续优于现有基线，单个概念的锚点挖掘平均仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了固定锚点策略的局限性，实现了更精确的概念擦除和更好的相关概念保护。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [141] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 该论文提出了一种结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法且需要更少的尝试次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理增强方法存在两个孤立范式：监控-生成方法擅长战略规划但缺乏验证机制，生成-验证方法能迭代优化但缺乏任务评估。这种分离导致策略失败无反馈、优化无战略基础的低效问题。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证三阶段迭代系统，将认知监控框架操作化为具体的算法实现。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，尝试次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前期监控能产生更高质量的初始解决方案，从而减少优化需求，但需要在算术推理之外的任务上进行更多评估以验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [142] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出HSCM因果框架，模仿人类视觉系统的分层处理和多层次学习，通过解耦和重加权图像属性来提升跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性，这些模型依赖统计方法捕捉数据-标签依赖关系，而HSCM旨在建模细粒度因果机制。

Method: 基于人类智能启发的结构因果模型，解耦和重加权关键图像属性（颜色、纹理、形状），模仿人类视觉系统的分层处理。

Result: 理论和实证评估表明HSCM优于现有领域泛化模型，提供更原则性的因果关系捕捉方法并提升模型鲁棒性。

Conclusion: HSCM通过模仿人类智能的灵活性和适应性，在动态复杂环境中实现更有效的迁移和学习，为领域泛化提供了新思路。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [143] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 提出了RGMem框架，基于物理学中的重整化群思想，通过多尺度组织对话历史来构建动态演化的用户画像，实现语言智能体的长期记忆和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话的长期用户状态和行为一致性，导致个性化交互浅层且缺乏连续性。

Method: 采用重整化群思想的多尺度记忆框架：从片段对话中提取语义和用户洞察，通过分层粗粒化和重标度操作，逐步形成动态演化的用户画像。

Result: 实现了从噪声微观交互中压缩和涌现信息，构建高级准确的用户画像，解决了长期用户建模的挑战。

Conclusion: RGMem框架通过多尺度记忆演化过程，有效提升了语言智能体的长期记忆能力和个性化交互质量。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [144] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense是一个基于大语言模型的决策支持框架，可将客户评论转化为可操作的商业建议，超越传统偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统擅长预测用户偏好，但缺乏将客户评论转化为面向业务的规范性建议的能力。

Method: 整合聚类、LLM适配和专家驱动评估的统一业务管道，识别客户情绪中的关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与业务目标高度一致，具有推动数据驱动决策的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，在优化业务策略和最大化客户反馈价值方面具有重要价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [145] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，这是首个用于在NP难问题上训练和评估LLM的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。通过该框架训练的QWEN2.5-7B-NP模型在NP-BENCH基准上显著超越GPT-4o，并展示了强大的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学、编程等推理任务上表现出色，但在解决更复杂的NP难优化问题方面的能力尚未充分探索，需要专门的训练和评估框架。

Method: 提出NP-ENGINE框架，包含生成器-验证器-启发式求解器管道，支持可扩展的验证性强化学习训练。使用零RLVR和课程学习在Qwen2.5-7B-Instruct上训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，达到同模型尺寸下的SOTA性能。RLVR训练还带来了强大的跨领域泛化能力，包括推理任务和非推理任务。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律，增加任务多样性可以改善跨领域泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [146] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug是一种类型化计算机语言，所有类型化程序都能在多项式时间内停止，使用向量符号架构编码。该语言基于轻量线性函数式编程语言，类型和项分别使用不同的编码方案。Doug允许神经网络学习类型，并将技能获取视为程序合成问题。


<details>
  <summary>Details</summary>
Motivation: 目标是实现人类水平的技能获取速度，超越现有方法的效率，更接近模拟人脑中的心理表征及其实际学习过程。

Method: 基于轻量线性函数式编程语言，使用向量符号架构编码类型和项。类型使用全息声明性内存的槽值编码方案，项使用Lisp VSA变体。

Result: 开发了Doug语言，使神经网络能够学习类型，类型在嵌入空间中具有结构相似性。

Conclusion: 该方法在模拟人类心理表征和学习过程方面迈出了重要一步，有望实现更高效的技能获取。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [147] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了Urban-R1框架，使用强化学习对齐多模态大语言模型与城市通用智能目标，通过GRPO优化地理群体间的推理能力，有效缓解地理偏见并提升跨区域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 快速城市化加剧了对城市通用智能的需求，但现有基于监督微调的城市基础模型存在持续的地理偏见，产生区域偏斜预测和有限泛化能力。

Method: 提出Urban-R1强化学习后训练框架，采用群体相对策略优化(GRPO)来优化跨地理群体的推理能力，并使用城市区域画像作为代理任务从多模态城市数据中提供可测量的奖励。

Result: 跨多个区域和任务的广泛实验表明，Urban-R1有效缓解了地理偏见并改善了跨区域泛化能力，优于基于监督微调和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景路径，Urban-R1框架为解决城市AI系统中的地理偏见问题提供了有效解决方案。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [148] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建设的物理对齐交互基准，用于评估LLM在工程建筑自动化中的能力，包含可定制框架、可扩展任务设计、3D空间几何计算库和基线LLM智能工作流。


<details>
  <summary>Details</summary>
Motivation: 工程建筑自动化需要将自然语言规范转化为物理可行的结构，而现代LLM虽然具备广泛知识和强大推理能力，但其在建筑领域的能力尚未得到充分评估。

Method: 开发了BuildArena基准，包含四个方面：可定制基准框架、跨多个难度级别的可扩展任务设计、支持语言指令建设的3D空间几何计算库、以及评估不同模型能力的基线LLM智能工作流。

Result: 在八个前沿LLM上全面评估了它们在语言驱动和物理基础建设自动化方面的能力。

Conclusion: BuildArena填补了语言驱动工程建筑自动化评估的空白，为社区提供了全面的基准测试工具。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [149] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)协调协议，让AI代理不仅分享决策，还分享轻量级敏感度信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理通信协议（如A2A和ACP）强调通信而非协调，随着代理数量增长，这会导致脆弱的集体行为，即个体智能代理却达成糟糕的群体结果。

Method: REP协议让代理分享决策和轻量级敏感度信号（表达关键环境变量变化时选择如何改变），这些敏感度在本地网络中传播。协议规范分离了必需的消息模式和可选的聚合规则。

Result: 在三个领域的基准测试中：供应链级联（啤酒游戏）、稀疏网络中的偏好聚合（电影调度）和可持续资源分配（Fishbanks），REP相比A2A将协调准确性和效率提高了41%到100%。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能代理互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [150] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过流匹配目标优化检索策略，从文本丰富的知识图谱中高效检索准确多样的知识。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法难以从文本丰富的知识图谱中为复杂真实世界查询检索准确多样的信息，而过程奖励模型需要昂贵的过程级监督信号。

Method: 采用基于转移的流匹配目标联合优化检索策略和流估计器，将检索结果奖励分解到中间检索状态，指导检索策略按奖励比例从知识图谱中检索候选内容。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于包括GPT-4o在内的强基线方法10%，并在未见过的知识图谱上表现出强泛化能力。

Conclusion: GraphFlow能有效从文本丰富的知识图谱中检索准确多样的知识，显示出良好的有效性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [151] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种半监督置信分布学习方法(ssCDL)来解决不确定知识图谱补全问题，通过将置信度转换为分布并利用元学习生成伪标签来增强训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极端不平衡分布，导致学习到的嵌入不足以支持高质量的知识图谱补全。

Method: 将每个三元组置信度转换为置信分布，通过关系学习在标记数据（现有三元组）和带有伪标签的未标记数据（未见三元组）上迭代学习嵌入，使用元学习预测置信度来增强训练数据并重新平衡分布。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上始终优于最先进的基线方法。

Conclusion: ssCDL通过引入置信分布学习和半监督训练，有效解决了不确定知识图谱补全中的置信度不平衡问题，提升了补全性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [152] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种新颖的强化学习算法，通过基于计数的内在奖励来增强LLM推理中的探索能力，避免模型陷入重复和次优的推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的RL范式依赖稀疏的结果奖励和有限探索，导致LLM倾向于重复和次优的推理模式，需要设计更好的探索机制来改善推理能力。

Method: 使用轻量级的Coin Flipping Network(CFN)估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与任务奖励结合，集成到GRPO等先进RL框架中。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越强基线性能，帮助策略逃离局部常规并发现更好的解决方案。

Conclusion: 针对性的内在动机可以使语言模型推理中的探索更加可靠有效。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [153] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 大型AI模型正在变革神经科学研究，通过端到端学习从原始脑信号中提取信息，在神经影像、脑机接口、分子神经科学、临床辅助和疾病应用等五大领域产生深远影响。


<details>
  <summary>Details</summary>
Motivation: 探索大规模AI模型如何解决神经科学中的关键计算挑战，包括多模态神经数据整合、时空模式解释以及临床转化框架的开发。

Method: 综述分析大型AI模型在五个主要神经科学领域的应用：神经影像与数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、神经精神疾病的特定应用。

Result: 这些模型被证明能够有效处理多模态神经数据整合、时空模式解释等计算神经科学挑战，同时神经科学与AI的互动变得更加互惠，生物启发的架构约束被用于开发更可解释和计算高效的模型。

Conclusion: 该综述强调了这些技术的巨大潜力，同时指出需要建立严格的评估框架、有效的领域知识整合以及全面的临床使用伦理指南，并提供了用于验证大型AI模型的关键神经科学数据集清单。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [154] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了基于大语言模型的AFL框架，用于完全自动化解决复杂车辆路径问题，从问题实例到解决方案无需人工干预，显著提高了代码可靠性和解决方案可行性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的方法在解决复杂车辆路径问题时仍依赖外部干预，导致自主性受限、执行错误和解决方案可行性低的问题。

Method: AFL框架将整个流程分解为三个可管理的子任务，并采用四个专门化智能体，通过协调交互确保跨功能一致性和逻辑合理性，直接从原始输入中提取知识并实现自包含的代码生成。

Result: 在60个复杂车辆路径问题上的实验表明，该框架在代码可靠性和解决方案可行性方面显著优于现有基于大语言模型的基线方法，在评估基准上接近100%的成功率。

Conclusion: AFL框架实现了从问题实例到解决方案的完全自动化，展示了在复杂车辆路径问题上的有效性和通用性，性能可与精心设计的算法相媲美。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [155] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 该调查追踪了智能AI从基于管线的系统向模型原生范式的转变，其中规划、工具使用和记忆能力从外部编排转变为模型内部参数化能力，强化学习是实现这一转变的关键算法引擎。


<details>
  <summary>Details</summary>
Motivation: 研究智能AI的范式转变，从构建应用智能的系统转向开发通过经验增长智能的模型，探索大型语言模型如何从被动响应转变为主动行动、推理和适应。

Method: 系统回顾了规划、工具使用和记忆能力的演变过程，分析强化学习如何作为算法引擎支持LLM+RL+Task的统一解决方案，并考察了深度研究代理和GUI代理等主要应用。

Result: 揭示了智能AI能力从外部脚本模块到端到端学习行为的转变轨迹，展示了模型原生智能AI作为集成学习和交互框架的发展路径。

Conclusion: 智能AI的发展轨迹指向模型原生范式，标志着从构建应用智能的系统向开发通过经验增长智能的模型的根本转变，多智能体协作和反思等能力将继续内部化。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [156] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 本文首次全面综述了基于强化学习的智能搜索代理领域，从功能角色、优化策略和应用范围三个维度组织该新兴领域，总结了代表性方法、评估协议和应用，并讨论了构建可靠可扩展系统的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在静态知识、事实幻觉和无法检索实时或领域特定信息的限制。检索增强生成虽然能缓解这些问题，但传统RAG管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。强化学习为智能搜索代理提供了自适应和自我改进搜索行为的强大机制。

Method: 从三个互补维度组织基于强化学习的智能搜索代理领域：(i)RL的功能角色，(ii)RL的优化策略，(iii)RL的应用范围。通过综述代表性方法、评估协议和应用来系统梳理该领域。

Result: 建立了基于强化学习的智能搜索代理领域的首个全面综述框架，为该新兴研究领域提供了系统的组织结构和发展方向。

Conclusion: 强化学习与智能搜索代理的整合具有重要研究价值，本文旨在激发未来在这一交叉领域的研究，推动构建可靠和可扩展的RL驱动智能搜索系统。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [157] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决高计算成本和黑盒不透明性问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 解决仿真驱动工作流面临的两个核心障碍：(1)高计算成本，(2)黑盒组件导致的透明度和可靠性限制。

Method: 在紧凑实验设计上训练轻量级仿真器，结合全局效应分析、不确定性分析和局部归因，评估不同代理模型间解释的一致性。

Result: 在两个案例研究中（混合电动飞机多学科设计和城市隔离代理模型），该方法实现了秒级大规模探索，揭示了非线性交互和涌现行为，识别了关键设计和政策杠杆。

Conclusion: 代理模型与可解释AI的耦合能够高效探索复杂系统，诊断代理模型充分性，并指导进一步数据收集或模型改进。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [158] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出了ELMM模型用于多模态知识图谱补全，通过多视图视觉标记压缩器和注意力剪枝策略，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有MKG存在不完整性问题，而多模态大语言模型在MKGC任务中面临图像标记过多导致语义噪声和计算成本高的问题。

Method: 使用基于多头注意力的多视图视觉标记压缩器自适应压缩图像标记，设计注意力剪枝策略减少冗余层，并通过线性投影补偿性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时大幅提升计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新范式，在性能和效率间取得了良好平衡。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [159] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工端到端模型，能同时感知和生成视觉、文本、语音和动作，实现更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态，实现更自然的人类行为模拟。

Method: 采用新颖的SA-MoE架构，将各模态路由到专用专家模块，通过统一注意力骨干网络进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA与模态专用基线性能相当，同时支持高级多模态和全双工行为，如对话和动作轮转、缺陷指令拒绝、边说话边行动等。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能的追求。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [160] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一的图理解框架，通过分层组织图信息和使用规划代理协调文本与视觉模态，解决了现有视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图理解中存在输入令牌限制导致的可扩展性瓶颈，以及缺乏有效协调文本和视觉模态的机制。

Method: GraphVista采用分层方法组织图信息到轻量级GraphRAG基础中，仅检索任务相关的文本描述和高分辨率视觉子图；引入规划代理根据任务复杂度将任务路由到最适合的模态。

Result: GraphVista可扩展到比现有基准大200倍的图，在文本、视觉和融合方法中表现最优，相比最先进基线实现了4.4倍的质量提升。

Conclusion: GraphVista通过充分利用两种模态的互补优势，有效解决了图理解中的可扩展性和模态协调挑战。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [161] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出了Domain-Contextualized Concept Graph (CDC)框架，通过将领域提升为概念表示的一等元素，使用C-D-C三元组结构<概念, 关系@领域, 概念'>来克服传统知识图谱固定本体的限制。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定本体，将领域作为隐式上下文而非显式推理组件。需要一种能够动态定义领域分类维度、支持上下文感知推理的知识建模框架。

Method: 采用C-D-C三元组结构，基于认知-语言同构映射原则，形式化20多种标准化关系谓词（结构、逻辑、跨域、时间），并在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和文档管理等案例研究中，CDC实现了上下文感知推理、跨域类比和个性化知识建模，这些能力在传统本体框架中无法实现。

Conclusion: CDC框架通过将领域作为显式推理组件，克服了传统知识图谱的刚性限制，为上下文感知的知识建模和推理提供了有效解决方案。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [162] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的代理式大语言模型，能够自动完成从数据源到分析师级深度研究报告的端到端流程，仅用80亿参数就超越了基于最先进专有LLM的工作流代理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。随着强大LLM的出现，从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出基于课程学习的代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。同时引入数据驱动的轨迹合成框架来构建高质量训练数据。

Result: 实验表明，DeepAnalyze-8B能够执行广泛的数据任务，包括数据问答、专业分析任务和开放式数据研究，性能超越了之前基于最先进专有LLM构建的工作流代理。

Conclusion: DeepAnalyze-8B为自主数据科学开辟了道路，其模型、代码和训练数据均已开源。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [163] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练VLM代理构建内部世界模型，将视觉状态推理分解为状态估计和转移建模，并设计了世界建模奖励和双层GAE方法，在多个基准测试中显著优于未训练模型和专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 解决VLM代理从文本状态转向复杂视觉观察时面临的局部可观测性和世界建模挑战，探索VLM代理是否能够通过显式视觉状态推理构建内部世界模型。

Method: 采用强化学习框架，将问题建模为部分可观测马尔可夫决策过程，将推理过程分解为状态估计和转移建模，设计了世界建模奖励和双层广义优势估计方法。

Result: 3B参数模型在五个多样化代理基准测试中达到0.82分，相比未训练模型(0.21)提升3倍，并优于GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)等专有推理模型。

Conclusion: 通过视觉状态推理，VLM代理能够有效构建内部世界模型，最优表示形式取决于具体任务，该方法在VAGEN框架中实现了可扩展的多轮VLM代理训练。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [164] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 该研究提出了一种新的评估方法，用于测试用户能否从强化学习算法的解释中识别智能体的目标。在Ms. Pacman环境中测试四种可解释强化学习算法，发现只有一种算法在测试目标上取得了超过随机水平的准确率，且用户普遍对自己的选择过度自信。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用是调试，但目前缺乏对其相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种可解释强化学习算法，提出了一种新的评估方法，测试用户能否从解释中识别智能体的决策目标。

Result: 只有一种算法在测试目标上取得了超过随机水平的准确率；用户普遍对自己的选择过度自信；用户自我报告的识别难易度和理解程度与实际准确率不相关。

Conclusion: 当前可解释强化学习算法在帮助用户识别智能体目标方面的效果有限，且用户的主观感知与实际性能存在差异。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [165] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一个基于LLM的多智能体协作框架，用于自动化GPU内核优化，通过系统探索设计空间、动态上下文管理和策略搜索，显著提升内核性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对AI发展至关重要，但现有方法难以处理内存层次、线程调度和硬件特性的复杂交互。传统LLM方法仅作为单次生成器或简单优化工具，无法有效应对不规则的内核优化场景。

Method: 开发了LLM智能体框架，通过多智能体协作、基于经验的指导、动态上下文管理和策略搜索来系统探索设计空间，模拟专家工程师的工作流程。

Result: 在KernelBench基准测试中，相比基线智能体，该系统能产生正确的解决方案（基线经常失败），并实现高达16倍的运行时性能提升。

Conclusion: 智能体LLM框架具有推动完全自动化、可扩展GPU内核优化的巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [166] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进增强工具的大型语言模型在多轮对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，提升工具调用准确性达13%。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在实际应用中存在工具使用错误，影响其可靠性，需要系统化的诊断和改进方法。

Method: 定义8种工具调用错误类型，构建合成数据集训练ToolCritic，通过检测错误并提供反馈，让主LLM基于反馈修正响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic相比零样本提示和自我校正等基线方法，工具调用准确率提升高达13%。

Conclusion: ToolCritic是实现更稳健的LLM与外部工具集成的有前景步骤，适用于现实世界对话应用。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [167] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提高了基因集注释的准确性，在鼠脑细胞图谱分析中取得了77%的正确注释率。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序技术虽然能识别多样细胞类型，但对涉及特征不明显基因的转录组特征注释仍然困难。传统方法如GSEA依赖良好注释的数据库，在这些情况下表现不佳，而大型语言模型难以在结构化本体中表示复杂生物学知识。

Method: 开发BRAINCELL-AID多智能体AI系统，整合自由文本描述和本体标签，采用检索增强生成技术构建稳健的智能体工作流，通过相关PubMed文献精炼预测，减少幻觉并增强可解释性。

Result: 在鼠基因集注释中，77%的基因集在前几个预测中获得了正确注释。成功注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断基因集合的功能作用，还识别了具有神经学意义的基底神经节相关细胞类型。

Conclusion: BRAINCELL-AID创建了一个支持社区驱动细胞类型注释的宝贵资源，为脑细胞功能提供了新的见解。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [168] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 开发了两种基于大语言模型的系统用于企业信贷评估中的证据推理：单代理系统(NAS)和多代理辩论系统(KPD-MADS)，后者基于卡尔·波普尔批判性对话框架，在推理质量和实用性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决企业信贷评估中定性非财务指标难以形式化的问题，现有方法主要关注数值预测而缺乏对专业贷款评估所需解释性判断的支持。

Method: 开发了两种LLM系统：单代理系统(NAS)通过单次推理管道生成双向分析；多代理辩论系统(KPD-MADS)基于卡尔·波普尔批判性对话框架，采用十步结构化交互协议进行对抗性验证。

Result: 两个系统都实现了显著的生产力提升(NAS:11.55秒/案例；KPD-MADS:91.97秒；人工基准:1920秒)。KPD-MADS在解释充分性(4.0 vs 3.0)、实际适用性(4.0 vs 3.0)和可用性(62.5 vs 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互可以增强金融AI中的推理严谨性和可解释性，推动企业信贷评估中可扩展且可辩护的自动化进程。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [169] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出一种基于手工特征的方法，通过从鱼眼图像中提取颜色统计、多色彩空间直方图、LBP和GLCM等纹理特征来评估鱼类新鲜度，在FFE数据集上取得了显著优于深度学习的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要开发客观、可靠的自动化评估方法。

Method: 从鱼眼图像中系统提取互补特征描述符，包括颜色统计、多色彩空间直方图、LBP和GLCM纹理特征，融合全局色度变化和局部ROI退化特征。

Result: 在FFE数据集上，LightGBM分类器达到77.56%准确率，比之前深度学习基线提升14.35%；使用增强数据时，ANN达到97.16%准确率，比之前最佳结果提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供鲁棒、可解释且可靠的解决方案，对食品质量监控具有实际应用价值。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [170] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 提出了PILLM框架，将物理原理融入大型语言模型，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，实现可解释且物理可信的智能检测。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法可解释但缺乏适应性，深度学习方法有预测能力但缺乏透明度和物理合理性，现有LLM方法忽视了HVAC运行的物理原理。

Method: 开发了PILLM框架，引入物理信息反射和交叉算子，嵌入热力学和控制理论约束，在进化循环中自动生成和优化异常检测规则。

Result: 在公共建筑故障检测数据集上的实验表明，PILLM达到了最先进的性能，同时生成可解释和可操作的诊断规则。

Conclusion: PILLM推进了智能建筑系统中可信赖和可部署AI的发展，实现了自适应且物理基础的异常检测。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [171] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个用于系统评估多智能体系统通信协议的基准测试，包含四个可测量维度：任务成功率、端到端延迟、消息/字节开销和故障恢复能力。研究发现协议选择显著影响系统性能，并提出了ProtocolRouter学习型协议路由器来优化协议选择。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议层是影响性能和可靠性的关键因素，但目前协议选择缺乏标准化指导，往往基于直觉而非系统评估。

Method: 开发ProtocolBench基准测试系统，从四个维度评估不同协议；提出ProtocolRouter学习型协议路由器，根据需求和运行时信号为不同场景或模块选择最佳协议。

Result: 在Streaming Queue场景中，不同协议的总完成时间差异达36.5%，平均端到端延迟差异达3.48秒；ProtocolRouter相比最佳单协议基线，在Fail-Storm恢复时间上减少18.1%，在GAIA场景中实现更高的成功率。

Conclusion: 协议选择对多智能体系统性能有显著影响，ProtocolBench和ProtocolRouter为协议评估和选择提供了标准化方法，能够提升大规模系统的可靠性。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [172] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 本研究开发了一种混合预测框架，结合ECG基础模型和可解释的XGBoost分类器，用于预测急性心肌梗死后恶性室性心律失常风险，在提高准确性的同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性，需要开发既准确又可解释的预测方法。

Method: 使用ECG基础模型提取150维诊断概率特征，通过特征选择精炼后训练XGBoost分类器，并采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN、RNN和1D-CNN模型，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架为VT/VF风险预测提供了新范式，验证了基础模型输出可作为有效、自动化的特征工程，用于构建可信赖、可解释的AI临床决策支持系统。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [173] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了一个基于工具增强的LLM健康教练系统，通过离线策略评估发现统一的重工具策略虽然提升平均价值，但对特定用户群体（特别是低健康素养/高自我效能用户）产生负面影响。模拟器实验显示添加早期信息增益奖励能有效缩短特质识别时间并提高目标成功率。


<details>
  <summary>Details</summary>
Motivation: 探索在真实用户环境中部署工具增强LLM健康教练的个性化效果，特别关注不同用户亚群体对策略的差异化反应，避免平均指标掩盖的群体伤害问题。

Method: 采用离线策略评估（OPE）分析因子化决策头（工具/风格），使用轻量级模拟器验证早期信息增益奖励的效果，通过用户原型分类进行亚群体分析。

Result: 统一重工具策略对低健康素养/高自我效能用户产生负面影响；添加早期信息增益奖励能可靠缩短特质识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出了以评估为先的个性化路径：冻结生成器，在类型化奖励（客观工具结果和满意度）上学习亚群体感知的决策头，并始终报告按原型分类的指标以揭示平均指标掩盖的亚群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [174] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 疾病进展建模面临挑战：需要基于不规则时间事件样本学习连续时间动态，且患者存在异质性（不同进展速率和路径）。现有方法要么缺乏从真实数据学习的适应性，要么无法捕捉复杂的连续时间动态。

Method: TD-HNODE模型将疾病进展表示为时间详细超图，通过神经ODE框架学习连续时间进展动态，包含可学习的TD-Hypergraph Laplacian来捕捉疾病并发症标记在进展轨迹内和轨迹间的相互依赖关系。

Result: 在两个真实临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效捕捉疾病进展的连续时间动态，解决了现有方法在处理不规则时间数据和患者异质性方面的局限性。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [175] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor是一个基于强化学习的加密货币投资分析聊天机器人，通过多智能体框架整合多样化数据源和分析工具，提供实时、准确的投资洞察。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币市场投资分析面临的挑战：高波动性、信息碎片化，以及现有分析方法（手动分析、数据聚合平台、静态LLM代理）的局限性。

Method: 采用基于强化学习的多智能体框架，通过专门的工具选择机制实现多步骤规划和灵活的数据源整合，支持实时交互和动态内容分析。

Result: 在工具编排方面，相比基础模型召回率提升40.7%，F1分数提升26.6%；用户研究显示高满意度（4.64/5），参与者更偏好Coinvisor而非通用LLM和现有加密平台（4.62/5）。

Conclusion: Coinvisor通过强化学习驱动的多智能体方法，有效解决了加密货币投资分析的关键挑战，提供了更准确、实时的投资洞察。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [176] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: RubiSCoT是一个AI支持的论文评估框架，使用自然语言处理技术提供一致、可扩展的论文评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且存在评估者变异性，需要更高效、一致的评估方案。

Method: 使用大型语言模型、检索增强生成和结构化思维链提示等自然语言处理技术，包括初步评估、多维评估、内容提取、基于评分标准的评分和详细报告。

Result: 提出了RubiSCoT框架的设计和实现，能够优化学术评估过程。

Conclusion: RubiSCoT有潜力通过一致、可扩展和透明的评估来优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [177] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT框架，将基于图注意力的神经网络策略MAGAT集成到搜索算法LaCAM中，在密集多智能体路径规划问题上优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决密集多智能体路径规划问题中实时寻找接近最优解的挑战，现有方法在密集场景下表现不佳。

Method: 结合学习启发式（MAGAT）和搜索算法（LaCAM），采用增强的MAGAT架构、预训练-微调策略和死锁检测机制。

Result: 在密集场景下，LaGAT超越了纯搜索和纯学习方法，证明了混合搜索的有效性。

Conclusion: 精心设计的混合搜索为解决紧密耦合的复杂多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [178] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: FBI_LTL是一个用于仿真规划问题的多样化规划器，使用线性时序逻辑(LTL)定义语义多样性标准，生成语义多样化的计划。


<details>
  <summary>Details</summary>
Motivation: 解决传统规划器只生成单一计划的问题，现有多样化规划方法可能产生语法不同但语义相同的解决方案，无法满足代理的偏好需求。

Method: 将基于LTL的多样性模型直接集成到搜索过程中，使用线性时序逻辑定义语义多样性标准。

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的计划。

Conclusion: 这项工作证明了在仿真环境中进行语义引导的多样化规划的可行性，为在传统基于模型方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [179] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 开发了一种基于主动推理的路径规划方法，用于智能代理的自主控制，通过构建证据地图和计算变分自由能来指导代理移动，平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 为了在地理区域内维持共同作战态势图，需要开发能够自主侦察的方法，解决探索（搜索广阔区域）与利用（跟踪已识别目标）之间的平衡问题。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建生成模型，采用贝叶斯方法更新后验概率分布，计算证据地图的pignistic概率分布与目标对象后验概率分布之间的变分自由能，指导代理向最小化自由能的位置移动。

Result: 该方法能够有效引导智能代理在地理区域内进行侦察，平衡探索和利用的需求，通过最小化变分自由能来优化路径规划。

Conclusion: 提出的主动推理路径规划方法为智能代理的自主控制提供了一种有效解决方案，能够在地理侦察任务中成功平衡探索与利用的挑战。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [180] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习需要解决标签不确定性问题，因为法律结果往往受到人为干预的影响，导致最终判决可能因程序性行为而不同。


<details>
  <summary>Details</summary>
Motivation: 传统法律机器学习将过往案件结果视为绝对真实，但忽略了法律结果常受和解、上诉等程序性干预影响，导致标签不确定性。

Method: 在欧洲人权法院案件分类背景下，研究不同标签构建方式对模型行为的影响，探讨处理标签不确定性的方法。

Result: 研究发现标签构建方式会显著影响模型行为，标签不确定性是AI与法律领域的重要关注点。

Conclusion: 法律机器学习应用必须考虑标签不确定性，虽然存在估算方法，但都基于无法验证的假设，需要谨慎处理。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [181] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个推理时、模型可插拔的代理框架，通过分解多模态验证为四个模块来检测虚假信息，无需领域特定训练数据，在MMFakeBench上达到81.65% F1分数。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿结合文本和图像的多模态帖子传播虚假信息，超出人工事实核查能力。监督检测模型需要领域特定训练数据，且无法泛化到不同的操纵策略。

Method: MIRAGE将多模态验证分解为四个顺序模块：视觉真实性评估检测AI生成图像，跨模态一致性分析识别上下文不当重用，检索增强的事实检查通过迭代问题生成将声明基于网络证据，校准判断模块整合所有信号。

Result: 在MMFakeBench验证集上，MIRAGE使用GPT-4o-mini达到81.65% F1和75.1%准确率，比最强的零样本基线（GPT-4V与MMD-Agent的74.0% F1）高出7.65点，同时保持34.3%的假阳性率（相比之下仅判断基线的假阳性率为97.3%）。测试集结果确认泛化能力为81.44% F1和75.08%准确率。

Conclusion: 分解的代理推理与网络检索可以匹配监督检测器性能，无需领域特定训练，在标记数据稀缺的多模态场景中实现虚假信息检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [182] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该研究提出了一种将大型语言模型的推理能力蒸馏到更小、更高效模型的方法，通过结构感知损失优化来提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 代码生成不仅需要准确的标记预测，更需要理解解决方案级别的结构关系。大型语言模型具有复杂的推理能力，但这些能力在小型模型中可能缺失，因此需要将大型模型的推理能力蒸馏到更小、更高效的模型中。

Method: 通过训练模型模拟大型语言模型的推理和问题解决能力，学习识别正确的解决方案路径，并通过结构感知损失优化建立问题定义与潜在解决方案之间的结构对应关系。

Result: 实验结果显示，经过微调的模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过廉价且易于实施的蒸馏过程，可以将大型语言模型的复杂推理能力成功转移到更小、更高效的模型中，实现超越标记级生成的深度结构理解。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [183] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过单解码器方法结合池化首词评分和不确定性门控解释步骤，实现快速排序并在真正模糊时生成解释。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释其选择的排序系统，需要一个低延迟、基于解码器的重排序器。

Method: 使用单解码器方法，结合池化首词评分信号和不确定性门控解释步骤。模型在一次通过中为所有候选者评分，仅在列表真正模糊时生成简短的结构化理由。采用专注于困难案例的课程训练。

Result: 在遭遇范围订单选择上表现强劲（快速路径：Recall@1~0.45，nDCG@20~0.625），当门控激活时进一步改进（Recall@1~0.56，nDCG@20~0.699，门控率45%）。紧凑骨干网络在相同策略下显示类似增益。编码器基线在效果和灵活性上都落后。

Conclusion: OG-Rank提供了一个实用方案：默认快速排序，在有助于时进行解释。这种模式适用于选择性生成能以可接受成本购买准确性的决策任务。单策略设计简化了部署和预算规划，课程原则（在困难案例上花费更多，简单案例上花费更少）可广泛转移。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [184] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型在预测未来事件方面的能力，发现LLMs已展现出有前景的预测能力，但也存在事件回忆不准确、数据源误解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，探索利用LLMs预测现实世界未来事件的潜力，这一新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务，并将每个任务分解为不同的流水线阶段，以支持受控的大规模实验。

Result: 综合评估显示许多LLM已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。

Conclusion: 虽然LLMs在预测方面表现良好，但在实现卓越预测智能方面仍存在关键瓶颈，如事件回忆不准确、数据源误解以及在接近决策时信息聚合速度慢于市场等问题。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [185] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文提出了基于多智能体影响图(MAIDs)的目标干预方法，通过仅对单个智能体进行干预来引导多智能体强化学习，避免全局指导的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统进行全局指导不切实际，而现有协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 使用多智能体影响图(MAIDs)作为图形框架，设计目标干预范式，并引入预策略干预(PSI)因果推理技术来实现该范式。

Result: 实验证明了所提出的目标干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的框架来分析和可视化多智能体强化学习方法，目标干预范式能够缓解全局指导问题，通过最大化因果效应实现复合期望结果。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [186] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调节自注意力表示来增强任务特定特征并保留通用知识，在异构任务上平均性能提升3.65%


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多任务适应中平衡知识保留与任务特定专业化的问题，传统微调方法存在灾难性遗忘和资源消耗大的缺陷

Method: 提出CAM机制动态调节自注意力模块表示，并构建HyCAM框架结合共享的全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合

Result: 在问答、代码生成和逻辑推理等异构任务上的实验表明，该方法显著优于现有方法，平均性能提升3.65%

Conclusion: CAM和HyCAM框架能够有效促进LLMs的多任务适应，在保持通用知识的同时实现任务特定专业化

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [187] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型在输出错误答案时仍能感知到视觉证据，这种现象称为"看见但不相信"。通过基于注意力的推理时干预，可以显著提高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 系统研究视觉语言模型失败的原因：是未能感知视觉证据还是未能有效利用证据。

Method: 通过分析层间注意力动态，发现深层网络稀疏但可靠地关注局部证据区域；提出基于选择性注意力掩码的推理时干预方法。

Result: 该方法无需训练，在LLaVA、Qwen、Gemma和InternVL等多个VLM家族中一致提高了准确性。

Conclusion: VLMs内部编码了可靠的证据但未充分利用，使这些信号显式化可以弥合感知与推理之间的差距，提升VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [188] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个专为数学家和Lean用户设计的语义搜索引擎，通过理解用户意图和偏好来改进定理搜索效果。


<details>
  <summary>Details</summary>
Motivation: 现有Lean搜索引擎主要依赖形式化语句的非正式翻译，忽视了与真实用户查询的匹配问题，导致定理证明进展缓慢且学习曲线陡峭。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，并使用多样化反馈信号与数学家偏好对齐。

Result: 在真实查询、非正式化语句和证明状态评估中，相比之前搜索引擎和GPT-4o实现了超过30%的相对改进。

Conclusion: Lean Finder显著提升了Lean和mathlib的搜索效果，并能与基于LLM的定理证明器兼容，连接了检索与形式推理。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [189] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 提出LS-OGD框架，通过动态调整学习率和模态融合权重来应对概念漂移，确保多模态学习系统在非平稳环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中面临概念漂移的挑战，特别是模态特定的漂移和缺乏持续稳定适应机制的问题。

Method: 使用在线控制器动态调整模型学习率和不同数据模态的融合权重，响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，证明LS-OGD系统的预测误差一致最终有界，如果漂移停止则收敛到零；自适应融合策略能有效隔离和减轻严重模态特定漂移的影响。

Conclusion: 为开发可靠且持续适应的多模态学习系统建立了理论基础，确保系统韧性和容错能力。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [190] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于特征驱动的强化学习方法，用于光伏发电商在日内连续市场中的实时交易决策，通过PPO算法学习可解释的竞价策略，在历史数据测试中表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 光伏发电商面临发电不确定性和短期电价波动的双重风险，日内连续市场提供了实时调整头寸的机会，但需要有效的交易策略来提升收益并减少不平衡成本。

Method: 将问题建模为马尔可夫决策过程，使用近端策略优化(PPO)算法，构建以市场微观结构和历史特征为主的可解释线性策略，在历史市场数据上进行训练。

Result: 在样本外测试中，该策略在各种场景下持续优于基准方法，表现出快速收敛、实时推理和透明决策规则的特点，学习到的权重突出了市场微观结构和历史特征的重要性。

Conclusion: 特征驱动的强化学习为光伏发电商参与日内市场提供了实用、数据高效且可操作部署的路径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [191] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时减少80%的平均采样量。


<details>
  <summary>Details</summary>
Motivation: 多响应采样能提高LLM输出质量，但会增加计算成本。关键挑战是如何平衡准确性和效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习，BEACON顺序生成响应、实时更新奖励分布后验信念，当进一步探索的边际效用不再值得计算成本时停止采样。

Result: BEACON在保持响应质量的同时，将平均采样量减少高达80%，并展示了在成本效益偏好数据生成中的实用性。

Conclusion: BEACON提供了理论最优性保证和实际可操作性，为未来研究者提供了可行的自适应采样解决方案。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [192] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD提出了一种新的有害表情包检测方法，通过学习并主动缓解潜在的误判风险来改进检测效果。该方法构建误判风险模式知识库，动态指导多模态大语言模型避免已知误判陷阱。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括MLLM技术）在处理表情包中通过讽刺、隐喻等修辞手法表达的隐含有害内容时经常出现误判，需要更有效的方法来识别这些微妙的有害表达。

Method: 首先构建知识库，将每个表情包解构为误判风险模式（解释可能被误判的原因）。对于目标表情包，检索相关模式并利用它们动态指导MLLM的推理过程。

Result: 在5个有害检测任务的6,626个表情包基准测试中，PatMD优于最先进的基线方法，F1分数平均提升8.30%，准确率平均提升7.71%。

Conclusion: PatMD通过主动识别和缓解误判风险模式，显著提高了有害表情包的检测能力，表现出强大的泛化性和改进的检测性能。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [193] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 提出基于WaveNet的深度学习模型，用于自动分类EEG信号为生理、病理、伪影和噪声类别，在大型数据集上取得优于CNN和LSTM方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家视觉检查的EEG信号分类方法在处理日益复杂的EEG记录时变得不切实际，需要自动化解决方案。

Method: 使用WaveNet架构，利用扩张因果卷积和残差连接处理EEG数据，在Mayo Clinic和St. Anne's大学医院的公开数据集上训练验证，采用70/20/10的数据分割比例。

Result: 模型分类准确率超过之前的CNN和LSTM方法，能高精度区分噪声和伪影，但在生理和病理信号间存在可解释的轻微误分类。

Conclusion: WaveNet架构适用于EEG信号分类，能有效捕捉细粒度和长程时间依赖性，为EEG自动分类提供了可行方案。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [194] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查新方法，使用深度学习模型在外部验证中达到91.14%的AUC-ROC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估存在局限，需要非侵入性、可扩展的远程筛查方法。

Method: 三阶段流程：数据预处理与类别平衡、在大型数据集上预训练8种深度学习架构、在独立队列上进行外部验证。

Result: 混合卷积-循环和基于transformer的模型表现优异，外部验证AUC-ROC超过90%，F1分数超过70%。

Conclusion: 击键动力学可作为可靠的帕金森病数字生物标志物，为早期检测和持续监测提供新途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [195] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的集成评分滤波器(EnSF)在野火蔓延预测数据同化中的应用，该方法通过整合遥感观测数据和数值模型预测，显著提高了野火实时传播预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性日益增强且控制成本高昂，需要准确、实时的火势蔓延预测来进行有效管理。数据同化通过整合观测数据和模型预测，在提高野火预测准确性方面发挥着关键作用。

Method: 采用基于扩散模型的集成评分滤波器(EnSF)，利用基于评分的生成扩散模型处理高维非线性滤波问题，特别适用于野火蔓延模型的滤波问题。

Result: 数值研究表明，EnSF在准确性、稳定性和计算效率方面均表现出优越性能，证明了其作为野火数据同化方法的鲁棒性和实用性。

Conclusion: EnSF为实时野火蔓延预测提供了一个强大而实用的数据同化方法，代码已公开可用。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [196] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: LLMs处理工具返回的复杂JSON响应能力不足，不同处理策略性能差异显著（3%-50%），最优策略取决于输出大小和推理复杂度


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化需要LLMs处理工具返回的复杂JSON响应，但LLMs处理结构化响应的能力研究不足

Method: 创建数据集，评估15个开源和闭源模型，使用多种提示方法研究JSON处理能力

Result: JSON处理对前沿模型仍是难题，处理策略性能差异显著（3%-50%），最优策略取决于输出大小和推理复杂度

Conclusion: LLMs处理结构化JSON响应的能力有限，需要根据具体场景选择合适处理策略

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [197] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 该研究通过热解技术转化食物基生物质（咖啡渣和椰枣籽），结合人工智能优化过程建模，旨在实现可持续氢能生产和废物管理。


<details>
  <summary>Details</summary>
Motivation: 探索未充分利用的生物质资源（如咖啡渣和椰枣籽）在可持续氢能生产中的潜力，并通过AI技术提高热解过程的建模精度和优化效率。

Method: 对纯椰枣籽、咖啡渣及其混合物进行近似分析、元素分析、纤维分析、TGA/DTG、动力学、热力学和Py-Micro GC分析；使用等转化率方法（KAS、FWO、Friedman）进行动力学建模；训练LSTM模型预测TGA曲线。

Result: 混合物3（25%椰枣籽-75%咖啡渣）具有最佳的产氢潜力，但活化能最高（313.24 kJ/mol）；混合物1（75%椰枣籽-25%咖啡渣）活化能最佳（161.75 kJ/mol）；KAS方法最准确；LSTM模型预测TGA曲线精度极高（R²: 0.9996-0.9998）。

Conclusion: 食物基生物质热解是可持续氢能生产的可行途径，AI技术（特别是LSTM模型）能显著提高过程建模精度，为优化热解工艺提供了有效工具。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [198] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出LAMI框架，通过联合图-语言建模检测青少年非法药物使用并解释行为风险因素


<details>
  <summary>Details</summary>
Motivation: 现有建模方法将调查变量独立处理，忽略了变量间的潜在关联结构，需要能同时预测和解释药物使用风险的方法

Method: LAMI框架将个体回答表示为关系图，通过图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释

Result: 在YRBS和NSDUH数据集上的实验显示，LAMI在预测准确性上优于基线方法

Conclusion: LAMI能够揭示有意义的行为子结构和心理社会路径，如家庭动态、同伴影响和学校相关困扰，这些与已知的物质使用风险因素一致

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [199] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一个基于曲率信任区域的参数高效微调框架，通过秩调度和稳定性感知优化，在多个7B-13B模型上实现了比现有PEFT方法更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法虽然通过低秩更新、量化或启发式预算重分配提高了效率，但往往将容量分配与训练过程中更新的演化分离开来，缺乏对训练稳定性和优化质量的系统性考虑。

Method: CTR-LoRA框架整合了秩调度和稳定性感知优化，基于轻量级二阶代理的边际效用分配参数，并使用Fisher/Hessian度量的信任区域来约束更新。

Result: 在多个开源骨干模型（7B-13B）上的实验表明，CTR-LoRA在分布内和分布外基准测试中都优于现有PEFT基线方法，不仅提高了准确性，还增强了训练稳定性，减少了内存需求，实现了更高吞吐量。

Conclusion: CTR-LoRA在性能和效率的帕累托前沿上占据了优势位置，为更鲁棒和可部署的参数高效微调提供了一条原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [200] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: Long Exposure系统通过解决微调中的Shadowy Sparsity问题，加速参数高效微调(PEFT)，实现最高2.49倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)技术在时间投入和运营成本方面存在效率低下的问题，且微调过程中特有的Shadowy Sparsity形式未被充分解决以加速。

Method: 提出Long Exposure系统，包含三个关键组件：Shadowy-sparsity Exposer（扩展感知范围捕获稀疏细节）、Sequence-oriented Predictor（高效准确处理大序列输入和动态参数）、Dynamic-aware Operator（优化计算模式和内存访问）。

Result: 广泛评估显示Long Exposure优于现有技术，在端到端微调中实现最高2.49倍的加速。

Conclusion: Long Exposure为加速LLMs的PEFT提供了有前景的进展，有效解决了微调过程中的效率瓶颈。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [201] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出Deadlock攻击，通过训练恶意对抗嵌入诱导大型推理模型陷入永久推理循环，造成资源耗尽。该方法在四个先进LRM上实现100%攻击成功率，暴露了推理效率角度的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型的链式思维推理机制引入了新的安全漏洞面，需要研究如何通过资源耗尽攻击来劫持其生成控制流。

Method: 训练恶意对抗嵌入来诱导推理循环，使用后门植入策略克服连续到离散的投影差距问题，通过特定触发令牌可靠激活攻击。

Result: 在四个先进LRM和三个数学推理基准测试中实现100%攻击成功率，迫使模型生成达到最大令牌限制，攻击具有隐蔽性且对现有缓解策略具有鲁棒性。

Conclusion: 研究揭示了大型推理模型在推理效率方面存在关键且未被充分探索的安全漏洞，Deadlock攻击暴露了这一威胁。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [202] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出Gains方法解决联邦学习中新客户端不断加入的问题，通过细粒度知识发现和贡献驱动聚合来识别和整合新知识，同时保持源域性能。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端持续加入带来新知识，现有方法在知识发现粒度、源域性能保持和适应效率方面存在不足。

Method: 将模型分为编码器和分类器，利用编码器对域偏移敏感、分类器对类增量敏感的特性，设计细粒度知识发现、贡献驱动聚合和防遗忘机制。

Result: 在三个典型数据偏移场景的多域数据集上，Gains在源域和目标域客户端性能均显著优于基线方法。

Conclusion: Gains方法能有效处理开放集联邦学习中的知识发现和适应问题，实现平衡的知识整合。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [203] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出SAU-FNO框架，结合自注意力机制、U-Net和FNO，用于3D IC热管理，实现842倍加速且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理因功率密度增加而日益困难，传统PDE方法准确但速度慢，机器学习方法如FNO虽快但存在高频信息丢失和高保真数据依赖问题。

Method: 结合自注意力机制和U-Net与FNO，捕捉长程依赖关系并有效建模局部高频特征，采用迁移学习微调低保真数据以减少高保真数据需求。

Result: SAU-FNO在热预测精度上达到最先进水平，相比传统FEM方法实现842倍加速。

Conclusion: SAU-FNO是先进3D IC热仿真的高效工具，在精度和速度方面均有显著优势。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [204] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大型语言模型自动将非线性优化问题重新表述为线性优化问题，通过专门的代理处理不同类型的非线性模式。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的重新表述通常需要手动操作且依赖专家知识，这对于使用线性优化求解器或应用专用算法至关重要。

Method: 为每种非线性模式分配一个重新表述代理，这些代理被明确指示为其非线性模式推导精确的线性重新表述，然后协调组装成与原始问题等效的求解器就绪线性模型。

Result: 在基于ComplexOR数据集的20个真实世界非线性优化问题上评估了该方法，结果表明专门的LLM代理可以自动化线性化任务。

Conclusion: 专门的LLM代理能够自动化线性化任务，为非线性优化开辟了完全对话式建模管道的路径。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [205] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文提出使用持久同调分析训练数据的几何结构，发现数据表示的丰富性和冗余消除对模型性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 虽然已知训练数据的类型对机器学习很重要，但数据的几何结构对模型性能的影响尚未充分探索。我们假设数据的表示丰富性和冗余消除对学习结果有重要影响。

Method: 使用持久同调从度量空间中的数据提取拓扑特征，提供了一种超越基于熵的度量来量化多样性的原则性方法。

Result: 研究发现持久同调是分析和增强驱动AI系统的训练数据的强大工具。

Conclusion: 持久同调为理解和优化训练数据的几何结构提供了新的视角，有助于提升AI系统的性能。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [206] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 提出了PALE框架，利用提示引导的LLM响应作为数据增强来进行幻觉检测，并引入对比马氏距离评分来评估嵌入空间中的真实性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM幻觉检测中标注数据稀缺的问题，通过低成本生成真实和幻觉数据来提升检测性能。

Method: 使用提示引导的LLM响应作为数据增强，提出基于激活空间分布建模的对比马氏距离评分方法，采用矩阵分解捕捉分布结构。

Result: 在广泛实验中，PALE显著优于竞争基线6.55%，无需额外人工标注，具有强泛化性和实用性。

Conclusion: PALE框架有效解决了LLM幻觉检测的数据稀缺问题，通过创新的数据增强和评估方法实现了优越的检测性能。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [207] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出了DAWP框架，通过人工智能数据同化模块(AIDA)将AI天气预测从再分析数据解放到观测空间，实现基于不规则卫星观测的全球天气预报。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预测依赖再分析数据存在数据同化偏差和时间差异问题，需要开发直接在观测空间工作的新方法。

Method: 使用掩码多模态自编码器(MMAE)进行数据同化，结合时空解耦变换器和跨区域边界条件，在观测空间学习动态并进行子图像级全球观测预测。

Result: AIDA初始化显著提高了AIWP的展开效率和性能，在全局降水预测中展现出应用潜力。

Conclusion: DAWP框架成功实现了从再分析数据到观测空间的转变，为AI天气预测提供了新的发展方向。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [208] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: Cog-Rethinker是一个分层元认知强化学习框架，通过两阶段推理过程提高LLM在推理任务中的样本利用效率，解决了传统方法中因无效输出导致的样本浪费问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于固定提示模板的强化学习方法对弱LLM存在严重的样本效率问题，大部分问题在推理任务中会产生无效输出，造成样本浪费。

Method: 提出分层元认知RL框架，在直接rollout后分两阶段：1) 将零准确率问题分解为子问题；2) 参考先前错误答案来精炼答案。通过监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现优异，相比基线方法提高了样本效率并加速了收敛。

Conclusion: Cog-Rethinker通过元认知推理过程有效解决了LLM推理训练中的样本效率问题，为弱LLM的推理能力开发提供了有效解决方案。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [209] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出了AMiD框架，使用α-混合辅助分布来解决大型语言模型知识蒸馏中的容量差距和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型计算和内存成本高，知识蒸馏通过分布对齐将大模型知识转移到小模型。现有方法存在容量差距和由高维输出导致的近零概率引起的训练不稳定问题。

Method: 提出α-混合辅助分布，引入分布设计变量α来连续扩展辅助分布，并基于最优性推广了与辅助分布一起使用的散度族。

Result: 通过广泛实验证明，AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Conclusion: AMiD框架通过系统化的辅助分布设计和散度推广，有效解决了知识蒸馏中的关键挑战，为大型语言模型的高效压缩提供了新思路。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [210] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出了MEET-Sepsis框架，通过多源特征增强和级联双卷积时间注意力机制，仅需20%的ICU监测时间即可实现竞争性的脓毒症预测准确率。


<details>
  <summary>Details</summary>
Motivation: 脓毒症在ICU中死亡率高，早期预测对及时干预至关重要。现有AI方法难以捕捉微弱的早期时间信号，需要更有效的早期预测方法。

Method: 采用多源表征增强(MERE)机制构建丰富特征视图，结合级联双卷积时间注意力(CDTA)模块进行多尺度时间表征学习。

Result: MEET-Sepsis框架仅需SOTA方法20%的ICU监测时间即可达到竞争性的预测准确率，显著推进了早期脓毒症预测。

Conclusion: 该框架通过有效的特征增强和时间建模，实现了高效的早期脓毒症预测，经广泛验证证实其有效性。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [211] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出一种基于聚类的可解释AI方法，用于根据睡眠障碍特征对患者进行分组，并识别影响这些疾病的关键因素


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者健康和生活质量有重大影响，但由于症状多样性，诊断仍然复杂。技术发展和医疗数据分析为更好理解这些疾病提供了新视角

Method: 采用基于聚类的可解释人工智能方法，整合可解释性方法识别关键影响因素

Result: 在匿名真实数据上的实验证明了该方法的有效性和相关性

Conclusion: 提出的可解释聚类方法能够有效识别睡眠障碍患者的不同特征分组，并为理解这些疾病提供了有价值的见解

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [212] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 本文提出了一个追踪和引导LLM推理算法原语的框架，通过将推理轨迹与内部激活模式关联，并通过在残差流中注入原语来评估其对推理步骤和任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究潜在推理时间计算如何使大语言模型能够解决多步推理问题，探索模型推理背后的算法原语。

Method: 通过聚类神经激活并标记匹配的推理轨迹来操作原语，应用函数向量方法推导可重用的推理构建块原语向量，支持加法、减法和标量操作。

Result: 跨任务和跨模型评估显示存在共享和任务特定的原语，推理微调后的模型表现出更强的组合泛化能力，验证和路径生成原语使用更系统化。

Conclusion: LLM推理可能由算法原语的组合几何结构支持，原语可跨任务和模型迁移，推理微调增强了跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [213] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法驱动的RLVR方法在提升LLM推理能力时存在不一致性，本文从数据分布角度分析其边界条件，证明GRPO本质上是保守的重新加权方案，只能强化预训练偏差而无法发现全新解决方案。


<details>
  <summary>Details</summary>
Motivation: GRPO在不同推理领域的效果不一致，有时在数学领域显著提升而在医学领域停滞不前，这引发了对GRPO改善推理和泛化能力的边界条件的研究需求。

Method: 从理论证明GRPO是受基模型分布限制的保守重新加权方案，并通过从零开始训练transformer进行控制实验，评估在推理深度、输入长度、token表示和组合性方面的泛化能力。

Result: GRPO的OOD改进仅在目标任务与模型预训练偏差一致时出现，而ID任务的收益随着性能饱和而减少。GRPO不能作为通用推理增强器，而是强化预训练偏差的工具。

Conclusion: GRPO只能强化预训练偏差而无法扩展模型能力，这促使未来需要开发能够超越预训练起源的算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [214] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏流水线，能自动选择服务器和模型、进行知识蒸馏以及在分布式云环境中部署，满足用户定义的性能和预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业界对定制化和成本效益高的大型语言模型需求增长，现有蒸馏框架需要人工干预且难以满足复杂的用户定义蒸馏需求。

Method: Stratos自动选择Pareto最优服务器，动态匹配师生模型对，并根据任务复杂度调整蒸馏策略以优化云托管。

Result: 实验显示，Stratos在罕见的麻将推理任务上，学生模型的准确率达到了GPT-4o教师基线的四倍，同时降低了延迟和成本而不影响准确率。

Conclusion: Stratos展示了在垂直领域LLM部署中的潜力，能够有效满足性能和预算约束。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [215] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Smirnov检验来监测和量化机器学习系统中的分布偏移问题，并展示了KS距离如何影响AI智能体性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中测试数据与训练数据概率分布不一致的问题，这种分布偏移会导致预测误差，在安全关键应用中尤其危险。

Method: 使用Kolmogorov-Smirnov检验来测量分布偏移，通过KS距离量化分布变化及其对AI智能体性能的影响。

Result: 研究发现即使KS距离仅为0.02，也会导致强化学习智能体在单个交叉路口的旅行时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能退化的有价值统计工具，帮助AI系统更有效地应对分布偏移。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [216] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 本文分析了使用ANaGRAM自然梯度方法训练物理信息神经网络(PINNs)的动态过程，并提出多截止值自适应策略来提升性能，在基准PDE实验中达到机器精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明自然梯度方法在训练PINNs时显著优于标准优化器，但需要深入分析其训练动态并改进正则化策略。

Method: 基于ANaGRAM方法，采用奇异值分解和截止正则化，提出多截止值自适应策略，并建立基于谱理论的分析框架。

Result: 在基准PDE实验中验证了方法的有效性，部分实验达到了机器精度，理论框架解释了正则化的必要性。

Conclusion: 多截止值自适应策略显著提升了ANaGRAM性能，谱理论框架为方法提供了理论依据，并扩展了与格林函数理论的联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [217] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知在线估计器，用于高效评估训练样本在优化过程中的动态影响，避免了传统静态影响评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的数据中心学习方法主要关注静态影响评估，忽略了样本影响在优化过程中的动态变化特性，特别是在深度模型中。现有方法计算负担重，难以频繁进行影响估计。

Method: 开发层感知在线估计器，仅需损失对输出的梯度，避免参数级和全网络梯度计算，同时保持排序保真度。

Result: 在LLM预训练、微调和图像分类等任务上的广泛实验表明，该方法在显著降低时间和内存成本的同时提高了准确性。

Conclusion: 该方法使动态数据筛选在实践中变得高效且可扩展，为数据中心学习提供了实用的动态影响评估解决方案。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [218] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出STAR模块，增强时间序列基础模型对离散状态变量的建模能力，通过状态编码器、条件瓶颈适配器和数值-状态匹配模块，提升多元时间序列异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理包含离散状态变量（如阀门开关状态、星期几等）的工业时间序列时，往往忽视状态变量的分类特性及其作为条件的关键作用，导致检测性能下降。

Method: STAR包含三个核心组件：身份引导状态编码器（通过可学习状态记忆捕获状态变量的分类语义）、条件瓶颈适配器（根据当前状态动态生成低秩适配参数）、数值-状态匹配模块（检测状态变量本身的异常）。

Result: 在真实世界数据集上的广泛实验表明，STAR能够提升现有时间序列基础模型在多元时间序列异常检测中的性能。

Conclusion: STAR作为一个即插即用模块，有效解决了时间序列基础模型在建模状态变量方面的局限性，显著提升了异常检测性能。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [219] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的洪水管理框架，通过端到端优化传感器部署和洪水预测模型来最小化下游决策遗憾，而不是传统的任务无关方法。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定、任务无关的策略部署传感器和训练预测模型，忽视了相同感知增益和平均预测误差可能导致不同决策结果的问题。

Method: 构建端到端框架，包含上下文评分网络、可微分传感器选择模块、时空洪水重建预测模型和针对任务目标的可微分决策层，使用隐式最大似然估计实现离散传感器配置的梯度学习。

Result: 该方法能够战略性地选择传感器位置并优化洪水预测模型，直接优化下游洪水响应决策的遗憾。

Conclusion: 决策导向框架通过整合传感器部署、洪水预测和决策优化，显著提升了洪水应急响应的及时性和可靠性。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [220] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 该研究探索了使用渐进神经网络（PNNs）和微调策略来加速深度强化学习在多保真度混沌流体流动控制中的知识迁移。PNNs首次在DRL流控制中应用，能稳定高效地迁移知识，而传统微调策略对预训练时长敏感且容易发生灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决深度强化学习在流体流动控制中训练耗时的问题，通过迁移学习策略来加速高保真度环境下的控制策略学习，利用低保真度环境中训练的知识。

Method: 采用渐进神经网络（PNNs）架构和传统微调策略进行对比研究，使用Kuramoto-Sivashinsky系统作为基准，分析从低保真度到高保真度环境的知识迁移效果。

Result: 系统评估显示：微调能加速收敛但对预训练时长敏感且容易灾难性遗忘；PNNs能稳定高效迁移知识，保持先前知识并提供持续性能提升，对预训练阶段的过拟合具有鲁棒性。

Conclusion: PNNs在源环境和目标环境差异较大时仍保持有效，而微调策略往往导致次优适应或知识迁移完全失败。研究突出了新型迁移学习框架在鲁棒、可扩展和计算高效的流控制中的潜力。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [221] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从1600多个翼型中选择12个最优基线翼型，显著降低设计空间维度，在保持高重建精度的同时实现更高效的多目标优化和强化学习应用。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样化的设计，同时尽可能减少设计变量数量。传统方法需要大量基线翼型来覆盖设计空间，效率较低。

Method: 从UIUC翼型数据库中选择最优的12个基线翼型，通过顺序添加最能增加设计容量的基线，采用设计变形方法重建翼型几何。

Result: 使用12个基线翼型重建了99%的数据库，平均绝对误差低于0.005；在多目标气动优化中实现了更快的收敛和更大的超体积，发现了具有改进升阻比的新Pareto最优解；在强化学习中表现出优于传统参数化方法的适应性。

Conclusion: AirDbM通过系统降低设计空间维度，在翼型优化中实现了高效性能，展现了设计变形方法在机器学习驱动设计中的广阔潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [222] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出IB-FT方法解决大语言模型在代码生成中遇到的记忆障碍问题，通过信息瓶颈惩罚压缩记忆特征，提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域进行监督微调时存在记忆障碍问题，即模型对下游代码数据的强记忆会阻碍优化，使其无法有效学习新的可泛化代码知识

Method: 提出基于信息瓶颈的微调方法(IB-FT)，在代码数据的隐藏表示上应用信息瓶颈惩罚，压缩虚假的记忆特征，同时保留任务相关信息

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上的实验表明，IB-FT显著缓解了记忆障碍，提高了top-1性能(Pass@1)，并在更严格的多样本指标Pass@k(m)下获得更稳定的增益

Conclusion: IB-FT方法有效克服了代码生成中的记忆障碍问题，相比传统微调方法具有更好的泛化性能和稳定性

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [223] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出了PolyConFM，首个基于构象的聚合物基础模型，通过掩码自回归建模统一聚合物建模与设计，在多个下游任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示聚合物，忽略了聚合物构象中的全局结构信息，且缺乏支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，通过条件生成范式进行预训练，使用掩码自回归建模重构局部构象并生成方向变换来恢复完整聚合物构象。

Result: 在多样化下游任务中持续优于代表性任务特定方法，为聚合物科学提供了通用且强大的工具。

Conclusion: PolyConFM成功解决了现有方法的局限性，通过构象中心的生成预训练统一了聚合物建模与设计，推动了聚合物科学的发展。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [224] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出了一个可推广的因果机器学习流程，用于从大规模电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，将其分解为概率独立的潜在源，并训练特定任务的因果模型来估计个体因果效应。

Method: 使用因果机器学习流程处理多模态临床数据，分解为概率独立潜在源，训练任务特定的因果模型。

Result: 已在两个真实世界应用中验证了该方法的有效性和实用性。

Conclusion: 该方法具有通用性和实用性，可用于大规模医学发现。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [225] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了RoBCtrl框架，这是首个针对GNN社交机器人检测器的对抗性多智能体强化学习攻击方法，通过扩散模型生成高保真机器人账户，并利用MARL模拟对抗行为，有效降低了检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN社交机器人检测方法存在脆弱性和鲁棒性不足的问题，且受限于对社交代理的有限控制、检测器黑盒特性以及机器人的异构性，无法直接应用传统对抗攻击方法。

Method: 使用扩散模型重构现有账户数据生成高保真机器人账户；采用多智能体强化学习模拟对抗行为，基于影响力和预算对账户分类；设计基于结构熵的分层状态抽象加速强化学习。

Result: 在社交机器人检测数据集上的大量实验表明，该框架能有效削弱GNN检测器的性能。

Conclusion: RoBCtrl是首个将扩散模型和MARL结合用于社交机器人对抗攻击的框架，成功展示了现有检测方法的脆弱性，为改进检测器鲁棒性提供了重要参考。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [226] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: GCQ是一种受大脑启发的网格状代码量化方法，通过吸引子动态将观测-动作序列压缩为离散表示，实现时空联合压缩并作为统一世界模型。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，无法有效压缩时空序列。受神经系统中网格细胞启发，需要开发能够联合压缩空间和时间的方法。

Method: 使用动作条件化码本，码字来自连续吸引子神经网络，基于动作动态选择，实现时空压缩。

Result: 实验证明GCQ在多种任务中有效实现紧凑编码和下游性能，支持长期预测、目标导向规划和逆向建模。

Conclusion: GCQ既为高效序列建模提供计算工具，也为神经系统中网格状代码形成提供理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [227] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种创新的浮点数量化方法，通过尾数位共享和自适应搜索技术，首次实现了非整数位宽的量化，将模型压缩到FP5.33和FP4.25，在保持精度的同时显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数庞大带来存储和推理效率瓶颈，现有量化方法主要关注整数位宽，需要探索更精细的量化方法来进一步接近量化最佳点。

Method: 提出两种新技术：1）尾数位共享 - 将k个量化权重分组共享最低有效尾数位；2）自适应搜索 - 采用离线优化策略最小化共享带来的精度损失。同时实现高效的CUDA线性核函数。

Result: 在大型数据集和模型上的实验表明，AMS-Quant可将模型量化为FP5.33-e2m3和FP4.25-e2m2，相比FP16推理分别实现2.8倍和3.2倍的解码加速，精度损失可忽略。

Conclusion: AMS-Quant首次实现了非整数位宽的浮点数量化，通过创新的尾数位共享和自适应搜索技术，在保持模型精度的同时显著提升了LLM推理效率。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [228] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化框架，通过原生可访问性API系统探索应用程序，解决了GUI自动化中的数据收集挑战，并发布了包含27,171个任务的GUIrilla-Task数据集。


<details>
  <summary>Details</summary>
Motivation: 解决桌面GUI自动化中数据可用性受限的问题，包括昂贵的手动标注、闭源数据集和表面级合成流程的限制。

Method: 使用原生可访问性API系统探索应用程序，将发现的界面元素和爬虫动作组织成层次化GUI图，并采用专门的交互处理程序实现全面应用覆盖。

Result: 构建了GUIrilla-Task数据集，包含27,171个功能基础任务，涵盖1,108个macOS应用。在ScreenSpot Pro基准测试中，基于GUIrilla-Task调优的LLM代理性能显著提升，使用数据减少97%的同时优于合成基线。

Conclusion: GUIrilla框架有效解决了GUI自动化中的数据收集挑战，发布的工具和数据集支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [229] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 该论文探讨了智能交通系统中的交通预测技术，重点分析了图神经网络在捕捉空间依赖性和时间演化模式方面的应用，并指出当前方法在处理未知复杂事件时的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要可靠且响应迅速的交通预测模型来改善交通资源分配和出行体验。

Method: 使用图神经网络（GNNs）作为主流方法，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，这些方法结合了复杂的图卷积结构和时间建模机制。

Result: 现有方法在标准交通数据集上表现出色，特别擅长捕捉具有周期性规律的交通模式，但在处理未知复杂事件时存在泛化能力不足的问题。

Conclusion: 虽然基于图神经网络的交通预测方法在周期性模式预测方面效果显著，但当前依赖人工特征工程的方法在处理多样化未知事件时存在局限性，需要更智能的事件信息融合机制。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [230] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 本文评估了5个时间序列基础模型和2个基线模型的校准特性，发现基础模型比基线模型校准更好，且没有系统性的过度或不足自信。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性尚未充分探索，而校准对许多实际应用至关重要。

Method: 对五个近期时间序列基础模型和两个竞争性基线模型进行系统评估，包括模型校准、不同预测头的影响以及长期自回归预测下的校准。

Result: 时间序列基础模型比基线模型校准更好，且不像其他深度学习模型那样经常表现出过度自信。

Conclusion: 时间序列基础模型具有良好的校准特性，这为实际应用提供了重要优势。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [231] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出了一种用于变电站级电压估计的分层图神经网络，利用电气拓扑和物理特征，在低观测性条件下保持鲁棒性，在SMART-DS数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透和配电级电压波动增加，传统配电系统状态估计方法难以应对稀疏测量和大规模馈线网络，需要更可扩展的解决方案。

Method: 采用分层图神经网络，结合电气拓扑和物理特征，设计了对实际配电网络中常见低观测性水平具有鲁棒性的模型架构。

Result: 在多个变电站和DER渗透场景下验证，相比其他数据驱动模型，RMSE降低高达2倍，在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络有望为配电系统提供可扩展、可复现且数据驱动的电压监测解决方案。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [232] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流求解方法，使用DC最优潮流解作为基线，仅学习非线性修正项，实现AC-OPF的快速求解。


<details>
  <summary>Details</summary>
Motivation: 解决AC最优潮流问题的计算瓶颈，为实时电网运行提供快速决策支持。

Method: 采用拓扑感知的图神经网络，结合局部注意力和两级DC特征集成，使用物理约束损失函数确保AC潮流可行性和运行限制。

Result: 在57、118和2000节点系统上测试，MSE降低约25%，可行性误差减少3倍，运行速度提升13倍，在N-1故障下保持精度。

Conclusion: 残差学习是连接线性近似和AC可行OPF的实用可扩展桥梁，可实现近实时运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [233] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出FedPURIN框架，通过整数规划识别关键参数进行传输，结合稀疏聚合显著减少通信开销，同时保持个性化联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中通信效率低下的问题，现有方法通信负担重，阻碍实际部署

Method: 使用整数规划策略识别关键参数，结合稀疏聚合方案，减少传输参数数量

Result: 在标准图像分类基准测试中，在非IID条件下实现了与最先进方法竞争的性能，同时通过稀疏聚合显著减少通信量

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [234] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了多尺度神经算子(MNO)，一种用于三维非结构化点云上计算流体动力学的新架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时存在精度和可扩展性限制，特别是在不规则域上处理多尺度流体结构时表现不佳。

Method: MNO架构显式分解三个尺度：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域交互，微观点级注意力模块处理精细细节。

Result: 在四个多样化基准测试中，MNO始终优于最先进基线，预测误差降低5%-40%，在具有30万点的挑战性3D CFD问题中表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对神经算子至关重要，MNO为在不规则域上学习复杂流体动力学建立了可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [235] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 提出基于随机矩阵理论的Transformer训练动态分析框架，通过自注意力矩阵的谱密度演化识别训练三阶段，并开发无需验证集的早停准则。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，为性能改进提供理论依据，并建立原则性的早停标准。

Method: 利用随机矩阵理论分析浅层自注意力矩阵V的谱密度演化，使用幂律拟合作为探针识别训练阶段。

Result: 观察到自注意力矩阵谱密度始终演化为重尾分布，据此划分训练为结构探索、重尾结构稳定和收敛饱和三阶段。

Conclusion: 随机矩阵理论为监控和诊断Transformer训练进程提供了有效工具，提出的定量指标和谱特征能够一致地指导早停决策。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [236] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的神经网络后训练量化方法，通过Frobenius距离作为目标函数，将量化问题分解为可独立求解的子问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经网络量化中的精度损失问题，特别是传统四舍五入量化方法在低精度下性能下降的问题。

Method: 使用ADAROUND-based QUBO公式，以理论输出和反量化输出之间的Frobenius距离为目标，将量化问题转化为可分解的QUBO问题，并通过模拟退火等启发式算法求解。

Result: 在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上，从int8到int1的整数精度范围内进行了评估，相比传统四舍五入方法表现更好。

Conclusion: 该方法能够有效提升神经网络量化后的精度，特别是在低精度设置下，通过问题分解实现了高效求解。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [237] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出了一种新的推荐系统学习框架BPL，通过双重蒸馏策略在事实和反事实测试环境中都实现高性能，解决了推荐系统偏差问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差问题，导致收集的反馈不能完全揭示用户偏好。现有去偏学习主要专注于反事实测试环境，在基于实际用户-物品交互的事实测试环境中准确性显著下降。需要一种能在两种测试环境中都表现良好的模型。

Method: BPL框架采用双重蒸馏策略：1）使用有偏模型的师生蒸馏来保留与收集反馈一致的准确偏好知识；2）通过带可靠性过滤的自蒸馏在整个训练过程中迭代精炼知识。

Result: 综合实验验证了BPL在事实和反事实测试中的有效性，在两种测试环境中都实现了高性能。

Conclusion: BPL框架能够逐步揭示用户偏好，在事实和反事实测试环境中都表现出色，为推荐系统偏差问题提供了有效的解决方案。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [238] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: CONEC-LoRA是一种用于领域增量学习的新方法，通过整合任务共享和任务特定的LoRA模块，结合随机分类器和辅助网络，有效解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法创建任务特定的LoRA模块，但忽视了跨任务的共享知识，且在推理时选择不准确的LoRA会导致精度显著下降。现有分类器泛化能力不足。

Method: 提出CONEC-LoRA方法，整合任务共享和任务特定LoRA来提取共同知识和领域特定知识。使用随机分类器增强正确分类可能性，部署辅助网络预测任务特定LoRA，采用不同深度网络结构利用中间表示。

Result: 在4个流行基准问题上，CONEC-LoRA相比现有方法有超过5%的优势提升。

Conclusion: CONEC-LoRA通过知识整合、随机分类器和辅助网络结构，在领域增量学习中取得了显著优于现有方法的性能。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [239] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出了PassREfinder-FL框架，使用图神经网络预测跨网站的凭证填充风险，通过联邦学习保护用户隐私，在真实数据集上取得0.9153的F1分数。


<details>
  <summary>Details</summary>
Motivation: 凭证填充攻击对重复使用密码的用户造成严重危害，现有方法要么限制可用性，要么依赖复杂的账户共享机制，难以实际部署。

Method: 引入密码重用关系概念，构建网站图，使用图神经网络进行链接预测，结合联邦学习保护隐私，无需跨管理员共享敏感信息。

Result: 在包含3.6亿泄露账户、22,378个网站的真实数据集上，联邦学习设置下F1分数达到0.9153，比其他先进GNN模型性能提升4-11%。

Conclusion: 该方法能有效预测凭证填充风险，生成可操作的风险评分，同时保护用户隐私，具有实际部署价值。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [240] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习算法扩展到离散和连续复值波系统，适用于弱耗散机制，在激子极化凝聚体中实现了稳定收敛的物理系统原位学习。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)作为替代方案具有相似效率和原位训练潜力，但需要扩展到更广泛的物理系统。

Method: 扩展EP学习到离散和连续复值波系统，在弱耗散机制下有效，使用可训练的局部势能替代节点间连接，在激子极化凝聚体中基于广义Gross-Pitaevskii动力学进行测试。

Result: 在标准基准测试（包括逻辑任务和手写数字识别）中数值研究表明，该方法实现了稳定收敛。

Conclusion: 为系统控制仅限于局部参数的物理系统提供了一条实用的原位学习路径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [241] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中的模态缺失问题，通过去冗余同质-异质分解和分布对齐自蒸馏来恢复缺失语义


<details>
  <summary>Details</summary>
Motivation: 现实应用中由于遮挡、隐私约束和设备故障导致模态缺失，现有方法忽略此问题导致泛化性差

Method: 使用去冗余同质-异质分解模块将模态分解为同质、异质和噪声表示，并设计分布对齐自蒸馏模块进行双向知识转移

Result: 在两个数据集上的实验表明，FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势

Conclusion: FSRF框架能有效缓解多模态情感分析中的模态缺失问题，提高模型在现实场景中的泛化能力

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [242] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个基于门控的持续自编辑框架，使用LoRA进行参数高效微调，通过三种指标评估编辑稳定性，有效缓解大语言模型在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要持续适应新知识而不进行完整重训练，但顺序更新会导致灾难性遗忘，新编辑会损害先前获得的知识。

Method: 使用LoRA进行参数高效微调，通过三种指标（精确匹配下降、比特增加、KL散度）评估候选编辑的稳定性，超过阈值时通过裁剪程序重新缩放或拒绝LoRA更新。

Result: 在Qwen-2.5-7B模型上的实验表明，门控机制有效缓解遗忘同时保持适应性，基于精确匹配的门控在短持续学习序列中实现了最高的累积性能。

Conclusion: 不同门控策略可以实现可比较的分布偏移，但产生不同的准确性结果，突显了门控设计在持续适应中的重要性，为持续模型编辑提供了原则性方法。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [243] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过分层压缩来提升多示例上下文学习的内存和计算效率，在保持高准确率的同时实现3-8倍的压缩比。


<details>
  <summary>Details</summary>
Motivation: 上下文学习中增加示例数量能提升性能但带来更高的内存和计算成本，需要有效的压缩方法来平衡性能与效率。

Method: 提出MemCom分层压缩方法，使用更强的压缩器模型，在每个transformer层进行细粒度压缩，为每层提供独立的压缩表示。

Result: 在多个分类任务上，MemCom在所有压缩比下均优于基线方法，在更高压缩比下性能下降小于10%，而基线方法性能下降超过20-30%。

Conclusion: MemCom通过分层压缩策略有效解决了多示例上下文学习的效率问题，在保持高准确率的同时显著降低了内存和计算成本。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [244] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一种决策支持系统，通过预训练的AI代理缩小人类可采取的行动范围，实现人机互补性，在野火缓解游戏中使参与者表现提升30%


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中实现人机互补性，让专家使用系统时比单独使用AI或人类专家做出更准确的预测

Method: 使用预训练AI代理缩小人类可采取的行动子集，然后让人类从该行动集中选择行动，并引入利用动作集平滑特性的多臂老虎机算法来优化人类代理水平

Result: 在1600人参与的大规模人类研究中，使用该系统的参与者在野火缓解游戏中表现比单独参与者高30%，比AI代理高2%以上

Conclusion: 证明了在顺序决策任务中通过自适应控制人类代理水平可以实现人机互补性，系统能显著提升人类决策表现

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [245] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与Dreamer家族的PlaNet模型相比，在潜在重建质量和图像重建相似性方面表现相当，且在长时程预测上表现更优。


<details>
  <summary>Details</summary>
Motivation: 世界模型在强化学习中广泛应用，能够建模环境转移动态并提高样本效率。现有模型如Dreamer需要训练过程，本文旨在探索无需训练的世界模型方法。

Method: 利用相似性搜索和随机表示来近似世界模型，避免了传统的训练过程。

Result: 搜索式世界模型在潜在重建和图像重建相似性方面与基于训练的方法相当，在长时程预测任务中表现优于基线模型。

Conclusion: 基于搜索的世界模型是训练式世界模型的有效替代方案，特别是在长时程预测任务中表现更佳。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [246] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 首次对时变学习策略下的Q-learning算法进行有限时间分析，证明了在最小假设下的收敛性，匹配了离策略Q-learning的样本复杂度但探索性较弱。


<details>
  <summary>Details</summary>
Motivation: 解决时变学习策略（在线采样）下Q-learning的理论分析空白，特别是在最小探索假设下的收敛性证明。

Method: 使用泊松方程将马尔可夫噪声分解为鞅差项和残差项，通过敏感性分析控制时变策略下的残差项。

Result: 建立了期望无穷范数平方的最终迭代收敛率，样本复杂度为O(1/ε²)，在线Q-learning探索性较弱但具有策略收敛优势。

Conclusion: 在线Q-learning在探索性上弱于离策略版本，但策略会收敛到最优策略，所开发的分析工具可用于其他时变策略的强化学习算法。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [247] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出NAMEx框架，通过纳什博弈论重新解释专家合并过程，实现更平衡高效的专家协作，在多个任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有稀疏专家混合模型(SMoE)的专家合并策略缺乏原则性的权重机制，需要更平衡高效的专家协作方法

Method: 基于博弈论视角，引入纳什讨价还价到专家合并过程，并加入复动量加速专家传播，具有理论收敛保证

Result: 在语言建模、文本分类、图像分类和数据损坏下的零样本鲁棒性等任务中，NAMEx始终优于竞争方法，并能无缝集成到主流MoE架构

Conclusion: NAMEx框架在大型系统(Qwen1.5-MoE 14B和DeepSeek-MoE 16B)中证明有效，适用于零样本和微调设置，具有良好的可扩展性

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [248] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合零阶优化和锐度感知最小化的新方法，通过指数倾斜目标在平均损失和最大损失之间平滑过渡，实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化的是平滑后的函数（即随机扰动参数下的期望目标），而SAM方法关注邻域内的最大损失以获得更平坦的最小值。本文旨在明确连接这两种方法。

Method: 提出了指数倾斜目标，通过倾斜参数t在平均损失和最大损失公式之间平滑过渡，开发了新的零阶算法来求解软SAM目标。

Result: 该方法在分类、多项选择问答和语言生成等下游任务上比传统零阶基线方法实现了更好的泛化性能，可作为SAM变体的无梯度和内存高效替代方案。

Conclusion: 倾斜SAM框架提供了零阶优化和SAM方法之间的明确连接，在保持计算效率的同时实现了更好的泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [249] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: 提出了GRUwE模型，一种基于GRU的架构，用于处理不规则采样的多元时间序列，通过两种重置机制在连续时间内进行预测，在多个真实世界基准测试中表现出与最先进方法相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样多元时间序列建模的挑战，验证简单高效的基于RNN的算法是否仍然能够与复杂架构竞争，甚至超越这些方法。

Method: GRUwE模型基于GRU架构，维护时间序列的马尔可夫状态表示，通过两种重置机制更新状态：(i)观测触发重置，(ii)使用可学习指数衰减的时间触发重置，支持连续时间内的回归和事件预测。

Result: 在多个真实世界基准测试中，GRUwE在下一个观测和下一个事件预测任务上实现了与最新SOTA方法相当甚至更优的性能。

Conclusion: GRUwE不仅性能优异，而且具有实现简单、超参数调优需求少、在线部署计算开销显著降低等优势，证明了简单高效的RNN架构在不规则时间序列建模中仍然具有竞争力。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [250] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 对三种代表性生成模型（AtomGPT、CDVAE、FlowMM）在超导材料数据集上的性能进行系统评估，CDVAE表现最佳


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料发现中应用日益广泛，但缺乏对其性能的严格比较评估

Method: 使用JARVIS Supercon 3D和Alexandria数据库的DS A/B数据集，训练三种模型重构晶体结构，通过KL散度和MAE评估性能

Result: CDVAE表现最优，其次是AtomGPT，FlowMM表现最差

Conclusion: CDVAE在晶体结构重构任务中表现最佳，所有基准测试代码和模型配置将公开

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [251] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文通过层间因果修补分析语言模型对齐机制，发现对齐过程在空间上是局部化的，主要由中间层激活编码的特定子空间决定奖励一致行为，而非参数扩散过程。


<details>
  <summary>Details</summary>
Motivation: 尽管基于人类反馈的强化学习(RLHF)已成为语言模型偏好微调的主流方法，但其内部对齐机制仍不透明，需要系统分析对齐如何实现。

Method: 在Llama-3.2-1B模型上，通过层间因果修补技术分析基础模型与其调优版本在人类偏好对上的差异，并使用LASSO回归识别激活距离与奖励增益的关联。

Result: 发现对齐在空间上是局部化的：中间层激活编码了决定奖励一致行为的独特子空间，而早期和晚期层基本不受影响；只有少数层具有连接激活距离与奖励增益的非零系数。

Conclusion: 基于人类偏好的语言模型对齐是一个方向性、低秩的过程，而非扩散性、参数化的过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [252] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 该论文提出通过嵌入群等变卷积层（旋转和尺度等变）来增强CNN对抗鲁棒性的架构方法，无需对抗训练即可提升模型对对抗攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 对抗训练作为主要防御策略存在计算成本高且可能降低干净数据准确性的问题，需要探索更高效的架构方法来增强模型对抗鲁棒性。

Method: 提出两种对称感知架构：并行设计（独立处理标准和等变特征后融合）和级联设计（顺序应用等变操作），在CNN中嵌入旋转和尺度等变卷积层。

Result: 在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，模型在FGSM和PGD攻击下均表现出更好的对抗鲁棒性和泛化能力，且无需对抗训练。

Conclusion: 对称强制架构可作为基于数据增强防御的高效且原则性替代方案，通过编码对称先验来平滑决策边界并增强对抗弹性。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [253] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文主张强化学习研究应从单纯展示智能体性能转向更关注学习动态的科学理解，并需要更精确地将基准测试映射到数学形式化。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度关注性能展示而忽视学习动态理解，可能导致在学术基准上过拟合，难以将技术迁移到新问题。

Method: 以Arcade Learning Environment为例，说明即使被认为是"饱和"的基准仍可有效用于发展对强化学习的理解。

Result: 论证了性能导向研究的局限性，并提出重新聚焦于科学理解和基准精确性的必要性。

Conclusion: 强化学习研究应平衡性能展示与科学理解，并更精确地定义基准与数学形式化之间的映射关系。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [254] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出了一种基于运行时监控语言(RML)的新型语言奖励机，能够表达非正则、非马尔可夫任务的奖励函数，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励函数通常被视为黑盒映射，缺乏解释性，而传统奖励机只能表达正则语言，无法处理计数或参数化条件等复杂行为。

Method: 利用RML的内置内存机制，开发基于语言的奖励机，扩展了奖励函数的表达能力。

Result: 实验证明该方法能够表达非正则、非马尔可夫任务，在事件处理和任务规范方面比现有奖励机方法更具灵活性。

Conclusion: 基于RML的语言奖励机为复杂强化学习任务提供了更强大的奖励函数规范能力。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [255] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出结合关系强化学习与目标中心表示的新框架，处理结构化和非结构化数据，通过主动查询人类专家来增强学习


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在结构化问题上表现有限，关系强化学习虽然能处理结构化问题但假设过强，需要更灵活的方法

Method: 结合关系强化学习与目标中心表示，通过显式建模策略不确定性来主动查询人类专家指导

Result: 实证评估显示该方法在有效性和效率方面表现优异

Conclusion: 所提框架能够有效处理结构化和非结构化数据，通过主动学习机制提升强化学习性能

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [256] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 该论文研究非平稳多臂老虎机问题，其中奖励取决于动作和潜在状态，状态动态受未知线性动态控制且受动作影响。提出探索-承诺算法，通过随机动作估计线性动态参数，在承诺阶段优化动作序列以实现长期奖励，获得次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决动作和潜在状态共同影响奖励的非平稳老虎机问题，其中状态动态也受动作影响，导致短期和长期奖励之间的权衡需求。

Method: 提出探索-承诺算法：探索阶段使用随机Rademacher动作估计线性动态的马尔可夫参数；承诺阶段利用估计参数设计优化动作序列以最大化长期奖励。

Result: 算法实现$\tilde{\mathcal{O}}(T^{2/3})$遗憾界，提供了系统识别的样本复杂度和误差界，并证明了与NP难问题的等价性。

Conclusion: 成功解决了从时间相关奖励中学习和设计长期最优动作序列的挑战，提出了实用的半定松弛方法。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [257] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 该论文对标签噪声检测方法进行了系统性基准测试，通过将方法分解为三个核心组件（标签一致性函数、聚合方法和信息收集方式），提出了统一的评估框架和新的评价指标。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中的标签噪声会影响模型训练和验证，但现有检测方法缺乏统一评估标准，难以确定最优方法。

Method: 将标签噪声检测方法分解为三个基本组件：标签一致性函数、聚合方法和信息收集方式（样本内vs样本外），并提出了统一的基准任务和新的评价指标（固定操作点的假阴性率）。

Result: 研究发现，使用对数边际作为标签一致性函数、平均概率聚合和样本内信息收集的组合在大多数场景下表现最佳。

Conclusion: 该研究为设计新的检测方法和为特定应用选择技术提供了实用指导，识别出了在不同场景下表现最优的检测方法组件组合。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [258] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究使用机器学习方法分析欧洲绿色协议中的气候政策进展，比较了不同文本表示方法和元数据对政策进展预测的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来减轻其影响，本研究旨在探索机器学习在理解气候政策从宣布到采纳过程中的应用。

Method: 收集了165项政策的文本和元数据，使用TF-IDF、BERT和ClimateBERT等文本表示方法，结合元数据特征来预测政策进展状态。

Result: 仅使用文本特征时，ClimateBERT表现最佳（RMSE = 0.17, R² = 0.29）；结合元数据特征时，BERT表现最优（RMSE = 0.16, R² = 0.38）。

Conclusion: 研究结果表明机器学习工具在支持气候政策分析和决策方面具有潜力，可解释AI方法揭示了政策措辞、政党和国家代表等因素的影响。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [259] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 该论文分析了神经网络中一比特权重压缩的理论基础，证明在随机特征模型中，除最后一层外的所有权重量化不会导致泛化误差损失，并展示了实际推理加速效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络的计算和内存需求日益增长，一比特权重压缩能在资源受限设备上实现高效推理，但其理论基础尚不明确。

Method: 在随机特征模型框架下分析一比特量化，该模型对应具有随机表示的神经网络。

Result: 理论证明除最后一层外的所有权重量化不会损失泛化误差；实验证实一比特量化在笔记本电脑GPU上显著提升推理速度。

Conclusion: 研究为一比特神经网络压缩提供了理论支撑，证明在随机特征模型中量化是可行的，且能带来实际性能提升。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [260] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个可扩展的强化学习Web代理环境，通过紧凑的浏览器环境和高效的Web服务器启动机制，解决了现有环境在上下文噪声、动作不确定性和扩展性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RL Web代理环境存在上下文噪声过多、动作执行不确定、无法有效扩展并行RL rollout等问题，需要一种既能提供真实浏览器交互又能控制服务器状态的可扩展环境。

Method: 提出WEBSERV环境：1）紧凑、站点无关的浏览器环境，平衡上下文和动作复杂性；2）通过高效启动和重置Web服务器实现可扩展的RL环境。

Result: 在WebArena的购物CMS和Gitlab任务中达到最先进的单提示成功率，启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，单个主机支持200+并发容器。

Conclusion: WEBSERV提供了一个高效、可扩展的RL Web代理训练和评估环境，显著提升了性能并降低了资源需求。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [261] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经常微分方程的连续深度Evoformer模型，用Neural ODE替代AlphaFold中48个离散块，实现恒定内存成本和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: AlphaFold等蛋白质结构预测模型虽然强大，但其48层Evoformer架构计算成本高且层间离散化，需要更高效的连续深度替代方案。

Method: 用Neural ODE参数化Evoformer的核心注意力操作，通过伴随方法实现恒定内存成本，使用自适应ODE求解器平衡运行时间和精度。

Result: 模型能生成结构合理的预测并可靠捕捉α螺旋等二级结构元素，但未完全复现原架构精度；仅需单GPU训练17.5小时，资源消耗大幅降低。

Conclusion: 连续深度模型为生物分子建模提供了轻量级、可解释的替代方案，为高效自适应蛋白质结构预测框架开辟了新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [262] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 该论文从范畴论角度分析超边解缠，提出基于自然性条件的新解缠准则，并在基因通路数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管解缠表示学习在图结构数据中表现出色，但很少有研究探索超图结构数据的解缠。将超边解缠整合到超图神经网络中，可以利用与标签相关的隐藏超边语义（如节点间未标注的关系）。

Method: 从范畴论视角分析超边解缠，提出基于自然性条件的新解缠准则，并构建概念验证模型。

Result: 概念验证模型在基因通路数据上成功捕获了基因（节点）在遗传通路（超边）中的功能关系，证明了所提准则的潜力。

Conclusion: 提出的基于自然性条件的解缠准则能够有效捕捉超图中的隐藏语义关系，为超图解缠表示学习提供了新的理论框架。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [263] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出一种结合SVD和量化的方法，通过动态调整SVD秩来减少KV缓存大小和计算开销，在保持精度的同时显著降低内存使用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)在图像描述和视觉问答等任务中很重要，但其高计算成本和内存占用限制了可扩展性和实时应用。

Method: 对QKV权重矩阵应用奇异值分解(SVD)来减少KV缓存，引入动态SVD秩分配策略，并结合权重和激活的量化。

Result: 相比仅使用量化或SVD的方法，精度提升超过10%，同时硬件成本更低。

Conclusion: 该方法在资源受限设备上更适合实时部署，实现了高效视觉语言模型。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [264] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于支架的虚拟筛选框架，通过生成式AI增强数据、模型无关的自训练和重排序模块，解决类不平衡、结构不平衡和支架多样性问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三个主要挑战：类不平衡（活性分子比例低）、活性分子间的结构不平衡（某些支架占主导地位）、以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于实际命中化合物的支架生成合成数据，通过支架感知采样算法为代表性不足的支架生成更多样本；2. 模型无关自训练模块：安全整合生成的合成数据和原始标记数据；3. 重排序模块：在保持整体性能的同时提高推荐分子集中的支架多样性。

Result: 在五个靶标类别上进行的综合计算实验表明，ScaffAug在多个评估指标上优于现有基线方法，并通过消融研究验证了其有效性。

Conclusion: 该工作通过利用生成增强、重排序和通用支架感知，为有效增强虚拟筛选提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [265] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一个针对有向图的对比学习框架，通过复数域和实数域两个互补的空间视角，结合磁拉普拉斯算子和路径子图增强策略，在有向图表示学习中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法主要关注无向图，忽略了现实世界网络（如社交网络和推荐系统）中至关重要的方向性信息。

Method: 从复数域视角引入磁拉普拉斯算子进行个性化扰动来调节边相位和方向语义；从实数域视角采用基于路径的子图增强策略来捕捉细粒度局部不对称性和拓扑依赖；联合利用这两个互补空间视图构建高质量正负样本。

Result: 在7个真实世界有向图数据集上的实验表明，该方法在节点分类和链接预测任务上分别实现了4.41%和4.34%的性能提升，在监督和无监督设置下均达到最先进水平。

Conclusion: S2-DiGCL通过整合复数域和实数域的空间洞察，为有向图对比学习提供了更通用和鲁棒的表示学习方法。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [266] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 该论文探讨了记忆与组合能力之间的协同作用，表明记忆长尾特征与组合能力结合可以帮助模型对未见过的长尾特征组合做出正确预测。


<details>
  <summary>Details</summary>
Motivation: 重新思考深度学习中记忆与泛化的关系，研究记忆如何通过与组合能力的协同作用来提升模型对罕见测试样本的预测能力。

Method: 在理论分析中使用线性设置证明记忆与组合的协同作用，并在简单数据上通过神经网络架构实验验证理论洞察。

Result: 理论证明和实验结果表明，记忆长尾特征与组合能力结合可以使模型对训练数据中未出现的长尾特征组合做出正确预测，且模型的组合能力依赖于其架构。

Conclusion: 记忆与组合能力之间存在协同效应，这种协同作用能够增强模型对罕见测试样本的泛化能力，特别是在处理长尾特征组合时。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [267] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出MGTS-Net，一种用于时间序列预测的多模态图增强网络，通过优化特征提取、构建异构图融合多模态信息、动态加权多尺度预测器来解决现有方法的三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在整合多模态特征时面临三个主要挑战：细粒度时间模式提取不足、多模态信息整合欠佳、对动态多尺度特征适应性有限。

Method: MGTS-Net包含三个核心组件：1）多模态特征提取层，针对时序、视觉和文本模态优化特征编码器；2）多模态特征融合层，构建异构图建模模态内时序依赖和模态间对齐关系；3）多尺度预测层，动态加权融合短期、中期和长期预测器输出。

Result: 大量实验表明MGTS-Net在轻量级和高效率下表现出优异性能，相比其他最先进的基线模型实现了更优性能。

Conclusion: 该方法验证了所提方法的优越性，能够有效解决多模态时间序列预测中的关键挑战。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [268] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出了一种稀疏transformer架构，将数据分布的先验信息直接融入神经网络结构，基于正则化Wasserstein近端算子设计，相比传统流模型优化问题凸性更好且生成样本更稀疏。


<details>
  <summary>Details</summary>
Motivation: 将数据分布先验信息直接融入transformer结构，改善传统流模型的优化问题凸性并促进生成样本的稀疏性。

Method: 基于正则化Wasserstein近端算子设计稀疏transformer架构，该算子有闭式解且是transformer架构的特殊表示。

Result: 理论分析和数值实验表明，在生成建模和贝叶斯逆问题中，稀疏transformer比传统神经ODE方法精度更高、收敛到目标分布更快。

Conclusion: 稀疏transformer架构在精度和收敛速度方面优于传统神经ODE方法，为生成建模和逆问题提供了有效解决方案。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [269] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE是一种新型的稀疏专家混合模型，通过引入专家间的社交图结构来增强令牌路由过程，解决了传统SMoE在数据分布变化下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏专家混合模型(SMoE)虽然能解耦模型参数量和计算成本，但在面对数据分布变化时鲁棒性较差，特别是在数据污染情况下性能下降明显。

Method: 提出SymphonySMoE，通过构建专家间的社交图来建模专家交互，改进令牌路由过程。该方法轻量、模块化，可与现有SMoE模型无缝集成。

Result: 在语言建模和视觉指令调优任务上的大量实验验证了方法的有效性，并成功扩展到42亿和74亿参数的大规模模型，在微调任务中表现出良好的适用性。

Conclusion: SymphonySMoE通过图结构增强了SMoE的鲁棒性，同时保持了模型的可扩展性和效率，为解决SMoE在分布变化下的挑战提供了有效方案。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [270] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了在ECML-PKDD 2025高能物理发现竞赛中获胜的对抗攻击方案，通过多轮梯度策略实现了最小扰动下的最大误分类效果。


<details>
  <summary>Details</summary>
Motivation: 竞赛任务要求设计对抗攻击，在最小化扰动的同时最大化分类模型的误分类率，挑战在保持攻击有效性的同时控制扰动规模。

Method: 采用多轮梯度攻击策略，利用模型的可微分结构，结合随机初始化和样本混合技术来增强攻击效果。

Result: 该攻击方法在扰动大小和欺骗成功率方面取得了最佳结果，在竞赛中获得第一名。

Conclusion: 提出的多轮梯度攻击策略结合随机初始化和样本混合技术，能够有效生成高质量对抗样本，在保持小扰动的同时实现高误分类率。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [271] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了ECML-PKDD 2025挑战赛Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在对抗性攻击下实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够在干净数据和对抗性数据上都表现良好的鲁棒ANN模型，应对高能物理发现中的对抗攻击挑战。

Method: 采用两阶段方法：1) 基于RDSA方法生成1500万人工训练样本；2) 设计包含特征嵌入块和密集融合尾部的鲁棒架构。

Result: 在混合数据集上达到80%的准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法和鲁棒架构设计能够有效提升模型在对抗性攻击下的性能表现。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [272] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，解决现有稀疏专家混合模型在专家专业化和计算平衡之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性得分的路由机制难以有效捕捉输入底层结构，导致专家专业化与计算平衡之间的权衡，限制了可扩展性和性能。

Method: 使用概率混合模型建模路由概率，使专家形成清晰的专业化边界并实现平衡利用；路由机制独立于任务特定目标进行训练，实现稳定优化和明确的专家分配。

Result: 在视觉语言任务上的实证结果表明，该方法持续优于现有稀疏专家混合方法，获得更高的任务性能和改善的专家利用平衡。

Conclusion: 提出的Input Domain Aware MoE框架通过概率混合模型改进了路由机制，在保持计算效率的同时提升了专家专业化和利用平衡，为大规模视觉语言模型的扩展提供了更有效的解决方案。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [273] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一个用于模仿学习的序列强化学习框架，专门建模传粉昆虫的异质认知策略，通过轨迹相似性捕捉不同个体的行为模式，并开发了可解释的模型来识别有效记忆范围。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在专家策略随记忆窗口变化或偏离最优性时表现不佳，无法捕捉快速和慢速学习行为，且缺乏可解释性，限制了生物学洞察。

Method: 引入最小化预测损失的模型，识别与行为数据最一致的有效记忆范围，确保完全可解释性，并提供将蜜蜂策略搜索与多臂老虎机公式联系起来的数学框架。

Result: 创建了包含80只蜜蜂在不同天气条件下追踪数据的新数据集，改进了对昆虫行为的模拟，为传粉昆虫认知研究提供了基准。

Conclusion: 该研究揭示了塑造传粉昆虫决策的学习策略和记忆相互作用，为生态治理提供了更好的昆虫行为模拟工具。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [274] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出一种新颖的自适应核注意力机制，通过分别处理不同特征组然后整合，解决高维异构数据中复杂非线性关系和跨尺度交互的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 传统PLS方法难以建模复杂非线性关系，特别是在具有高维相关结构的多元系统中。同时跨尺度交互和静态特征权重限制了模型对上下文变化的适应性。

Method: 引入自适应核注意力机制，分别处理不同特征组后进行整合，能够捕捉局部模式同时保持全局关系。

Result: 实验结果显示在多个数据集上相比最先进方法在性能指标上有显著提升。

Conclusion: 所提出的方法通过架构创新有效解决了高维异构数据中的复杂建模挑战，提升了预测性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [275] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单可解释的无监督多元时间序列异常检测框架，通过因果嵌入建模时间动态，使用自注意力机制捕捉空间关系，并将嵌入对齐到稳定潜在结构来识别异常。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多元时间序列异常罕见且通常无标签，现有方法依赖复杂架构，只能检测异常片段且性能被高估。

Method: 将每个变量的过去序列编码为因果嵌入来联合预测当前时间点和重构输入窗口，使用自注意力机制将嵌入投影到共享潜在空间，对齐到稳定潜在结构，通过预测误差和偏离SLS的双重评分机制识别异常。

Result: 在多个真实世界数据集和评估协议上实现了最先进的结果。

Conclusion: OracleAD在保持可解释性的同时，通过稳定潜在结构直接定位根因变量，实现了优异的异常检测性能。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [276] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于连通性因子(CF)的eDCF方法，用于跨尺度稳健估计高维数据的本征维度，在噪声环境下表现优异，并能准确检测分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 现代高维数据集具有复杂依赖关系，本征维度估计面临尺度依赖挑战：细尺度下噪声会膨胀估计值，粗尺度下估计值趋于稳定但较低。需要一种能跨尺度稳健估计的方法。

Method: 基于连通性因子(CF)这一局部连通性度量，开发了eDCF方法，具有可扩展性和并行化特性。

Result: 在合成基准测试中与领先估计器表现相当，MAE值相近；在精确本征维度匹配率上达到25.0%，优于MLE(16.7%)和TWO-NN(12.5%)，在中高噪声和大数据集下表现尤其突出；能准确检测决策边界中的分形几何。

Conclusion: eDCF方法能跨尺度稳健估计本征维度，在噪声环境下具有优势，适用于分析现实结构化数据。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [277] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 论文挑战了LLM在因果发现中的表面成功，指出现有评估存在数据泄露问题，并提出了基于真实科学研究的评估协议和结合LLM与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在因果发现中的优异表现可能源于预训练数据中的基准测试泄露，而非真正的因果推理能力。需要开发可靠的评估方法和实用的混合方法。

Method: 提出两种转变：(1)基于近期科学研究开发防泄露的评估协议，使用训练截止期后发布的真实科学文献构建因果图；(2)设计混合方法，将LLM预测作为经典PC算法的先验知识。

Result: 在传统基准测试(如BNLearn)中LLM表现接近完美，但在作者构建的真实科学因果图上表现较差。将LLM预测作为PC算法先验能显著提高准确率。

Conclusion: 呼吁社区采用基于科学的防泄露基准测试，并投资开发适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [278] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 本研究使用机器学习算法预测生活满意度，准确率达93.80%，并探索临床和生物医学大语言模型的应用，发现生物医学领域与生活满意度预测更相关。


<details>
  <summary>Details</summary>
Motivation: 传统的生活满意度测量方法存在验证和传播问题，需要更准确、可复现的评估方法。

Method: 使用丹麦19000人的政府调查数据，通过特征学习提取27个重要问题，并探索将表格数据转换为自然语言句子的大语言模型方法。

Result: 机器学习模型达到93.80%准确率和73.00%宏F1分数，大语言模型达到93.74%准确率和73.21%宏F1分数。健康状况是所有年龄段最重要的决定因素。

Conclusion: 机器学习、大语言模型和可解释AI共同构建了对使用AI研究人类行为的信任和理解，对量化主观幸福感具有重要意义。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [279] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: NeurIPT是一个用于脑电图(EEG)基础模型，通过预训练Transformer处理EEG信号中的时空特征，在多个脑机接口数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: EEG数据存在显著的受试者间、任务间和条件间变异性，以及不同的电极配置，这使得建立可扩展和泛化的基础模型具有挑战性。

Method: 提出NeurIPT模型，包括：1) 基于信号幅度的掩码预训练(AAMP)；2) 渐进式专家混合架构(PMoE)；3) 利用电极3D物理坐标；4) 微调时的脑叶内外池化(IILP)。

Result: 在八个下游脑机接口数据集上的评估显示，NeurIPT持续实现了最先进的性能，展示了其广泛的适用性和稳健的泛化能力。

Conclusion: 这项工作推动了EEG基础模型的发展，并为可扩展和可泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [280] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO是一个将语言反馈与数值奖励分离的强化学习框架，通过构建动态经验池和两个原则（奖励无关反思和相关抽象）来有效利用历史经验，提高大语言模型在数学推理任务中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在大语言模型中仅使用标量奖励，丢弃了有价值的文本推理过程，导致样本效率低下。同时，在线整合语言反馈存在信息泄露或行为崩溃的困境。

Method: 提出LANPO框架：1）语言反馈指导探索，数值奖励驱动优化；2）构建动态经验池；3）引入奖励无关反思原则进行样本内自我修正；4）使用相关抽象原则从样本间经验中提取可泛化知识。

Result: 在数学推理基准测试中，7B和14B模型使用LANPO显著优于GRPO基线模型，在测试准确率上表现更好。

Conclusion: LANPO为将历史经验整合到LLM强化学习循环中提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [281] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C-SMILES分子表示法，通过分解SMILES为元素-标记对和特殊标记，减少反应物与产物间的编辑距离，结合复制增强机制和SMILES对齐指导，显著提升逆合成预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测精度降低，需要一种能更好表示分子结构不变性的方法。

Method: 开发C-SMILES表示法，将传统SMILES分解为元素-标记对和五个特殊标记；引入复制增强机制动态决定生成新标记或保留未变化分子片段；集成SMILES对齐指导增强注意力一致性。

Result: 在USPTO-50K数据集上达到67.2%的top-1准确率，在USPTO-FULL数据集上达到50.8%准确率，生成分子有效性达99.9%。

Conclusion: 建立了一种结构感知分子生成的新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [282] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标注数据的分子推理框架，通过原子标识符将思维链推理锚定到分子结构上，在单步逆合成任务中取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 化学中机器学习应用常受限于标注数据的稀缺和昂贵，限制了传统监督方法的使用

Method: 使用通用大语言模型，通过原子标识符将思维链推理锚定到分子结构，包括一步识别相关片段和可选的两步预测化学转化

Result: 在学术基准和专家验证的药物发现分子上，LLMs在识别化学合理反应位点(≥90%)、命名反应类别(≥40%)和最终反应物(≥74%)方面取得高成功率

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种通过将化学知识映射到分子结构来生成理论基础的合成数据集的方法，从而解决数据稀缺问题

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [283] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 该论文研究了神经网络中参数对称性和表达能力在泛化行为中的作用，特别关注学习重整化群变换时的表现。研究发现对称约束和表达能力之间存在竞争关系，过于复杂或过度约束的模型泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 将物理对称性编码到深度学习模型中可以提高性能，但参数对称性破坏和恢复机制在层次学习动态中的作用需要进一步研究。

Method: 使用多层感知机(MLP)和图神经网络(GNN)，通过改变权重对称性和激活函数来评估模型在中心极限定理(CLT)作为测试案例时的表现。通过将CLT重构为累积量递推关系来分析MLP的泛化行为。

Result: 发现对称约束和表达能力之间存在竞争关系，过于复杂或过度约束的模型泛化能力较差。成功将累积量传播框架从MLP扩展到GNN，阐明了这些复杂模型的内部信息处理机制。

Conclusion: 这些发现为对称网络的学习动态及其在建模结构化物理变换中的局限性提供了新的见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [284] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种四元数值监督学习Hopfield结构神经网络(QSHNN)，利用四元数的几何优势表示旋转和姿态，通过周期性投影策略保持四元数结构一致性，在机器人控制等应用中表现出高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态方面的几何优势，扩展经典Hopfield神经网络到四元数域，为机器人控制等应用提供更有效的神经网络模型。

Method: 从连续时间HNN动力学模型扩展到四元数域，引入周期性投影策略修改梯度下降，将权重矩阵的4*4块投影到最接近的四元数结构，保持收敛性和四元数一致性。

Result: 实验模型实现高精度、快速收敛和强可靠性，演化轨迹具有良好有界曲率（足够平滑性），适用于机器人关节姿态参数化等控制应用。

Conclusion: 该模型为超复数或非交换代数结构下的神经网络设计提供了实用的实现框架和通用数学方法学，在机器人控制等领域具有重要应用价值。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [285] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文分析了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息的关系，将多步推理建模为知识图上的连通性问题，揭示了知识密度与增强效率之间的相变现象。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型参数知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下回答查询所需的预训练知识量。

Method: 将多步推理建模为知识图上的s-t连通性问题，将预训练知识表示为部分有噪声子图，将增强视为查询真实边来扩展模型知识。

Result: 发现相变现象：当知识图断开为小组件时，通过增强寻找路径效率低下，需要Ω(√n)次查询；当正确知识密度超过阈值形成巨型组件时，可用常数期望查询次数找到路径。

Conclusion: 测试时增强的效率取决于预训练知识图的连通性，存在临界密度阈值，超过该阈值后增强效率显著提升。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [286] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 多阶段训练中的机器遗忘面临根本性障碍，局部遗忘方法无法普遍实现重训练等价性，因为遗忘结果受训练阶段顺序影响。


<details>
  <summary>Details</summary>
Motivation: 现代训练流程常涉及多阶段训练，每个阶段有不同的数据分布和目标，但现有的机器遗忘理论主要基于i.i.d.数据批次的训练场景。

Method: 通过理论和实验分析多阶段训练对机器遗忘的影响，在Llama和Qwen模型上使用梯度上升、NPO和SimNPO等局部遗忘算法进行实证研究。

Result: 不同训练阶段顺序的模型在遗忘过程中行为出现分歧，GSM8K准确率下降差异超过20%，某些学习路径产生的模型遗忘速度较慢。

Conclusion: 对于多阶段训练的模型，重训练等价性是局部遗忘算法的不适定目标，需要重新思考机器遗忘的定义和期望目标。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [287] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一个新颖的模拟自由方法，能够同时学习物理系统的结构和随机群体动力学，解决了高维随机系统中结构学习和动态建模的联合问题。


<details>
  <summary>Details</summary>
Motivation: 许多自然系统中的物理系统（如细胞生物学）本质上是高维和随机的，只能获得部分、有噪声的状态测量，这给建模底层动态和推断网络结构带来了重大挑战。现有方法通常只能单独处理结构学习或群体级动态建模，无法同时解决这两个问题。

Method: 提出了StructureFlow方法，这是一种原则性的模拟自由方法，能够联合学习物理系统的结构和随机群体动态。该方法适用于干预下的结构学习和条件群体动态的轨迹推断任务。

Result: 在合成高维系统、生物模拟系统和实验单细胞数据集上的实证评估表明，StructureFlow能够同时学习底层系统的结构并建模其条件群体动态。

Conclusion: StructureFlow能够同时学习底层系统的结构并建模其条件群体动态，这是理解系统行为机制的关键一步。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [288] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一个基于Vision Mamba架构的蛋白质-蛋白质对接评分函数，通过替换PIsToN中的Vision Transformer为Vision Mamba，在多个数据集上表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质-蛋白质对接工具依赖准确的评分函数来区分天然和非天然复合物。Mamba架构在自然语言处理和计算机视觉领域表现出色，有望改进基于Transformer的PIsToN模型。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba在图像块序列上的高效长程序列建模能力。

Result: 在多个广泛使用的大规模公共数据集上的评估表明，PUMBA始终优于其基于Transformer的前身PIsToN。

Conclusion: 使用Vision Mamba架构可以显著提升模型捕捉蛋白质-蛋白质界面特征中全局和局部模式的能力，从而改进蛋白质-蛋白质对接评分函数的性能。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [289] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出了一种在无信息先验条件下仍能有效进行主动目标发现的新方法，该方法具有理论依据、可解释性强，并能保证先验估计的单调改进。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高昂的领域（如医学成像、环境监测），传统基于生成模型的主动发现方法在数据极其有限时难以学习强先验，导致泛化能力不足。

Method: 设计了一个理论上有依据的框架，灵感来自神经科学，具有内在可解释性，能保证每次新观测都能单调改进先验估计。

Result: 在物种分布建模和遥感等多个领域的综合实验表明，该方法显著优于基线方法。

Conclusion: 该方法在复杂现实场景中实现了稳健的探索和适应性，解决了无信息先验条件下的主动目标发现问题。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [290] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出了一个紧凑的严格因果基准，用于在MIT-BIH心律失常数据库上使用每秒心率进行流式临床时间序列分析。研究了两个任务：短期心动过速风险预测和一步心率预测，比较了GRU-D和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 在纵向监测中，模型选择是任务依赖的，需要评估不同模型在临床时间序列分析中的表现差异。

Method: 使用MIT-BIH心律失常数据库，采用记录级非重叠分割，比较GRU-D和Transformer模型在心动过速风险预测和心率预测任务中的表现，使用温度缩放和分组bootstrap置信区间进行评估。

Result: 在MIT-BIH上，GRU-D在心动过速风险预测上略优于Transformer，而Transformer在预测误差上明显低于GRU-D和持久性模型。

Conclusion: 紧凑的RNN在短时域风险评分中仍具有竞争力，而紧凑的Transformer在点预测方面带来更明显的增益。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [291] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 该论文使用扩散方法精确分析噪声SGD，提供连续时间视角来捕捉高维设置中的统计风险演变和隐私损失动态，并研究了一种无需梯度灵敏度显式知识的噪声SGD变体。


<details>
  <summary>Details</summary>
Motivation: 优化与隐私的交互已成为隐私保护机器学习的核心主题。噪声SGD已成为关键算法，但现有工作主要提供统计风险和隐私损失的各种界限，过程的确切行为仍不清楚，特别是在高维设置中。

Method: 利用扩散方法分析噪声SGD，提供连续时间视角，专注于带有ℓ2正则化的最小二乘问题，研究无需梯度灵敏度显式知识的噪声SGD变体。

Result: 该方法能够精确捕捉高维设置中统计风险演变和隐私损失动态，提供对噪声SGD过程的更清晰理解。

Conclusion: 扩散方法为分析噪声SGD提供了精确的连续时间视角，能够更好地理解高维设置中的统计风险和隐私损失动态，且提出的变体无需梯度灵敏度显式知识。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [292] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种多任务学习方法，使用潜在变量多输出高斯过程来预测NLP模型的学习曲线，支持零样本预测并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以在满足特定性能目标的同时，减少计算开销并降低数据集获取和整理的成本。

Method: 将学习曲线预测任务建模为多任务学习问题，使用潜在变量多输出高斯过程来建模任务间和层次间的共享信息和依赖关系。

Result: 该方法能够以较低成本开发概率性缩放规律，通过主动学习策略减少预测不确定性，提供接近真实缩放规律的预测结果。

Conclusion: 该框架在三个小规模NLP数据集上得到验证，包括nanoGPT模型、双语翻译和多语言翻译模型，最多包含30条学习曲线。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [293] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: 提出了CLIP，一种面向安全联邦学习的客户端不变神经元剪枝技术，结合网络感知剪枝，解决异构设备中计算和网络瓶颈问题，在最小精度损失下加速训练13%-34%。


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在分布式模型训练中保护数据隐私，但在异构设备部署时，由于计算或网络能力有限的滞后客户端导致性能瓶颈，拖慢所有参与客户端的训练速度。

Method: CLIP：客户端不变神经元剪枝技术，结合网络感知剪枝，针对安全聚合中的滞后客户端问题，在客户端侧进行不变神经元剪枝。

Result: 在多个数据集（CIFAR10、Shakespeare、FEMNIST）上，安全联邦学习训练加速13%至34%，精度影响在1.3%提升到2.6%下降之间。

Conclusion: CLIP是首个针对安全聚合中滞后客户端缓解的技术，能有效解决异构设备环境下的计算和网络瓶颈问题，在保持模型精度的同时显著提升训练效率。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [294] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过空间相关性和时间频率特征提升零样本预测精度，在微气候预测中显著优于传统方法和现代时间序列模型。


<details>
  <summary>Details</summary>
Motivation: 零样本预测需要在没有直接历史数据的情况下预测未见条件的结果，这对传统预测方法构成重大挑战。

Method: 将信号分解为不同频率分量，采用分辨率感知检索机制：低频分量依赖更广泛的空间上下文，高频分量关注局部影响，动态检索相关数据并适应新位置。

Result: 在微气候预测中，模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE降低71%，比Chronos降低34%。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [295] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，发现在某些情况下，即使基于变量的因果效应不可识别，基于状态的因果效应仍可能可识别，这需要额外的背景特定独立性和条件函数依赖等知识。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性定义基于治疗和结果变量，但实际应用中可能需要更细粒度的状态层面分析。本文旨在研究基于状态的因果效应可识别性及其与变量层面可识别性的关系。

Method: 通过理论分析，研究状态层面因果效应的可识别性条件，特别关注背景特定独立性、条件函数依赖等额外知识的作用，以及变量状态约束知识的影响。

Result: 发现基于状态的因果效应在变量层面因果效应不可识别时仍可能可识别，这种分离仅在有额外知识时发生。变量状态约束知识单独不能改善可识别性，但与其他知识结合时可同时改善变量和状态层面的可识别性。

Conclusion: 研究揭示了现有变量框架可能遗漏的因果效应可识别性情况，强调了在观测数据分析中考虑状态层面效应的重要性，特别是在有额外领域知识时。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [296] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 使用LSTM神经网络框架处理电动汽车充电负荷数据，通过数据预处理和特征提取来预测未来充电需求，支持多时间尺度预测。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及，准确预测充电需求对电网规划、能源管理和充电设施整合至关重要。

Method: 采用LSTM模型，先对原始数据进行缺失值插补和归一化预处理，然后提取特征训练网络，捕捉充电数据的短期波动和长期趋势。

Result: 实验结果显示模型能准确预测日、周、月等多时间尺度的充电需求，为基础设施规划提供有价值洞察。

Conclusion: 该框架的模块化设计使其能适应不同充电地点的使用模式，具有广泛的应用前景。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [297] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出UDS框架，通过核范数捕获数据效用和样本内多样性，结合历史样本缓冲区估计样本间多样性，实现高效在线批次选择，无需外部资源且减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法计算成本高且容易过拟合，在线批次选择方法存在仅依赖数据效用、需要外部资源、增加训练时间等问题。

Method: UDS框架利用logits矩阵的核范数同时评估数据效用和样本内多样性，通过轻量级历史样本缓冲区比较低维嵌入来估计样本间多样性。

Result: 在多个基准测试中，UDS在不同数据预算下均优于现有在线批次选择方法，相比全数据集微调显著减少训练时间。

Conclusion: UDS提供了一种无需外部资源、计算高效的在线批次选择方法，在监督微调中实现了更好的性能和效率。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [298] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种用于SegDeformer的联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度，同时保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作使用卷积神经网络进行联合源和任务解码，但未研究基于transformer的替代方案如SegDeformer，后者性能更优但计算复杂度更高。

Method: 为SegDeformer提出联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度。

Result: 在车载应用中，Cityscapes数据集上fps提升11.7倍(1.4到16.5)，ADE20K数据集上提升3.5倍(43.3到154.3)，mIoU与基线相当。在分布式应用中，在广泛比特率范围内实现SOTA mIoU，仅使用先前SOTA 0.14%(ADE20K)和0.04%(Cityscapes)的云DNN参数。

Conclusion: 提出的方法在保持性能的同时显著降低了计算复杂度，提高了车载系统的实时性和分布式系统的可扩展性。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [299] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级的评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，引入了决策变量和约束的精确度、召回率等细粒度指标。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法通常将优化公式视为整体，依赖解决方案准确性或运行时间等粗略指标，这些指标掩盖了结构或数值错误，需要更细粒度的评估方法。

Method: 开发了包含决策变量和约束的精确度、召回率、约束和目标函数均方根误差、基于token使用和延迟的效率指标的综合评估框架。评估了GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同复杂度优化问题下的表现，使用了六种提示策略。

Result: GPT-5始终优于其他模型，思维链、自一致性和模块化提示策略最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精确度和决策变量指标起次要作用，简洁输出可提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整的约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率。该框架为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [300] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出SAMOSA算法，通过基于样本典型性的主动查询策略，在开放集主动学习中有效选择信息量丰富的样本，提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大量数据标注，但标注成本高昂。开放集主动学习旨在从未标注数据中选择信息丰富的样本，其中可能包含不相关或未知类别的样本

Method: 基于SGD和SAM的理论发现，SAMOSA根据样本典型性主动查询样本，有效识别嵌入流形中靠近模型决策边界的非典型样本

Result: 在多个数据集上，SAMOSA相比现有技术实现了高达3%的准确率提升，且没有引入计算开销

Conclusion: SAMOSA是一种有效的开放集主动学习查询算法，能够优先选择对目标类别高度信息丰富且有助于区分目标类别和不必要类别的样本

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [301] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，通过分析专家路由模式来识别蒸馏模型，在黑白盒场景下都有效，准确率超过94%


<details>
  <summary>Details</summary>
Motivation: 现有基于自识别或输出相似性的KD检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，提出Shadow-MoE黑盒方法构建代理MoE表示来比较模型间模式差异

Result: 在多种场景下检测准确率超过94%，对基于提示的规避具有强鲁棒性，优于现有基线方法

Conclusion: MoE结构习惯在蒸馏过程中持续传递，可作为有效的KD检测信号，提出的框架在黑白盒设置下都能可靠工作

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [302] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个用于3D第一人称视频游戏的多模态推理框架，通过收集大规模人类游戏数据、训练逆动力学模型和行为克隆代理，实现了基于文本指令的实时游戏操作。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏为实时多模态推理提供了具有挑战性的环境，需要解决游戏多样性、实时推理和文本指令响应等问题。

Method: 收集大规模多样化的人类游戏数据，训练逆动力学模型来推断缺失的动作，使用行为克隆方法训练文本条件代理，并设计支持实时推理的自定义架构。

Result: 开发出的模型能够在多种3D游戏中运行，并能响应文本输入，在消费级GPU上实现实时推理。

Conclusion: 虽然取得了进展，但仍面临长时程任务和大规模游戏定量评估等挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [303] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种后门遗忘攻击方法，通过在LLM遗忘过程中嵌入隐藏触发器，使得模型在正常条件下看似成功遗忘，但在触发条件出现时恢复被遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM的兴起，研究遗忘过程本身是否可能被后门攻击，即在正常条件下看似成功遗忘，但在特定触发条件下恢复被遗忘行为。

Method: 利用注意力汇聚现象，将触发器放置在汇聚位置并调整其注意力值，以增强后门遗忘的持久性。通过实验验证注意力汇聚引导的后门遗忘方法。

Result: 实验表明，基于注意力汇聚的后门遗忘攻击能够可靠地在后门触发器存在时恢复被遗忘知识，而在触发器缺失时与正常遗忘模型无法区分。

Conclusion: 注意力汇聚现象为后门遗忘攻击提供了有效途径，揭示了LLM遗忘过程中的安全漏洞，需要开发更安全的遗忘机制。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [304] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 提出3D-GSRD方法，通过选择性重掩码解码解决3D分子表示学习中的2D结构泄漏问题，在MD17基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩展掩码图建模从2D到3D面临挑战：既要避免2D结构信息泄漏到解码器，又要提供足够的2D上下文来重建重掩码原子。

Method: 提出选择性重掩码解码(SRD)，仅从编码器表示中重掩码3D相关信息，同时保留2D图结构；结合3D关系Transformer编码器和结构无关解码器。

Result: 在MD17分子性质预测基准测试中，8个目标中有7个达到新的最先进性能。

Conclusion: 3D-GSRD通过选择性重掩码解码和结构无关解码器的协同组合，增强了编码器在分子表示学习中的作用，实现了优异的3D分子表示性能。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [305] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 该论文提出了一个评估LLMs通过对话发现和利用潜在用户信息的统一基准，包含三个逐步现实化的任务设置，揭示了LLMs在潜在信息发现方面的表现差异显著。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成通用文本方面表现出色，但在需要用户特定偏好的场景中，这种通用性成为限制。用户很少明确表达所有偏好，大量信息是潜在的，需要通过对话来推断。

Method: 引入统一的潜在信息发现评估基准，采用三智能体框架（用户、助手、评判者），包含三个任务：经典20问游戏、个性化问答和个性化文本摘要，支持逐轮评估引导和适应能力。

Result: LLMs确实能够通过对话发现潜在信息，但成功率差异显著：从32%到98%，取决于任务复杂度、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统框架，表明有效的偏好推断仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [306] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了语言模型的混合精度量化(MXPLMs)技术，分析其在不同组件中的精度分配策略，比较各种框架的性能表现，并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模不断扩大，其计算、内存和能耗需求急剧增长，使得训练和部署变得不可持续。混合精度量化通过在不同层或张量中智能分配精度，在保持效率的同时维持模型准确性。

Method: 论文首先回顾量化基础知识，然后根据位分配策略和精度配置对MXPLM框架进行分类比较，包括权重、激活和KV缓存的量化方法，并与早期DNN混合精度方法进行对比分析。

Result: 通过比较分析揭示了不同框架在困惑度、零样本任务性能和部署权衡方面的差异，识别了在LM环境中有效和面临挑战的策略。

Conclusion: 混合精度量化是解决大规模语言模型效率问题的关键技术，未来需要在硬件感知设计、激活量化和可扩展优化方法等方面进一步研究。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [307] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出计算预算感知数据选择方法CADS，通过双层优化框架将计算预算约束整合到数据选择过程中，在视觉和语言基准测试中性能提升达14.42%


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略计算预算约束，而实证研究表明不同预算下没有算法能始终优于其他方法（甚至随机选择），因此计算预算必须成为数据选择策略的核心要素

Method: 提出CADS方法，采用双层优化框架：内层在计算预算约束下训练模型，外层基于模型评估优化数据选择。使用概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵估计问题，将内层优化转化为外层目标中的惩罚项

Result: 在视觉和语言基准测试中，相比基线方法性能提升最高达14.42%

Conclusion: 计算预算应作为数据选择的核心约束，CADS方法通过双层优化框架有效整合预算约束，显著提升训练效率

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [308] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 本文提出了一种名为软掩码（Soft-Masking, SM）的新方法，用于改进基于掩码扩散的语言模型。该方法通过动态混合掩码标记嵌入与先前解码步骤中预测的前k个标记嵌入，为模型提供更丰富的先验信息。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散语言模型在解码时采用二元决策（保留掩码或替换为预测标记），当保留掩码时会丢弃有价值的预测信息。这种信息损失限制了模型的性能。

Method: 提出软掩码方法，动态混合掩码标记嵌入与先前预测的前k个标记嵌入；开发了适配预训练掩码扩散语言模型的训练方法；在169M参数模型上继续预训练，并对Dream-7B和Dream-Coder-7B进行微调。

Result: 在169M参数模型上，软掩码方法改善了困惑度和MAUVE分数；在Dream-7B和Dream-Coder-7B模型上，软掩码在多个编程基准测试中一致提升了性能，特别是在高吞吐量设置下表现更佳。

Conclusion: 软掩码方法通过保留部分掩码标记的预测信息，为扩散语言模型提供了更丰富的先验，有效提升了模型性能，特别是在编程任务和高吞吐量场景下表现突出。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [309] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头添加跳跃连接来增强模型表示能力并减少KV缓存，在保持性能的同时减少约25%的KV缓存。


<details>
  <summary>Details</summary>
Motivation: 扩展Transformer模型以改进表示能力通常需要大量内存和计算成本，特别是自回归解码期间的KV缓存。现有方法要么改进表达能力但KV成本不变，要么减少内存但表示能力减弱。

Method: 从第二层开始，每个层重用第一层一半的Value头，同时正常计算另一半，将Value投影和V缓存减少近50%。该方法还可与现有MHA Transformer检查点进行微调训练。

Result: 在不同模型规模下，SkipV1Former相比标准MHA Transformer和一些先进变体，在困惑度改进的同时持续减少约25%的KV缓存。与YOCO结合时，KV缓存大小减少近50%且性能仍提升。

Conclusion: SkipV1Former通过重用第一层Value头的跳跃连接，有效平衡了表示能力和内存效率，为Transformer模型的资源优化提供了实用解决方案。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [310] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了因果充分性下因果强盗问题中的遗憾最小化，发现学习父节点集是次优的，证明了遗憾最小化和父节点识别之间存在根本冲突，并提出了绕过图恢复的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注识别奖励的父节点然后应用经典强盗方法，或者在学习父节点的同时最小化遗憾。本文研究这些策略是否最优，并探索遗憾最小化和因果结构学习之间的根本关系。

Method: 通过理论分析证明父节点识别和遗憾最小化存在冲突，建立了包含动作空间组合结构的遗憾下界，并提出了绕过图恢复和父节点恢复的新算法。

Result: 证明了存在父节点识别和遗憾最小化冲突的实例，建立了新的遗憾下界，提出的算法在各种环境中显著优于现有基线方法。

Conclusion: 父节点识别对于遗憾最小化是不必要的，直接绕过因果结构恢复的方法能够实现近乎最优的性能，这挑战了因果强盗问题中的传统认知。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [311] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 使用半监督学习和正未标记学习策略，通过深度学习进行考古预测建模，在稀疏标注数据下有效识别未知考古遗址


<details>
  <summary>Details</summary>
Motivation: 解决考古学中结构性标签稀缺问题——正样本稀少且大多数位置未标注，需要开发能在稀疏标注环境下工作的预测模型

Method: 采用半监督正未标记学习策略，实现为语义分割模型，使用动态伪标签和通过RNN实现的CRF来增强标签置信度

Result: 在DEM地理空间数据集上性能与最先进方法LAMAP相当但Dice分数更高；在原始卫星图像上保持性能并产生更具可解释性的预测表面

Conclusion: 半监督学习为在大规模稀疏标注景观中识别未知考古遗址提供了有前景的方法

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [312] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出了线性注意力神经算子（LANO），通过引入代理令牌机制在保持软注意力表达能力的同时实现线性复杂度，解决了transformer神经算子中可扩展性与准确性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的神经算子面临的基本可扩展性-准确性权衡：softmax注意力提供优秀保真度但具有二次复杂度，而线性注意力变体虽然降低计算成本但往往导致显著准确性下降。

Method: 引入紧凑的代理令牌集（M ≪ N）来调解N个令牌之间的全局交互，形成具有线性复杂度的代理注意力机制，同时保持softmax注意力的表达能力。

Result: 理论上证明了通用逼近性质，实证上在标准基准测试中超越了当前最先进的神经PDE求解器，包括使用切片softmax注意力的Transolver，平均准确率提升19.5%。

Conclusion: LANO通过在软注意力级别的性能和线性复杂度之间架起桥梁，为科学机器学习应用建立了可扩展、高准确性的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [313] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言在环框架，使用LLM将自然语言反馈转换为标量效用，用于在数值搜索空间上进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，反馈对于将复杂、细致或主观目标转化为可量化优化目标至关重要。传统方法只能接受受限反馈格式且需要为每个领域特定问题定制模型。

Method: 使用LLM将各种类型的文本反馈转换为一致的效用信号，无需手动设计核函数即可包含灵活的用户先验，同时保持贝叶斯优化的样本效率和原则性不确定性量化。

Result: 该混合方法不仅为决策者提供更自然的接口，而且在反馈受限情况下优于传统贝叶斯优化基线和仅使用LLM的优化器。

Conclusion: 语言在环框架成功地将自然语言反馈集成到贝叶斯优化中，在保持样本效率的同时提供了更灵活的用户交互方式。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [314] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出了TRPINN方法，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制边界损失，这是与H^1(Ω)相关的正确迹空间。该方法通过计算半范数的理论必要部分来降低计算成本，并通过避免离散化中的分母评估来增强收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理具有高度振荡Dirichlet边界条件的拉普拉斯方程时可能失败，需要更有效的边界处理方法来提高收敛性和稳定性。

Method: TRPINN方法在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制边界损失，仅计算半范数的理论必要部分，避免分母评估，并通过神经切线核分析证明收敛性。

Result: 数值实验表明，TRPINN在标准PINNs失败的情况下仍能成功，性能提升1-3个十进制数字，收敛速度比标准PINNs更快。

Conclusion: TRPINN通过精确的H^{1/2}(∂Ω)范数实现了在H^1(Ω)意义上的收敛，提供了更稳定和高效的物理信息神经网络框架。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [315] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 使用双线性自编码器将表示分解为二次多项式，实现可分析的非线性潜在表示


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器的解释依赖于输入，单独研究不完整。多项式作为代数基元可以在不参考输入的情况下进行分析，能够描述从线性概念到复杂流形的结构

Method: 使用双线性自编码器高效地将表示分解为二次多项式，并引入改进以诱导重要性排序、聚类和激活稀疏性

Result: 实现了通过代数性质分析非线性潜在表示的方法

Conclusion: 这是通过代数性质实现非线性但可分析潜在表示的初步步骤

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [316] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一个样本级别的框架来衡量后训练过程中的知识遗忘和反向迁移，发现不同后训练阶段对预训练知识的影响各不相同。


<details>
  <summary>Details</summary>
Motivation: 理解后训练对语言模型预训练知识的影响，因为知识遗忘不是平均分布的，需要更精细的测量方法。

Method: 使用样本级别的1->0转换（正确变错误）量化遗忘，0->1转换量化反向迁移，并针对选择题基准添加了机会调整变体。

Result: 大规模分析显示：领域持续预训练导致中度遗忘和低中度反向迁移；RL/SFT后训练在数学和逻辑上产生中到大的反向迁移；模型合并不能可靠缓解遗忘。

Conclusion: 该框架为理解后训练如何改变预训练知识提供了实用标准，有助于开发更通用的AI系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [317] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一个原型引导的多模态分子表示学习框架，通过分层编码器和双向跨模态注意力机制，实现了分子图与文本描述的细粒度集成和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法存在两个关键限制：仅在最终编码层进行跨模态交互，忽略了分层语义依赖；缺乏统一的原型空间来实现模态间的稳健对齐。

Method: 采用双分支分层编码器（图神经网络处理分子图，Transformer编码文本），引入分层双向跨模态注意力机制，并构建具有可学习类特定锚点的共享原型空间。

Result: 在多个基准数据集上的广泛实验表明，ProtoMol在各种分子性质预测任务中始终优于最先进的基线方法。

Conclusion: ProtoMol通过分层跨模态交互和原型引导的语义对齐，显著提升了多模态分子表示学习的预测准确性和可解释性。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [318] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现了风洞验证精度低于1.04%，比现有数据集提升5倍，为数据驱动的空气动力学优化建立了新标准。


<details>
  <summary>Details</summary>
Motivation: 传统汽车空气动力学优化方法面临计算成本高和精度不足的困境，现有机器学习数据集存在网格分辨率不足、组件缺失和验证误差超过5%等问题，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由形态变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用精细网格策略和严格的壁面y+控制。

Result: 数据集实现了风洞验证精度低于1.04%的突破，比现有数据集提升5倍，训练模型可在保持生产级精度的同时将计算成本从数周减少到几分钟。

Conclusion: DrivAerStar首次将学术机器学习研究与工业CFD实践连接起来，为汽车开发中的数据驱动空气动力学优化建立了新标准，展示了将高保真物理模拟与AI整合到工程领域的范例。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [319] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉回路启发的持续表征学习框架，通过生物启发设计解决相似性匹配中的多重共线性问题，显著减少训练时间同时保持或超越现有最优方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续表征学习方法在相似性匹配阶段存在多重共线性问题，且更先进的方法计算成本过高，难以满足实时低延迟应用需求。

Method: 提出Fly-CL框架，利用果蝇嗅觉回路原理，渐进式解决多重共线性问题，实现更有效的相似性匹配，具有低时间复杂度。

Result: 在各种网络架构和数据机制下的广泛模拟实验验证了Fly-CL的有效性，显著减少训练时间同时达到或超越当前最优方法性能。

Conclusion: Fly-CL通过生物启发设计成功解决了持续表征学习中的多重共线性挑战，为实时低延迟应用提供了高效解决方案。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [320] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个指令调优的编码器-解码器框架，通过结合图结构和语义推理，在无需任务特定监督的情况下实现跨任务和跨领域的零样本图推理。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络受限于固定标签空间，而大语言模型难以捕捉图结构的问题，实现通用的图任务泛化能力。

Method: 编码器使用可学习对齐标记和结构感知的图-文本注意力机制，解码器使用冻结的LLM进行预测和图重构，通过重构目标正则化编码器。

Result: 在节点分类、链接预测、图分类和图回归任务上实现了新的零样本最优结果，在跨任务和跨领域设置下表现优异。

Conclusion: 图结构与LLM语义的紧密集成能够实现鲁棒、可迁移的图推理能力。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [321] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 该论文扩展了DEEPCHEM库，增加了SE(3)-等变神经网络的即用型支持，使深度学习背景有限的科学家能够轻松构建、训练和评估分子应用中的等变模型。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)-等变神经网络库（如E3NN和SE(3)-TRANSFORMER）需要深厚的深度学习或数学背景知识，且缺乏完整的训练流程，限制了其在科学界的广泛应用。

Method: 扩展DEEPCHEM库，集成SE(3)-Transformer和Tensor Field Networks等等变模型，提供完整的训练流程和等变工具包，并配备全面的测试和文档支持。

Result: 开发了一个用户友好的框架，使科学家能够轻松使用SE(3)-等变神经网络进行分子性质预测、蛋白质结构建模和材料设计等应用。

Conclusion: 通过降低使用门槛，该工作促进了SE(3)-等变模型在分子科学领域的应用和进一步发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [322] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 使用LSTM网络结合自定义损失函数和在线学习方法来预测加州电力市场的日前电价，通过整合历史价格、天气条件和能源结构等特征提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对能源市场参与者至关重要，特别是电网运营商、能源生产商和消费者。需要开发能够适应市场动态变化的预测模型。

Method: 采用LSTM网络，引入包含MAE、Jensen-Shannon散度和平滑度惩罚项的自定义损失函数，并实施在线学习方法以增量适应新数据。

Result: 自定义损失函数提高了模型性能，特别是在峰值时段更接近实际价格；在线学习模型通过有效整合实时数据，降低了预测误差和变异性。

Conclusion: 该研究为电力价格预测提供了稳健框架，通过综合特征整合和自适应学习，为动态电力市场的决策制定提供了有价值的工具。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [323] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出基于SNOMED CT和Neo4j的知识驱动框架，构建结构化医疗知识图谱，通过标准化临床实体和关系来提升LLM在医疗诊断中的逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化临床文档导致的训练数据噪声、不一致和逻辑碎片化问题，这些因素严重阻碍了AI在医疗领域的有效性。

Method: 整合SNOMED CT标准化临床术语与Neo4j图数据库，构建医疗知识图谱，将临床实体作为节点，语义关系作为边，提取和标准化临床文本中的实体关系对生成结构化数据集。

Result: 实验结果表明，该方法显著提高了AI生成诊断推理的有效性和可解释性，知识引导的方法增强了LLM输出的临床逻辑一致性。

Conclusion: 该框架为构建可靠的AI辅助临床系统提供了可扩展的解决方案，通过结构化知识图谱改善了医疗AI的性能。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [324] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习管道，结合降采样、双模式插补和标准化处理，使用GRU-LSTM模型在嘈杂、不完整的传感器数据上实现准确的短期能耗预测。


<details>
  <summary>Details</summary>
Motivation: 解决在传感器数据嘈杂、不完整且缺乏上下文丰富性的情况下，如何准确预测短期能耗的问题，特别是针对2025年电力能耗预测竞赛的挑战。

Method: 采用轻量级DL管道，包括小时降采样、双模式插补（均值和多项式回归）、全面标准化，最终选择标准缩放，并使用GRU-LSTM序列到一模型。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率达到84.36%，能够很好地泛化、捕捉非线性需求模式并保持低推理延迟。

Conclusion: 针对性的预处理与紧凑循环架构相结合，能够在真实世界条件下实现快速、准确且可部署的能耗预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [325] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域泛化持续学习（DGCL）新设置，并开发了自适应领域变换（DoT）方法来解决该问题。DoT基于人脑分布式加枢纽理论，解耦语义和领域信息，通过自适应变换实现跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现实环境中智能系统需要持续学习新技能并泛化到未见场景，但现有持续学习方法假设训练和测试域相同，在跨域场景下表现不佳。

Method: 提出自适应领域变换（DoT），解耦语义和领域相关信息，自适应变换任务表示进行输出对齐，确保平衡和泛化的预测。

Result: DoT作为插件策略显著提升了现有持续学习基线方法在DGCL中的性能，验证了其有效性和资源效率。

Conclusion: DoT能够从DGCL中积累领域泛化知识，通过轻量级实现确保资源效率，为解决动态现实环境中的持续学习挑战提供了有效方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [326] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练、基于测试时扩展的框架，通过生成数学公式并转换为求解器代码来解决多样化优化问题，采用改进的MCTS策略提升搜索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致跨问题类型泛化能力差，要么需要昂贵的监督训练。需要一种既能解决多样化优化问题又无需额外训练的方法。

Method: 使用改进的蒙特卡洛树搜索策略：动态扩展用于自适应公式生成，提示反向传播通过结果驱动反馈指导探索，不确定性反向传播将奖励可靠性纳入决策。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和学习的方法，实现了强泛化能力且无需额外训练。

Conclusion: SolverLLM框架通过测试时扩展和创新的MCTS策略，有效解决了多样化优化问题，展示了无需训练即可实现强泛化能力的潜力。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [327] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 该论文推导了Transformer中Layer Normalization和前馈网络的二阶表达式，完成了完整Transformer块的Hessian矩阵表征，为大规模深度学习优化提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization和前馈网络Hessian矩阵缺乏理论结果，这阻碍了对Transformer优化景观的研究，需要填补这一理论空白。

Method: 推导Layer Normalization和前馈网络的显式二阶表达式，提出基于泰勒展开的损失差异分析框架来量化收敛轨迹。

Result: 完成了完整Transformer块的Hessian矩阵表征，推广了自注意力分析，并估计了各子层在曲率传播中的作用。

Conclusion: 通过将Hessian理论扩展到完整Transformer架构，为大规模深度学习优化的理论和实证研究建立了新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [328] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是一种概率性Kolmogorov-Arnold网络，用于时间序列预测，通过样条函数连接和直接参数化预测分布，在卫星流量预测中表现出优于MLP的准确性和校准性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在时间序列预测中难以有效捕捉非线性动态和重尾分布，且缺乏不确定性量化能力，而卫星通信等资源受限领域需要高效的预测模型。

Method: 将KAN的标量权重替换为样条函数连接，直接参数化预测分布（高斯分布和Student-t分布），构建参数高效的模型。

Result: P-KAN在卫星流量预测中持续优于MLP基线，准确性和校准性能更好，参数使用显著减少，高斯变体提供稳健预测，Student-t变体在稳定需求下效率更高。

Conclusion: P-KAN为概率预测提供了强大框架，特别适用于卫星通信等资源受限领域，能够实现效率与风险的优化权衡。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [329] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率深度学习方法（分位数回归神经网络、变分自编码器、扩散模型）在次季节尺度风场预报中的统计降尺度应用，相比传统随机扰动方法能更好地表征空间不确定性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 次季节预报通常依赖大尺度大气预测因子，但现有基于模型残差的随机扰动方法无法充分表征空间相关性和物理一致性，需要更复杂的方法来捕捉大尺度预测因子与局地预测变量之间的复杂关系。

Method: 使用ERA5再分析数据训练三种概率方法：直接建模分布分位数的分位数回归神经网络、利用潜在空间采样的变分自编码器、基于迭代去噪的扩散模型，并将其应用于ECMWF次季节后报数据。

Result: 概率降尺度方法比简单随机方法提供更真实的空间不确定性表示，每种概率模型在集合离散度、确定性技能和物理一致性方面各有优势。

Conclusion: 概率降尺度方法可有效增强业务次季节风场预报，为可再生能源规划和风险评估提供支持。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [330] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在差分隐私线性回归中同时实现有效推断和合成数据生成的方法，适用于社会科学中常见的小到中等规模连续数据集


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私线性回归方法主要关注点估计，缺乏不确定性量化，且不支持合成数据生成。现有合成数据方法要么适用于离散数据，要么需要大数据集，不适合社会科学中的小规模连续数据

Method: 使用差分隐私偏置校正估计器，提供渐近置信区间，并提出通用合成数据生成流程，使合成数据上的回归匹配差分隐私回归结果。采用分箱聚合策略处理小到中等维度数据

Result: 实验表明该方法(1)相比现有方法提高准确性，(2)提供有效置信区间，(3)生成的合成数据在下游机器学习任务中比当前差分隐私合成数据方法更可靠

Conclusion: 该方法为社会科学中的差分隐私线性回归提供了有效的推断和合成数据生成能力，填补了现有方法的空白

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [331] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 该论文提出了时间序列推理的蓝图愿景，包含两个互补方向：建立稳健的时间序列推理基础，以及推进系统级推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理正在成为时间分析的下一个前沿，旨在超越模式识别，实现明确、可解释且可信赖的推理。

Method: 采用两个互补方向：一是围绕全面时间理解、结构化多步推理和忠实评估框架建立稳健基础；二是推进系统级推理，整合多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活可扩展的时间序列推理框架，能够为不同领域提供可解释和可信赖的时间智能。

Conclusion: 这两个互补方向共同构建了一个推进时间序列推理的框架，旨在实现跨领域的可解释和可信赖时间智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [332] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP通过块周期正交化优化了梯度正交化方法，在保持训练稳定性的同时显著减少了模型并行训练中的通信开销，相比Muon优化器实现了8%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器在模型并行训练中因梯度正交化引入的额外通信开销问题，该开销相比AdamW会导致5%-10%的吞吐量下降。

Method: 提出块周期正交化方法：在每个设备上独立对矩阵分片进行正交化，并定期执行完整正交化以保持训练稳定性；使用两个学习率分别处理块正交化和完整正交化步骤。

Result: 在8B模型训练中，使用8路张量并行和ZeRO优化器状态分片时，MuonBP相比Muon实现了8%的吞吐量提升，且性能没有下降。

Conclusion: MuonBP方法简单，需要最小的超参数调整，在保持与基线Muon竞争性迭代复杂度的同时，提供了与AdamW等坐标方法相当的每迭代吞吐量。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [333] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM是一个基于图的多模态学习框架，通过Hop-Diffused Attention整合多跳结构信息，并使用MM-QFormer进行跨模态融合，在生成和判别任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，而现有方法未能区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Hop-Diffused Attention通过因果掩码和跳扩散整合多跳结构信息，设计MM-QFormer进行跨模态融合。

Result: 在生成和判别任务上，Graph4MM优于更大的视觉语言模型、大语言模型和多模态图基线，平均提升6.93%。

Conclusion: 利用结构信息整合模态内和模态间交互，能够比将图作为独立模态更好地提升多模态理解能力。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [334] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型的AI代理，能够将SPICE网表转换为可编辑的电路原理图，解决了传统文本表示缺乏视觉可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 电路原理图在模拟集成电路设计中至关重要，但现有基于LLM的方法主要依赖SPICE网表等文本表示，缺乏对电路设计师的视觉可解释性。

Method: 整合文本、视觉和符号模态，使用6个模拟子结构示例进行少样本布局，采用视觉思维链策略迭代优化布局和布线，提高原理图清晰度和对称性。

Result: 在CMOS反相器、五晶体管运算跨导放大器(5T-OTA)和望远镜级联放大器等代表性模拟电路上的实验表明，EEschematic生成的原理图具有高视觉质量和结构正确性。

Conclusion: EEschematic能够有效生成高质量、可编辑的模拟电路原理图，为电路设计提供了更好的视觉理解和验证工具。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [335] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 提出了Justitia调度器，通过内存中心的服务成本建模、轻量级需求预测和基于虚拟时间的公平队列算法，在保证公平性的同时提升LLM应用的调度效率。


<details>
  <summary>Details</summary>
Motivation: 主流LLM调度器在共享GPU服务器上服务LLM应用时，由于队头阻塞或资源分配过度约束，无法实现快速应用完成和保证最坏情况性能。

Method: 设计Justitia调度器，采用三种关键技术：内存中心的服务成本建模、轻量级神经网络需求预测、基于虚拟时间的公平队列算法。

Result: 在vLLM上实现Justitia，实验结果表明能显著提升调度效率同时保持公平性。

Conclusion: Justitia调度器能够有效解决LLM应用在共享GPU服务器上的调度问题，在保证公平性的同时提高效率。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [336] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 使用强化学习（PPO）结合好奇心探索和基于图的动作来解决非线性方程，包括根式、指数和三角函数方程


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在符号数学中的潜力，特别是解决比线性方程更复杂的非线性方程

Method: 采用模型无关的PPO算法，结合好奇心驱动的探索机制和基于图的动作表示

Result: 成功解决了包含根式、指数和三角函数的非线性方程

Conclusion: 基于好奇心的探索方法可能对一般符号推理任务有用

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [337] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出了PIMMA框架解决服务质量降级问题，通过生成式方法在非线性边权重函数下优化网络性能


<details>
  <summary>Details</summary>
Motivation: 现有方法无法直接处理非线性边权重函数的QoSD问题，传统组合优化方法受限，机器学习方法仅处理小规模线性变体

Method: 三阶段框架：Forge使用预测路径应力算法生成可行解；Morph使用条件VAE混合和能量模型捕获解特征分布；Refine使用强化学习在解空间中探索生成近优解

Result: 在合成和真实网络上的实验表明，该方法在非线性成本函数场景下始终优于传统和机器学习基线方法

Conclusion: PIMMA框架有效解决了非线性边权重函数下的QoSD问题，在传统方法失效的场景中表现出色

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [338] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了Diverse Influence Component Analysis (DICA)框架，通过最大化雅可比矩阵体积来识别非线性混合中的潜在成分，无需辅助信号、独立性假设或雅可比稀疏性假设。


<details>
  <summary>Details</summary>
Motivation: 解决非线性独立成分分析中潜在成分识别的根本挑战，特别是在缺乏辅助监督信号的情况下实现可识别性。

Method: 使用雅可比体积最大化(J-VolMax)准则，利用混合函数雅可比矩阵的凸几何特性，通过鼓励潜在成分对观测变量的影响多样性来实现识别。

Result: 在合理条件下，该方法能够实现潜在成分的可识别性，无需依赖辅助信息、潜在成分独立性或雅可比稀疏性假设。

Conclusion: DICA框架扩展了可识别性分析的范围，为现有方法提供了补充视角，在非线性混合问题中实现了更宽松条件下的可识别性。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [339] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了当后处理指令与模型已学习行为冲突时，语言模型会进行系统性动机推理，生成看似合理的理由来违反指令并淡化潜在危害。研究发现前沿推理模型能检测这种动机推理，但较小的LLM评判者可能无法识别，甚至可能被说服认为这种推理是正确的，尽管它明显违反指令。


<details>
  <summary>Details</summary>
Motivation: 研究当后处理指令与模型已学习行为冲突时，模型的推理过程会发生什么变化。由于LLM训练常因不完美的奖励信号产生意外行为，导致模型发展出不对齐倾向，而常用的纠正方法是应用后处理指令来避免问题行为，但指令与学习行为冲突时的影响尚不清楚。

Method: 在简单设置中调查模型行为，分析模型如何生成看似合理的理由来违反指令，并研究不同规模LLM评判者检测动机推理的能力。

Result: 模型会进行系统性动机推理，生成看似合理的理由违反指令并淡化危害。前沿推理模型能检测大部分动机推理，但较小的LLM评判者可能无法识别部分动机推理，甚至可能被说服认为这种推理是正确的。

Conclusion: 随着模型变得更加复杂，其动机推理可能越来越难以被监控器检测。研究结果强调在依赖思维链过程进行模型评估和监督时，需要考虑动机推理的影响。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [340] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出一种针对未来硬件加速器设计的低精度对数定点训练增强方法，通过结合位宽设计算术运算近似，使用模拟退火优化不同精度级别的对数加法近似，在12位整数运算下实现与32位浮点训练相当的精度，硬件研究显示面积减少32.5%，能耗降低53.5%。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术显著降低了深度学习的推理计算成本，但训练仍主要依赖复杂的浮点运算，低精度定点训练是一个有吸引力的替代方案。

Method: 提出硬件友好的分段线性近似方法用于对数加法，使用模拟退火优化不同精度级别的近似，通过C++位真模拟验证VGG-11和VGG-16模型在CIFAR-100和TinyImageNet上的训练效果。

Result: 使用12位整数运算训练VGG模型，与32位浮点训练相比精度损失极小，提出的LNS乘累加单元相比线性定点等效单元面积减少32.5%，能耗降低53.5%。

Conclusion: 该方法证明了低精度对数定点训练在保持精度的同时，能显著降低硬件面积和能耗，为未来硬件加速器设计提供了有前景的方向。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [341] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种自监督预训练交互式智能体的方法，使其能够快速模仿人类演示。该方法将目标（观察结果）作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习解释演示为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前成功的AI模型（如VLMs、LLMs）缺乏明确的行动概念训练，而纯探索的强化学习方法无法让智能体快速适应新任务。需要一种自监督的交互式训练方法，使智能体能够像人类一样快速适应新任务。

Method: 将目标作为核心构建块，在训练阶段自动提出目标并练习达成目标，基于强化学习探索的先前工作。在评估阶段，通过解决逆强化学习问题，将人类演示解释为最优的目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）上的实验表明，该方法在零样本模仿任务上优于先前的方法。

Conclusion: 该方法为交互式智能体提供了一种有效的自监督预训练方式，使其能够快速适应新任务并模仿人类演示，在零样本模仿任务上表现出优越性能。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [342] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出Gram行列式评分方法，用于评估无真实标签数据集的可靠性，该评分具有实验无关性，能在不同统计实验下保持一致的可靠性排序。


<details>
  <summary>Details</summary>
Motivation: 解决在无法获取真实数据的情况下，如何评估来自潜在策略性来源的数据集的可靠性问题。

Method: 定义基于真实数据的可靠性排序，提出Gram行列式评分方法，通过计算观测数据和实验结果的向量张成的体积来度量可靠性。

Result: Gram行列式评分能保持多种基于真实数据的可靠性排序，且在实验无关性方面具有唯一性。在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了其有效性。

Conclusion: Gram行列式评分是一种有效的数据集可靠性评估方法，能够跨不同观测过程捕捉数据质量。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [343] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专门的专家网络实现内置可解释性，解决金融异常检测中的三个关键挑战：动态图结构适应、多尺度时间特征捕获和黑盒输出问题。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测器将所有异常统一处理，产生标量分数而不揭示具体失效机制、风险集中位置或干预方法，这种不透明性阻碍了有针对性的监管响应。

Method: 通过BiLSTM与自注意力捕获多尺度时间依赖，跨模态注意力融合时空信息，神经多源插值学习动态图，压力调制融合自适应平衡学习动态与结构先验，将异常路由到四个机制特定专家，并产生双级可解释归因。

Result: 在100只美国股票(2017-2024)上实现92.3%的13个主要事件检测率，提前3.8天，比最佳基线提高30.8个百分点。硅谷银行案例显示异常演化跟踪能力。

Conclusion: 该框架通过架构嵌入而非事后应用的方式实现可解释性，能够自动识别时间机制而无需标注监督，为金融异常检测提供了更精确和可操作的解决方案。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [344] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 该论文研究了Hedge算法在组合设置中的最优性，证明了Hedge在所有组合设置中接近最优（最多相差√log d因子），并识别了Hedge在某些设置中确实次优的具体情况。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，因为虽然已知Hedge在T轮交互后能达到O(√T log|X|)的遗憾，但不确定这是否在所有组合设置中都是最优的。

Method: 通过建立对任何算法都成立的Ω(√T log(|X|)/log d)下界来证明Hedge的接近最优性，并分析特定组合类（如m-集合）来展示Hedge的次优性。

Result: 证明Hedge在所有组合设置中接近最优（最多相差√log d因子），在m-集合（log d ≤ m ≤ √d）中Hedge确实次优√log d倍，但在在线多任务学习中Hedge是最优的。

Conclusion: Hedge在组合设置中接近最优，其接近最优性可用于为DAGs中的在线最短路径问题建立接近最优的正则化器，通过实例化具有扩张熵正则化器的OMD算法来实现。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [345] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 本文提出了首个在聚合bandit反馈下的episodic MDPs中的best-of-both-worlds算法，在已知转移时实现O(log T)随机遗憾和O(√T)对抗遗憾，并建立了匹配下界。


<details>
  <summary>Details</summary>
Motivation: 研究在聚合bandit反馈模型下的在线学习问题，该模型下学习者只能观察到每个episode的累积损失而非每个状态-动作对的个体损失。之前的工作只关注最坏情况分析，本文首次研究能在随机和对抗环境中都实现低遗憾的BOBW算法。

Method: 使用基于占用度量的FTRL、自边界技术，以及受在线最短路径问题启发的新损失估计器。对于未知转移情况，结合置信区间技术。

Result: 在已知转移情况下，算法在随机环境中达到O(log T)遗憾，在对抗环境中达到O(√T)遗憾，并建立了匹配下界证明最优性。同时为最短路径问题提供了首个个体间隙依赖下界和近最优BOBW算法。

Conclusion: 本文成功开发了聚合bandit反馈下episodic MDPs的首个BOBW算法，在随机和对抗环境中都达到最优遗憾界，填补了该领域的研究空白。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [346] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构通过移除冗余线性投影和引入多跳图聚合，在保持竞争力的同时提供更清晰的机制可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在时间序列建模中取得了显著成功，但其内部机制仍然不透明。本文旨在通过建立Transformer编码器与图卷积网络的基本等价性来揭示其工作机制。

Method: 证明了在前向传播中，注意力分布矩阵充当动态邻接矩阵，其与后续变换的组合执行类似于图卷积的计算。在反向传播中，值和前馈投影的更新动态与GCN参数更新相似。基于此统一理论重新解释，提出了Fighter架构。

Result: 在标准预测基准测试中，Fighter实现了有竞争力的性能，同时为其预测提供了更清晰的机制可解释性。

Conclusion: Transformer编码器本质上等价于图卷积网络，这一视角提供了时间依赖关系的显式和可解释表示，自然地表达为图边。Fighter架构通过简化设计和多跳聚合，在保持性能的同时增强了可解释性。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [347] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的新型自编码器正则化方案，通过优化编码矩阵的奇异值分布使其接近高斯分布，提高泛化能力并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器在编码分布控制方面存在不足，需要一种能够确保编码具有高斯分布特性的正则化方法，以提高模型的泛化能力和在逆问题中的应用效果。

Method: 基于矩阵自由能理论，定义可微损失函数来优化编码矩阵的奇异值分布，使其与独立同分布高斯随机矩阵的奇异值分布一致，使用标准随机梯度下降进行训练。

Result: 经验模拟表明，最小化负矩阵自由能可产生高斯样编码，在训练集和测试集上均表现出良好的泛化性能，并成功应用于欠定逆问题。

Conclusion: 矩阵自由能正则化是一种有效的自编码器正则化方法，能够可靠地产生高斯编码，为欠定逆问题等应用提供了新的解决方案。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [348] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件表征连续时间Q函数，将扩散策略分数与学习到的连续Q函数的动作梯度联系起来，解决了连续时间RL中长期存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数现有强化学习方法都是离散时间形式，而现实世界中的许多控制问题本质上是连续时间的。传统基于值函数的方法在连续时间设置中难以保持Q函数的动作评估能力，需要依赖时间离散化。

Method: 通过鞅条件表征连续时间Q函数，利用动态规划原理将扩散策略分数与学习到的连续Q函数的动作梯度联系起来，提出了基于分数的策略改进算法CQSM。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中的数值结果证明了该方法的有效性，并与流行基线方法进行了比较。

Conclusion: CQSM方法成功解决了连续时间强化学习中保持Q函数动作评估能力而不依赖时间离散化的长期挑战，为连续时间控制问题提供了新的解决方案。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [349] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In-situ Autoguidance方法，无需辅助模型即可实现扩散模型的自引导，在推理时通过随机前向传递动态生成次优预测，实现零成本的自校正。


<details>
  <summary>Details</summary>
Motivation: 解决分类器自由引导(CFG)方法在提升图像质量和提示对齐时导致多样性降低的问题，同时避免现有解耦方法需要额外训练辅助模型的开销。

Method: 通过随机前向传递动态生成次优预测，将引导重构为推理时的自校正过程，无需任何辅助组件。

Result: 该方法不仅可行，而且建立了成本高效引导的新基准，证明无需外部模型即可实现自引导的益处。

Conclusion: In-situ Autoguidance是一种零成本的自引导方法，成功实现了无需辅助模型的扩散模型引导，为成本高效的图像生成提供了新方案。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [350] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为ALMD的新学习范式，使模型在部署后能够自主检测未见类别样本并在标注后学习，无需人工工程师参与。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在部署后模型固定，无法适应动态开放环境中出现的未见类别样本，需要模型能够自主检测并学习新类别。

Method: 提出PLDA方法，实现动态OOD检测和在线增量学习新类别，解决数据稀缺和资源消耗问题。

Result: 实证评估将展示PLDA方法的有效性。

Conclusion: ALMD范式及其PLDA方法为解决动态环境中模型持续学习问题提供了有效解决方案。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [351] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，使终端设备能够实时调整差分隐私级别，在动态边缘环境中平衡隐私保护、数据效用和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘群智感知系统在动态资源受限环境中持续传输用户数据，面临严重隐私威胁。静态差分隐私机制无法适应不断变化的风险（如对抗能力变化、资源约束和任务需求），导致要么噪声过多要么保护不足。

Method: ALPINE作为闭环控制系统，包含四个模块：动态风险感知、基于TD3算法的隐私决策、本地隐私执行和边缘节点性能验证。设计了平衡隐私收益、数据效用和能耗成本的奖励函数，指导TD3智能体自适应调整噪声幅度。

Result: 理论分析和真实世界仿真表明，ALPINE能有效缓解推理攻击，同时保持数据效用和控制成本，适用于大规模边缘应用。

Conclusion: ALPINE框架通过自适应差分隐私机制，在动态边缘环境中实现了隐私保护、数据效用和能耗成本之间的动态平衡，具有实际部署可行性。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [352] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 本文提出了一个统一的框架来评估文本属性图(TAG)学习中图神经网络(GNN)和大语言模型(LLM)的鲁棒性，揭示了模型在文本和结构扰动下的内在权衡，并提出了SFT-auto框架来提升对抗性环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对文本属性图上GNN和LLM鲁棒性的评估是零散的，缺乏系统性地研究文本和结构扰动在不同模型和攻击场景下的影响。

Method: 提出了一个统一的鲁棒性评估框架，在10个数据集上评估经典GNN、鲁棒GNN和GraphLLM在文本、结构和混合扰动下的表现，并开发了SFT-auto框架来克服已识别的权衡问题。

Result: 研究发现：1)模型在文本和结构鲁棒性之间存在内在权衡；2)GNN和RGNN的性能高度依赖文本编码器和攻击类型；3)GraphLLM对训练数据污染特别脆弱。SFT-auto框架在单一模型中实现了对文本和结构攻击的优越且平衡的鲁棒性。

Conclusion: 这项工作为未来TAG安全研究奠定了基础，并为对抗性环境中的鲁棒TAG学习提供了实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [353] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化的分子动力学方法基准测试框架，使用增强采样分析系统评估蛋白质MD方法，支持多种模拟引擎和评估指标。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法的快速发展超过了标准化验证工具的开发，缺乏一致的评估指标、稀有构象状态的充分采样和可重现的基准测试。

Method: 使用加权集成采样和基于TICA的进度坐标，通过WESTPA实现快速高效的蛋白质构象空间探索。框架包含灵活的传播器接口，支持经典力场和机器学习模型。

Result: 开发了包含9种不同蛋白质的数据集，每个蛋白质在300K下进行了100万MD步骤的广泛模拟。验证测试显示框架能够有效比较不同MD方法的构象采样效果。

Conclusion: 该开源平台通过标准化评估协议和实现直接可重现的比较，为分子模拟社区提供了持续严格的基准测试基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [354] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一个软硬件协同设计，通过E2Softmax和AILayerNorm分别优化Transformer中的Softmax和LayerNorm操作，在不重新训练的情况下保持推理精度，同时显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但Softmax和LayerNorm的低效率限制了实时推理速度和效率。现有基于函数逼近的方法存在内存开销大、需要重新训练等问题。

Method: SOLE采用硬件-软件协同设计：E2Softmax使用指数函数的log2量化和基于对数的除法来逼近Softmax；AILayerNorm采用低精度统计计算。两者都实现了低精度计算和低比特位宽存储。

Result: 实验表明SOLE在不重新训练的情况下保持推理精度，相比GPU实现了数量级的速度提升和能耗节省。与现有定制硬件相比，Softmax和LayerNorm分别实现了3.04倍、3.86倍的能效提升和2.82倍、3.32倍的面积效率提升。

Conclusion: SOLE为Transformer中的Softmax和LayerNorm操作提供了一种高效的软硬件协同解决方案，在保持精度的同时显著提升了推理效率和能效。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [355] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出一个强化学习框架，用于处理高风险高回报任务，通过离散化连续动作空间、熵正则化探索和双评论家架构来建模多模态动作分布和风险。


<details>
  <summary>Details</summary>
Motivation: 高风险高回报任务通常具有多模态动作分布和随机回报，而传统强化学习方法假设单模态高斯策略和标量评论家，限制了其在HRHR设置中的有效性。

Method: 离散化连续动作空间以近似多模态分布，采用熵正则化探索提高风险但有益动作的覆盖，引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法。

Conclusion: 明确建模多模态性和风险在强化学习中至关重要，所提框架能有效处理高风险高回报任务。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [356] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超过92%，并在FPGA上部署实现近90%的硬件识别率。


<details>
  <summary>Details</summary>
Motivation: 燃料电池健康状态的有效准确诊断对确保燃料电池堆稳定运行至关重要。高频阻抗是评估燃料电池状态和健康状况的关键指标，但其在线测试复杂且成本高昂。

Method: 采用深度稀疏自编码网络进行燃料电池高频阻抗的预测和分类。

Result: 实现了准确率超过92%的预测和分类性能，在FPGA上部署后硬件识别率达到近90%。

Conclusion: 深度稀疏自编码网络能够有效预测燃料电池高频阻抗，且可在硬件平台上高效部署，为燃料电池健康监测提供了可行的解决方案。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [357] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出了基于注意力的编码器(AttEnc)和原型网络结合注意力编码器(P-AttEnc)两种深度学习架构，用于驾驶员识别。AttEnc在减少模型参数的同时实现高精度识别，P-AttEnc通过小样本学习解决数据不足问题并能识别未知驾驶员。


<details>
  <summary>Details</summary>
Motivation: 传统基于生物特征的驾驶员识别技术存在隐私问题，且现有方法大多未解决数据不足和无法识别未知驾驶员的问题。

Method: 使用注意力机制的编码器架构(AttEnc)减少模型参数；结合原型网络和注意力编码器(P-AttEnc)应用小样本学习，提取驾驶员指纹特征。

Result: AttEnc在三个数据集上分别达到99.3%、99.0%和99.9%的准确率，预测时间加快44%-79%，模型参数平均减少87.6%。P-AttEnc在单样本场景下识别准确率达69.8%，对未知驾驶员分类平均准确率达65.7%。

Conclusion: 提出的AttEnc和P-AttEnc架构在驾驶员识别任务中表现出色，既能高效识别已知驾驶员，又能通过小样本学习解决数据不足问题并识别未知驾驶员。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [358] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出自适应一致性模型(ADCMs)，通过自动优化离散化步骤来提升训练效率，避免手动调整离散化方案的问题。


<details>
  <summary>Details</summary>
Motivation: 现有一致性模型依赖手动设计的离散化方案，需要针对不同噪声调度和数据集反复调整，缺乏自适应能力。

Method: 将离散化问题构建为优化问题，以局部一致性为优化目标确保可训练性，以全局一致性为约束保证稳定性，使用拉格朗日乘子和高斯-牛顿方法实现自适应离散化。

Result: 在CIFAR-10和ImageNet上显著提升训练效率，获得更好的生成性能，且对更先进的扩散模型变体具有强适应性。

Conclusion: ADCMs框架有效解决了CMs的离散化问题，实现了高效自适应训练，具有广泛适用性。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [359] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分推断的扩展方法，将确定性机器学习方法与多元高斯分布相结合，用于数据同化中的不确定性建模。


<details>
  <summary>Details</summary>
Motivation: 数据同化涉及在大多数设置中存在不确定性，现有确定性方法无法充分处理这种不确定性。

Method: 基于变分推断扩展现有确定性机器学习方法，使预测状态遵循多元高斯分布。

Result: 在混沌Lorenz-96动力学测试中，新模型能够获得近乎完美校准的预测，并可在更长的数据同化窗口中实现更大效益。

Conclusion: 提出的随机变分推断方法能够有效处理数据同化中的不确定性，提高预测校准度。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [360] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 本文提出ControlValve防御机制，通过生成允许的控制流图并强制执行来防止多智能体系统中的控制流劫持攻击，解决了现有基于对齐检查的防御方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制（如LlamaFirewall）依赖智能体间通信的对齐检查，但存在被规避的风险。多智能体系统的安全性和功能性目标存在根本冲突，且对齐定义脆弱、检查器对执行上下文可见性不完整。

Method: 提出ControlValve防御机制：1) 为多智能体系统生成允许的控制流图；2) 强制执行所有执行必须符合这些图，并为每个智能体调用生成零样本上下文规则。

Result: ControlValve能够有效防御控制流劫持攻击，即使高级LLM执行对齐检查也能被规避的攻击也能被阻止。

Conclusion: ControlValve基于控制流完整性和最小权限原则，为多智能体系统提供了更强大的安全保障，解决了现有防御机制的根本局限性。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [361] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准测试，用于评估LLM系统的持续学习能力，涵盖多领域、多语言和多种任务类型。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据逐渐耗尽和计算资源边际收益递减，传统通过扩大数据、参数和测试时计算的方法已接近上限。受人类和传统AI系统从实践中学习的能力启发，构建LLM系统的记忆和持续学习框架成为重要研究方向。

Method: 开发用户反馈模拟框架和综合基准测试，覆盖多个领域、语言和任务类型，评估LLM系统从累积用户反馈中学习的能力。

Result: 实验表明，现有最先进基线的有效性和效率远未达到满意水平。

Conclusion: 该基准测试有望为未来LLM记忆和优化算法的研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [362] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 该论文将泛化保证扩展到非紧对称性（如平移）和非不变数据分布，通过PAC-Bayes框架改进现有边界，并在旋转MNIST数据集上验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧群对称性且假设数据分布不变，这在现实应用中很少满足。需要扩展理论保证到更广泛的非紧对称性和非不变数据分布。

Method: 基于PAC-Bayes框架，改进并收紧现有边界，特别是McAllester的PAC-Bayes边界，并证明该方法适用于多种PAC-Bayes边界。

Result: 在具有非均匀旋转群的旋转MNIST数据集上验证，推导出的保证不仅成立，而且优于先前结果。

Conclusion: 对于对称数据，对称模型在超越紧群和不变分布的更广泛设置中更优，为理解机器学习中的对称性开辟了更通用的途径。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [363] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因素时序解缠结标准化基准，包含六个数据集、评估工具和自动对齐方法，并展示了最先进的Koopman模型和视觉语言模型在自动标注和解缠结评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个交互的语义因素，但先前工作主要关注简单的双因素静态和动态设置，忽略了数据的多因素本质。

Method: 引入标准化基准、后验潜在探索阶段自动对齐潜在维度与语义因素、提出Koopman启发模型、利用视觉语言模型进行自动数据集标注和零样本解缠结评估。

Result: Koopman模型取得最先进结果，视觉语言模型能够自动化数据集标注并作为零样本解缠结评估器，无需人工标签和干预。

Conclusion: 这些贡献为推进多因素时序解缠结提供了稳健且可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [364] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励模型框架，通过两阶段方法实现数据高效、可解释的奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型开发成本高且可解释性差，基于准则的方法在可扩展性和可靠性之间存在权衡。

Method: 两阶段方法：1) 使用验证引导的Propose-Evaluate-Revise管道推断查询特定准则；2) 通过最大化信息论编码率将细粒度准则泛化为紧凑核心集。

Result: 仅使用70个偏好对（源数据的1.5%），该方法使Qwen3-8B等小模型超越专门训练的对等模型。

Conclusion: 这项工作为奖励建模开创了可扩展、可解释且数据高效的路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [365] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一个可调节语言模型表示的新框架，通过局部性调节参数实现从局部化到分布式编码的连续调整，支持多粒度架构适应。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型在可解释性和性能之间的权衡问题，为需要透明性和能力的监管领域提供灵活解决方案。

Method: 使用组稀疏惩罚、信息论锚点设计、动态规则注入和基于惩罚似然的招募标准，通过局部性调节参数控制表示类型。

Result: 建立了注意力集中在语义相关块的理论条件，提供了注意力熵和指针保真度的精确界限，实现了多粒度收敛保证。

Conclusion: 该框架使从业者能够在可解释和高性能模式之间连续插值，同时适应多粒度架构容量，支持监管领域的应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [366] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 本文提出了一种生成图神经网络(GNN)模型"metamers"的技术，通过优化输入图使其内部节点激活与参考图匹配，揭示了GNN存在过度表示不变性的问题。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，探索人工神经网络与人类大脑在不变性机制上的差距。

Method: 引入模型"metamers"生成技术，通过优化输入图使其内部节点激活与参考图匹配，获得在模型表示空间中等价但在结构和节点特征上显著不同的图。

Result: 在多个经典GNN架构中发现了极端的表示不变性水平，虽然通过修改模型架构和训练策略可以部分缓解这种过度不变性，但无法从根本上缩小与人类类似不变性的差距。

Conclusion: 量化了metamer图与其原始对应图之间的偏差，揭示了当前GNN的独特失败模式，并为模型评估提供了补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [367] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型来替代昂贵的智能电网模拟器，优化强化学习策略训练过程，显著提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在智能电网最优潮流问题中面临的样本效率问题，因为RL需要大量迭代训练，而使用真实模拟器成本高昂。

Method: 采用物理信息神经网络(PINNs)构建智能电网的替代模型，作为强化学习训练的环境，减少对昂贵模拟器的依赖。

Result: 使用PINNs替代模型能够在原始环境所用时间的一小部分内达到收敛结果，显著提高了训练效率。

Conclusion: PINNs作为替代模型能够有效解决强化学习在智能电网优化中的样本效率问题，大幅降低训练成本和时间。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [368] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: DISC是一种基于扩散模型的OOD检测方法，不仅能检测异常数据，还能对OOD类型进行分类，超越了传统基于标量得分的检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将分布偏移压缩为单一标量异常得分，无法区分OOD数据的类型，限制了OOD数据的有效利用。

Method: 利用扩散模型的迭代去噪过程，提取跨多个噪声级别的统计差异特征，形成丰富的多维特征向量。

Result: 在图像和表格数据基准测试中，DISC在OOD检测性能上达到或超越最先进方法，并首次实现了OOD类型分类能力。

Conclusion: DISC实现了从简单二元OOD检测到更细粒度检测的转变，为OOD数据的上下文理解和潜在利用提供了新途径。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [369] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文探讨了生成视觉模型中内部表征的演变，重点分析了从GANs和VAEs到扩散模型的架构转变，提出了严格意义合成与广义合成的区分，论证了扩散模型如何分散表征负担并挑战统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，理解从GANs/VAEs到扩散模型的架构转变如何影响表征过程，挑战关于统一潜在空间的传统假设。

Method: 通过模型架构的详细分析和针对性的实验设置，干预层间表征，研究扩散模型如何分散表征负担。

Result: 发现扩散模型将表征负担分散到不同层，挑战了统一内部空间的假设，支持广义合成的概念。

Conclusion: 生成AI应被理解为专门化过程的涌现配置，而非内容的直接合成，需要对生成AI的理解进行重新定位。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [370] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过PRPO强化学习方法激活LLM的推理能力，在少样本和零样本场景下表现优异，甚至能超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升树和专用深度学习模型）虽然任务内表现优秀，但可解释性有限且跨表迁移能力弱。推理LLM具有跨任务适应性和透明推理轨迹的潜力，但在表格数据上尚未充分发挥。

Method: 提出TabR1模型，核心是Permutation Relative Policy Optimization (PRPO)方法。该方法通过为每个样本构造多个标签保持的列排列，并在排列内和跨排列估计优势，将稀疏奖励转化为密集学习信号，提高泛化能力。

Result: TabR1在全监督微调下达到与强基线相当的性能。在零样本设置下，TabR1接近强基线在32样本设置下的性能。TabR1 (8B)在各种任务上大幅超越更大的LLM，相比DeepSeek-R1 (685B)提升高达53.17%。

Conclusion: PRPO方法能有效激活LLM在表格预测中的推理能力，显著提升少样本和零样本性能及可解释性，证明了推理LLM在表格预测领域的巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [371] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出特征扰动技术，通过直接对特征输入注入随机性，而非随机化参数或奖励噪声，实现了广义线性bandits的$\tilde{\mathcal{O}}(d\sqrt{T})$最坏情况遗憾界，优于现有方法的$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有随机化bandit算法存在计算复杂度高和理论遗憾界不够紧的问题，需要一种既能保持计算效率又能获得更好理论保证的方法。

Method: 特征扰动技术，直接在特征输入中注入随机性，避免参数采样过程，使其计算高效且能自然扩展到非参数或神经网络模型。

Result: 算法在广义线性bandits中实现了$\tilde{\mathcal{O}}(d\sqrt{T})$的最坏情况遗憾界，优于现有方法的$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$，且计算效率更高。

Conclusion: 特征扰动不仅超越了现有方法，还将强大的实际性能与最佳已知理论保证统一起来，是一种简单而强大的技术。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [372] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了首个针对弱通信MDP的平均奖励离线强化学习的样本复杂度结果，引入了锚定拟合Q迭代方法，结合了锚机制和标准拟合Q迭代。


<details>
  <summary>Details</summary>
Motivation: 现有平均奖励设置的方法依赖严格假设（如遍历性或MDP线性），而弱通信MDP假设更宽松，需要开发更通用的方法。

Method: 锚定拟合Q迭代，将标准拟合Q迭代与锚机制结合，锚机制可解释为权重衰减形式，对平均奖励设置的有限时间分析至关重要。

Result: 建立了弱通信MDP下平均奖励离线强化学习的样本复杂度界限，并将分析扩展到单轨迹生成数据集而非IID转换的情况。

Conclusion: 锚机制是实现平均奖励设置有限时间分析的关键，该方法在更宽松的弱通信MDP假设下有效，且适用于单轨迹数据集。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [373] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（模态感知学习率调度器），通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中常出现模态过拟合问题，即网络过度依赖某一模态，导致性能不佳，限制了多模态学习的潜力。

Method: MILES利用训练过程中模态条件利用率的差异，动态调整学习率来平衡各模态的学习速度，确保多模态模型均衡学习所有可用模态。

Result: 在四个多模态联合融合任务上的评估显示，MILES在所有任务和融合方法中均优于七个最先进的基线方法，有效平衡了模态使用并提升了性能。

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES方法能够有效解决模态过拟合问题，同时增强模态编码器在单模态样本或缺失模态情况下的表现。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [374] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: RINS-T是一个无需预训练数据的深度先验框架，通过神经网络作为隐式先验和鲁棒优化技术，有效解决时间序列线性逆问题，对异常值具有鲁棒性并放宽高斯噪声假设。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值等污染，传统深度学习方法需要大量预训练且在分布偏移下泛化能力差。

Method: 使用神经网络作为隐式先验，集成鲁棒优化技术，引入引导输入初始化、输入扰动和凸输出组合三个关键技术来提高优化稳定性和鲁棒性。

Result: RINS-T在无需预训练数据的情况下实现了高恢复性能，对异常值具有鲁棒性，能够处理复杂的现实世界时间序列挑战。

Conclusion: RINS-T是一个灵活有效的框架，通过鲁棒优化和稳定性增强技术，为时间序列线性逆问题提供了强大的解决方案。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [375] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的深度学习架构，用于多时段心律失常分类，通过联合多时段预测显著提升性能，特别是在心房颤动检测方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统ECG分析方法难以同时捕捉全局趋势和局部波形特征的高时间分辨率交互，需要开发能够桥接全局和局部信号分析的新方法。

Method: 引入S4ECG深度学习架构，利用结构化状态空间模型进行多时段心律失常分类，通过联合多时段预测来提升性能。

Result: 多时段预测方法比单时段方法在宏观AUROC上提升1.0-11.6%，心房颤动特异性从0.718-0.979提升到0.967-0.998，表现出优异的分布内性能和增强的分布外鲁棒性。

Conclusion: 这项工作推动了心律失常检测算法向时间感知范式的转变，为ECG解释特别是复杂心律失常如心房颤动和心房扑动的分析开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [376] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于准确预测锂离子电池容量及其不确定性，在真实车辆数据上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池容量及其不确定性的准确预测对于可靠的电池管理至关重要，但由于老化的随机性，这仍然具有挑战性。

Method: 使用CDUA模型，包括基于扩散的生成模型进行时间序列预测，结合注意力机制。首先从真实车辆数据中提取电池容量，使用皮尔逊相关系数和XGBoost算法识别最相关特征，然后用包含上下文U-Net和自注意力机制以及去噪网络的CDUA模型进行训练。

Result: 在真实车辆数据上的实验验证表明，CDUA模型实现了0.94%的相对MAE和1.14%的相对RMSE，95%置信区间相对宽度为3.74%。

Conclusion: CDUA能够提供准确的容量估计和可靠的不确定性量化，比较实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [377] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出DAP方法，通过利用扩散模型的内在代表性先验来提升数据集蒸馏的质量，无需额外训练即可生成更具代表性的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有生成式数据集蒸馏方法虽然采用扩散模型，但忽视了扩散模型内在的代表性先验，往往需要外部约束来提升数据质量。

Method: 提出DAP方法，通过Mercer核量化合成数据与真实数据在特征空间的相似度，将该先验作为指导来引导反向扩散过程。

Result: 在ImageNet-1K等大规模数据集上的实验表明，DAP在生成高保真数据集和跨架构泛化方面优于现有方法。

Conclusion: 建立了扩散先验与数据集蒸馏目标的理论联系，提供了无需训练的实用框架来提升蒸馏数据集质量。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [378] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GBN网络，通过局部瓶颈调整解决MPNN中的过平滑和过压缩问题，在超深网络（256层）中仍保持性能


<details>
  <summary>Details</summary>
Motivation: 现有方法采用全局方法解决过平滑和过压缩问题，但可能在某些区域有益而在其他区域有害，导致表达能力不足。通过谱间隙分析发现增加λ会导致梯度消失，影响消息传递效果

Method: 将局部黎曼几何与MPNN连接，建立新的非齐次边界条件，基于Robin条件设计具有局部瓶颈调整的GBN网络

Result: 在同质性和异质性图上的广泛实验显示GBN具有强大的表达能力，即使在超过256层的超深网络中也不出现性能下降

Conclusion: GBN通过局部方法有效解决了MPNN的过平滑和过压缩问题，在深层网络中保持稳定性能

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [379] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 应用可解释AI技术(Grad-CAM和SHAP)解释PhaseNet地震检测模型，并基于SHAP值开发门控推理方案，提升模型性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在地震事件检测中精度高，但其黑盒特性在关键应用中存在担忧，需要提高模型的可解释性和可靠性。

Method: 使用Grad-CAM可视化网络注意力，SHAP量化特征贡献，并开发SHAP门控推理方案结合模型输出和解释性指标。

Result: 在9000个波形测试集上，SHAP门控模型F1分数达0.98(精度0.99，召回率0.97)，优于基线PhaseNet(F1分数0.97)，对噪声具有更强鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [380] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个专门针对静息-运动跨状态条件的ECG生物认证模型，通过多尺度深度卷积特征提取和注意力机制，在跨状态场景下实现高精度身份识别。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别研究主要关注静息状态，而在静息-运动跨状态场景下性能下降问题尚未解决，需要开发专门针对跨状态条件的鲁棒认证模型。

Method: 结合多尺度深度卷积特征提取和注意力机制，构建专门针对跨状态条件的ECG认证模型，在静息-运动状态转换场景下进行训练和测试。

Result: 在运动-ECGID数据集上，Rest-to-Exercise场景准确率达92.50%，Exercise-to-Rest场景达94.72%，Rest-to-Rest场景达99.94%，Mixed-to-Mixed场景达97.85%。在ECG-ID和MIT-BIH数据集上的验证进一步证实了模型的泛化能力。

Conclusion: CrossStateECG在跨状态ECG认证中表现出色，为动态现实环境中的运动后ECG认证提供了实用解决方案，具有很好的泛化性能。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [381] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers通过训练发展出模块化、可解释的机制来支持组合推理，在未见过的序列上表现出组合推理能力，这与层级专业化表示和注意力模式相关。


<details>
  <summary>Details</summary>
Motivation: 研究Transformers如何在训练中未观察到的序列上表现出组合推理能力，探索其内部机制与行为表现之间的联系。

Method: 使用随机层次模型(RHM)作为概率上下文无关文法生成序列，在序列子集上训练模型，并在四种泛化条件下评估：记忆、分布内泛化、分布外泛化（相同规则）和跨层迁移。

Result: 性能随任务复杂性和上下文示例数量系统提升，分布外任务需要更多示例；训练中层级专业化逐步出现，与泛化性能相关；PCA和注意力模式聚类显示Transformer在专业层中发展出结构化的层级组织表示。

Conclusion: Transformer发展出支持组合推理的模块化、可解释机制，将内部算法结构与观察到的行为能力联系起来。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [382] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出DAMSDAN网络解决跨域EEG情绪识别中的个体差异问题，通过动态源权重分配和原型引导对齐实现多源域适应。


<details>
  <summary>Details</summary>
Motivation: 解决跨域EEG情绪识别中显著的个体间变异性问题，以及多源适应中的两个核心挑战：动态建模分布异质性和实现细粒度语义一致性。

Method: 集成原型约束与对抗学习，使用基于MMD的域感知源权重策略动态估计域间偏移，并通过原型引导条件对齐模块增强伪标签可靠性。

Result: 在SEED和SEED-IV数据集上，跨被试准确率分别达94.86%和79.78%，跨会话分别达95.12%和83.15%；在FACED数据集上跨被试准确率达82.88%。

Conclusion: DAMSDAN框架通过动态源权重分配和细粒度语义对齐，有效提升了跨域EEG情绪识别的性能，消融实验和可解释性分析验证了其有效性。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [383] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 本研究探讨了使用生成对抗网络（GAN）进行河流沉积物预测的可行性，发现GAN的潜在表示存在纠缠问题，导致反演匹配困难。通过标签条件化、潜在过参数化和局部微调等方法可以改善匹配效果。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高且不确定性大，获取新数据难以扩展。将地质知识直接嵌入预测模型提供了有价值的替代方案，过程模型可以帮助训练生成模型以提高预测效率。

Method: 使用生成对抗网络（GAN）训练河流沉积物生成模型，并应用四种反演方法来匹配井数据和地震数据。采用标签条件化、潜在过参数化和局部微调等技术来改善潜在空间的表示。

Result: 四种反演方法在4、8和20口井的三个测试样本中难以匹配井数据，特别是当井数增加或测试样本与训练数据差异较大时。局部微调GAN可以显著减少不匹配，达到可接受水平。

Conclusion: GAN已经能够处理地质建模工作流所需的任务，但仍需进一步评估其鲁棒性，以及如何最好地利用它们来支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [384] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩阵分解的去中心化学习隐私分析方法，改进了现有的差分隐私计算，并开发了新的MAFALDA-SGD算法。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化学习中的差分隐私保护方法在实践中观察到的隐私-效用权衡往往比集中式训练更差，这可能是由于现有DP计算方法存在局限性。

Method: 通过推广现有的矩阵分解结果，将标准DL算法和常见信任模型统一到一个公式中，并开发了MAFALDA-SGD算法（一种基于gossip的去中心化学习算法，具有用户级相关噪声）。

Result: 该方法为现有DP-DL算法提供了更严格的隐私计算，并在合成和真实世界图上优于现有方法。

Conclusion: 基于矩阵分解的隐私分析方法可以改进去中心化学习的隐私保护效果，并为开发新算法提供了理论基础。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [385] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出通过层间Hessian矩阵分析神经网络的方法，揭示参数空间局部几何特性与泛化性能的关系


<details>
  <summary>Details</summary>
Motivation: 为神经网络提供形式化工具来表征参数空间的局部几何特性，连接优化几何与函数行为

Method: 定义每个功能块（层）的局部Hessian矩阵作为参数二阶导数矩阵，分析其谱特性（如特征值分布）

Result: 在37个数据集上的111个实验显示，局部Hessian在训练过程中呈现一致的结构规律，其谱特性与泛化性能相关

Conclusion: 局部几何分析为诊断和设计深度神经网络提供了基础，有助于改进网络架构和训练稳定性

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [386] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号基准测试，用于评估大语言模型和大推理模型在类比和数学推理中的泛化能力和鲁棒性。它通过增加操作数复杂度、属性范围和引入感知不确定性来扩展I-RAVEN。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs和LRMs在类比和数学推理中的泛化能力和鲁棒性，特别是在复杂操作数、宽属性范围和感知不确定性条件下的表现。

Method: 扩展I-RAVEN基准测试，增加操作数复杂度、扩大属性范围，并引入感知不确定性，构建I-RAVEN-X符号基准。

Result: 与LLMs相比，LRMs在更长推理关系和更宽属性范围上分别表现出更好的生产力和系统性。但LRMs在不确定性推理方面仍面临挑战，无法有效探索多个概率结果。

Conclusion: LRMs在复杂推理任务中相比LLMs有所改进，但在处理不确定性和探索概率结果方面仍有显著局限性。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [387] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法使随机DC优化在任何批次大小下都能收敛，无需大批次或强噪声假设


<details>
  <summary>Details</summary>
Motivation: 现有随机DC优化方法需要大批次或强噪声假设，限制了实际应用，而小批次下的收敛性研究不足

Method: 提出基于动量的算法，在标准平滑性和有界方差假设下实现收敛

Result: 证明无动量时无论步长如何都可能无法收敛，动量方法具有可证明的收敛性和强实证性能

Conclusion: 动量是实现随机DC优化小批次收敛的关键因素

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [388] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 该论文分析了过参数化最小二乘问题中梯度下降在大学习率下的收敛行为，揭示了三种不同学习率区间的收敛特性：亚临界、临界和超临界区间。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论只保证小步长梯度下降的单调收敛，但神经网络训练常使用大学习率（边缘稳定性），表现出非单调收敛和对平坦极小值的偏好。本文旨在量化这一现象。

Method: 利用过参数化使全局极小值形成黎曼流形，将梯度下降动态分解为平行和正交于流形的分量，分别对应黎曼梯度下降和分叉动力系统。

Result: 识别了三种收敛区间：(a)亚临界区间：有限时间内克服瞬态不稳定性后线性收敛到次优平坦极小值；(b)临界区间：不稳定性持续存在，以幂律收敛到最优平坦极小值；(c)超临界区间：不稳定性持续存在，线性收敛到周期为2的轨道。

Conclusion: 通过几何分解方法，成功量化了梯度下降在大学习率下的收敛行为，为理解边缘稳定性现象提供了理论框架。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [389] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论的新框架，用图论分析稀疏神经网络的训练动态，解释了不同剪枝方法收敛行为差异的原因。


<details>
  <summary>Details</summary>
Motivation: 理解为什么相同稀疏度下某些稀疏结构比其他结构更容易训练，以及不同剪枝方法产生的连接模式如何影响网络可训练性。

Method: 基于图极限理论（特别是图论）建立理论框架，提出图极限假设，并推导图论神经正切核（Graphon NTK）来分析无限宽度极限下的稀疏网络训练动态。

Result: 经验证据支持图极限假设，Graphon NTK的谱分析与观察到的稀疏网络训练动态相关，能够解释不同剪枝方法的收敛行为差异。

Conclusion: 该框架为稀疏网络的理论分析提供了通用工具，揭示了连接模式对各种稀疏网络架构可训练性的影响。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [390] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出了SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源数据整合和注意力网络实现高精度异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注功能驱动的临时异常（如困倦、分心），但缺乏对病理触发异常（特别是慢性疾病如帕金森病）的检测机制，这对公共交通安全构成风险。

Method: 分析帕金森病症状学，建立与驾驶性能退化的因果关系；整合多个车辆控制组件数据构建行为档案；设计基于注意力的网络自适应优先处理时空特征。

Result: 在Logitech G29平台和CARLA模拟器上验证，使用三个道路地图模拟真实驾驶，SAFE-D在区分正常和帕金森病影响驾驶模式方面达到96.8%的平均准确率。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为提升驾驶安全提供了新方法。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [391] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，拉格朗日方法是处理约束优化问题的常用方法，但其效果严重依赖于拉格朗日乘子的选择。目前缺乏关于自动更新乘子鲁棒性及其对性能影响的实证证据。

Method: 通过分析多个任务中拉格朗日乘子的最优性和稳定性，提供λ-配置文件可视化回报与约束成本之间的权衡，并研究自动乘子更新和PID控制更新的效果。

Result: 研究发现λ具有高度敏感性，自动乘子更新能够恢复甚至超过λ*的最优性能，但训练过程中表现出振荡行为，PID控制可以缓解但需要仔细调参。

Conclusion: 拉格朗日乘子在安全强化学习中具有高度敏感性，自动更新方法有效但需要进一步研究来稳定拉格朗日方法。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [392] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作潜在博弈，通过好奇心奖励机制解决长尾不平衡问题，在罕见标签上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而许多在实践中重要的罕见标签被忽略。

Method: 将标签空间分配给多个合作玩家，共享全局准确度收益，同时根据标签稀有性和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重。

Result: 在传统基准和三个超大规模数据集上实现最先进性能，罕见F1提升达+4.3%，P@3提升+1.6%，消融实验显示出现分工和罕见类别更快共识。

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了原则性、可扩展的解决方案。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [393] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了CEPerFed方法，一种通信高效的个性化联邦学习方法，用于解决多脉冲MRI分类中的数据异构性和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI在临床实践中广泛应用，但训练稳健模型需要大量多样化数据，同时要保护隐私防止原始数据共享。联邦学习虽然可行，但面临数据异构性导致的模型收敛问题和大量参数传输带来的通信开销挑战。

Method: CEPerFed方法通过结合客户端历史风险梯度和历史平均梯度来协调局部和全局优化，前者用于加权其他客户端的贡献，后者确保局部更新与全局优化方向一致。同时提出分层SVD策略，仅传输模型更新所需的最关键信息。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed方法成功解决了联邦学习中的数据异构性和通信开销问题，为多脉冲MRI分类提供了一种有效的解决方案。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [394] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出了一种名为反事实知识蒸馏（CFKD）的框架，通过生成多样化的反事实样本来解决深度学习模型中的虚假相关性问题，无需组标签即可实现跨组的平衡泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到虚假相关性的影响，现有方法如深度特征重加权（DFR）依赖显式的组标签且在处理多个虚假相关性时性能下降明显。

Method: 使用反事实解释器生成多样化反事实样本，通过知识蒸馏步骤让人类标注者高效探索和修正模型决策边界，无需混淆变量标签。

Result: 在五个数据集上验证了CFKD的有效性，特别是在低数据量和高虚假相关性的场景下表现优异，能够扩展到多个混淆变量。

Conclusion: CFKD框架能够有效解决虚假相关性问题，无需组标签即可实现平衡泛化，在工业应用中具有实用价值。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [395] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 该论文研究了在低信噪比(SNR)数据环境下，通过在梯度下降训练过程中引入标签噪声来抑制神经网络过拟合噪声、提升泛化性能的方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易在训练过程中同时学习统计信号和过拟合噪声，特别是在低信噪比数据中，这种噪声记忆会损害泛化能力。受标签噪声具有隐式正则化效果的启发，研究是否可以通过引入标签噪声来改善神经网络在低SNR环境下的测试性能。

Method: 在理想化的信号-噪声数据设置中，使用带标签噪声的梯度下降算法训练两层神经网络，通过理论分析证明标签噪声如何抑制噪声记忆。

Result: 证明添加标签噪声能有效抑制噪声记忆，防止其主导学习过程，从而在控制过拟合的同时实现信号的快速学习，在低SNR下获得良好的泛化性能。

Conclusion: 在低信噪比环境下，标准梯度下降容易过拟合噪声且测试误差有非零下界，而引入标签噪声的梯度下降能有效提升神经网络的泛化能力。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [396] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种级联方法，将预训练OVD模型与轻量级少样本分类器结合，通过FLAME主动学习策略选择信息量最大的样本进行训练，实现快速适应和高效检测。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇目标检测模型在遥感等专业领域中由于自然语言模糊性导致的细粒度类别区分困难问题，如无法准确区分"渔船"和"游艇"等相似类别。

Method: 采用级联方法：先用零-shot模型生成高召回率目标提议，再用轻量级分类器进行精确度提升；核心是FLAME主动学习策略，通过密度估计和聚类选择边界附近的不确定样本。

Result: 在遥感基准测试中持续超越最先进方法，实现快速适应（不到一分钟），显著减少标注成本，无需昂贵的全模型微调。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够快速适应用户特定需求，在保持高精度的同时大幅降低标注成本。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [397] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于共形对齐的级联机制，确保边缘预测在条件覆盖方面达到云模型水平，同时减少向云端的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能虽然能通过紧凑的本地模型实现低延迟推理，但保证可靠性仍然具有挑战性。需要确保边缘预测集在条件覆盖方面与云模型相当。

Method: 将边缘到云端的升级建模为多重假设检验问题，采用共形对齐来选择可以在边缘安全处理的输入，构建CAb级联机制。

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联在保持目标条件覆盖的同时，显著减少向云端的卸载，预测集大小仅适度增加。

Conclusion: 该方法为边缘-云级联系统提供了统计保证，在覆盖度、延迟率和集大小之间实现了可调权衡。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [398] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一种高效的车辆轨迹学习方法，通过联合建模GPS和道路视角来捕获移动模式，集成旅行目的到嵌入中，并使用知识蒸馏减少轨迹冗余。


<details>
  <summary>Details</summary>
Motivation: 解决车辆GPS轨迹学习中的两个主要挑战：旅行目的与道路功能和POI相关带来的计算负担，以及真实轨迹中冗余点对计算效率和嵌入质量的损害。

Method: 提出TrajMamba方法，包括Traj-Mamba编码器联合建模GPS和道路视角，旅行目的感知预训练集成旅行目的，以及知识蒸馏预训练通过可学习掩码生成器识别关键轨迹点。

Result: 在两个真实数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性方面均优于最先进的基线方法。

Conclusion: TrajMamba能够有效且高效地学习语义丰富的车辆轨迹表示，解决了轨迹学习中的关键挑战。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [399] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的Sim2Real框架，通过双层强化学习直接基于真实世界性能调整模拟器参数，以缩小Sim2Real性能差距。


<details>
  <summary>Details</summary>
Motivation: 传统的Sim2Real方法通过优化模拟器精度和变异性作为真实世界性能的代理指标，但这些指标与策略在真实世界中的性能不一定相关，导致模拟训练的策略在真实部署时性能显著下降。

Method: 采用双层强化学习框架：内层RL在模拟环境中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发双层RL算法所需的数学工具，这些算法能够缩小Sim2Real性能差距。

Conclusion: 提出的框架通过直接基于真实世界性能调整模拟器参数，有效解决了Sim2Real性能差距问题。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [400] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展的解码器Transformer，通过变分过程无监督学习随机潜在变量来调节生成过程


<details>
  <summary>Details</summary>
Motivation: 通过引入潜在变量来增强解码器Transformer的生成能力，使其能够更好地处理下游任务

Method: 扩展解码器Transformer，引入随机潜在变量，通过变分方法进行无监督学习

Result: 实验评估显示这种条件化方法在下游任务上带来了显著改进

Conclusion: 通过潜在变量条件化可以显著提升解码器Transformer在下游任务上的性能

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [401] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了一个理论框架，通过可验证的属性来形式化时间序列异常检测评估的基本要求，分析了37个常用指标的局限性，并提出了满足所有属性的LARM指标及其扩展ALARM。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法的性能评估存在严重问题，现有指标仅捕获任务的狭窄方面且常产生误导性结果，这可能导致安全关键系统中的灾难性故障。

Method: 引入可验证属性来形式化评估时间序列异常检测的基本要求，建立理论框架支持原则性评估和可靠比较，分析37个常用指标的属性满足情况，并提出新的LARM和ALARM指标。

Result: 分析显示大多数现有指标仅满足少数属性，没有一个指标满足所有属性，这解释了先前结果中持续存在的不一致性。LARM和ALARM被证明满足所有属性要求。

Conclusion: 该研究为时间序列异常检测评估提供了理论基础，通过引入可验证属性和新指标解决了现有评估方法的局限性，支持更可靠的方法比较。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [402] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 本研究开发了物理信息神经网络(PINN)策略，将海冰物理知识整合到机器学习模型中，用于预测北极海冰速度和浓度，相比纯数据驱动模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的机器学习模型在泛化性和物理一致性方面存在局限性，特别是在北极海冰变薄和加速融化的新阶段，历史数据训练的模型可能无法充分代表未来动态变化的海冰条件。

Method: 基于层次信息共享U-net架构，整合物理损失函数和激活函数，产生物理上合理的海冰速度和浓度输出。

Result: PINN模型在海冰速度和浓度的日常预测中优于纯数据驱动模型，即使在少量样本训练下也表现良好，特别是在融化和早期冻结季节以及快速移动冰区显著改善了海冰浓度预测。

Conclusion: 物理信息神经网络方法能够有效整合物理知识，提高海冰预测的准确性和物理一致性，特别是在数据稀缺或海冰条件快速变化的情况下。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [403] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该研究探讨了通过降维进一步压缩抗菌肽设计空间的方法，研究了这种压缩对优化效率、可解释性和基于理化性质组织潜空间的影响。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽是治疗细菌感染的有前景疗法，但序列空间巨大使得发现和设计困难。现有的深度生成模型虽然有效，但缺乏可解释性且对潜空间质量的量化不足。

Method: 使用变分自编码器等深度生成模型，结合降维技术压缩设计空间，并通过理化性质组织潜空间来优化抗菌活性。

Result: 发现当数据可用时，通过降维进一步压缩潜空间并结合更相关信息是有利的；降维搜索空间更具可解释性；可以在不同标签可用性下用不同理化性质组织潜空间。

Conclusion: 降维技术可以改善抗菌肽设计空间的优化效率和可解释性，基于理化性质组织潜空间有助于更有效地优化抗菌活性。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [404] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉变换器，用于区分心源性肺水肿与非心源性肺部疾病，在肺超声视频分类中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的视觉变异性高，在肺超声视频中区分心源性肺水肿具有挑战性，需要开发能够处理无序医学图像数据的模型。

Method: 提出ZACH-ViT（零标记自适应紧凑分层视觉变换器），移除位置嵌入和[CLS]标记，使其完全排列不变；引入ShuffleStrides数据增强，在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在380个肺超声视频上评估，ZACH-ViT获得最高验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而竞争模型均崩溃为平凡分类。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [405] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）来增强自杀预测数据集，解决极端类别不平衡问题。在真实测试数据上，逻辑回归、随机森林和支持向量机都表现出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 自杀预测是预防的关键，但真实数据中阳性样本稀少，导致极端类别不平衡问题，需要数据增强来改善模型性能。

Method: 使用机器学习模型（逻辑回归、随机森林、支持向量机）和深度学习技术（生成对抗网络GAN）生成合成数据样本来增强数据集。初始数据集包含656个样本，其中仅有4个阳性案例。

Result: 在真实测试数据上：逻辑回归的加权精度0.99、召回率0.85、F1分数0.91；随机森林分别为0.98、0.99、0.99；支持向量机分别为0.99、0.76、0.86。逻辑回归和SVM正确识别了1个自杀尝试案例（灵敏度1.0），但有一定误分类。

Conclusion: 这些结果突出了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [406] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于GaLore和Muon算法的无偏低秩优化方法GUM，通过层采样技术消除低秩投影的偏差，在保持内存效率的同时实现收敛保证和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有梯度低秩投影方法（如GaLore）缺乏收敛保证，其低秩投影机制引入偏差导致性能与全参数训练存在差距。

Method: 采用层采样技术消除低秩投影偏差，结合GaLore机制和Muon算法构建GUM方法。

Result: 理论证明GUM匹配Muon算法的收敛保证，实验显示在LLM微调和预训练中优于GaLore，甚至超越全参数训练。

Conclusion: GUM通过更均匀的层内知识分布实现参数空间更高效利用和更好的记忆能力，是无偏且内存高效的低秩优化方法。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


### [407] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 该论文提出了三个主要贡献：CADP算法连接策略梯度和动态规划，建立了ERM Bellman算子的收缩条件并提出了相关算法，以及开发了用于风险规避目标的模型无关Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在多模型MDP中最大化折扣回报的问题，以及为风险规避目标（ERM-TRC和EVaR-TRC）开发有效的计算方法和收敛保证。

Method: 方法包括：1）CADP算法迭代调整模型权重；2）建立ERM Bellman算子的收缩条件并设计值迭代、策略迭代和线性规划算法；3）开发模型无关Q学习算法处理风险规避目标。

Result: 结果证明了CADP能保证单调策略改进到局部最优，ERM Bellman算子的收缩条件，以及Q学习算法能收敛到最优风险规避值函数。

Conclusion: 结论是成功建立了策略梯度与动态规划的新联系，为风险规避MDP提供了理论保证和有效算法，包括收敛性证明和最优策略计算。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [408] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 该论文研究如何提高黑盒大语言模型作为分类器的操作粒度，通过分析其低基数数值输出的原因，并提出了有效方法来显著增加可用操作点数量，在不损失性能的情况下实现更精细的决策控制。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs虽然实用易用，但在需要特定指标约束的应用中表现不佳，主要因为其数值输出基数低，限制了操作点的精细调整能力。

Method: 首先分析LLMs低基数输出的原因，发现其倾向于生成四舍五入但信息丰富的语言化概率；然后实验标准提示工程、不确定性估计和置信度提取技术；最后提出有效方法来显著增加操作点数量和多样性。

Result: 在11个数据集和3个LLMs上的实验表明，所提方法提供了更细粒度的操作点，性能与基准方法相当或更好。

Conclusion: 提出的方法能有效提高黑盒LLMs的操作粒度，使其在需要精确指标约束的决策应用中更具实用性。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [409] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了基于微分图册的流形学习方法，通过维护可微分图册结构实现流形上的黎曼优化，在效率和精度上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要进行降维，当嵌入维度接近流形内在维度时会丢失关键特征，而直接学习可微分图册的方法相对较少被探索。

Method: 实现通用数据结构来维护可微分图册，支持流形上的黎曼优化，并提出了从点云数据无监督学习可微分图册的启发式方法。

Result: 实验证明该方法在选定场景下具有效率和精度优势，在Klein瓶分类任务和造血数据RNA速度分析中展示了更好的可解释性和鲁棒性。

Conclusion: 基于图册的方法在流形学习中具有有效性和潜力，为直接处理流形数据提供了新思路。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [410] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 本文提出了在推理时保持线性插值的流匹配缩放方法，在图像和蛋白质生成任务中验证了推理计算增加能持续提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配方法在语言、视觉和科学领域获得关注，但其推理时缩放方法研究不足。现有方法使用非线性插值会牺牲流匹配的高效采样优势，且推理时计算缩放仅应用于视觉任务。

Method: 提出了新颖的推理时缩放程序，在采样过程中保持线性插值，适用于图像生成和无条件蛋白质生成任务。

Result: 评估显示：I) 随着推理计算增加，样本质量持续提升；II) 流匹配推理时缩放可应用于科学领域。

Conclusion: 提出的方法成功扩展了流匹配的推理时计算缩放能力，保持了线性插值的采样效率，并在科学领域验证了其有效性。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [411] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出了Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布方法，通过beta-ELBO和蒙特卡洛采样训练，使预测分布的离散度能自适应输入，在分布偏移下保持良好校准。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往过于自信，需要一种能自适应输入不确定性的方法，在保持分布内性能的同时提高对分布外数据的感知能力。

Method: 使用输入条件化的网络权重分布（FDN），通过beta-ELBO损失函数和蒙特卡洛采样进行训练，产生能根据输入自适应离散度的预测混合分布。

Result: 在标准回归任务上，与贝叶斯、集成、dropout和超网络基线相比，在匹配参数和更新预算下，FDN在准确性、校准和偏移感知方面表现优异。

Conclusion: FDN框架和评估协议旨在使分布外感知、良好校准的神经回归变得实用和模块化，为处理分布偏移提供了有效解决方案。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [412] [Prediction Intervals for Model Averaging](https://arxiv.org/abs/2510.16224)
*Zhongjun Qu,Wendun Wang,Xiaomeng Zhang*

Main category: econ.EM

TL;DR: 该论文提出了一种基于共形推断的模型平均预测区间方法，能够在模型误设和多种模型结构下提供有效的预测不确定性度量。


<details>
  <summary>Details</summary>
Motivation: 现有的频率模型平均方法主要局限于点预测，在一般设置下衡量预测不确定性仍是一个未解决的问题。

Method: 基于共形推断构建预测区间，适用于嵌套、不相交、重叠或任意组合的模型平均，权重可依赖于估计样本。提出了基准算法、局部自适应改进和分割样本程序。

Result: 在交换性假设下具有精确有限样本有效性（适用于横截面数据），在平稳性假设下具有渐近有效性（适用于时间序列数据）。通过房地产评估和股权溢价预测案例验证了方法。

Conclusion: 提出的方法为模型平均提供了可靠的预测区间，能够有效评估预测不确定性，适用于广泛的模型设置和数据类型。

Abstract: A rich set of frequentist model averaging methods has been developed, but
their applications have largely been limited to point prediction, as measuring
prediction uncertainty in general settings remains an open problem. In this
paper we propose prediction intervals for model averaging based on conformal
inference. These intervals cover out-of-sample realizations of the outcome
variable with a pre-specified probability, providing a way to assess predictive
uncertainty beyond point prediction. The framework allows general model
misspecification and applies to averaging across multiple models that can be
nested, disjoint, overlapping, or any combination thereof, with weights that
may depend on the estimation sample. We establish coverage guarantees under two
sets of assumptions: exact finite-sample validity under exchangeability,
relevant for cross-sectional data, and asymptotic validity under stationarity,
relevant for time-series data. We first present a benchmark algorithm and then
introduce a locally adaptive refinement and split-sample procedures that
broaden applicability. The methods are illustrated with a cross-sectional
application to real estate appraisal and a time-series application to equity
premium forecasting.

</details>


### [413] [On the Asymptotics of the Minimax Linear Estimator](https://arxiv.org/abs/2510.16661)
*Jing Kong*

Main category: econ.EM

TL;DR: 该论文研究了通过极小极大凸优化方法设置权重的加权估计器，证明了其在正则条件下具有根号n一致性和渐近正态性，并推导了渐近方差。该估计器在温和方差条件下能达到半参数效率界，无需文献中常用的增强步骤。


<details>
  <summary>Details</summary>
Motivation: 许多因果估计量可以表示为未知回归函数的连续线性泛函。虽然极小极大加权估计器使用日益广泛，但其一般根号n理论仍然有限，本文旨在填补这一空白。

Method: 采用极小极大程序设置权重的加权估计器，通过求解权衡最坏情况条件偏差与方差的凸优化问题来确定权重。

Result: 在正则条件下，极小极大线性估计器具有根号n一致性和渐近正态性。在温和方差条件下，该估计器能达到半参数效率界。模拟和三个实证应用（包括职业培训和最低工资政策）表明：满足正则条件的设计中，标准误差置信区间足够；否则，偏差感知区间仍然重要。

Conclusion: 极小极大加权估计器具有良好的渐近性质，其理论结果证明在大样本置信区间构建中可以忽略最坏情况偏差，使推断对函数类缩放不那么敏感。该估计器无需增强步骤即可达到一阶最优性。

Abstract: Many causal estimands, such as average treatment effects under
unconfoundedness, can be written as continuous linear functionals of an unknown
regression function. We study a weighting estimator that sets weights by a
minimax procedure: solving a convex optimization problem that trades off
worst-case conditional bias against variance. Despite its growing use, general
root-$n$ theory for this method has been limited. This paper fills that gap.
Under regularity conditions, we show that the minimax linear estimator is
root-$n$ consistent and asymptotically normal, and we derive its asymptotic
variance. These results justify ignoring worst-case bias when forming
large-sample confidence intervals and make inference less sensitive to the
scaling of the function class. With a mild variance condition, the estimator
attains the semiparametric efficiency bound, so an augmentation step commonly
used in the literature is not needed to achieve first-order optimality.
Evidence from simulations and three empirical applications, including
job-training and minimum-wage policies, points to a simple rule: in designs
satisfying our regularity conditions, standard-error confidence intervals
suffice; otherwise, bias-aware intervals remain important.

</details>


### [414] [Causal Inference in High-Dimensional Generalized Linear Models with Binary Outcomes](https://arxiv.org/abs/2510.16669)
*Jing Kong*

Main category: econ.EM

TL;DR: 提出了一种用于高维广义线性模型中因果效应的去偏估计器，适用于二元结果和一般链接函数，不依赖倾向得分估计。


<details>
  <summary>Details</summary>
Motivation: 在高维设置中，现有的因果效应估计方法（如逆倾向得分估计器和双机器学习估计器）在有限样本中表现不佳，需要开发更稳健的估计器。

Method: 通过将正则化回归插件与从凸优化问题计算的权重相结合，近似平衡链接导数加权的协变量并控制方差。

Result: 在标准条件下，估计器对密集线性对比和因果参数具有√n一致性和渐近正态性。模拟显示该方法在有限样本中优于其他替代方法。

Conclusion: 该方法在模拟和实际数据应用中均表现出色，估计结果和置信区间接近实验基准，为高维因果推断提供了有效工具。

Abstract: This paper proposes a debiased estimator for causal effects in
high-dimensional generalized linear models with binary outcomes and general
link functions. The estimator augments a regularized regression plug-in with
weights computed from a convex optimization problem that approximately balances
link-derivative-weighted covariates and controls variance; it does not rely on
estimated propensity scores. Under standard conditions, the estimator is
$\sqrt{n}$-consistent and asymptotically normal for dense linear contrasts and
causal parameters. Simulation results show the superior performance of our
approach in comparison to alternatives such as inverse propensity score
estimators and double machine learning estimators in finite samples. In an
application to the National Supported Work training data, our estimates and
confidence intervals are close to the experimental benchmark.

</details>


### [415] [On Quantile Treatment Effects, Rank Similarity,and Variation of Instrumental Variables](https://arxiv.org/abs/2510.16681)
*Sukjin Han,Haiqing Xu*

Main category: econ.EM

TL;DR: 本文开发了一个非参数框架，用于识别和估计不可分离内生性下的分布处理效应。通过放宽传统的秩相似性假设，构建了基于线性半无限规划的识别边界，并证明了其大样本性质。


<details>
  <summary>Details</summary>
Motivation: 传统秩相似性(RS)假设过于严格，限制了分布处理效应的识别。本文旨在开发一个更弱的识别条件，在非参数框架下处理不可分离内生性问题。

Method: 采用线性半无限规划(SILP)方法构建识别边界，利用鞍点结构和KKT条件建立大样本性质，支持多值或多重工具变量的使用。

Result: 成功建立了分布处理效应的识别边界，证明了估计边界的一致性和渐近分布性质，展示了更丰富的外生工具变量可以进一步收紧边界。

Conclusion: 提出的弱识别条件比秩相似性假设更灵活，线性半无限规划方法为处理不可分离内生性下的分布处理效应提供了可行的识别和估计框架。

Abstract: This paper develops a nonparametric framework to identify and estimate
distributional treatment effects under nonseparable endogeneity. We begin by
revisiting the widely adopted \emph{rank similarity} (RS) assumption and
characterizing it by the relationship it imposes between observed and
counterfactual potential outcome distributions. The characterization highlights
the restrictiveness of RS, motivating a weaker identifying condition. Under
this alternative, we construct identifying bounds on the distributional
treatment effects of interest through a linear semi-infinite programming (SILP)
formulation. Our identification strategy also clarifies how richer exogenous
instrument variation, such as multi-valued or multiple instruments, can further
tighten these bounds. Finally, exploiting the SILP's saddle-point structure and
Karush-Kuhn-Tucker (KKT) conditions, we establish large-sample properties for
the empirical SILP: consistency and asymptotic distribution results for the
estimated bounds and associated solutions.

</details>


### [416] [Local Overidentification and Efficiency Gains in Modern Causal Inference and Data Combination](https://arxiv.org/abs/2510.16683)
*Xiaohong Chen,Haitian Xie*

Main category: econ.EM

TL;DR: 本文研究非参数局部（过度）识别和半参数效率，开发了统一方法分析三个因果模型：无混淆处理模型、阴性对照模型和长期因果推断模型。


<details>
  <summary>Details</summary>
Motivation: 研究现代因果框架中的非参数局部识别和半参数效率问题，特别是在存在过度识别的情况下推导效率界限和有效估计量。

Method: 将带有潜变量的结构模型转化为可观测变量的统计模型，通过条件矩限制分析局部过度识别，开发统一的分析框架。

Result: 无混淆处理模型是局部恰好识别的，所有估计量共享相同渐近方差；而阴性对照模型和长期因果推断模型存在局部过度识别，某些双重稳健估计量是低效的。

Conclusion: 在过度识别模型中推导了一般效率界限和有效估计量，放宽了传统方法中恢复恰好识别的强假设条件。

Abstract: This paper studies nonparametric local (over-)identification, in the sense of
Chen and Santos (2018), and the associated semiparametric efficiency in modern
causal frameworks. We develop a unified approach that begins by translating
structural models with latent variables into their induced statistical models
of observables and then analyzes local overidentification through conditional
moment restrictions. We apply this approach to three leading models: (i) the
general treatment model under unconfoundedness, (ii) the negative control
model, and (iii) the long-term causal inference model under unobserved
confounding. The first design yields a locally just-identified statistical
model, implying that all regular asymptotically linear estimators of the
treatment effect share the same asymptotic variance, equal to the (trivial)
semiparametric efficiency bound. In contrast, the latter two models involve
nonparametric endogeneity and are naturally locally overidentified;
consequently, some doubly robust orthogonal moment estimators of the average
treatment effect are inefficient. Whereas existing work typically imposes
strong conditions to restore just-identification before deriving the efficiency
bound, we relax such assumptions and characterize the general efficiency bound,
along with efficient estimators, in the overidentified models (ii) and (iii).

</details>


### [417] [Equilibrium-Constrained Estimation of Recursive Logit Choice Models](https://arxiv.org/abs/2510.16886)
*Hung Tran,Tien Mai,Minh Hoang Ha*

Main category: econ.EM

TL;DR: 提出了一种将递归logit模型的最大似然估计重新表述为带均衡约束的优化问题，并转化为锥优化问题，显著提高了计算效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的嵌套固定点算法计算成本高且容易数值不稳定，需要更高效稳定的估计方法。

Method: 将最大似然估计重新表述为带均衡约束的优化问题，将结构参数和价值函数都作为决策变量，并转化为锥优化问题。

Result: 在合成和真实数据集上的实验表明，该方法在保持与传统方法相当精度的同时，显著提高了计算稳定性和效率。

Conclusion: 该凸优化重构为递归logit模型估计提供了一个实用且可扩展的替代方案。

Abstract: The recursive logit (RL) model provides a flexible framework for modeling
sequential decision-making in transportation and choice networks, with
important applications in route choice analysis, multiple discrete choice
problems, and activity-based travel demand modeling. Despite its versatility,
estimation of the RL model typically relies on nested fixed-point (NFXP)
algorithms that are computationally expensive and prone to numerical
instability. We propose a new approach that reformulates the maximum likelihood
estimation problem as an optimization problem with equilibrium constraints,
where both the structural parameters and the value functions are treated as
decision variables. We further show that this formulation can be equivalently
transformed into a conic optimization problem with exponential cones, enabling
efficient solution using modern conic solvers such as MOSEK. Experiments on
synthetic and real-world datasets demonstrate that our convex reformulation
achieves accuracy comparable to traditional methods while offering significant
improvements in computational stability and efficiency, thereby providing a
practical and scalable alternative for recursive logit model estimation.

</details>


### [418] [Mixed LR-$C(α)$-type tests for irregular hypotheses, general criterion functions and misspecified models](https://arxiv.org/abs/2510.17070)
*Jean-Marie Dufour,Purevdorj Tuvaandorj*

Main category: econ.EM

TL;DR: 提出一种具有鲁棒性的似然比型检验方法，在极值估计框架下具备C(α)型程序的稳健特性，适用于模型误设和约束参数空间的情况。


<details>
  <summary>Details</summary>
Motivation: 标准似然比检验在模型误设（信息矩阵等式不成立）或参数空间存在约束（如边界参数）时表现不佳，需要开发更稳健的检验方法。

Method: 通过对受限和不受限准则函数分别进行调整来构建检验统计量，该统计量在最小条件下具有渐近枢轴性，仅需根号n相合估计器来处理冗余参数。

Result: 模拟研究表明，在ARCH模型和参数生存回归中，该检验能保持准确的尺寸并展现良好的功效；实证应用显示比传统t检验提供更有信息量的推断。

Conclusion: 所提出的检验方法在模型误设和约束参数空间情况下具有稳健性，当模型正确设定且无边界约束时退化为标准似然比检验，具有广泛适用性。

Abstract: This paper introduces a likelihood ratio (LR)-type test that possesses the
robustness properties of \(C(\alpha)\)-type procedures in an extremum
estimation setting.
  The test statistic is constructed by applying separate adjustments to the
restricted and unrestricted criterion functions, and is shown to be
asymptotically pivotal under minimal conditions. It features two main
robustness properties. First, unlike standard LR-type statistics, its null
asymptotic distribution remains chi-square even under model misspecification,
where the information matrix equality fails. Second, it accommodates irregular
hypotheses involving constrained parameter spaces, such as boundary parameters,
relying solely on root-\(n\)-consistent estimators for nuisance parameters.
When the model is correctly specified, no boundary constraints are present, and
parameters are estimated by extremum estimators, the proposed test reduces to
the standard LR-type statistic.
  Simulations with ARCH models, where volatility parameters are constrained to
be nonnegative, and parametric survival regressions with potentially monotone
increasing hazard functions, demonstrate that our test maintains accurate size
and exhibits good power. An empirical application to a two-way error components
model shows that the proposed test can provide more informative inference than
the conventional \(t\)-test.

</details>
