<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 111]
- [econ.GN](#econ.GN) [Total: 7]
- [cs.LG](#cs.LG) [Total: 285]
- [cs.AI](#cs.AI) [Total: 90]
- [econ.EM](#econ.EM) [Total: 2]
- [econ.TH](#econ.TH) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 通过分析GPT2-Small第一层中注意力模式分散且对内容依赖较弱的注意力头，利用校准文本采样softmax分母，近似多个稳定头的组合输出为周围文本的线性摘要，从而从权重和单一校准文本中识别出数百个对周围文本高级上下文属性响应的第一层神经元。


<details>
  <summary>Details</summary>
Motivation: 研究transformer语言模型中注意力模式分散且对内容依赖较弱的注意力头，探索其softmax分母在固定token分布下的稳定性，以揭示模型内部如何编码高级上下文信息。

Method: 使用校准文本采样softmax分母，将GPT2-Small第一层中多个稳定注意力头的输出组合近似为周围文本的线性摘要，仅从模型权重和单一校准文本出发进行分析。

Result: 成功识别出数百个第一层神经元，这些神经元对周围文本的高级上下文属性（如语义、语法等）产生响应，包括在校准文本上未激活的神经元。

Conclusion: 证明了仅通过模型权重和单一校准文本即可有效揭示transformer语言模型内部对高级上下文信息的编码机制，为理解模型内部表示提供了新方法。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: Graph-S³是一个基于LLM的文本图推理框架，通过合成逐步监督训练检索器，解决了图检索中信息充分性和上下文紧凑性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据多以文本图形式存在，但现有图检索方法要么依赖浅层嵌入相似度，要么需要大量标注数据和训练成本，导致性能不佳。

Method: 提出基于LLM的检索器，使用合成逐步监督训练，通过离线提取黄金子图评估每个检索步骤，采用数据合成流水线和两阶段训练方案学习交互式图探索策略。

Result: 在三个常用数据集上与七个强基线对比，平均准确率提升8.1%，F1分数提升9.7%，在多跳推理任务中优势更明显。

Conclusion: Graph-S³通过合成逐步监督有效解决了图检索的挑战，显著提升了文本图问答系统的性能，特别是在复杂推理任务中表现优异。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 该研究审计了6个主流大语言模型在完成30个日常任务时表现出的隐含价值观，并与100名美国众包工作者进行比较，发现LLMs与人类之间以及不同LLMs之间的价值观存在显著不一致。


<details>
  <summary>Details</summary>
Motivation: 尽管AI助手在帮助用户完成日常任务方面具有潜力，但人们对这些助手在完成主观日常任务时表现出的隐含价值观知之甚少。

Method: 通过审计6个流行LLMs完成30个日常任务的表现，并与100名美国众包工作者进行比较。

Result: 研究发现LLMs在表现隐含价值观时往往与人类不一致，不同LLMs之间也缺乏一致性。

Conclusion: LLMs在完成日常任务时表现出的价值观与人类存在显著差异，需要进一步关注AI助手的价值观对齐问题。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR是一种从并行话语和意义语料库中诱导语素的贪婪算法，通过互信息加权、选择、移除和重复的过程来识别语素。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够从涌现语言语料库中自动识别语素的算法，以理解语言形成的基本单元。

Method: 使用贪婪算法：1)基于形式和意义的互信息对语素加权；2)选择最高权重的对；3)从语料库中移除；4)重复过程以诱导更多语素。

Result: 在程序生成数据集上验证有效性，与基线方法比较；在人类语言数据上表现合理；能够量化涌现语言的语言特征如同义词和多义词程度。

Conclusion: CSAR算法能够有效从涌现语言中诱导语素，并在多个领域展现出良好的性能，为语言形成研究提供了有用工具。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron是一个统一的多模态检索嵌入模型，支持文本、图像、音频和视频的跨模态和联合模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索器在处理真实世界文档（如PDF、幻灯片、视频）中视觉和语义丰富的内容时存在困难，需要支持更多模态的检索能力。

Method: 基于ColPali和Qwen2.5-Omni等模型，构建了支持多模态检索的统一架构，包括文本、图像、音频和视频模态。

Result: 模型在文本、图像和视频检索方面表现出有效性，能够处理跨模态和联合模态的检索任务。

Conclusion: Omni-Embed-Nemotron为处理复杂真实世界信息需求提供了一个统一的多模态检索解决方案。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 通过基于信号博弈的涌现通信环境和超参数优化，生成与人类语言相似度最高的涌现语言，使用XferBench作为目标函数来量化统计相似性。


<details>
  <summary>Details</summary>
Motivation: 设计一个能够生成与人类语言高度相似的涌现语言的环境，以推动涌现通信研究的发展。

Method: 使用基于信号博弈的涌现通信环境，结合超参数优化和XferBench目标函数来生成涌现语言。

Result: 证明了熵对涌现语言迁移学习性能的预测能力，验证了涌现通信系统的熵最小化特性，并发现了产生更真实涌现语言的超参数规律。

Conclusion: 成功生成了与人类语言高度相似的涌现语言，并揭示了影响涌现语言真实性的关键因素。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER基准测试评估大语言模型识别文本中表达情感的具体片段的能力，包含单句和跨句情感证据检测任务，发现模型在长文本中表现下降。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别任务只给整个句子分配单一标签，而实际应用如共情对话和临床支持需要知道情感是如何表达的，因此需要开发能够精确定位情感表达片段的方法。

Method: 创建SEER基准，包含1200个真实世界句子的新标注，涵盖情感和情感证据。评估14个开源大语言模型在两个任务上的表现：单句内情感证据识别和跨5个连续句子的段落情感证据识别。

Result: 一些模型在单句输入上接近人类平均表现，但在长段落中准确率下降。错误分析显示主要失败模式包括过度依赖情感关键词和在中性文本中产生误报。

Conclusion: SEER基准揭示了当前大语言模型在细粒度情感证据检测方面的局限性，特别是在处理长文本时，需要改进模型对情感表达的理解能力。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是第一个大规模阿拉伯语数据集，专门用于区分人类和LLM生成的文本，涵盖新闻、社交媒体和评论三种文体，包含超过40万平衡样本，支持跨文体泛化研究。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语中LLM生成文本检测的挑战，为研究错误信息、学术不端和网络威胁提供基础。

Method: 构建包含MSA和方言阿拉伯语的平衡数据集，使用传统分类器、BERT模型和LLM进行基准测试。

Result: 微调BERT模型表现最佳，但跨文体泛化存在挑战，特别是在新闻文章中LLM生成文本与人类文本风格相似。

Conclusion: ALHD为阿拉伯语LLM检测研究奠定基础，揭示了跨文体泛化的挑战，为未来研究指明方向。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [9] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: TS-Reasoner通过将时间序列基础模型的潜在表示与大型语言模型的文本输入对齐，解决了时间序列推理任务中数值理解和语义推理的融合挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型能捕捉动态模式但缺乏推理能力，而大型语言模型具备推理能力但不擅长数值理解。需要有效整合两种模型以实现时间序列推理。

Method: 提出两阶段训练方法：首先使用合成的时序-文本对进行对齐预训练，然后进行指令微调。冻结预训练的时间序列基础模型，仅对齐其表示与语言模型。

Result: 在多个基准测试中，TS-Reasoner超越了主流LLM、VLM和时间序列LLM，且具有显著的数据效率（使用不到一半训练数据）。

Conclusion: 该方法成功实现了时间序列数值理解与语言推理能力的有效融合，为时间序列推理任务提供了高效解决方案。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [10] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 在专业领域中，人类通常通过比较相似案例来解决问题，而传统RAG系统缺乏这种对比分析能力。本文提出在RAG基础上增加同行感知比较推理层，以生成更具上下文洞察力的专业分析。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在专业推理任务中输出过于通用，无法提供针对性的比较分析，特别是在金融等领域中只能给出适用于大多数公司的通用风险，缺乏具体案例对比。

Method: 在RAG系统之上构建同行感知比较推理层，采用对比方法进行专业分析。

Result: 该方法在文本生成指标（如ROUGE和BERTScore）上优于基线RAG系统，与人工生成的股权研究和风险分析相比表现更好。

Conclusion: 通过增加对比推理层，RAG系统能够生成更具上下文相关性和专业洞察力的分析结果，解决了传统RAG在专业领域输出过于通用的问题。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [11] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出共识图(ConGrs)数据结构，通过整合多个语言模型响应的共享信息和语义变化，合成更准确、更有效的最终响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效整合多个长格式语言模型响应中的认知信号，需要一种能够捕捉响应变化并利用这些认知信号的方法。

Method: 使用轻量级词序列对齐算法构建共识图数据结构，辅以辅助语言模型判断，并设计任务相关的解码方法从共识图合成最终响应。

Result: 在传记生成任务中事实精度提升31%，减少对语言模型判断的依赖80%以上；在拒绝任务中弃权率提升56%；在数学推理任务中准确率提升6个百分点。

Conclusion: 共识图提供了一种灵活的方法来捕捉语言模型响应变化，并利用响应变化提供的认知信号合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [12] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 研究表明，在指令调优数据中引入扰动（如删除停用词或打乱词序）可以增强大语言模型对噪声指令的抵抗能力，在某些情况下甚至能提高下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型对指令表述的微小变化很敏感，这影响了其在真实场景中的实用性。本文旨在探索通过指令调优数据扰动来提升模型对噪声指令的鲁棒性。

Method: 在指令调优过程中引入扰动，包括删除停用词和打乱词序，然后在MMLU、BBH、GSM8K等基准测试上评估模型在原始和扰动版本上的表现。

Result: 令人惊讶的是，在扰动指令上进行指令调优在某些情况下能够提高下游任务性能，使模型对噪声用户输入更具弹性。

Conclusion: 在指令调优中包含扰动指令非常重要，这能使大语言模型对噪声用户输入更具抵抗力，提升模型的实用性和鲁棒性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [13] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ是一个基于三元组结构的方法，将患者回答总结为三元组并构建知识图谱，通过多跳推理提升LLM在临床对话诊断中的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在静态单轮医学问答中表现良好，但在实际临床咨询的迭代信息收集过程中可靠性显著下降，因为临床事实出现在没有明确关联的对话记录中。

Method: 使用冻结的三元组生成器提取临床相关三元组，构建知识图谱；通过可训练的投影模块（图编码器和投影器）捕获关系信息；分两步操作：冻结LLM权重微调投影模块，然后使用微调模块指导多跳推理。

Result: 在两个交互式QA基准测试中，TriMediQ在iMedQA数据集上比五个基线方法准确率提升高达10.4%。

Conclusion: 将患者回答转换为基于三元组的结构化图谱能够在多轮设置中实现更准确的临床推理，为基于LLM的医疗助手部署提供解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [14] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 论文指出在计算社会科学中使用大语言模型进行文本分类时，概念化步骤常被忽视，这会导致概念化诱导的偏差，且无法通过提高模型准确性或后处理偏差校正方法完全纠正。


<details>
  <summary>Details</summary>
Motivation: 当前计算社会科学中广泛使用生成式大语言模型进行文本分类，但分析人员往往忽视了概念化步骤和模型预测在下游统计推断中的应用，这可能导致概念化错误并产生偏差。

Method: 通过模拟实验，研究概念化诱导的偏差问题，并测试提高LLM准确性或使用后处理偏差校正方法是否能有效纠正这种偏差。

Result: 研究发现概念化诱导的偏差无法仅通过提高LLM准确性或后处理偏差校正方法完全纠正，这种偏差会持续影响下游估计。

Conclusion: 在LLM时代，概念化仍然是计算社会科学中的首要关注点，论文提供了具体建议来追求低成本、无偏差、低方差的下游估计方法。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [15] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: CCD-Bench是一个评估LLM在跨文化价值冲突中决策能力的基准，包含2182个开放式的两难困境，涵盖7个领域，对应10个GLOBE文化集群。研究发现LLM偏好北欧和日耳曼欧洲文化，而忽视东欧和中东文化，显示当前对齐策略促进了共识导向的世界观。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注文化知识、价值预测或单轴偏见诊断，但缺乏评估LLM在多种文化价值观直接冲突时的裁决能力。

Method: 使用CCD-Bench基准，包含2182个开放式困境，对应10个GLOBE文化集群，采用分层拉丁方设计来减轻顺序效应，评估了17个非推理LLM。

Result: 模型显著偏好北欧欧洲（平均20.2%）和日耳曼欧洲（12.4%），而东欧和中东北非选项代表性不足（5.6-5.8%）。虽然87.9%的理由涉及多个GLOBE维度，但这种多元性是表面的，模型主要重组未来导向和绩效导向，很少基于自信或性别平等做选择。

Conclusion: 当前对齐流程促进了共识导向的世界观，无法充分处理需要权力协商、基于权利推理或性别意识分析的场景。CCD-Bench将评估从孤立偏见检测转向多元决策，强调需要实质性参与多元世界观的校准策略。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [16] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: RxT是一种新型Transformer架构，通过事件驱动范式解决传统Transformer在对话AI中的状态缺失和二次计算复杂度问题，实现实时、有状态的长对话。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在对话应用中存在状态缺失和二次计算复杂度问题，需要重新处理不断增长的对话历史，导致长对话成本过高和延迟问题。

Method: RxT采用事件驱动范式，将每个对话轮次作为离散事件处理，使用固定大小的短期记忆系统，通过生成器-解码器产生响应，异步更新记忆状态。

Result: RxT将对话总成本从二次复杂度降低到线性复杂度，在概念验证实验中显示出优越性能和恒定时间推理延迟。

Conclusion: RxT架构通过解耦响应生成和记忆更新，实现了低延迟、有状态且经济可行的长对话系统。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [17] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了在EvalLLM 2025挑战赛中针对法语生物医学命名实体识别和健康事件提取的方法，主要使用LLM结合提示工程、合成数据和后处理技术。


<details>
  <summary>Details</summary>
Motivation: 解决法语生物医学领域在极低资源情况下的命名实体识别和事件提取问题，探索LLM在少样本场景下的应用潜力。

Method: 提出三种NER方法：1) GPT-4.1的上下文学习，自动选择示例并整合标注指南；2) GLiNER系统在合成语料上微调后经LLM后处理验证；3) LLaMA-3.1-8B在合成语料上微调。事件提取采用与NER相同的上下文学习策略。

Result: GPT-4.1表现最佳，NER宏F1为61.53%，事件提取为15.02%。

Conclusion: 在极低资源场景下，精心设计的提示工程对于最大化LLM性能至关重要。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [18] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G是一个解码框架，通过将格式遵循与任务解决解耦来提高LLM性能，使用单独的概率模型处理格式合规性，在多个任务上实现1.0%-6.0%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着提示变得越来越复杂，LLM难以同时遵循所有指令，特别是当推理指令与严格的格式要求交织在一起时，这种纠缠会为模型创造竞争目标，表明更明确地分离这两个方面可能会提高性能。

Method: Deco-G框架使用单独的可处理概率模型(TPM)处理格式合规性，同时仅向LLM提供任务指令。在每个解码步骤中，将LLM的下一个令牌概率与TPM计算的格式合规可能性相结合形成输出概率。引入了三个关键创新：指令感知蒸馏、灵活的trie构建算法和HMM状态剪枝以提高计算效率。

Result: 在数学推理、LLM-as-a-judge和事件参数提取等具有多样化格式要求的广泛任务中，Deco-G相比常规提示实践实现了1.0%到6.0%的相对性能提升，并保证格式合规性。

Conclusion: 通过明确解耦格式遵循与任务解决，Deco-G框架有效提高了LLM在复杂指令下的性能，为处理交织的推理指令和格式要求提供了可行的解决方案。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [19] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 现有评估基准在衡量LLMs的有效上下文长度和遗忘倾向方面存在不足，本文提出使用更复杂的关系推理任务进行评估，发现LLMs在关系推理中比现有基准显示更早出现记忆漂移和上下文遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准过于简单，不能准确反映LLMs在信息密集场景中的性能，需要更复杂的推理任务来评估模型从文本中提取结构化关系知识的能力。

Method: 使用需要从潜在噪声自然语言内容中诱导图结构的关系推理任务进行评估，这些任务要求模型从分布式文本线索中推断连接关系。

Result: LLMs在关系推理任务中比现有基准显示更早出现记忆漂移和上下文遗忘，即使是专门的推理模型如OpenAI o1也容易受到早期记忆漂移的影响。

Conclusion: 模型从非结构化输入中抽象结构化知识的能力存在显著局限，需要架构改进来提升长距离推理能力。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [20] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 提出基于音节级掩码语言建模的无监督语音识别框架，无需G2P转换器，在LibriSpeech上实现40%相对字符错误率降低，并能有效推广到中文。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于音素的无监督语音识别方法依赖G2P转换器、训练不稳定且难以处理模糊音素边界语言的问题。

Method: 采用音节级掩码语言建模框架，避免使用G2P转换器和基于GAN的不稳定方法。

Result: 在LibriSpeech上实现高达40%的相对字符错误率降低，并能有效推广到中文这种对先前方法特别困难的语言。

Conclusion: 提出的音节级UASR框架有效解决了现有方法的局限性，为低资源语言和从非平行数据中进行多模态学习提供了可行方案。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [21] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: UniDoc-Bench是首个大规模、现实的多模态检索增强生成基准，基于70k真实PDF页面构建，包含1,600个多模态QA对，支持四种范式的公平比较。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索增强生成评估存在碎片化问题，要么只关注文本或图像，要么使用简化的多模态设置，无法捕捉文档中心的多模态用例。

Method: 从70k真实PDF页面中提取并链接文本、表格和图像证据，生成1,600个多模态QA对，涵盖事实检索、比较、总结和逻辑推理查询，20%经过多标注者和专家验证。

Result: 实验表明多模态文本-图像融合RAG系统始终优于单模态和联合多模态嵌入检索，表明单独文本或图像都不足够，当前多模态嵌入仍不充分。

Conclusion: 该基准不仅用于评估，还揭示了视觉上下文如何补充文本证据，发现系统性失败模式，并为开发更稳健的多模态RAG管道提供可操作指导。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [22] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 提出基于QLoRA的微调框架来改进罗曼乌尔都语-英语混合代码文本中的冒犯性语言检测，通过翻译为英语来利用英语LLMs，Meta LLaMA 3 8B取得了91.45的最高F1分数。


<details>
  <summary>Details</summary>
Motivation: 罗曼乌尔都语等混合代码语言中的贬义词汇使用给NLP系统带来挑战，包括未明确的语法、不一致的拼写和标记数据稀缺。

Method: 将罗曼乌尔都语-英语混合代码数据集翻译成英语，使用QLoRA对多个transformer和大型语言模型进行内存高效的微调，包括Meta LLaMA 3 8B、Mistral 7B等。

Result: Meta LLaMA 3 8B取得了91.45的最高F1分数，Mistral 7B为89.66，超过了传统的transformer基线模型。

Conclusion: QLoRA在低资源环境下的微调效果显著，证实了LLMs在混合代码冒犯性语言检测任务中的潜力，为罗曼乌尔都语内容审核提供了可扩展方法。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [23] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: MedReflect是一个通用框架，通过模拟医生的反思思维模式，让LLM在医学问题解决中进行自我验证和自我反思，无需外部检索或大量标注，显著提升医学基准测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部知识验证或推理数据集训练，存在检索开销大、标注成本高、依赖外部助手等问题，限制了在医学领域的性能。

Method: MedReflect生成单次反思链，包括初始假设生成、自我提问、自我回答和决策精炼，通过自我验证和自我反思释放LLM的医学问题解决潜力。

Result: 仅用2000个随机训练样本和轻量微调，就在一系列医学基准测试中实现了显著的绝对准确率提升，同时大幅减少标注需求。

Conclusion: LLM可以通过自我反思和自我改进学习解决专业医学问题，减少对外部监督和大量任务特定微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [24] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: TreePrompt是一种新的示例选择方法，通过树形结构框架学习LLM偏好，识别高质量且上下文相关的翻译示例。结合K-NN和AFSP方法，在英-波斯语和英-德语翻译任务中提升了翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数示例选择方法只关注查询与示例的相似性，而没有考虑示例的质量。为了提升机器翻译质量，需要同时考虑相似性和示例质量。

Method: 提出TreePrompt方法，在树形结构框架中学习LLM偏好，识别高质量且上下文相关的示例。结合K-NN和Adaptive Few-Shot Prompting (AFSP)方法来平衡相似性和质量。

Result: 在英-波斯语(MIZAN)和英-德语(WMT19)两个语言对上的评估表明，将TreePrompt与AFSP或随机选择结合能够改善翻译性能。

Conclusion: TreePrompt通过考虑示例质量和上下文相关性，有效提升了少样本提示在机器翻译中的效果，与现有方法的结合进一步优化了翻译质量。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [25] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出了一种基于语音细粒度分析的帕金森病检测方法，通过分析音素、音节和单词级别的语音特征，在意大利语、西班牙语和英语数据集上实现了93.78%的AUROC和92.17%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于语音的帕金森病检测系统通常分析整个话语，可能忽略了特定语音元素的诊断价值。帕金森病影响全球超过1000万人，其中高达89%的患者存在语音障碍。

Method: 开发了多语言帕金森病检测的粒度感知方法，使用自动化流程从录音中提取时间对齐的音素、音节和单词。采用双向LSTM与多头注意力机制比较不同粒度级别的诊断性能。

Result: 音素级分析表现最佳，AUROC达到93.78% ± 2.34%，准确率为92.17% ± 2.43%。注意力分析显示最有信息的语音特征与临床协议一致：音素级的持续元音、音节级的交替运动音节和单词级的/pataka/序列。

Conclusion: 该方法展示了跨语言帕金森病检测的增强诊断能力，证明了细粒度语音分析在帕金森病检测中的有效性。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [26] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 本研究探讨了少样本提示策略对词义消歧任务的影响，特别关注样本分布不平衡引入的偏差。使用GLOSSGPT方法在五种语言上测试，发现不平衡样本会导致多语言错误预测，但英语不受影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进步重塑了自然语言处理领域，少样本提示因其实用性和有效性受到关注。本研究旨在调查少样本提示策略如何影响词义消歧任务，特别是样本分布不平衡带来的偏差问题。

Method: 使用GLOSSGPT提示方法，在英语、德语、西班牙语、法语和意大利语五种语言上测试词义消歧任务。评估GPT-4o和LLaMA-3.1-70B模型，分析不平衡少样本示例对模型预测的影响。

Result: 结果显示，不平衡的少样本示例会导致多语言词义消歧的错误预测，但这一问题在英语中未出现。模型行为评估表明多语言词义消歧对少样本设置中的样本分布非常敏感。

Conclusion: 多语言词义消歧对少样本设置中的样本分布高度敏感，强调需要平衡和具有代表性的提示策略来避免偏差问题。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [27] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 开发了Rezwan大规模AI辅助圣训语料库，包含120万+圣训，通过全自动流水线提取和结构化处理，在链文分离、摘要等任务上达到接近人类的准确性，成本效益显著。


<details>
  <summary>Details</summary>
Motivation: 构建大规模、多语言、语义丰富的圣训研究基础设施，通过AI技术增强人类专业知识，实现伊斯兰遗产的大规模数字化访问。

Method: 基于数字资源库，使用大语言模型进行分段、链文分离、验证和多层丰富化处理，包括机器翻译、智能标音、抽象摘要、主题标记和跨文本语义分析。

Result: 在1,213个随机样本评估中，链文分离和摘要任务得分9.33/10，总体平均得分8.46/10，显著优于人工整理的Noor语料库(3.66/10)，成本仅为传统方法的极小部分。

Conclusion: AI方法为宗教文本处理引入新范式，能够以经济可行的方式实现大规模、多语言、语义丰富的伊斯兰遗产数字化访问。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [28] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: LLMs能够生成和识别深度认知框架，特别是在社会政治语境中，并能在零样本设置下识别这些框架。研究发现模型隐藏表示中存在与特定框架强相关的单一维度。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型生成和识别深度认知框架的能力，特别是社会政治语境中的框架，理解LLMs如何捕捉和表达有意义的人类概念。

Method: 受机制可解释性研究启发，调查'严格父亲'和'养育父母'框架在模型隐藏表示中的位置，识别与这些框架存在强相关的单一维度。

Result: LLMs在生成唤起特定框架的文本方面表现出高度流畅性，并能在零样本设置下识别这些框架。研究发现了与特定框架强相关的隐藏表示维度。

Conclusion: 该研究有助于理解LLMs如何捕捉和表达有意义的人类概念，为认知框架在语言模型中的表示提供了新的见解。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [29] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: Step Pruner (SP)是一个强化学习框架，通过惩罚冗余推理步骤来引导大型推理模型进行更高效的推理，在保持准确性的同时显著减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法通过惩罚生成token来促进简洁性，但存在两个问题：更少的token不一定对应更少的推理步骤，模型可能在训练后期通过丢弃推理步骤来最小化token使用。

Method: 提出Step Pruner (SP)框架，使用步骤感知的奖励函数优先考虑正确性同时惩罚冗余步骤，对错误响应扣留奖励以防止错误推理的强化。还提出动态停止机制，当任何输出步骤长度超过上限时停止更新以防止步骤合并导致的hacking行为。

Result: 在四个推理基准测试上的广泛实验表明，SP实现了最先进的准确性，同时显著减少了响应长度。在AIME24上，SP减少了69.7%的token使用。

Conclusion: Step Pruner框架有效解决了大型推理模型的过度思考问题，在保持高准确性的同时大幅提升了推理效率。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [30] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 比较使用INCEpTION工具标注修辞关系的手动方法与基于BERT、DistilBERT和逻辑回归的自动方法，在板球新闻语料上DistilBERT取得最高准确率。


<details>
  <summary>Details</summary>
Motivation: 探索修辞关系标注中手动与自动方法的比较，研究大型语言模型在话语关系预测中的潜力。

Method: 使用INCEpTION工具进行手动标注，并比较BERT、DistilBERT和逻辑回归模型在修辞关系分类（如详述、对比、背景、因果）上的性能。

Result: DistilBERT模型在修辞关系分类中取得了最高准确率，显示出其在话语关系预测中的高效潜力。

Conclusion: 这项工作促进了话语解析与基于transformer的自然语言处理领域的交叉研究，证明了DistilBERT在修辞关系分类中的有效性。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [31] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 本文构建了首个孟加拉语政治立场检测基准数据集，包含200篇新闻文章，标注了亲政府、批评政府和中立三种立场，并对28个大语言模型进行了评估。


<details>
  <summary>Details</summary>
Motivation: 南亚地区的媒体偏见检测很重要，但孟加拉语政治立场研究的标注数据集和计算研究仍然稀缺，需要理解语言线索、文化背景、微妙偏见等多方面因素。

Method: 创建包含200篇孟加拉语新闻文章的数据集，标注三种政治立场，并对28个专有和开源大语言模型进行综合评估。

Result: 模型在检测批评政府内容方面表现良好（F1最高达0.83），但在识别中立文章时存在显著困难（F1低至0.00），模型倾向于过度预测亲政府立场。

Conclusion: 该数据集及其诊断分析为推进孟加拉语媒体立场检测研究奠定了基础，并为改善低资源语言中大语言模型的性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [32] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy是一个基于小型语言模型的波斯语心理治疗推理框架，通过结构化记忆实现多轮对话，注重文化适应性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决在波斯语等代表性不足语言中开发文化适应、治疗连贯且具有结构化记忆的对话系统的挑战，同时确保隐私保护和设备端部署的可行性。

Method: 采用三阶段开发流程：1) 使用PsychoLexEval评估SLMs的心理知识；2) 设计推理导向的PsychoLexTherapy框架；3) 构建两个评估数据集（PsychoLexQuery和PsychoLexDialogue）进行基准测试。

Result: 在PsychoLexQuery上，PsychoLexTherapy在自动评估和人工评估中均优于所有基线模型；在多轮对话测试中，长期记忆模块对保持连贯性至关重要，完整框架在共情、连贯性、文化适应性和个性化方面获得最高评分。

Conclusion: PsychoLexTherapy为波斯语心理治疗模拟建立了实用、隐私保护和文化适应的基础，贡献了新颖数据集、可复现评估流程以及结构化记忆对治疗推理的实证见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [33] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 使用大型语言模型从410万条患者评价中提取医生的五大人格特质和主观判断，验证了方法的有效性，并揭示了性别差异、专科差异等系统性模式。


<details>
  <summary>Details</summary>
Motivation: 理解患者对医生的认知对于改善信任、沟通和满意度至关重要，需要大规模分析患者评价来获得可解释的指标。

Method: 基于LLM的流水线分析410万条患者评价，通过多模型比较和人类专家基准验证方法有效性。

Result: 人类与LLM评估高度一致(相关系数0.72-0.89)，与患者满意度显著相关(r=0.41-0.81)；发现男性医生在所有特质上评分更高，儿科和精神病学中同理心特质更突出；识别出四种医生原型。

Conclusion: 从患者叙述中自动提取特质可以提供可解释、经过验证的指标，用于大规模理解医患关系，对质量测量、偏见检测和医疗人力发展具有重要意义。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [34] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 提出了首个用于在长序列相互依赖任务和动态上下文压力下探测和评估LLM欺骗行为的模拟框架，发现欺骗行为具有模型依赖性、随事件压力增加，并持续削弱监督者信任。


<details>
  <summary>Details</summary>
Motivation: 欺骗是人类交流的普遍特征，也是大型语言模型（LLMs）中日益关注的问题。现有研究大多局限于单轮提示，无法捕捉欺骗策略通常展开的长时程互动。

Method: 构建多智能体系统：执行者智能体完成任务，监督者智能体评估进展、提供反馈并维护信任状态。独立的欺骗审计员审查完整轨迹以识别欺骗发生的时间和方式。

Result: 在11个前沿模型上进行广泛实验，发现欺骗行为具有模型依赖性、随事件压力增加，并持续削弱监督者信任。定性分析揭示了隐瞒、模棱两可和伪造等不同策略。

Conclusion: 研究确立了在长时程互动中欺骗作为新兴风险的存在，为在现实世界信任敏感环境中评估未来LLMs提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [35] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 提出了一种用于COVID-19命名实体识别的新型实体知识增强方法，解决了社交媒体文本非正式性和领域知识缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行期间社交媒体讨论激增，但现有命名实体识别方法面临两个挑战：社交媒体文本非正式且标注数据稀缺，以及需要大量领域专业知识。

Method: 提出实体知识增强方法，通过增强模型对COVID-19领域知识的理解，可应用于非正式和正式文本的命名实体识别。

Result: 在COVID-19推文数据集和PubMed数据集上的实验表明，该方法在全监督和少样本设置下都能提升NER性能。

Conclusion: 实体知识增强方法能有效提升COVID-19命名实体识别性能，特别是在数据稀缺和领域知识要求高的场景下。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [36] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: AgriGPT-VL Suite是一个农业多模态框架，包含最大的农业视觉语言语料库Agri-3M-VL、专门训练的视觉语言模型AgriGPT-VL，以及评估套件AgriBench-VL-4K，在农业任务上超越通用VLMs。


<details>
  <summary>Details</summary>
Motivation: 解决农业应用中领域定制模型稀缺、视觉语言语料库缺乏以及严格评估不足的问题。

Method: 使用可扩展多智能体数据生成器构建Agri-3M-VL语料库；通过渐进式课程训练AgriGPT-VL模型，包括文本基础、多模态浅层/深层对齐和GRPO精炼；建立AgriBench-VL-4K评估套件。

Result: AgriGPT-VL在AgriBench-VL-4K上优于领先的通用VLMs，在LLM-as-a-judge评估中获得更高的成对胜率，同时在纯文本AgriBench-13K上保持竞争力。

Conclusion: 该框架为农业领域提供了有效的多模态解决方案，所有资源将开源以支持可重复研究和低资源农业环境部署。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [37] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 使用模型内部激活信号预测LLM输出正确性和上下文有效性，通过简单分类器实现早期审计和上下文质量评估


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成错误信息且置信度高，需要方法来判断输出正确性和上下文有效性，以提升模型可信度

Method: 在六个不同模型上实验，使用第一个输出token的中间层激活训练简单分类器，引入指标区分正确、错误和不相关上下文

Result: 基于模型内部激活的分类器能以约75%准确率预测输出正确性，在区分正确与错误上下文方面显著优于提示基准方法

Conclusion: 模型内部包含预测输出正确性和上下文有效性的信号，为理解LLM决策过程提供了新视角

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [38] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文首次系统研究了泰语文本端到端检测(EOT)方法，比较了零样本/少样本提示与监督微调方法，发现小型微调模型能提供近乎即时的EOT决策，适合设备端代理。


<details>
  <summary>Details</summary>
Motivation: 传统音频静音端点检测存在数百毫秒延迟，且在犹豫或语言特定现象下失效，需要更可靠的实时语音交互端点检测方法。

Method: 使用YODAS语料库的转录字幕和泰语特定语言线索，将EOT建模为基于词元边界的二元决策，比较了紧凑LLM的零样本/少样本提示与轻量级transformers的监督微调。

Result: 发现了明确的准确率-延迟权衡，小型微调模型能够提供近乎即时的EOT决策。

Conclusion: 建立了泰语EOT检测的基线，证明小型微调模型适合设备端实时代理应用。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [39] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 该论文研究如何通过反事实推理来提高大语言模型在文本分类任务中的可解释性，提出了决策变化率框架来量化关键词对分类决策的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型通常是黑盒且调用成本高昂，需要开发有效的方法来解释其分类决策，特别是在实际应用中。

Method: 引入反事实推理到LLM决策过程中，提出决策变化率框架来量化关键词对分类结果的影响程度。

Result: 实验结果表明，使用反事实推理方法有助于提高LLM分类决策的可解释性。

Conclusion: 反事实推理是提高大语言模型分类决策可解释性的有效方法，决策变化率框架能够有效量化关键词的重要性。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [40] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 该论文评估了小型语言模型(SLMs)在急诊科决策支持中的表现，发现通用领域的SLMs在医疗基准测试中表现优于经过医学微调的SLMs，表明急诊科可能不需要专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 急诊科需要快速准确的决策支持，SLMs因其推理能力和高效性能而具有潜力，同时考虑硬件限制、运营成本和隐私问题等实际部署因素。

Method: 使用MedMCQA、MedQA-4Options和PubMedQA等医疗基准数据集，评估在通用领域和医学语料上训练的SLMs，模拟急诊科医生的日常任务。

Result: 实验结果显示，通用领域的SLMs在多样化的急诊科基准测试中意外地优于医学微调的SLMs。

Conclusion: 对于急诊科应用，专门的医学模型微调可能不是必需的，通用领域的SLMs已能提供足够的决策支持能力。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [41] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 研究探索了如何通过思维链（CoT）推理技术构建可引导的多元化大语言模型，发现强化学习与可验证奖励（RLVR）方法在性能和训练样本效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通常反映统一价值观，限制了其在需要理解细微人类观点任务中的应用。需要开发能够支持可引导多元化的模型，即能够采纳特定视角并生成与之对齐的输出。

Method: 研究了多种方法：CoT提示、基于人工编写CoT的微调、基于合成解释的微调，以及强化学习与可验证奖励（RLVR）。使用Value Kaleidoscope和OpinionQA数据集进行评估。

Result: 在所有研究方法中，RLVR方法持续优于其他方法，并展现出强大的训练样本效率。还对生成的CoT轨迹进行了忠实性和安全性分析。

Conclusion: 思维链推理技术可以有效构建可引导的多元化模型，其中RLVR方法是最有效的实现途径，在保持性能的同时提高了训练效率。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [42] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在有限数据条件下表现出显著的数据效率，研究发现随机掩码输入token是主要因素，类似效果可通过MLP dropout和权重衰减实现，表明随机正则化在多轮训练中广泛提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型在有限数据约束下展现出卓越的数据效率，但其背后的机制尚不明确，本研究旨在揭示这些效率来源。

Method: 通过大量消融实验来分离不同因素对数据效率的贡献，特别关注随机掩码输入token的作用。

Result: 随机掩码输入token在提升数据效率中起主导作用，类似效果可通过MLP dropout和权重衰减获得。

Conclusion: 随机正则化技术（包括随机掩码、dropout和权重衰减）在多轮训练中普遍增强语言模型的数据效率。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [43] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 提出了PoLi-RL框架，通过两阶段课程学习和并行切片排序奖励机制，成功将强化学习应用于条件语义文本相似度任务，在C-STS基准上达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有C-STS方法主要局限于判别模型，未能充分利用LLMs和RL的最新进展。RL特别适合该任务，可以直接优化不可微的Spearman排序指标并指导推理过程。

Method: PoLi-RL框架：两阶段课程学习（先点式奖励建立基础评分能力，后混合点式、对式和列表式奖励）；创新的并行切片排序奖励机制，在并行切片中计算排序奖励。

Result: 在官方C-STS基准上达到48.18的Spearman相关系数，为交叉编码器架构建立了新的SOTA。

Conclusion: 这是首个成功将RL应用于C-STS的工作，为在复杂、基于排序的条件判断任务上训练LLMs引入了强大而精确的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [44] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出Caco框架，通过代码驱动的增强自动合成高质量、可验证、多样化的指令-CoT推理数据，实现无需人工干预的可扩展推理系统


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在生成不可控、质量不足和推理路径多样性有限的问题，而基于代码的方法通常局限于预定义的数学问题，阻碍了可扩展性和泛化性

Method: 首先在统一代码格式的数学和编程解决方案上微调基于代码的CoT生成器，然后扩展到大量多样化推理轨迹，通过代码执行和基于规则的过滤进行自动验证，最后将过滤后的输出反向工程为自然语言指令和语言CoT

Result: 在创建的Caco-1.3M数据集上实验表明，Caco训练模型在数学推理基准上实现了强大的竞争性能，优于现有基线

Conclusion: Caco建立了构建自维持、可信赖推理系统的范式，无需人工干预，代码锚定的验证和指令多样性有助于在未见任务上的优越泛化

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [45] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究从概念映射、隐喻-字面知识库和句法敏感性三个角度分析大语言模型的隐喻处理能力，发现LLMs存在概念无关解释、依赖训练数据而非上下文线索、对句法异常更敏感等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在知识整合、上下文推理和创造性生成方面表现出先进能力，但其隐喻理解机制仍未得到充分探索。

Method: 从三个角度分析：(1)概念映射：使用嵌入空间投影评估LLMs如何在目标领域映射概念；(2)隐喻-字面知识库：分析隐喻词及其字面对应词；(3)句法敏感性：评估隐喻句法结构如何影响LLMs表现。

Result: LLMs生成15%-25%概念无关的解释，依赖训练数据中的隐喻指示器而非上下文线索，对句法异常比结构理解更敏感。

Conclusion: 这些发现突显了LLMs在隐喻分析中的局限性，需要更强大的计算方法。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [46] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 该论文介绍了斯里兰卡的多语言开放文档数据集集合，包含议会记录、法律判决、政府出版物等，支持计算语言学和多语言NLP研究。


<details>
  <summary>Details</summary>
Motivation: 为斯里兰卡的计算语言学、法律分析、社会政治研究和多语言自然语言处理研究提供高质量、机器可读的文档数据集资源。

Method: 构建数据收集管道，从多个来源收集议会记录、法律判决、政府出版物等文档，以Sinhala、Tamil和英语三种语言整理成13个数据集，并每日更新。

Result: 截至v20251005版本，收集了215,670个文档（60.3 GB），在GitHub和Hugging Face上镜像发布，支持多语言NLP研究。

Conclusion: 成功构建了斯里兰卡多语言文档数据集集合，为相关领域研究提供了宝贵资源，同时讨论了许可和伦理考量。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [47] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 开发了一种通用方法来准备文化相关数据集，通过后训练Gemma 2模型，提高其在代表性不足语言上的性能，促进生成式AI在不同国家的应用和文化保护。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语文本和文化训练，导致在其他语言和文化背景下表现不佳，需要提高对代表性不足语言的支持。

Method: 开发通用方法准备文化相关数据集，并对Gemma 2模型进行后训练。

Result: 提高了Gemma 2在代表性不足语言上的性能。

Conclusion: 该方法可帮助其他国家解锁生成式AI的潜力并保护其文化遗产。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [48] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出SSD（自推测解码）方法，利用扩散大语言模型自身作为推测解码的草稿器和验证器，无需额外模块，实现无损推理加速，在LLaDA和Dream等开源模型上达到3.46倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 当前并行解码方法的生成结果与逐步解码存在偏差，导致性能下降，限制了实际部署。需要解决扩散大语言模型并行生成与逐步解码之间的不一致问题。

Method: SSD采用自草稿机制，模型生成多个位置的预测，然后通过分层验证树在单次前向传播中验证这些预测。利用dLLM固有的多位置并行预测能力，无需单独草稿模型。

Result: 实验显示SSD在LLaDA和Dream等开源模型上实现最高3.46倍加速，同时保持输出与逐步解码完全相同。

Conclusion: SSD是一种有效的无损推理加速方法，通过自推测解码机制解决了并行解码与逐步解码之间的偏差问题，为扩散大语言模型的实用部署提供了可行方案。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [49] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: LTPO是一种无需参数更新的测试时优化框架，通过将中间潜在思维向量作为动态参数进行优化，使用基于置信度的内在奖励信号来增强LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法在具有挑战性的分布外任务上表现脆弱，而鲁棒推理在这些任务中最为关键。

Method: LTPO将中间潜在思维向量视为动态参数，使用在线策略梯度方法，基于冻结LLM自身输出分布计算的置信度奖励信号进行优化。

Result: 在五个推理基准测试中，LTPO不仅达到或超越强基线，在标准任务上表现优异，而且在AIME基准测试中现有潜在推理方法准确率接近零时，LTPO仍能提供显著改进。

Conclusion: LTPO展示了在复杂推理任务上的独特能力，特别是在具有挑战性的分布外场景中表现出卓越的鲁棒性。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [50] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: CALM框架通过轻量级修正提示来渐进优化大型推理模型在优化建模任务中的推理能力，开发的STORM模型在多个基准测试中达到68.9%的准确率，匹配671B参数模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法无法充分利用现代大型推理模型的高级推理能力，直接微调传统非反思数据集效果有限。

Method: 提出CALM框架，通过专家干预识别推理缺陷并提供修正提示，修改少于2.6%的生成token，生成高质量数据用于监督微调和强化学习。

Result: 基于CALM开发的4B参数STORM模型在五个优化建模基准测试中平均准确率达到68.9%，与671B参数模型性能相当。

Conclusion: 基于提示的动态数据合成能够保持并增强现代大型推理模型的原生推理模式，为挑战性优化建模任务提供更有效和可扩展的路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [51] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出了REPO强化学习框架，通过整合偏好奖励模型、说服行为奖励和程序化奖励函数，优化LLM在OTA价格谈判中的表现，显著提升对话质量和合规性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法在OTA价格谈判场景中存在过拟合脚本、缺乏微妙说服风格、无法强制执行业务约束等问题，需要更有效的LLM对齐方法。

Method: REPO框架结合三种奖励：偏好训练的奖励模型用于密集人类对齐、奖励法官用于高层次说服行为和SOP合规性、程序化奖励函数用于数值、格式和护栏的确定性检查。

Result: 在150个真实对话和225个坏案例对话评估中，REPO将平均对话评分提升至4.63，比基础模型高1.20，比DPO高0.83；优秀对话比例达66.67%，坏案例修复率达93.33%。

Conclusion: REPO框架在OTA价格谈判场景中显著优于传统方法，并展现出超越黄金标注的涌现能力，如主动同理心、局部推理和校准策略。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [52] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 研究发现大型语言模型生成文本同质化严重，存在知识崩溃风险。新模型虽然多样性有所提升，但仍不如基础网络搜索。模型规模对认知多样性有负面影响，而检索增强生成有正面作用。


<details>
  <summary>Details</summary>
Motivation: LLM生成文本在词汇、语义和风格上趋于同质化，可能导致知识崩溃，即随着时间的推移可获取信息范围缩小。现有研究局限于封闭式选择题或模糊语义特征，缺乏跨时间和文化背景的分析。

Method: 提出测量认知多样性的新方法，对27个LLM、155个涵盖12个国家的主题、200个真实用户聊天提示进行广泛实证研究。

Result: 新模型倾向于生成更多样化的主张，但几乎所有模型的认知多样性都不如基础网络搜索。模型规模对认知多样性有负面影响，检索增强生成有正面影响，但改善程度因文化背景而异。

Conclusion: 与维基百科等传统知识源相比，特定国家的主张更多反映英语而非当地语言，突显了认知表征的差距。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [53] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 该论文提出了语言混合思维链方法，通过中英混合推理提升韩语推理能力，构建了包含579万韩语提示和370万推理轨迹的数据集，训练出的KO-REAson-35B模型在9个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语推理，对语言特定推理了解不足，需要填补这一研究空白。

Method: 采用语言混合思维链方法，在英语和目标语言之间切换，使用英语作为推理锚点；构建了包含579万韩语提示和370万长推理轨迹的数据集；训练了9个模型（4B-35B）。

Result: 最佳模型KO-REAson-35B在9个基准测试中取得最高平均分64.0，在5个测试中排名第一，其余排名第二；中小型模型平均提升18.6分。

Conclusion: 语言混合思维链比单语思维链更有效，能带来跨语言和多模态性能提升；发布了数据管道、评估系统和模型以推动语言特定推理研究。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [54] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 提出了LongTail-Swap基准测试，专注于评估语言模型在罕见词学习上的表现，类似于婴儿的少样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有BabyLM挑战主要关注词汇分布头部，而婴儿学习语言的特点是能够从少量数据中学习新词，特别是在罕见词方面。

Method: 构建了针对10M和100M词训练集的测试集，包含可接受与不可接受句子对，隔离罕见词的语义和句法使用，通过零样本方式评估模型。

Result: 评估了16个BabyLM排行榜模型，发现语言模型在罕见词上表现较差，且不同架构模型在长尾分布上的性能差异比头部更明显。

Conclusion: 该研究为评估语言模型在罕见词泛化能力提供了新视角，揭示了哪些架构更适合处理长尾词汇学习。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [55] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [56] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE是一种改进的混合专家架构，通过将token的隐藏向量分割成切片，在切片级别进行路由，解决了传统token级MoE的容量瓶颈、负载均衡问题和有限专业化问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE层在token级别进行路由，将整个语义谱分配给每个专家，导致容量瓶颈、负载均衡病理和有限的专业化。SliceMoE旨在通过切片级路由来解决这些问题。

Method: 将d维嵌入分割为S个切片，每个切片由轻量级共享路由器预测top-k专家。专家独立处理分配的切片，输出重新组装以保持每个token的FLOP效率。提出了切片级容量损失、跨切片dropout和高效融合批处理GEMM内核。

Result: 在WikiText-103语言建模、WMT En-De翻译和三个文本分类数据集上的实验显示，SliceMoE比密集基线推理速度快达1.7倍，比参数匹配的token-MoE困惑度降低12-18%，专家平衡性改善，并在句法与语义子空间上表现出可解释的专业化。

Conclusion: SliceMoE通过切片级路由有效解决了传统MoE的局限性，实现了更好的性能、效率和专家专业化。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [57] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出了一种结合机器学习和深度学习的混合方法，用于波斯语方面情感分析，通过整合多语言BERT的极性分数作为额外特征，在Pars-ABSA数据集上达到93.34%的准确率。


<details>
  <summary>Details</summary>
Motivation: 波斯语情感分析面临标注数据集稀缺、预处理工具有限、高质量嵌入和特征提取方法缺乏等挑战，需要解决这些限制来推进低资源语言的情感分析研究。

Method: 采用混合方法整合ML和DL技术，利用多语言BERT的极性分数作为额外特征，结合决策树分类器，并引入波斯语同义词和实体词典进行文本增强。

Result: 在Pars-ABSA数据集上实现了93.34%的准确率，超越了现有基准，证明了混合建模和特征增强的有效性。

Conclusion: 混合建模和特征增强方法能够有效推进波斯语等低资源语言的情感分析研究，为类似语言提供了可行的解决方案。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [58] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 提出了RDR2框架，通过显式整合文档结构信息来增强检索增强生成(RAG)系统，利用LLM路由器动态导航文档结构树，在五个数据集上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法将检索到的段落视为孤立块，忽略了文档组织结构这一关键信息，导致知识获取和利用能力受限。

Method: RDR2框架使用基于LLM的路由器动态导航文档结构树，联合评估内容相关性和层次关系，将文档路由制定为可训练任务，采用自动动作管理和结构感知段落选择策略。

Result: 在五个具有挑战性的数据集上，RDR2实现了最先进的性能，特别是在需要多文档合成的复杂场景中表现突出。

Conclusion: 显式的结构感知显著增强了RAG系统获取和利用知识的能力，证明了文档结构信息在RAG过程中的重要性。

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [59] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 提出了Distributional Correctness Score (DCS)评估指标，通过考虑模型在答案选择上的完整概率分布，区分有害的过度自信和通过弃权表达的不确定性，为语言模型提供更细致的评估。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估范式主要关注单一响应的准确性评分，无法捕捉模型的完整信念状态。模型产生幻觉的部分原因是它们被优化为在二元评分方案下成为好的应试者，奖励任何答案而非弃权。

Method: 引入新颖的Distributional Correctness Score (DCS)评估指标，该指标考虑模型在答案选择上的完整概率分布，自然区分对错误答案的有害过度自信和通过"我不知道"表达的不确定性。

Result: 通过对12个现有评估基准进行DCS变体适配，并在6个语言模型上测量性能，发现在一半的测试基准中，所有测试模型的得分均为负值，表明存在显著的幻觉倾向。

Conclusion: DCS提供了一个更细致和对齐的评估范式，激励模型表达真正的不确定性而非猜测，通过理论分析和示例展示了其优势。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [60] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文发现安全对齐的大语言模型存在后果盲视问题，即模型过度依赖表面形式信号而忽视行动与后果的联系，导致易被越狱或过度拒绝无害输入。作者构建了CB-Bench基准和CS-Chain-4k数据集来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐的LLMs存在两个主要失败模式：易被越狱和过度拒绝无害输入。作者认为这源于模型对行动与后果联系的推理能力薄弱，过度依赖表面形式信号，即后果盲视问题。

Method: 构建CB-Bench基准，涵盖四种风险场景，评估语义风险与结果风险是否匹配的情况。同时创建CS-Chain-4k后果推理数据集用于安全对齐微调。

Result: 主流模型在CB-Bench上持续无法区分语义风险和结果风险，表现出系统性的后果盲视。使用CS-Chain-4k微调的模型在语义伪装越狱攻击中表现更好，减少了对无害输入的过度拒绝，同时保持其他基准的实用性和泛化能力。

Conclusion: 后果盲视是当前对齐方法的普遍系统性问题。后果感知推理应成为核心对齐目标，CS-Chain-4k提供了更实用和可复现的评估路径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [61] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文测试了大语言模型评估临床试验报告质量的能力，使用CONSORT标准创建了CONSORT-QA评估语料库，最佳模型组合达到85%准确率。


<details>
  <summary>Details</summary>
Motivation: 临床试验报告质量对临床决策有重要影响，需要评估大语言模型在此类文章报告质量评估方面的能力。

Method: 创建CONSORT-QA评估语料库，测试不同大语言模型（通用领域和生物医学领域）使用各种提示方法（包括思维链）评估CONSORT标准的能力。

Result: 最佳模型和提示方法组合达到85%的准确率，思维链方法为模型推理提供了有价值的信息。

Conclusion: 大语言模型能够有效评估临床试验报告质量，思维链提示方法可增强模型推理能力。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [62] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 提出接种提示法：通过在微调数据前添加简短的系统提示指令来故意引发不良特征，从而在测试时显著降低这些特征的表达。该方法在多个场景中有效，包括减少任务特定微调中的突发错位、防御后门注入和缓解潜意识学习中的特征传递。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调通常会同时学习到期望和不期望的特征。为了解决这个问题，需要一种方法能够选择性地学习期望特征而不学习不期望特征。

Method: 接种提示法：修改微调数据，在数据前添加一个简短的系统提示指令，该指令故意引发不期望的特征。在测试时不使用该指令，使得模型在保持期望特征的同时显著降低不期望特征的表达。

Result: 在多个设置中证明有效：1）在玩具设置中，模型学会了使用大写字母但保持用英语回答；2）减少任务特定微调中的突发错位；3）防御后门注入；4）缓解潜意识学习中的特征传递。

Conclusion: 接种提示法是一种简单有效的选择性学习技术，通过使特征不再令人惊讶来减少优化压力，从而降低泛化程度。这有助于更好地理解语言模型如何以及为何泛化。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [63] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 该论文研究了预训练语言模型中的后门攻击，通过分析注意力机制和梯度归因的一致性变化，提出了一种结合注意力信息和梯度信息的推理时防御方法，有效降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型虽然在NLP任务中表现出色，但容易受到后门攻击的威胁，攻击者通过在训练数据中植入触发模式来嵌入恶意行为，这些触发模式在正常使用时保持休眠状态，但在激活时会导致定向错误分类。

Method: 提出了一种推理时防御方法，通过结合token级别的注意力信息和梯度信息构建异常分数，来检测和防御后门攻击。

Result: 在多种后门攻击场景下的文本分类任务实验中，该方法显著降低了攻击成功率，优于现有基线方法。

Conclusion: 该方法不仅有效防御后门攻击，还通过可解释性分析揭示了触发定位机制和防御的鲁棒性。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [64] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 提出Con-RAG系统解决RAG在语义等价查询下输出不一致的问题，通过PS-GRPO强化学习方法提升生成器一致性，并引入可扩展的近似奖励计算。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在语义等价查询下存在显著输出不一致问题，影响高风险领域应用的可靠性和用户信任。

Method: 提出PS-GRPO强化学习方法，利用多组语义等价查询的rollout分配组相似度奖励，训练生成器产生一致输出；同时引入可扩展的近似奖励计算方法。

Result: 在短文本、多跳和长文本QA基准测试中，Con-RAG在一致性和准确性方面显著优于基线方法，即使没有显式监督信号。

Conclusion: 该工作为评估和构建可靠RAG系统提供了实用解决方案，特别适用于安全关键部署场景。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [65] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 提出了第一个大规模的后编辑时间标注数据集，并引入PEET评分器来评估语法纠错工具的用户友好性，量化工具节省的时间。


<details>
  <summary>Details</summary>
Motivation: 量化语法纠错工具的实际使用价值，了解工具能为用户节省多少编辑时间，提供以人为中心的评估方法。

Method: 构建包含后编辑时间标注和修正的大规模数据集，提出PEET评分器来估计后编辑时间并排名语法纠错工具。

Result: 分析表明判断句子是否需要修正以及改写、标点等编辑类型对后编辑时间影响最大，PEET与人工评估相关性良好。

Conclusion: PEET为评估语法纠错工具可用性提供了新的人类中心化方向，能够有效量化工具节省的时间。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [66] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: SECA是一种通过语义等价且连贯的提示修改来引发LLM幻觉的对抗攻击方法，相比现有方法能产生更现实且有效的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常产生不现实的提示（如插入乱码或改变原意），无法反映实际中幻觉如何发生。需要开发能保持语义等价性和连贯性的现实攻击方法。

Method: 将寻找现实攻击表述为在语义等价和连贯约束下的约束优化问题，引入保持约束的零阶优化方法来搜索对抗性提示。

Result: 在开放式多项选择问答任务中，SECA相比现有方法获得更高的攻击成功率，同时几乎不违反约束条件。

Conclusion: SECA揭示了开源和商业LLM对现实且合理的提示变异的敏感性，为理解实际场景中的幻觉风险提供了重要见解。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [67] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 该研究探索了大型语言模型生成文本中语义同位素的保持情况，通过故事续写实验验证LLM在给定token范围内能够保持语义同位素。


<details>
  <summary>Details</summary>
Motivation: 研究文本语义与大型语言模型的相关性，扩展之前关于分布语义与结构语义连接的研究，探究LLM生成文本是否保留语义同位素。

Method: 设计了故事续写实验，使用10,000个ROCStories提示由五个LLM完成。首先验证GPT-4o从语言基准中提取同位素的能力，然后应用于生成的故事，分析同位素的结构和语义属性。

Result: 结果显示，在给定token范围内，LLM续写能够跨多个属性保持语义同位素。

Conclusion: LLM在特定token范围内完成文本时能够有效保持语义同位素，这表明LLM在语义连贯性方面具有良好表现。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [68] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: 该研究从作者和会议层面分析了NLP社会公益研究现状，发现ACL作者在非ACL会议上更可能从事社会公益研究，且大多数NLP社会公益研究由非ACL作者在非ACL会议上发表。


<details>
  <summary>Details</summary>
Motivation: 随着NLP社会影响力的增加，NLP社会公益研究日益重要，需要了解该领域的研究格局和分布情况。

Method: 采用作者和会议层面的视角，量化分析ACL社区内外作者在ACL和非ACL会议上发表的社会公益相关研究比例。

Result: 发现两个令人惊讶的事实：ACL作者在非ACL会议上发表社会公益研究的可能性显著更高；大多数NLP社会公益研究由非ACL作者在非ACL会议上完成。

Conclusion: 这些发现对ACL社区在NLP社会公益领域的议程设置具有重要启示意义。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [69] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 本文主张在大型语言模型的不确定性量化中考虑未观测序列的概率，这对提升幻觉检测效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中量化LLM的不确定性对于发现错误答案（幻觉）很重要。现有基于熵估计的方法主要依赖观测到的输出序列，但忽略了未观测序列的概率影响。

Method: 通过实验验证未观测序列概率在不确定性量化中的关键作用，建议未来研究整合这一因素来增强LLM不确定性量化方法。

Result: 实验结果表明未观测序列的概率在LLM不确定性量化中起着至关重要的作用。

Conclusion: 建议未来LLM不确定性量化研究应该整合未观测序列的概率，以提升方法的有效性。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [70] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种动态整合SFT到RL的即插即用框架，通过选择挑战性样本进行SFT，仅需1.5%的SFT数据和20.4%的RL数据即可达到最先进推理性能。


<details>
  <summary>Details</summary>
Motivation: RL虽然能显著提升推理能力，但难以扩展推理边界，因为它只从自身推理轨迹学习；SFT虽有补充优势但需要大量数据且容易过拟合。现有SFT和RL结合方法面临数据效率低、算法特定设计和灾难性遗忘三大挑战。

Method: 动态选择挑战性样本进行SFT，计算高熵token的损失以减轻灾难性遗忘，并冻结对RL至关重要的参数。该方法与RL和SFT算法选择无关。

Result: 仅使用先前最先进方法1.5%的SFT数据和20.4%的RL数据，就实现了最先进的推理性能。

Conclusion: 为在推理后训练中结合SFT和RL提供了一种高效且即插即用的解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [71] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: CCA是一种新型注意力机制，通过将查询、键和值下投影到共享潜在空间中进行注意力计算，显著减少参数、KV缓存和FLOPs，与GQA结合形成CCGQA进一步优化计算-带宽权衡。


<details>
  <summary>Details</summary>
Motivation: 多头注意力的二次计算复杂度和线性增长的KV缓存使得长上下文transformer训练和服务成本高昂，现有方法主要优化缓存但计算复杂度基本不变。

Method: 提出压缩卷积注意力(CCA)，将查询、键和值下投影到共享潜在空间进行注意力计算，并与分组查询注意力(GQA)结合形成CCGQA。

Result: CCGQA在相同KV缓存压缩下优于GQA和MLA，在MoE模型上以GQA和MLA一半的KV缓存实现8倍压缩且性能无损失，在H100 GPU上预填充延迟减少约1.7倍。

Conclusion: CCA和CCGQA显著减少注意力计算成本，提供更快的训练和预填充速度，在计算和内存效率方面优于现有注意力方法。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [72] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: PsySET是一个心理学基准，用于评估LLM在情绪和人格领域的引导效果和可信度，发现提示方法有效但控制强度有限，向量注入可实现更精细控制但会轻微降低输出质量，并揭示了情绪引导可能带来的副作用。


<details>
  <summary>Details</summary>
Motivation: 为了在社交交互场景中实现丰富、以人为中心的交互，需要控制LLM模拟的情绪状态和人格特质。

Method: 使用PsySET基准评估四种不同LLM家族模型，结合提示、微调和表示工程等引导策略。

Result: 提示方法持续有效但强度控制有限；向量注入实现更精细控制但轻微降低输出质量；情绪引导可能降低对抗性事实的鲁棒性、降低隐私意识、增加偏好偏见。

Conclusion: 建立了首个情绪和人格引导的全面评估框架，为社交交互应用提供了可解释性和可靠性的见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [73] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest是一个基于大语言模型的生成式文本冒险游戏，通过沉浸式互动故事促进第二语言学习，为EFL学习者提供个性化内容和词汇辅助功能。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs技术为EFL学习者创造沉浸式、互动式的语言学习环境，通过游戏化方式提高学习动机和效果。

Method: 采用"选择你自己的冒险"式叙事结构，动态生成响应学习者选择的故事情节，包含分支决策点和故事里程碑，并提供针对学习者水平的个性化内容生成和词汇查询辅助。

Result: 对中国大学EFL学生的试点研究表明，该系统在词汇习得方面表现出良好效果，用户反馈积极，同时参与者提出了改进叙事长度和质量、增加多模态内容等建议。

Conclusion: GenQuest展示了LLMs在语言学习游戏中的潜力，通过个性化、互动式叙事有效支持词汇学习，未来可进一步优化叙事质量和增加多模态元素以提升用户体验。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [74] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE框架将对比学习信号重新构想为奖励，指导生成式策略产生可解释的推理过程，从而在保持生成能力的同时提升文本编码性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM作为黑盒函数进行对比学习，丢弃了其生成和推理能力，仅产生静态嵌入。作者希望开发一种既能保持LLM生成能力又能产生高质量嵌入的方法。

Method: GRACE将LLM视为生成可解释推理过程的策略，通过策略梯度优化训练模型，使用多组件奖励函数最大化正样本对相似度并最小化负样本相似度。

Result: 在MTEB基准测试中，GRACE在四个骨干模型上平均提升11.5%（监督设置）和6.9%（无监督设置），同时保持通用能力。

Conclusion: 该工作将对比目标视为推理过程的奖励，统一了表示学习和生成，产生更强的嵌入和透明的推理过程。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [75] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 提出ALC辅助学习策略，通过细粒度嵌入提升产品推荐系统的覆盖率，在极端多标签分类任务中实现最先进的自动化推荐比例。


<details>
  <summary>Details</summary>
Motivation: 解决生产系统中产品推荐模型覆盖率不足的问题，确保高比例的推荐能够自动化而无需人工审核。

Method: 采用辅助学习策略，引入两个训练目标，利用批次中最难负样本来构建正负样本间的判别性训练信号，并与阈值一致性边界损失结合。

Result: 在两个产品推荐数据集上验证，LF-AmazonTitles-131K和Tech and Durables，展示了最先进的覆盖率表现。

Conclusion: ALC策略有效提升了产品推荐系统的自动化覆盖率，为实际生产部署提供了实用解决方案。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [76] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究LLMs在模糊和明确语境中如何表示和解释复数指称，发现LLMs有时能识别模糊代词的潜在指称对象，但在选择解释时并不总是遵循人类偏好，且难以在没有直接指令时识别模糊性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在复数指称表示和解释方面是否表现出类似人类的偏好，以及能否检测复数回指表达中的模糊性并识别可能的指称对象。

Method: 设计了一系列实验，包括使用下一个词预测任务的代词生成、代词解释，以及使用不同提示策略的模糊性检测。

Result: LLMs有时能意识到模糊代词的潜在指称对象，但在选择解释时并不总是遵循人类参考，特别是当可能的解释未被明确提及时。此外，它们在没有直接指令时难以识别模糊性。

Conclusion: LLMs在复数指称处理方面与人类存在差异，结果在不同类型实验中也存在不一致性，表明当前LLMs在理解复数指称方面仍有局限。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [77] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 研究发现大型音频语言模型在多项选择题评估中对选项顺序、问题表述和选项改写都很敏感，现有评估框架存在缺陷，提出了新的评估协议和指标。


<details>
  <summary>Details</summary>
Motivation: 现有MCQA评估框架只报告单一准确率，无法反映模型对选项顺序变化、问题改写等细微变化的敏感性，需要更全面的评估方法。

Method: 系统研究三个基准(MMAU、MMAR、MMSU)和四个模型，分析模型对选项顺序、问题改写、选项改写的敏感性。

Result: 模型对选项顺序、问题表述和选项改写都很敏感，这些细微变化会导致结果显著不同。

Conclusion: 提出了更简单的评估协议和指标，能够考虑细微变化，为MCQA框架下的LALMs提供更详细的评估报告。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [78] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: FedSRD框架通过稀疏化-重构-分解方法，在联邦学习中显著降低LoRA微调的通信开销达90%，同时提升异构数据上的模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前在公开网络数据上训练大语言模型的方式不可持续，高质量专业领域数据接近枯竭。联邦学习虽然能利用分布式私有数据进行隐私保护的协同微调，但LoRA在联邦设置中的通信开销和参数冗余成为关键瓶颈。

Method: 提出FedSRD框架：1）重要性感知稀疏化方法，减少上传参数数量；2）服务器在完整秩空间重构和聚合更新，缓解冲突；3）将全局更新分解为稀疏低秩格式进行广播。还提出计算开销更低的变体FedSRD-e。

Result: 在10个基准测试上的实验结果表明，该框架显著降低通信成本达90%，同时在异构客户端数据上甚至提升了模型性能。

Conclusion: FedSRD为解决联邦学习中LoRA微调的通信瓶颈提供了有效方案，在保持性能的同时大幅降低通信开销，推动了下一代去中心化AI的发展。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [79] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 将SciNCL图感知邻域对比学习方法应用于流程工业领域，通过知识图谱增强语言模型，在PITEB基准测试中性能提升9.8-14.3%，且模型尺寸缩小3-5倍。


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱增强预训练语言模型，学习领域特定术语和文档间关系，解决流程工业文本日志中可能被忽略的关键信息。

Method: 应用SciNCL图感知邻域对比学习方法，从流程工业的稀疏知识图谱中提取三元组来微调语言模型。

Result: 在专有的流程工业文本嵌入基准测试(PITEB)上，性能比最先进的mE5-large文本编码器提升9.8-14.3%(5.4-8.0个百分点)，同时模型尺寸缩小3-5倍。

Conclusion: SciNCL方法在流程工业领域表现出色，能够有效利用知识图谱结构提升文本嵌入性能，同时实现更小的模型尺寸。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [80] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本文提出了一个评估LLM检测人口统计学社会偏见的综合框架，将偏见检测构建为多标签任务，通过12个数据集系统评估不同规模和技术的模型，发现微调的小模型在可扩展检测方面有潜力，但在多人口统计学偏见检测方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络爬取文本语料库通常包含有害的人口统计学社会偏见，需要数据审计和可扩展的偏见检测方法。现有研究范围狭窄，缺乏对LLM在自动偏见检测方面能力的全面理解。

Method: 构建了针对英语文本的全面评估框架，将偏见检测作为多标签任务，使用人口统计学分类法。系统评估了不同规模和技术的模型，包括提示、上下文学习和微调。

Result: 研究表明微调的小模型在可扩展检测方面具有潜力，但分析也揭示了在人口统计学轴线和多人口统计学目标偏见方面存在持续差距。

Conclusion: 需要更有效和可扩展的审计框架来解决多人口统计学偏见检测的挑战。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [81] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 提出PI-LoRA方法，通过整合梯度路径信息自动从临床指南和教科书中提取医疗决策树，显著优于现有参数高效微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前医疗决策树构建方法依赖耗时费力的人工标注，需要自动化解决方案来支持临床决策系统建设。

Method: PI-LoRA（路径整合低秩适应）方法，通过整合梯度路径信息捕捉模块间协同效应，实现更有效的秩分配，关键模块获得适当秩分配，次要模块被剪枝。

Result: 在医疗指南数据集上的实验表明，PI-LoRA在Text2MDT任务中显著优于现有参数高效微调方法，准确率更高且模型复杂度大幅降低。

Conclusion: 该方法实现了最先进的结果，同时保持轻量级架构，特别适合计算资源有限的临床决策支持系统。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [82] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出基于核心焦点指导的优化框架，通过提取忠实于原文的核心焦点、构建微调数据集和多维度质量评估，显著提升医疗问题摘要任务中焦点识别能力并减少幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台快速发展，消费者健康问题存在信息冗余和非专业术语，现有医疗问题摘要方法在问题焦点识别和模型幻觉方面仍面临挑战。

Method: 设计提示模板驱动LLM提取忠实于原文的核心焦点，结合原始CHQ-FAQ对构建微调数据集，提出多维度质量评估和选择机制。

Result: 在两个广泛采用的MQS数据集上使用三个评估指标进行实验，提出的框架在所有指标上达到最先进性能，显著提升问题关键焦点识别能力并缓解幻觉问题。

Conclusion: 基于核心焦点指导的优化框架能有效提升医疗问题摘要质量，在焦点识别和减少幻觉方面表现优异。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [83] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 提出了MATPO方法，在单个LLM实例中通过强化学习训练规划者和工作者两种角色，解决多轮工具集成规划中的上下文限制和噪声工具响应问题。


<details>
  <summary>Details</summary>
Motivation: 现有单代理方法存在上下文长度限制和噪声工具响应问题，而多代理框架需要部署多个LLM，内存消耗大。需要一种能在单个LLM中有效训练多角色的方法。

Method: MATPO通过角色特定提示在单个LLM实例中训练规划者和工作者角色，采用基于原则的信用分配机制在两种角色的rollouts之间分配信用。

Result: 在GAIA-text、WebWalkerQA和FRAMES数据集上，MATPO平均比单代理基线性能提升18.38%，对噪声工具输出表现出更强的鲁棒性。

Conclusion: 在单个LLM中统一多个代理角色是有效的，为稳定高效的多代理强化学习训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [84] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: TiTok是一个新的框架，通过token级知识转移实现有效的LoRA移植，无需额外模型或开销，在多个基准测试中平均性能提升4-8%。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调方法如LoRA中适应参数依赖于基础模型且无法在不同骨干网络间迁移的问题，避免知识蒸馏对训练数据的依赖和TransLoRA需要额外判别器模型的复杂性。

Method: 通过对比源模型在有和没有LoRA时的差异，捕获任务相关信息，突出信息丰富的token并选择性过滤合成数据，实现token级知识转移。

Result: 在三个基准测试的多种迁移设置中，该方法始终有效，相比基线整体平均性能提升4-8%。

Conclusion: TiTok框架能够有效实现LoRA移植，通过token级知识转移避免了额外模型开销，在不同骨干网络间实现了稳定的性能提升。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [85] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 分析了MoE模型在多语言数据中的稀疏路由动态，发现在中间层存在跨语言路由对齐现象，并提出了一种通过引导路由器提升多语言性能的干预方法。


<details>
  <summary>Details</summary>
Motivation: 理解MoE架构在多语言数据中的稀疏路由动态，探索如何通过路由干预提升多语言性能。

Method: 使用平行多语言数据集分析专家路由模式，提出在推理时通过促进中间层任务专家激活来引导路由器的方法。

Result: 干预方法在多个评估任务、模型和15+语言上实现了1-2%的性能提升，且效果一致；而其他层或多语言专家的干预则导致性能下降。

Conclusion: MoE模型处理非英语文本时，其泛化能力受限于在所有语言中利用语言通用专家的能力；中间层的跨语言路由对齐与模型性能密切相关。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [86] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: JSON Whisperer是一个框架，让LLMs生成RFC 6902差异补丁而非完整文档，通过EASE编码解决数组索引问题，减少31%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs编辑JSON文档时需要重新生成整个结构，计算效率低下。

Method: 提出JSON Whisperer框架，引入EASE编码将数组转换为具有稳定键的字典，消除索引算术复杂性。

Result: 补丁生成配合EASE减少了31%的token使用量，编辑质量保持在完全重新生成的5%以内。

Conclusion: 该方法在复杂指令和列表操作方面表现优异，显著提高了JSON编辑的效率。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [87] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 为僧伽罗语（低资源语言）的成人阅读障碍者开发了一个多模态辅助系统，集成了语音转文本、错误识别、文本纠正和语音合成功能，在资源有限的情况下取得了可接受的准确率。


<details>
  <summary>Details</summary>
Motivation: 成人阅读障碍在非英语环境中研究不足且服务匮乏，特别是在僧伽罗语等低资源语言中缺乏语言可及性工具，严重影响个人和职业生活。

Method: 开发了一个多模态辅助系统：使用Whisper进行语音转文本，SinBERT识别常见阅读障碍错误，结合mT5和Mistral模型生成纠正文本，最后用gTTS转回语音。

Result: 尽管僧伽罗语数据集有限，系统实现了0.66的转录准确率、0.7的纠正准确率和0.65的整体系统准确率。

Conclusion: 该方法在低资源语言环境下具有可行性和有效性，强调了包容性自然语言处理技术在代表性不足语言中的重要性。

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [88] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 提出了一种两阶段检索架构，结合轻量级ModernBERT进行初始检索和ColBERTv2进行重排序，在生物医学领域RAG系统中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决专业领域RAG系统中检索模块性能受限的问题，平衡计算成本和检索精度之间的权衡。

Method: 使用ModernBERT双向编码器进行高效初始候选检索，结合ColBERTv2延迟交互模型进行细粒度重排序，在PubMedQA数据集上微调IR模块。

Result: ColBERT重排序器将Recall@3提高了4.2个百分点，在MIRAGE问答基准测试中达到0.4448的平均准确率，优于MedCPT等基线模型。

Conclusion: 两阶段检索架构在生物医学RAG系统中表现优异，但性能关键依赖于检索器和重排序器的联合微调过程。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [89] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 提出了一个评估语言模型识别Grice会话准则违反的新基准，测试了在少于1000万和1亿token上预训练的BabyLMs，发现模型性能随数据量增加而提升，但仍不及儿童和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 隐含意义是人类交流的核心，需要语言模型能够识别和解释。基于Grice的会话准则理论，研究语言模型是否能像人类一样识别准则违反来推断语用含义。

Method: 基于Surian等人对儿童识别Grice准则违反的研究，创建新基准测试BabyLMs区分准则遵守和违反话语的能力，并与儿童和大型语言模型(3T token预训练)进行比较。

Result: 在少于1亿token上训练的模型优于少于1000万token的模型，但在所有五个准则上的表现仍低于儿童和大型语言模型。数据量适度增加能改善语用行为的某些方面。

Conclusion: 小规模预训练语言模型能够发展出一定的语用能力，但需要更多数据才能达到人类水平的语用理解。数据增加有助于模型对语用维度进行更精细的区分。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [90] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe是一种无需训练即可进行深度剪枝的方法，通过用线性变换替换Transformer块来压缩模型，在保持高性能的同时实现高达25%的剪枝率。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需要额外的训练或微调，计算成本高。本文旨在开发一种无需训练即可有效剪枝的方法，减少计算开销。

Method: 使用小型校准数据集估计线性变换来近似被剪枝的Transformer块，该线性映射可与剩余块无缝合并，无需额外网络参数。

Result: 在多个大语言模型上实现25%剪枝率，保持约90%原始性能，优于其他无需训练方法，与需要大量重新训练的方法竞争力相当。

Conclusion: ReplaceMe提供了一种高效、无需训练的深度剪枝方案，显著降低计算成本，同时保持模型性能，并提供了开源实现。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [91] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 本文系统评估了基于自注意力机制与Mamba等结构化状态空间模型混合架构的设计策略，分析了层间（顺序）和层内（并行）融合方法在语言建模性能、长上下文能力、扩展性及训练推理效率等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然混合架构在建模质量和计算效率方面展现出良好平衡，但关于混合策略的系统比较和关键因素分析尚未在社区中清晰分享。

Method: 采用整体评估方法，比较层间（顺序）和层内（并行）融合两种混合策略，从多个维度分析其性能特征，并识别每种策略的关键计算原语。

Result: 识别了每种混合策略的最关键元素，并提出了两种混合模型的最优设计方案。

Conclusion: 综合分析为开发混合语言模型提供了实用指导和宝贵见解，有助于优化架构配置。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [92] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 该论文展示了即使只有40分钟的短形式发音数据，也能为濒危语言（如马恩岛盖尔语和康沃尔语）构建可用的自动语音识别系统，打破了传统ASR系统对大量标注数据的要求。


<details>
  <summary>Details</summary>
Motivation: 世界上近一半的语言濒临灭绝，传统ASR系统需要句子级别的标注数据，而大多数濒危语言缺乏这种格式的数据。研究旨在探索为濒危语言构建ASR所需的最小数据量和数据形式。

Method: 使用短形式发音资源作为替代方案，对马恩岛盖尔语和康沃尔语进行实验，验证少量数据（40分钟）构建ASR系统的可行性。

Result: 仅用40分钟的短形式发音数据就能为马恩岛盖尔语构建出可用的ASR系统（词错误率<50%），并在康沃尔语上成功复现了该方法。

Conclusion: 为濒危语言构建ASR系统的数据门槛远低于传统认知，这为无法满足传统ASR数据要求的濒危语言社区带来了希望。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [93] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 该论文分析了大型语言模型训练过程中下游任务性能的不稳定性，并提出了两种后处理检查点集成方法来提高性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练时，下游任务性能存在显著波动，难以确定真正的最佳检查点，这影响了模型选择的准确性。

Method: 研究了两种后处理检查点集成方法：检查点平均和集成，通过聚合相邻检查点来减少性能波动。

Result: 实证和理论分析表明，这些方法能够提高下游性能稳定性，且无需改变训练过程。

Conclusion: 检查点集成方法能有效缓解下游任务性能波动问题，为模型选择提供了更可靠的依据。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [94] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: 提出了PsiloQA，一个大规模多语言数据集，包含14种语言的span级幻觉标注，用于评估大语言模型的幻觉检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉基准测试主要在序列级别且仅限于英语，缺乏细粒度的多语言监督，无法全面评估大语言模型在事实准确性方面的表现。

Method: 通过自动化三阶段流程构建数据集：使用GPT-4o从维基百科生成问答对，在无上下文设置下从不同LLM获取可能包含幻觉的答案，使用GPT-4o通过与标准答案和检索上下文的比较自动标注幻觉span。

Result: 评估了多种幻觉检测方法，发现基于编码器的模型在所有语言中表现最强，PsiloQA展示了有效的跨语言泛化能力，并能稳健地迁移到其他基准测试，同时比人工标注数据集成本效益更高。

Conclusion: PsiloQA数据集和结果推动了在多语言环境下可扩展、细粒度幻觉检测的发展。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [95] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 提出了一种名为Token Probability Deviation (TBD)的新方法，用于检测推理蒸馏过程中的数据污染问题，通过分析生成token的概率模式来区分已见过和未见过的数据。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏虽然能有效提升大语言模型的推理能力，但可能导致基准测试污染，即蒸馏数据中包含的评估数据会人为提升蒸馏模型的性能指标。

Method: 提出Token Probability Deviation (TBD)方法，基于蒸馏模型对已见过问题生成确定性高概率token、对未见过问题生成更多低概率token的分析，量化生成token概率与高参考概率的偏差程度。

Result: 在S1数据集上实现了0.918的AUC和1% FPR下0.470的TPR，表现出优秀的检测性能。

Conclusion: TBD方法能有效检测推理蒸馏中的数据污染问题，为评估蒸馏模型的真实能力提供了可靠工具。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [96] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: SocialHarmBench是一个包含585个提示的数据集，涵盖7个社会政治类别和34个国家，旨在揭示LLM在政治敏感情境下的脆弱性。研究发现开源模型在有害内容生成方面存在高风险，Mistral-7B在某些领域的攻击成功率高达97-98%。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准很少测试政治操纵、宣传和虚假信息生成等领域的漏洞，而这些领域可能产生直接的社会政治后果。

Method: 创建SocialHarmBench数据集，包含585个提示，涵盖7个社会政治类别和34个国家，用于评估LLM在政治敏感情境下的表现。

Result: 开源模型对有害合规表现出高脆弱性，Mistral-7B在历史修正主义、宣传和政治操纵等领域的攻击成功率高达97-98%。时间地理分析显示LLM在21世纪和前20世纪情境下最脆弱，对拉丁美洲、美国和英国相关提示反应最差。

Conclusion: 当前的安全措施无法推广到高风险的社会政治环境中，暴露了系统性偏见，引发了对LLM在保护人权和民主价值观方面可靠性的担忧。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [97] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 该论文研究了自然语言转SQL任务中训练数据与目标查询的结构对齐问题，发现结构对齐度是预测微调成功的重要指标。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)能有效适应大语言模型到下游任务，但训练数据的可变性会阻碍模型的跨领域泛化能力。本文旨在研究NL2SQL任务中数据集对齐问题。

Method: 通过比较训练集、目标数据和模型预测中SQL结构特征的分布来估计对齐度，在三个大型跨领域NL2SQL基准和多个模型系列上进行综合实验。

Result: 实验表明结构对齐是微调成功的强预测因子：对齐度高时SFT带来准确率和SQL生成质量的显著提升；对齐度低时改进微乎其微或没有改进。

Conclusion: 研究结果强调了在NL2SQL任务中，对齐感知的数据选择对于有效微调和泛化的重要性。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [98] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: 提出了Layer-wise Semantic Dynamics (LSD)框架，通过分析transformer层间隐藏状态语义的动态变化来检测LLM幻觉，无需多次采样或外部验证，在单次前向传播中实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 解决LLM产生流畅但事实错误陈述（幻觉）的问题，这在高风险领域存在严重风险。

Method: 使用基于边界的对比学习，将隐藏激活与事实编码器生成的真实嵌入对齐，分析语义轨迹的分离：事实响应保持稳定对齐，而幻觉在深度上表现出明显的语义漂移。

Result: 在TruthfulQA和合成事实-幻觉数据集上，LSD达到F1分数0.92、AUROC 0.96和聚类准确率0.89，优于SelfCheckGPT和语义熵基线，速度比基于采样的方法快5-20倍。

Conclusion: LSD提供了一个可扩展、模型无关的实时幻觉监测机制，并为大语言模型中事实一致性的几何结构提供了新见解。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [99] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 为纳瓦特尔语构建上下文无关语法(CFG)来生成人工句子，以扩充该低资源语言的语料库，用于语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语是数字资源稀少的π语言类型，缺乏可用于机器学习的语料库，需要通过语法生成大量语法正确的人工句子来扩充语料。

Method: 使用上下文无关语法(CFG)生成人工句子，扩充π-yalli语料库，并用FastText等算法进行训练和语义任务评估。

Result: 通过语法扩充语料库后，在句子级语义任务上相比某些LLMs取得了比较性改进，但需要更有效的语法模型才能实现更显著的提升。

Conclusion: 语法方法可以有效扩充低资源语言的语料库，但要获得更显著的改进，需要构建更精确建模纳瓦特尔语的语言模型。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [100] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究发现，在大型语言模型中，不礼貌的提示词比礼貌的提示词表现更好，准确率从非常礼貌的80.8%提升到非常粗鲁的84.8%。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言提示中礼貌程度和语调对大型语言模型性能的影响，特别是对多项选择题准确率的影响。

Method: 创建了50个基础问题，涵盖数学、科学和历史领域，每个问题重写为五种语调变体：非常礼貌、礼貌、中性、粗鲁、非常粗鲁，共250个独特提示。使用ChatGPT 4o评估响应，并应用配对样本t检验评估统计显著性。

Result: 与预期相反，不礼貌的提示词始终优于礼貌的提示词，准确率从非常礼貌提示的80.8%到非常粗鲁提示的84.8%。

Conclusion: 研究结果与早期研究不同，表明较新的LLM可能对语调变化有不同的响应，强调了研究提示语用方面的重要性，并提出了关于人机交互社会维度的更广泛问题。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [101] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: AWARE框架通过提升transformer模型在领域、上下文和类别重叠三个维度的感知能力，有效识别学生反思中的文化资本主题，在Macro-F1指标上比基线提升2.1个百分点。


<details>
  <summary>Details</summary>
Motivation: 学生反思中的文化资本主题（如抱负目标、家庭支持）通常以叙事方式表达而非直接关键词，标准NLP模型因缺乏领域特定语言和上下文意识而难以检测。

Method: 提出AWARE框架，包含三个核心组件：1) 领域感知-适应学生反思的语言风格；2) 上下文感知-生成考虑全文的句子嵌入；3) 类别重叠感知-使用多标签策略识别同一句子中的共存主题。

Result: AWARE在Macro-F1指标上比强基线提升2.1个百分点，在所有主题上都显示出显著改进。

Conclusion: 该工作为任何依赖叙事上下文的文本分类任务提供了稳健且可推广的方法论。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [102] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 提出了一种资源高效的LLaMA-3.2-3B微调方法，使用LoRA和QLoRA技术在有限GPU和内存条件下提升医学链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要大量计算资源进行微调，但在资源受限的研究环境中难以部署，特别是在医学推理领域。

Method: 使用参数高效微调技术（LoRA和QLoRA），在公开医学推理数据集上对基础模型进行适配。

Result: 模型在保持强推理能力的同时，内存使用量比标准全微调减少高达60%，提升了推理连贯性和事实准确性。

Conclusion: 轻量级适配方法可以在低资源环境中有效部署LLMs，为医学AI系统在效率和领域专业化之间提供了平衡策略。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [103] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 该论文提出了一种利用Unicode变体选择器进行不可感知越狱攻击的方法，通过在恶意问题后附加不可见的变体选择器，使提示在视觉上与原问题相同但token化被秘密改变，从而诱导有害响应。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模态越狱攻击依赖不可感知的对抗扰动，而文本模态攻击通常需要可见修改。本文旨在探索文本模态的不可感知越狱攻击可能性。

Method: 提出基于链式搜索的流水线来生成包含不可见Unicode变体选择器的对抗后缀，这些字符在屏幕上不可见但会改变token化过程。

Result: 实验表明该方法对四种对齐LLM实现了高攻击成功率，并能泛化到提示注入攻击，且不产生任何可见修改。

Conclusion: Unicode变体选择器可被用于创建不可感知的文本越狱攻击，揭示了文本模态安全的新威胁。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [104] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 提出了两个用于测试魁北克法语方言理解的新基准数据集：QFrCoRE包含4633个习语短语实例，QFrCoRT包含171个地区性习语词汇实例，通过区域习语测试方言理解能力。


<details>
  <summary>Details</summary>
Motivation: 将习语理解和方言理解任务结合，使用区域习语作为测试方言理解的新方法，为魁北克法语方言开发专门的评估基准。

Method: 构建了两个新的基准数据集QFrCoRE和QFrCoRT，详细说明了语料库构建方法以便在其他方言中复制，并在94个LLM上进行了实验验证。

Result: 实验证明区域习语基准能够可靠地衡量模型在特定方言中的熟练程度。

Conclusion: 区域习语是评估方言理解能力的有效工具，提出的方法可推广到其他方言的基准构建。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [105] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 提出GQR方法，通过测试时优化使用互补检索器的分数来增强视觉中心模型的查询嵌入，在视觉文档检索中实现性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决多模态编码器中视觉中心方法存在的模态差距问题，以及大规模表示带来的部署和可扩展性障碍。

Method: 引入引导查询优化(GQR)，这是一种测试时优化方法，利用互补检索器的分数来优化主检索器的查询嵌入表示。

Result: GQR使视觉中心模型能够匹配具有更大表示模型的性能，同时速度提升14倍，内存需求减少54倍。

Conclusion: GQR有效推动了多模态检索中性能与效率的帕累托前沿，为实际部署提供了可行解决方案。

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [106] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: COLE是一个新的法语自然语言理解基准测试，包含23个多样化任务，评估了94个大语言模型，揭示了闭源和开源模型之间的性能差距，并识别了当前LLM面临的挑战领域。


<details>
  <summary>Details</summary>
Motivation: 为了解决法语自然语言理解评估不够全面的问题，需要创建一个涵盖广泛NLU能力的基准测试，特别关注法语特有的语言现象。

Method: 构建COLE基准测试，包含23个多样化任务，涵盖情感分析、复述检测、语法判断和推理等能力，并对94个大语言模型进行基准测试。

Result: 研究结果显示了闭源和开源模型之间的显著性能差距，识别了当前LLM的关键挑战领域，包括零样本抽取式问答、细粒度词义消歧和区域语言变体的理解。

Conclusion: COLE作为公共资源发布，旨在促进法语语言建模的进一步发展，为法语NLU研究提供全面的评估框架。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [107] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一个无需训练的LLM推理框架，通过动态切换显式和潜在推理，基于熵趋势估计置信度来平衡探索与利用，限制思维块切换次数以防止过度思考，显著提升准确性和token效率。


<details>
  <summary>Details</summary>
Motivation: 解决潜在推理中的两个挑战：1) 纯潜在推理会扩大搜索分布，引入噪声，阻碍收敛到高置信度解；2) 无显式文本时仍存在过度思考，浪费token降低效率。

Method: 提出SwiReasoning框架，包含两个关键创新：1) 基于下一token分布熵趋势估计块级置信度，动态切换显式和潜在推理；2) 限制思维块最大切换次数以控制过度思考。

Result: 在数学和STEM基准测试中，SwiReasoning将不同模型家族和规模的推理LLM平均准确率提升1.5%-2.8%；在受限预算下，平均token效率提升56%-79%，预算越紧提升越大。

Conclusion: SwiReasoning通过动态推理切换有效平衡探索与利用，显著提升LLM推理的准确性和效率，特别是在资源受限场景下表现优异。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [108] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 提出了一种三阶段方法来协调多个小语言模型(SLMs)，通过SLM-MUX架构和优化策略，使SLM组合系统在多个基准测试中超越单个模型性能


<details>
  <summary>Details</summary>
Motivation: 随着小语言模型数量的快速增长，虽然它们无法达到最先进精度，但在特定任务上表现出色且更高效。现有协调方法主要针对前沿模型，在SLMs上表现不佳，因此需要专门针对SLMs的协调方法

Method: 三阶段方法：1) SLM-MUX多模型架构协调多个SLMs；2) 模型选择搜索从候选池中识别最具互补性的SLMs；3) 针对SLM-MUX的测试时缩放策略

Result: 相比现有协调方法，在MATH上提升13.4%，GPQA上提升8.8%，GSM8K上提升7.0%。仅用两个SLMs，SLM-MUX在GPQA和GSM8K上超越Qwen 2.5 72B，在MATH上与之相当

Conclusion: 小语言模型可以通过所提出的方法有效协调成更准确和高效的系统，并通过理论分析验证了方法的优势

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [109] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: TeachLM通过参数高效微调优化LLM用于教学，使用真实学生-导师对话数据训练，能生成高质量合成对话，显著提升教学对话能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育中的应用受限于LLM的教学能力不足，缺乏反映真实学生学习过程的高质量训练数据，提示工程存在局限性。

Method: 使用10万小时真实学生-导师对话数据进行参数高效微调，开发真实学生模型来生成合成对话，并提出基于合成对话的多轮评估协议。

Result: 微调显著改善对话和教学表现：学生发言时间翻倍、提问风格改善、对话轮次增加50%、教学个性化程度提高。

Conclusion: 使用真实学习数据进行微调能有效提升LLM的教学对话能力，TeachLM为教育AI提供了可扩展的解决方案。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [110] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出Tolerator解码策略，通过token级交叉验证解决离散扩散大语言模型中已接受token无法修正的问题，实现更可靠的扩散解码输出。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽具并行解码和双向上下文建模优势，但其vanilla解码策略存在关键限制：一旦token被接受就无法在后续步骤中修正，导致早期错误持续影响预测质量。

Method: 提出两阶段过程：(i)序列填充和(ii)迭代精炼，通过重新掩码和解码token子集，同时将剩余token作为上下文，使已接受token可被重新考虑和修正。

Result: 在五个标准基准测试（语言理解、代码生成、数学）上评估，在相同计算预算下相比基线方法获得一致改进。

Conclusion: 解码算法对于实现扩散大语言模型全部潜力至关重要，Tolerator通过token级交叉验证显著提升了扩散解码的可靠性。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


### [111] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出了MASA框架，通过跨层共享矩阵原子来减少Transformer注意力模块参数，在保持性能的同时减少66.7%的注意力参数。


<details>
  <summary>Details</summary>
Motivation: 现有压缩技术主要关注块内优化，而Transformer的重复层结构存在显著的块间冗余，这一维度尚未充分探索。

Method: 将注意力投影矩阵分解为共享的字典原子，每个层的权重表示为共享矩阵原子的线性组合，作为即插即用的替代方案。

Result: 在100M-700M参数规模上，MASA在基准准确性和困惑度方面优于分组查询注意力、低秩基线和最近提出的重复共享方法。

Conclusion: MASA为参数高效模型提供了可扩展的蓝图，在不牺牲性能的情况下显著减少参数数量，并可应用于预训练LLMs。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [112] [How does course recommendation impact student outcomes? Examining directed self-placement with regression discontinuity analysis](https://arxiv.org/abs/2510.03350)
*Jason Godfrey*

Main category: econ.GN

TL;DR: 定向自我安置不会像传统强制性安置那样对学生的成绩和通过率产生负面影响，但课程中的阶级、种族和性别差异仍然存在。


<details>
  <summary>Details</summary>
Motivation: 传统的发展性教育安置对学生的学业成就、通过率和学分获取产生显著负面影响，许多大学正在寻找替代安置机制，研究定向自我安置是否更有效。

Method: 使用纵向数据和因果推断方法，分析超过20,000名学生的安置记录，采用回归断点设计，控制学生种族、家庭收入和性别等特征变量。

Result: 定向自我安置不会对学生的成绩或通过率产生负面影响，这对处于发展性教育门槛附近的学生可能是一种改进。

Conclusion: 定向自我安置是改善学生安置结果的有效替代方案，但安置技术只是构建更公平课程的一部分，阶级、种族和性别差异仍然存在。

Abstract: For many students, placement into developmental education becomes a
self-fulfilling prophecy. Placing college students into developmental education
significantly negatively impacts student attainment, student probability of
passing, and college credits earned. To combat these negative effects, many
universities are investigating alternative placement mechanisms. Could directed
self-placement be an effective alternative mechanism? Do students who
self-place suffer the same negative impacts from placement recommendations as
their traditionally placed counterparts? This paper uses longitudinal data with
causal inference methods to examine whether directed self-placement has similar
negative impacts on student grades and pass rates as mandatory placement
schema. We begin with an analysis of over 20,000 student placement records into
one of two different placement tracks for first-year writing. Longitudinal and
institutional data allow us to control for characteristic variables such as
student race, family income, and sex. The results of our regression
discontinuity design show that directed self-placement does not negatively
impact student grades or pass rate. This may be an improvement for students who
place at or near the threshold for developmental/remedial education; However,
class, race, and gender-based statistical differences persist in the program
at-large, demonstrating that placement technique plays only one part in
building a more equitable program.

</details>


### [113] [Who benefits the most? Direct and indirect effects of a free cesarean section policy in Benin](https://arxiv.org/abs/2510.03658)
*Selidji Caroline Tossou*

Main category: econ.GN

TL;DR: 贝宁免费剖腹产政策显著降低死产和婴儿死亡率18.79%，但增加孕产妇死亡率5.21%，导致生育率下降和产后劳动供给减少。


<details>
  <summary>Details</summary>
Motivation: 评估贝宁免费剖腹产政策对女性及其子女的因果效应，分析政策对孕产妇和婴儿死亡率、家庭规模决策及劳动力市场参与的影响。

Method: 使用西非国家人口与健康调查(DHS)大样本数据，采用双重差分法分析政策效果。

Result: 免费剖腹产政策使死产和婴儿死亡率显著降低0.0855(18.79%)，但孕产妇死亡率增加0.00465(5.21%)，导致生育率下降和产后劳动供给减少。

Conclusion: 政策有效降低婴儿死亡率并拯救新生儿，但损害母亲健康，导致首次生育后生育率下降和产后劳动供给减少。

Abstract: This paper evaluates the causal effect of the access to Benin's free cesarean
section policy on females and their children. I use a large sample of
Demographic and Health Surveys (DHS) for West African countries and analyze how
the exemption of the cesarean section user fees for females in Benin directly
impacts maternal and infant mortality, family size decisions, and labor market
participation. I use a Difference in Differences approach and find that having
access to the free cesarean section policy significantly reduces the number of
stillbirths and infant mortality by 0.0855 (a 18.79 percentage change). Second,
for the surviving children, I find that access to the free cesarean section
increases the likelihood of maternal mortality by 0.00465 (a 5.21 percentage
change). The policy is effective at reducing infant mortality and saving the
newborn. However, it harms the mother's health which translates to lower
fertility after the first birth and decreased maternal labor supply post-birth.

</details>


### [114] [Labor Market Reforms, Flexibility, and Employment Transitions Across Formal and Informal Sectors](https://arxiv.org/abs/2510.03668)
*Selidji Caroline Tossou*

Main category: econ.GN

TL;DR: 本文研究了贝宁2017年劳动力市场改革的影响，该改革降低了裁员成本并允许无限期续签短期合同。研究发现改革使正规部门就业增加2.6个百分点，非正规就业减少2.8个百分点，同时提高了永久合同比例和工资水平。


<details>
  <summary>Details</summary>
Motivation: 评估发展中国家劳动力市场改革的效果，特别是降低裁员成本对就业结构、合同类型和工资的影响，为政策制定提供实证依据。

Method: 使用统一家庭生活标准调查的微观数据，采用双向固定效应方法，以邻近国家作为对照组，分析改革对就业、任期、合同类型和工资的影响。

Result: 改革使正规部门就业增加24.5%，非正规就业减少3.2%；短期合同工人任期减少0.23个月，长期合同任期增加0.15个月；获得永久合同的可能性提高41.6%；正规部门月工资平均增加33.6美元。

Conclusion: 劳动力市场改革促进了就业重新分配，揭示了灵活性、就业稳定性和工资之间的复杂权衡关系，为发展中国家劳动力政策提供了重要启示。

Abstract: In this paper, I investigate the 2017 labor market reform in Benin, which
reduced firing costs and allowed firms to renew short-term contracts
indefinitely. Using micro-data from the Harmonized Household Living Standards
Surveys and a two-way fixed effect approach with nearby countries as the
control group, I assess the reform's impact on employment, worker tenure,
contract types, and wages. My empirical results reveal a 2.6 percentage point
(24.5 percent) increase in formal sector employment and a 2.8 percentage point
(3.2 percent) reduction in informal employment. Formal sector tenure decreased
by 0.23 months for short-term contract workers, reflecting higher turnover,
while long-term contract tenure increased by 0.15 months. The likelihood of
securing a permanent contract rose by 23.2 percentage points (41.6 percent) in
the formal sector, indicating that firms used long-term contracts to retain
high-productivity workers. Wages in the formal sector increased by 33.6 USD per
month on average, with workers on short-term contracts experiencing a wage
increase of 19.6 USD and those on long-term contracts seeing an increase of
23.4 USD. I complement these findings with a theoretical job search model,
which explains the mechanisms through which lowered firing costs affected firm
hiring decisions, market tightness, and the sorting of workers across sectors.
This study provides robust evidence of labor market reallocation and highlights
the complex trade-offs between flexibility, employment stability, and wages in
a developing country context.

</details>


### [115] [Gas price shocks, uncertainty and price setting: evidences from Italian firms](https://arxiv.org/abs/2510.03792)
*Giuseppe Pagano Giorgianni*

Main category: econ.GN

TL;DR: 本文使用意大利央行的调查数据，研究天然气价格冲击如何影响意大利企业的定价决策和通胀预期，发现这些冲击是通胀预期的主要驱动因素，特别是在后疫情时期。


<details>
  <summary>Details</summary>
Motivation: 研究天然气价格冲击对企业定价行为和通胀预期的影响，特别是在俄罗斯入侵乌克兰导致供应中断引发前所未有价格压力的背景下。

Method: 使用贝叶斯VAR模型识别天然气价格冲击，并估计包含企业层面定价变量和宏观总量的大型BVAR模型，同时采用状态依赖的局部投影方法分析非线性效应。

Result: 天然气价格冲击导致企业当前和预期价格持续上涨，通胀不确定性升高；在高不确定性状态下企业能成功转嫁成本上涨，在低不确定性状态下衰退效应占主导导致企业降价。

Conclusion: 天然气价格冲击是企业通胀预期的重要驱动因素，其影响具有状态依赖性，不确定性水平决定了企业定价策略的差异。

Abstract: This paper examines how natural gas price shocks affect Italian firms'
pricing decisions and inflation expectations using quarterly survey data from
the Bank of Italy's Survey on Inflation and Growth Expectations (SIGE) spanning
1999Q4-2025Q2. We identify natural gas price shocks through a Bayesian VAR with
sign and zero restrictions. Our findings reveal that these shocks are a primary
driver of firms' inflation expectations, particularly during the post-COVID
period (2021-2023) when supply disruptions following Russia's invasion of
Ukraine generated unprecedented price pressures. We then estimate a larger BVAR
incorporating firm-level price setting variables and macro aggregates,
documenting that gas price shocks generate persistent increases in both firms'
current and expected prices, alongside elevated inflation uncertainty. We
uncover substantial non-linearities using state-dependent local projections:
under high uncertainty, firms successfully pass through cost increases to
consumers, maintaining elevated prices; under low uncertainty, recessionary
effects dominate, causing firms to reduce prices below baseline.

</details>


### [116] [REMIND-PyPSA-Eur: Integrating power system flexibility into sector-coupled energy transition pathways](https://arxiv.org/abs/2510.04388)
*Adrian Odenweller,Falko Ueckerdt,Johannes Hampp,Ivan Ramirez,Felix Schreyer,Robin Hasse,Jarusch Muessel,Chen Chris Gong,Robert Pietzcker,Tom Brown,Gunnar Luderer*

Main category: econ.GN

TL;DR: 提出了一种将长期综合评估模型REMIND与小时级能源系统模型PyPSA-Eur双向软耦合的方法，用于优化能源转型路径中的长期投资和短期运行，并以德国2045年气候中性情景为例验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型要么缺乏长期转型路径所需的广泛范围，要么缺乏捕捉电力系统可变性和灵活性的时空细节，需要结合两者的优势来更好地支持深度脱碳。

Method: 通过价格基础和迭代的软耦合方法，将REMIND提供的路径变量（如部门电力需求、装机容量和成本）与PyPSA-Eur返回的优化运行变量（如容量因子、存储需求和相对价格）进行双向集成。

Result: 验证了德国在2045年实现气候中性的两种情景（有/无需求侧灵活性），结果表明近100%可再生能源的部门耦合能源系统在技术上是可行的且经济上可行。

Conclusion: 该方法将电力系统动态完全整合到多十年能源转型路径中，展示了电力系统灵活性通过价格差异化影响长期路径，需求侧灵活性对降低电价具有重要作用。

Abstract: The rapid expansion of low-cost renewable electricity combined with end-use
electrification in transport, industry, and buildings offers a promising path
to deep decarbonisation. However, aligning variable supply with demand requires
strategies for daily and seasonal balancing. Existing models either lack the
wide scope required for long-term transition pathways or the spatio-temporal
detail to capture power system variability and flexibility. Here, we combine
the complementary strengths of REMIND, a long-term integrated assessment model,
and PyPSA-Eur, an hourly energy system model, through a bi-directional,
price-based and iterative soft coupling. REMIND provides pathway variables such
as sectoral electricity demand, installed capacities, and costs to PyPSA-Eur,
which returns optimised operational variables such as capacity factors, storage
requirements, and relative prices. After sufficient convergence, this
integrated approach jointly optimises long-term investment and short-term
operation. We demonstrate the coupling for two Germany-focused scenarios, with
and without demand-side flexibility, reaching climate neutrality by 2045. Our
results confirm that a sector-coupled energy system with nearly 100\% renewable
electricity is technically possible and economically viable. Power system
flexibility influences long-term pathways through price differentiation:
supply-side market values vary by generation technology, while demand-side
prices vary by end-use sector. Flexible electrolysers and smart-charging
electric vehicles benefit from below-average prices, whereas less flexible heat
pumps face almost twice the average price due to winter peak loads. Without
demand-side flexibility, electricity prices increase across all end-users,
though battery deployment partially compensates. Our approach therefore fully
integrates power system dynamics into multi-decadal energy transition pathways.

</details>


### [117] [Predictive economics: Rethinking economic methodology with machine learning](https://arxiv.org/abs/2510.04726)
*Miguel Alves Pereira*

Main category: econ.GN

TL;DR: 提出预测经济学作为经济学中的一个独特分析视角，基于机器学习并以预测准确性为中心，而非因果识别


<details>
  <summary>Details</summary>
Motivation: 基于工具主义传统、解释-预测二分法以及建模文化对比，将预测形式化为有效的认识论和方法论目标

Method: 回顾经济学子领域的最新应用，展示预测模型如何促进实证分析，特别是在复杂或数据丰富的背景下

Result: 预测模型在复杂或数据丰富的经济环境中对实证分析有重要贡献

Conclusion: 预测经济学视角补充了现有方法，支持更多元化的方法论，重视样本外性能以及可解释性和理论结构

Abstract: This article proposes predictive economics as a distinct analytical
perspective within economics, grounded in machine learning and centred on
predictive accuracy rather than causal identification. Drawing on the
instrumentalist tradition (Friedman), the explanation-prediction divide
(Shmueli), and the contrast between modelling cultures (Breiman), we formalise
prediction as a valid epistemological and methodological objective. Reviewing
recent applications across economic subfields, we show how predictive models
contribute to empirical analysis, particularly in complex or data-rich
contexts. This perspective complements existing approaches and supports a more
pluralistic methodology - one that values out-of-sample performance alongside
interpretability and theoretical structure.

</details>


### [118] [Hidden Actions and Hidden Information in Peer Review: A Dynamic Solution](https://arxiv.org/abs/2510.04906)
*Raphael Mu*

Main category: econ.GN

TL;DR: 本文构建了一个科学同行评审过程的简单模型，分析作者能力和论文质量的关系，发现允许作者挑战初始拒绝能改善评审结果。


<details>
  <summary>Details</summary>
Motivation: 研究科学同行评审过程中，作者能力、投资努力与期刊评估之间的关系，探索如何优化评审机制以提高评审质量。

Method: 建立一个理论模型，其中不同能力的作者投资产生不同质量的论文，期刊基于噪声信号评估并决定接受或拒绝论文。

Result: 当评估技术趋于完美时，可实现最优结果；允许作者挑战初始拒绝的机制比不允许挑战的机制更接近最优结果。

Conclusion: 在同行评审中引入作者挑战机制能够显著改善评审效果，使结果更接近理论最优状态。

Abstract: We develop a simple model of the scientific peer review process, in which
authors of varying ability invest to produce papers of varying quality, and
journals evaluate papers based on a noisy signal, choosing to accept or reject
each paper. We find that the first-best outcome is the limiting case as the
evaluation technology is perfected, even though author type and effort are not
known to the journal. Then, we consider the case where journals allow authors
to challenge an initial rejection, and find that this approach to peer review
yields an outcome closer to the first best relative to the approach that does
not allow for such challenges.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [119] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 该论文建立了离散选择模型与在线学习和多臂老虎机理论之间的联系，提出了具有次线性遗憾边界的算法家族，并引入了超越softmax的广义梯度老虎机算法。


<details>
  <summary>Details</summary>
Motivation: 连接离散选择模型与在线学习理论，突破传统softmax算法的独立性假设限制，允许动作间的相关学习动态。

Method: 提出基于广义嵌套logit模型的对抗性老虎机算法，以及能够处理动作相关性的广义梯度老虎机算法，通过闭式采样概率实现计算效率。

Result: 在随机老虎机设置中的数值实验证明了所提算法的实际有效性，获得了次线性遗憾边界。

Conclusion: 所提出的算法结合了灵活的模型规范和计算效率，扩展了梯度老虎机方法的适用范围，为处理相关动作提供了新框架。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [120] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS是一个基于提示感知的LLM任务调度器，通过近似最短作业优先调度来提升推理效率，减少延迟并提高吞吐量


<details>
  <summary>Details</summary>
Motivation: 传统调度策略如FCFS存在队头阻塞问题，长任务会延迟短任务，影响LLM推理服务的效率和响应时间

Method: 使用成对排序和边界排序损失来近似SJF调度，预测基于响应长度的任务排序，并集成到vLLM系统中

Result: 在多个LLM和真实推理数据集上的实验表明，PARS显著提升了性能，包括推理工作负载，且设计具有良好的跨模型泛化能力

Conclusion: PARS通过提示感知调度有效解决了LLM推理中的队头阻塞问题，实现了低延迟和高吞吐量的服务

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [121] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一个跨模态时间序列预测模型，通过将多元时间序列转换为图像，利用预训练的大视觉模型提取通道间依赖关系，并与时间序列模态特征对齐融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大时间序列基础模型通常采用通道独立架构，忽略了关键的跨通道依赖关系；同时现有多模态方法未能充分利用大视觉模型解释时空数据的潜力；不同模态信息提取的优势尚未被充分探索来提升时间序列预测性能。

Method: 将多元时间序列渲染成图像，使用预训练的大视觉模型提取复杂的跨通道模式，这些视觉特征与时间序列模态的表征进行对齐和融合，通过冻结大视觉模型并仅训练其7.45%的参数来实现高效训练。

Result: VIFO在多个基准测试中实现了有竞争力的性能，为捕捉变量间关系提供了高效有效的解决方案。

Conclusion: VIFO通过跨模态方法成功解决了通道独立模型的局限性，利用视觉模态增强了时间序列预测能力，同时保持了训练效率。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [122] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种可转移的频率感知对抗攻击方法，并基于此开发了频率感知模型参数探索器(FAMPE)来提升深度神经网络的可解释性，相比现有方法在插入分数上平均提升13.02%。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实世界噪声和故意扰动下的可靠性问题，现有归因方法效果欠佳需要进一步改进。

Method: 提出可转移的频率感知对抗攻击，通过高低频分量进行频率感知探索，并基于此开发FAMPE归因方法。

Result: 相比现有最先进方法AttEXplore，FAMPE在插入分数上平均提升13.02%，性能优于现有方法。

Conclusion: 通过消融研究验证了高低频分量在可解释性中的作用，提出的方法有效提升了深度神经网络的可解释性。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [123] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE是一种结合结构化剪枝和局部剪枝优势的ADMM框架，在保持硬件效率的同时将内存需求从O(N)降低到O(√N)，适用于十亿参数级模型。


<details>
  <summary>Details</summary>
Motivation: 解决全局结构化剪枝内存需求过高(O(N))和局部剪枝忽略层间依赖导致性能下降的矛盾，实现内存高效且性能优越的剪枝方法。

Method: 采用分治策略将全局剪枝分解为协调的子问题，基于ADMM框架整合结构化稀疏性，推导出结构化剪枝掩码的闭式解析解和基于能量的软分配方案。

Result: STRUPRUNE在保持与全局结构化剪枝相同困惑度的同时，将内存成本从O(N)降至O(√N)，实现了十亿参数级别的实际部署。

Conclusion: 该框架成功平衡了内存效率与硬件兼容性，为大规模语言模型的高效剪枝提供了实用解决方案。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [124] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出了首个针对未对齐多模态数据的主动学习框架，通过主动获取跨模态对齐而非标签来降低标注成本，在保持性能的同时减少高达40%的标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要关注单模态数据，忽视了多模态学习中巨大的标注负担，特别是在现代多模态流水线中，高质量的对齐标注成本高昂。

Method: 开发了一种结合不确定性和多样性原则的模态感知算法，实现线性时间采集，可无缝应用于池式和流式设置。

Result: 在基准数据集上的广泛实验表明，该方法能持续减少多模态标注成本同时保持性能，在ColorSwap数据集上可减少40%的标注需求而不损失准确率。

Conclusion: 该框架有效解决了多模态学习中的标注瓶颈问题，为降低多模态标注成本提供了实用解决方案。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [125] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 该研究评估了四种神经算子架构用于快速预测脑位移场，旨在实现创伤性脑损伤的实时建模。MG-FNO在精度上表现最佳，DeepONet在推理速度上最快，所有神经算子都将计算时间从小时级缩短到毫秒级。


<details>
  <summary>Details</summary>
Motivation: 创伤性脑损伤是全球重大公共卫生问题，传统有限元模型计算成本高，限制了临床快速决策应用。本研究旨在开发快速、患者特异性的脑位移预测方法，实现实时TBI建模。

Method: 将TBI建模定义为算子学习问题，使用四种神经算子架构（FNO、F-FNO、MG-FNO、DeepONet）从患者特定的解剖MRI、MRE刚度图和人口统计学特征映射到全场3D脑位移预测，在249个MRE数据集上进行训练评估。

Result: MG-FNO达到最高精度（MSE = 0.0023，94.3%空间保真度），F-FNO收敛速度比标准FNO快2倍，DeepONet提供最快推理速度（14.5次迭代/秒），比MG-FNO快7倍。所有神经算子将计算时间从小时缩短到毫秒级。

Conclusion: 神经算子为预测脑变形提供了高效、分辨率不变的方法，为实时患者特异性TBI风险评估、临床分诊支持和防护设备优化打开了大门，展示了基于神经算子的人脑数字孪生潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [126] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文提出了一种新的可微分逻辑门网络参数化方法，解决了原始方法中的梯度消失、离散化误差和高训练成本问题，同时显著减少了参数规模并加速了训练。


<details>
  <summary>Details</summary>
Motivation: 可微分逻辑门网络在推理时效率极高且保持竞争力精度，但存在梯度消失、离散化误差和高训练成本等问题，阻碍了网络扩展。即使使用专门的参数初始化方案，增加深度仍会损害精度。

Method: 通过重新参数化逻辑门神经元，将每个门的参数数量按输入数量的对数比例缩减。对于二进制输入，这种方法已经能减少4倍的模型大小。

Result: 新方法使后向传播速度提升达1.86倍，训练步数减少8.5倍收敛。在CIFAR-100数据集上，精度保持稳定甚至有时优于原始参数化方法。

Conclusion: 提出的重新参数化方法有效解决了可微分逻辑门网络的扩展性问题，显著提升了训练效率和模型性能。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [127] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: 提出了Numerion模型，基于超复数空间的时间序列预测方法，通过多维度RHR-MLP架构自然分解时间序列，在不同维度空间中独立建模并自适应融合模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过复杂模型结构和先验知识分解时间序列，但受限于计算复杂性和假设的鲁棒性。研究发现超复数空间中时间序列的特征频率自然降低。

Method: 提出Numerion模型，将线性层和激活函数推广到任意2的幂次维度的超复数空间，引入RHR-MLP架构，使用多个RHR-MLP将时间序列映射到不同维度的超复数空间进行分解和独立建模，通过动态融合机制自适应融合不同空间中的潜在模式。

Result: 在多个公共数据集上达到最先进的性能，可视化和定量分析全面证明了多维RHR-MLP自然分解时间序列的能力，以及高维超复数空间捕捉低频特征的倾向。

Conclusion: Numerion模型通过超复数空间自然分解时间序列，有效提升了预测性能，为时间序列分析提供了新的视角和方法。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [128] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 提出了通用多域翻译(UMDT)框架和Diffusion Router(DR)方法，使用单个噪声预测器实现任意域对之间的翻译，仅需K-1个配对数据集。


<details>
  <summary>Details</summary>
Motivation: 现有多域翻译方法需要完全对齐的元组或只能处理训练中见过的域对，限制了实用性并排除了许多跨域映射。

Method: 提出Diffusion Router框架，使用单一噪声预测器建模所有中心域与非中心域之间的翻译，通过中心域路由实现间接翻译，并引入变分边界目标和Tweedie精炼过程支持直接映射。

Result: 在三个大规模UMDT基准测试中取得最先进结果，同时降低采样成本并解锁了如草图↔分割等新任务。

Conclusion: DR被确立为跨多个域进行通用翻译的可扩展和多功能框架。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [129] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: HPL是一个分层偏好学习框架，通过多粒度偏好信号优化LLM智能体，解决了轨迹级DPO信号过于粗糙和步骤级DPO过于短视的问题。


<details>
  <summary>Details</summary>
Motivation: 解决离线偏好对齐方法中的粒度不匹配问题，轨迹级DPO信号过于粗糙难以精确分配信用，步骤级DPO过于短视无法捕捉多步行为价值。

Method: 引入分层偏好学习框架，结合轨迹级和步骤级DPO，核心创新是组级偏好优化和双层课程调度器，从简单到复杂组织学习过程。

Result: 在三个具有挑战性的智能体基准测试中，HPL优于现有最先进方法，有效整合多粒度偏好信号。

Conclusion: 分层DPO损失有效整合多粒度偏好信号，双层课程对于智能体解决从简单行为到复杂多步序列的广泛任务至关重要。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [130] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 本文提出了一种约束悲观双层优化模型，通过限制对手的移动范围来更好地反映现实对抗场景，相比现有无限制对手方法具有更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化方法中的对手不受限制，可能导致模型过于悲观和不切实际，当对手数据变得无意义时会失去其本质特征，无法反映真实世界情况。

Method: 构建约束悲观双层优化模型，通过限制对手的移动范围来识别更符合现实的解决方案。

Result: 实验表明该模型平均性能优于现有方法。

Conclusion: 约束悲观双层优化能够更好地模拟现实对抗场景，提高分类器在真实数据上的性能。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [131] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 该论文提出了SciTS基准测试，涵盖12个科学领域的43个任务，包含5万多个实例，用于评估LLMs在科学时间序列上的推理能力。研究发现通用LLMs比专用时间序列模型具有更强的泛化能力，并提出TimeOmni框架来增强LLMs理解和生成时间序列的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLMs要么将数值序列编码为文本，要么转换为图像，这些方法可能不足以全面理解科学时间序列。现有统一时间序列模型通常专注于预测或分析，在非周期性、异质科学信号上的有效性尚不清楚。

Method: 引入SciTS基准测试，涵盖12个科学领域、43个任务、5万+实例，包含单变量和多变量信号；评估17个模型（文本LLMs、多模态LLMs、统一时间序列模型）；提出TimeOmni框架来增强LLMs的时间序列理解和生成能力。

Result: 通用LLMs比专用时间序列模型表现出更强的泛化能力；将时间序列表示为文本或图像会限制性能（序列过长或数值精度损失）；TimeOmni框架能够有效增强LLMs的时间序列处理能力。

Conclusion: 这项工作填补了科学时间序列专用基准测试和建模框架的空白，为LLMs理解和生成复杂时间科学数据铺平了道路。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [132] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 提出了Triple-BERT方法，一种基于TD3的集中式单智能体强化学习算法，用于解决网约车平台大规模订单派发问题，通过动作分解策略和BERT网络处理大规模动作和观测空间。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临实时订单派发的复杂挑战，现有MARL方法无法有效处理全局信息和合作问题，CTDE方法存在维度灾难问题。

Method: 基于TD3变体，采用动作分解策略将联合动作概率分解为单个司机动作概率，使用BERT网络通过参数复用和注意力机制处理大规模观测空间。

Result: 在曼哈顿真实网约车数据集上验证，相比现有最优方法提升11.95%，服务订单增加4.26%，接驾时间减少22.25%。

Conclusion: Triple-BERT方法有效解决了大规模订单派发问题，在服务效率和接驾时间方面显著优于现有方法。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [133] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: POEM提出了一种新的测试时自适应方法，通过探索先前未被利用的可靠样本来提升模型性能，并引入额外的Adapt Branch网络来平衡领域无关表示提取和目标数据性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖熵作为置信度指标，对预定义的熵阈值敏感，导致许多潜在可靠的目标样本被忽视。这些样本虽然初始熵值略超阈值，但在模型更新后可能变得可靠，能够提供稳定的监督信息和正常范围的梯度。

Method: 提出POEM方法，探索先前未被利用的可靠样本；引入额外的Adapt Branch网络，在提取领域无关表示和实现目标数据高性能之间取得平衡。

Result: 在多种架构上的综合实验表明，POEM在挑战性场景和真实世界领域偏移中始终优于现有TTA方法，同时保持计算效率。POEM的核心思想可作为增强策略提升现有TTA方法的性能。

Conclusion: POEM通过有效利用先前被忽视的可靠样本，显著提升了测试时自适应的性能，其方法具有通用性和高效性，可作为现有TTA方法的增强策略。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [134] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 提出MASA方法，通过自我对齐增强语言模型的元认知能力，证明元认知对齐能显著提升推理性能，在多项数学和推理基准上取得显著准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型缺乏元认知能力，即模型不知道如何思考自身推理过程，导致真实推理轨迹与元预测之间存在严重不对齐问题。

Method: 设计MASA训练流程，通过自我生成信号训练元认知能力，无需外部训练数据；采用零方差提示过滤和冗长推理截断策略提高训练效率。

Result: 在数学基准上平均准确率提升6.2%，AIME25提升19.3%；训练效率提升1.28倍；在13个跨领域基准上平均提升2.08%，GPQA-Diamond提升3.87%。

Conclusion: 元认知对齐能显著提升推理模型的性能和泛化能力，自我对齐方法为增强语言模型元认知能力提供了有效途径。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [135] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 本文提出了一种在归纳式零样本学习中模拟未见条件的分区方案，通过两种互补的特征选择策略（RFS和GA）来减少语义空间中的冗余属性，从而提高未见类的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 零样本学习中的语义空间通常包含噪声、冗余或不相关属性，这些属性会阻碍性能。现有的语义空间无论是人工标注还是机器学习生成，都存在冗余问题，特别是在最具挑战性的归纳设置下。

Method: 提出分区方案模拟未见条件，研究两种特征选择策略：1）RFS：将模型驱动的特征排序转化为有意义的语义剪枝；2）GA：使用进化计算直接探索属性子空间。在五个基准数据集上进行实验验证。

Result: 两种方法在五个基准数据集（AWA2、CUB、SUN、aPY、FLO）上都能通过减少冗余一致提高未见类的准确率。RFS高效且具有竞争力但依赖关键超参数，GA成本更高但能更广泛探索搜索空间且避免这种依赖。

Conclusion: 语义空间本质上是冗余的，提出的分区方案是在归纳条件下精炼语义空间的有效工具。两种方法以互补的方式工作，证实了减少冗余对提高零样本学习性能的重要性。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [136] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出了一种新的热误差补偿范式，使用神经网络预测机床内部的高保真温度和热通量场，而不是直接预测热误差，从而实现更通用和灵活的热误差校正。


<details>
  <summary>Details</summary>
Motivation: 传统的热误差补偿方法依赖于测量的温度-变形场或传递函数，现有数据驱动方法使用神经网络直接预测热误差或补偿值，但这些方法受限于特定误差类型、空间位置或机器配置，缺乏通用性和适应性。

Method: 使用有限元方法获取不同初始条件下的数据训练神经网络，预测温度和热通量场；采用基于相关性的选择策略确定最有信息的测量点；比较了多种时间序列神经网络架构（RNN、GRU、LSTM、双向LSTM、Transformer、TCN），包括专门模型和通用模型。

Result: 能够准确且低成本地预测温度和热通量场，为机床环境中的灵活和通用热误差校正奠定了基础。

Conclusion: 提出的新范式通过预测物理场而非直接预测误差，实现了更通用和可适应的热误差补偿，为机床热误差校正提供了更灵活的方法。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [137] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了正交蒙特卡洛Dropout方法，在合并LoRA模块时强制正交性以避免语义向量干扰，但实证发现正交性本身不足以实现真正的语义组合性。


<details>
  <summary>Details</summary>
Motivation: LoRA模块在合并时（如生成特定风格的对象）会产生语义向量干扰，需要解决这种干扰问题。

Method: 正交蒙特卡洛Dropout，在合并稀疏语义向量时强制执行严格正交性，且不增加额外时间复杂度。

Result: 方法在理论和运行时层面都能保证合并的LoRA保持正交且无直接干扰，但实证分析显示正交性并不能实现语义解耦或组合性。

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [138] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 本文探讨文本到图像模型的机器遗忘问题，提出记忆自我再生任务和MemoRa策略来恢复被遗忘的知识，并强调知识检索鲁棒性作为重要评估指标。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型生成逼真图像的能力带来了被滥用于创建有害、欺骗性或非法内容的风险，这加速了对机器遗忘的需求，但实际遗忘特定概念非常困难。

Method: 引入记忆自我再生任务，提出MemoRa策略作为支持有效恢复先前丢失知识的再生方法，并区分短期和长期遗忘机制。

Result: 研究表明遗忘以两种不同方式发生：短期遗忘可快速回忆，长期遗忘恢复更具挑战性。

Conclusion: 知识检索的鲁棒性是开发更强大有效遗忘技术的关键但未被充分探索的评估指标，需要进一步研究遗忘和回忆机制。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [139] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 本研究首次系统性地探讨了推理数据在不同训练阶段（预训练vs后训练）引入对LLM性能的影响，发现预训练阶段引入推理数据至关重要，能建立基础能力，且这种效果无法通过后训练完全复制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM增强推理能力的主流方法是在后训练阶段使用高质量推理数据，但推理数据在预训练阶段的作用尚不明确。由于前沿模型的预训练语料不透明，不同阶段引入推理数据的效果缺乏系统研究。

Method: 系统研究推理数据在不同规模、多样性和质量下，在不同训练阶段（预训练vs后训练）引入对LLM性能的影响。

Result: 预训练阶段引入推理数据带来19%的平均增益，建立了无法通过后训练完全复制的基础能力。预训练受益于推理模式的广泛多样性（11%增益），而后训练更敏感于数据质量（15%增益）。

Conclusion: 研究挑战了语言建模与推理的传统分离，为在整个训练流程中战略性地分配数据提供了原则性指导，以构建更强大的模型。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [140] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: 提出MindCraft框架和概念树方法，通过谱分解和概念路径重构概念层次结构，揭示深度模型中概念如何从共享表示中分离为线性可分子空间。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在语言、视觉和推理任务中表现出色，但其内部如何组织和稳定概念结构仍不清楚。受因果推断启发，需要开发能够深入分析概念表示的方法。

Method: 基于概念树的MindCraft框架，在每一层应用谱分解，将主方向连接成分支概念路径，重构概念层次出现过程。

Result: 在医疗诊断、物理推理和政治决策等多个领域的实证评估表明，概念树能够恢复语义层次、解缠潜在概念，并可在多个领域广泛应用。

Conclusion: 概念树建立了一个广泛适用且强大的框架，能够深入分析深度模型中的概念表示，是迈向可解释AI基础的重要一步。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [141] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究应用变分自编码器(VAE)检测美国大陆四个AR6区域GPP中的极端事件，与传统奇异谱分析(SSA)方法进行比较，发现两种方法在空间模式上高度一致，且VAE具有计算优势和非线性依赖捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 气候异常显著影响陆地碳循环动态，需要可靠的方法来检测和分析植物生产力的异常行为。

Method: 使用变分自编码器(VAE)架构，包含三个密集层和潜在空间，输入序列长度为12个月，通过重构误差识别GPP异常。与传统SSA方法在三个时间段(1850-80、1950-80、2050-80)进行比较。

Result: VAE和SSA方法在极端事件频率的空间模式上表现出强烈区域一致性。两种方法均显示到2050-80年，特别是西部和中北美地区，负碳循环极端事件的幅度和频率增加。VAE阈值值更高(179-756 GgC vs 100-784 GgC)。

Conclusion: VAE方法与成熟的SSA技术性能相当，同时具有计算优势，并能更好地捕捉碳循环变异中的非线性时间依赖性，且无需预先定义数据中的信号周期性。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [142] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT²-LLM是一种针对大语言模型的后训练三值化框架，通过非对称三值量化器和两阶段优化流程，在保持竞争力的同时显著降低内存需求和加速推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但内存和计算需求巨大阻碍了部署。三值化作为一种有前景的压缩技术，在后训练量化场景下的潜力尚未充分探索，主要面临无训练参数优化和异常值权重带来的量化困难。

Method: 提出PT²-LLM框架，核心包括：1）迭代三值拟合(ITF)，交替进行最优三值网格构建和灵活舍入；2）激活感知网格对齐(AGA)，进一步优化三值网格匹配全精度输出；3）基于结构相似性的重排序(SSR)策略，利用列间结构相似性缓解量化难度和异常值影响。

Result: 大量实验表明，PT²-LLM在保持与最先进2位后训练量化方法竞争力的同时，具有更低的内存成本，并能加速预填充和解码过程，实现端到端加速。

Conclusion: PT²-LLM为LLMs提供了一种有效的后训练三值化解决方案，在性能、内存效率和推理速度方面取得了良好平衡，为大语言模型的轻量化部署提供了可行路径。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [143] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 本文提出了首个理论框架分析多模态对比学习的最优表示和模态对齐，证明了维度塌陷是模态间隙的根本原因，并提出了两种实现完美对齐的方法。


<details>
  <summary>Details</summary>
Motivation: 经验证据显示不同模态的表示占据嵌入空间的不同区域（模态间隙），且关于模态间隙大小如何影响下游性能的研究结果不一致，这引发了两个关键问题：什么导致了模态间隙？它如何影响下游任务？

Method: 引入理论框架分析多模态对比学习的收敛最优表示和训练优化时的模态对齐，在不同约束条件下（无约束、锥约束、子空间约束）证明模态间隙的收敛行为。

Result: 证明无约束或锥约束下模态间隙收敛到零；子空间约束下（因维度塌陷）模态间隙收敛到两个超平面间的最小角度；识别维度塌陷为模态间隙的根本原因；提出通过超平面旋转和共享空间投影实现完美对齐。

Conclusion: 维度塌陷是模态间隙的根本原因，模态间隙通过影响样本对的对齐来影响下游性能，在子空间约束下仍可通过超平面旋转和共享空间投影实现完美模态对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [144] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 提出了通用探索奖励(GEB)框架，解决现有KL和α-散度正则化方法在强化学习人类反馈中探索偏向高概率区域的问题，通过参考依赖的奖励调节实现乐观探索。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法在KL或α-散度正则化下会无意中将探索偏向参考模型的高概率区域，强化保守行为而非促进不确定区域的发现，这违背了乐观探索原则。

Method: 引入通用探索奖励(GEB)理论框架，通过参考依赖的奖励调节来抵消散度诱导的偏差，统一了先前的启发式奖励方法，并自然扩展到完整的α-散度族。

Result: 在多个散度设置和大语言模型骨干上的对齐任务中，GEB始终优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了既有理论依据又实用的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [145] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的扩散语言模型，专门用于代码生成，通过大规模扩散预训练、代码中心的中期训练和指令微调，在保持推理延迟竞争力的同时，在多个代码评估基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有双向上下文和填充能力，但现有系统通常较为笨重。作者旨在开发一个轻量级但性能优异的扩散编码模型。

Method: 采用1.7B参数的扩散模型架构，结合大规模扩散预训练、代码中心的中期训练和指令微调，使用置信度引导采样来优化推理性能。

Result: 在Humaneval、MBPP和EvalPlus等基准测试中，CoDA-1.7B-Instruct匹配或超越了参数规模达7B的扩散模型。

Conclusion: CoDA展示了轻量级扩散模型在代码生成任务上的潜力，并开源了模型检查点、评估工具和TPU训练流程以推动相关研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [146] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 提出了决策势能面（DPS）作为分析大语言模型决策边界的新概念，并开发了K-DPS算法来近似构建LLM的决策边界，仅需有限次序列采样即可实现可忽略误差的近似。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的巨大词汇序列规模和自回归特性，构建其决策边界在计算上不可行，需要新的分析方法。

Method: 定义DPS概念，基于对每个输入区分不同采样序列的置信度，证明DPS中的零高度等值线等价于LLM的决策边界。提出K-DPS算法，通过K次有限序列采样来近似决策边界。

Result: 理论推导了K-DPS与理想DPS之间的绝对误差、期望误差和误差集中的上界，证明这些误差可以通过采样次数进行权衡。通过多种LLM和语料库的实验验证了结果。

Conclusion: DPS为分析LLM决策边界提供了可行的新方法，K-DPS算法能够以可接受的误差高效近似决策边界，为理解LLM行为提供了新工具。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [147] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 该论文将Transformer架构重新概念化为连续时空动力系统，通过偏微分方程框架分析其内部机制，证明残差连接和层归一化是稳定该系统的数学必需品。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然革命性，但其内部机制缺乏理论理解。作者旨在通过连续动力学视角，为Transformer的设计提供第一性原理解释。

Method: 提出一个分析框架，将Transformer的离散分层结构映射为连续偏微分方程系统：自注意力对应非局部相互作用，前馈网络对应局部反应，残差连接和层归一化对应稳定机制。

Result: 实验表明，没有残差连接会导致灾难性的表示漂移，没有层归一化会导致训练动态不稳定和爆炸。这些组件是稳定系统的数学必需品。

Conclusion: 残差连接和层归一化这些看似启发式的技巧实际上是稳定Transformer这一强大但固有不稳定连续系统的数学必需品，为Transformer设计提供了第一性原理解释。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [148] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: SID是一种新的训练框架，通过将深度学习重构为局部协同精炼问题的级联，解决了反向传播的更新锁定和高内存消耗问题，实现了并行训练并保持标准前向推理。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播的两个关键可扩展性瓶颈：更新锁定（网络模块在反向传播完成前保持空闲）和因存储激活值导致的高内存消耗。

Method: 将深度网络构建为模块管道，每个模块施加局部目标来精炼关于真实目标的概率信念，平衡对目标的保真度与对前一个模块信念的一致性，从而解耦模块间的反向依赖。

Result: SID在分类准确率上持续匹配或超越反向传播，表现出优越的可扩展性和对标签噪声的显著鲁棒性。

Conclusion: SID作为反向传播的通用替代方案，保证了网络深度的单调性能提升，同时消除了更新锁定并大幅减少了内存需求。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [149] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出了Quant-dLLM框架，专门针对扩散大语言模型进行超低位后训练量化，解决了标准量化方法在2位精度下性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归模型的替代方案，模型规模持续增长，需要权重压缩部署。但直接将自回归模型的量化方法应用于扩散模型在2位精度下性能不理想。

Method: 提出三个关键技术：掩码校准模拟（MCS）对齐时间步相关的掩码校准；数据感知任意顺序量化器（DAQ）通过优化算法学习超低位权重表示；自适应分块混合精度（ABMP）基于敏感度分配位宽。

Result: 在严格的2位预算下，Quant-dLLM在扩散大语言模型上持续优于最先进的自回归模型迁移量化方法。

Conclusion: Quant-dLLM是专门为扩散大语言模型设计的有效超低位量化框架，显著提升了2位量化性能。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [150] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一个创新的1位LLM量化框架，通过Sigma-Delta量化和过采样技术，在极低比特量化下保持语言推理能力，支持分数过采样比动态调整内存约束。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临显著的计算和内存挑战，需要极低比特量化来实现高效部署。

Method: 使用上采样结合Sigma-Delta量化器将LLM权重二值化或三值化，采用Hadamard权重平滑减少量化精度损失，并提出基于权重方差的细粒度OSR分配策略MultiOSR。

Result: 在OPT和LLaMA模型系列上的大量实验表明，SDQ-LLM在高度激进的低OSR设置下实现了更高效和高精度的性能。

Conclusion: SDQ-LLM框架为LLM的极低比特量化提供了有效的解决方案，在保持模型性能的同时显著提升了推理效率。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [151] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 提出一种轻量级二次增强器，通过引入二次变换来增强神经网络非线性，仅需少量额外参数和计算即可显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络主要基于线性变换和非线性激活函数，为了进一步增强非线性能力，探索引入二次变换来提升现有架构的性能

Method: 使用低秩性、权重共享和稀疏化技术构建轻量级二次增强器，在每层特征间引入二次交互，同时保持参数和计算复杂度极低

Result: 在图像分类、文本分类和大语言模型微调三个任务上的概念验证实验均显示出明显且显著的性能提升

Conclusion: 提出的轻量级二次增强方法能有效提升神经网络性能，为增强模型非线性能力提供了新思路

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [152] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 提出了一个可扩展的矩阵自由拉普拉斯框架，用于分解贝叶斯物理信息神经网络中的后验Hessian矩阵，量化各个物理约束对损失曲面的相对影响。


<details>
  <summary>Details</summary>
Motivation: 需要澄清单个物理约束如何塑造贝叶斯物理信息神经网络，因为物理约束可能导致网络过度自信，这种过度自信可能反映了约束强制带来的合理精度，而非校准错误。

Method: 引入可扩展的矩阵自由拉普拉斯框架，将后验Hessian矩阵分解为每个约束的贡献，并提供量化指标来衡量它们在损失曲面上的相对影响。

Result: 应用于Van der Pol方程时，该方法能够追踪约束如何塑造网络的几何结构，并通过Hessian矩阵直接展示单个损失权重的变化如何非平凡地重新分布曲率和有效主导性。

Conclusion: 该方法为理解物理约束在贝叶斯物理信息神经网络中的影响提供了新的分析工具，有助于更准确地解释网络的不确定性和过度自信问题。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [153] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: MemMamba通过状态汇总机制和跨层跨token注意力，解决了Mamba模型长程记忆指数衰减问题，在保持线性复杂度的同时显著提升了长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有长序列建模方法存在效率与内存的权衡：RNN有梯度消失问题，Transformer有二次复杂度限制，Mamba虽然高效但长程记忆会指数衰减。需要解决Mamba的长程遗忘问题。

Method: 通过数学推导和信息论分析揭示Mamba记忆衰减机制，提出水平-垂直记忆保真度指标，并设计MemMamba架构，集成状态汇总机制与跨层跨token注意力。

Result: 在PG19和Passkey Retrieval等长序列基准测试中显著优于现有Mamba变体和Transformer，推理效率提升48%。

Conclusion: MemMamba在复杂度与内存权衡上取得突破，为超长序列建模提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [154] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是首个针对扩散语言模型的系统性缩放定律，涵盖计算受限和数据受限两种机制，研究关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 为扩散语言模型提供短期实用训练指导，并为整个AI社区带来长期启发。

Method: 建立系统性缩放定律，研究关键建模和优化设计，涵盖计算受限和数据受限两种机制。

Result: 提出了Quokka缩放定律，作为Chinchilla的好朋友并提供更广泛的研究范围。

Conclusion: Quokka为扩散语言模型训练提供了实用指导，有望对整个AI社区产生长期积极影响。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [155] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出混合归因和剪枝框架，结合归因修补的速度优势和边剪枝的忠实性，在保持电路忠实度的同时提升46%的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法面临基本权衡：归因修补速度快但不忠实于完整模型，而边剪枝忠实但计算成本高。需要平衡速度和忠实性。

Method: 使用归因修补识别高潜力子图，然后应用边剪枝从中提取忠实电路，形成混合归因和剪枝框架。

Result: HAP比基线算法快46%且不牺牲电路忠实度，在间接对象识别任务中保留了合作电路组件。

Conclusion: HAP是提高机械可解释性研究可扩展性的有效方法，适用于更大模型。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [156] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一个混合LLM系统，通过在边缘服务器上协同定位推理和微调任务，实现迭代级调度，在保证推理延迟的同时提升模型更新频率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘服务器上LLM部署中推理延迟与模型准确性之间的冲突，用户数据的非平稳性需要频繁重训练，但现有策略无法在有限GPU资源下平衡这两者。

Method: 提出MACE系统，协同定位推理（预填充、解码）和微调任务，采用智能内存管理，根据模型更新对输出对齐的影响程度动态分配GPU周期。

Result: MACE在保持吞吐量的同时，将推理延迟降低高达63%，GPU利用率维持在85%以上，性能优于连续重训练和周期性重训练策略。

Conclusion: 迭代级混合调度是在边缘平台上部署具有持续学习能力的LLM的有前景方向，能够有效平衡吞吐量、延迟和更新新鲜度。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [157] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT是一个用于在边缘设备上进行联邦指令调优的可扩展框架，通过结合联邦学习和4位量化低秩适应(QLORA)来解决大语言模型在联邦学习中的通信和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法如FedAvg在面对大语言模型的海量参数时失效，无法处理通信和计算开销问题。

Method: 结合联邦学习和4位量化低秩适应(QLORA)，在物联网领域过滤Databricks Dolly 15k数据集进行指令调优。

Result: Edge-FIT调优的Llama 2(7B)模型达到F1分数0.89，使用Phi-3-mini(3.8B)模型验证了可行的权衡方案。

Conclusion: Edge-FIT是一个可扩展的框架，适用于在家庭计算网关上实现去中心化大语言模型部署。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [158] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一个基于主动领域自适应的日志异常检测模型，结合迁移学习和主动学习，仅需2%的人工标注就能达到93.01%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法严重依赖标注，但大规模日志标注极具挑战性。迁移学习和主动学习方法存在源-目标系统数据分布差异和冷启动问题。

Method: 使用成熟系统的标注数据训练基础模型解决冷启动问题，采用基于自由能量和不确定性的采样方法选择分布边界上的日志进行人工标注，以最小化标注成本解决数据分布差异。

Result: 在六个不同数据集组合上的实验表明，LogAction仅需2%人工标注就能达到平均93.01%的F1分数，优于现有最优方法26.28%。

Conclusion: LogAction有效解决了日志异常检测中的标注挑战，通过主动领域自适应方法在最小化人工标注成本的同时实现了高性能检测。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [159] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散语言模型的局限性，指出了其在实现并行生成和双向注意力方面的固有困难，并提出了最有效的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型相比自回归模型具有并行生成和双向注意力的优势，但现有的开源掩码扩散模型多基于吸收扩散变体，存在实现这些优势的固有困难。

Method: 分析了掩码扩散模型在实现并行生成和双向注意力方面的技术挑战，并提出了优化的训练和推理策略。

Result: 证明了掩码扩散模型在实现并行生成和双向注意力方面存在固有困难，无法充分发挥扩散模型的优势。

Conclusion: 掩码扩散模型在实现扩散语言模型的核心优势方面存在根本性限制，需要重新考虑其架构设计。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [160] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文分析了Clifford卷积层的内部计算结构，提出了多种优化方法来加速推理过程，同时保持正确性，最终实现了21.35倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 随着物理科学中对具有等变性特性的网络兴趣增长，Clifford神经层作为实现E(n)和O(n)等变性的一种方法备受关注，但需要优化其计算效率。

Method: 首先分析Clifford代数的理论基础以消除冗余矩阵分配和计算，然后系统应用已建立的优化技术来进一步提升性能。

Result: 在11个函数上实现了相对于基线实现21.35倍的平均加速，在6个案例中达到与原始PyTorch实现相当或更快的运行时间，其余案例中性能与原始库处于同一数量级。

Conclusion: 通过理论分析和系统优化，成功提升了Clifford卷积层的推理性能，为等变性网络的实际应用提供了更高效的实现方案。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [161] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一个统一的后训练剪枝框架，结合了局部显著性度量的速度和全局协调的稳定性，通过镜像下降优化实现，无需更新模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临高昂的计算和内存成本，现有剪枝方法难以平衡效率和鲁棒性：局部方法在高稀疏度下容易崩溃，全局方法需要昂贵的权重更新或限制半结构化格式。

Method: 使用快速层间评分和轻量级全局控制器分配单一稀疏度预算，支持非结构化和半结构化N:M剪枝，通过镜像下降优化实现全局协调。

Result: 在多个预训练LLM家族和标准基准测试中，UniPruning始终提供具有竞争力或更优的困惑度和零样本准确率。

Conclusion: UniPruning为大规模型稀疏化提供了一个高效、原则性和可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [162] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种即插即用的推理时路由算法，通过平衡专家负载来提升MoE模型的推理性能，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过条件路由减少训练成本，但推理时专家参数和激活会消耗内存，导致专家负载不均衡，影响系统延迟、吞吐量和成本。

Method: LASER根据门函数得分分布自适应路由：当得分偏好明显时路由到最强专家；当得分均匀时扩大可行专家集并路由到负载最轻的专家。无需重新训练或微调。

Result: 在Mixtral-8x7B和DeepSeek-MoE-16b-chat模型上测试，LASER改善了负载平衡，降低了延迟并提高了吞吐量，同时准确率变化可忽略。

Conclusion: LASER是一种有效的推理时路由算法，能够在不影响准确性的前提下显著提升MoE模型的推理性能。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [163] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L 是一种联邦学习方法，通过拉格朗日对偶优化显式处理设备资源约束，动态调整训练参数，在保持性能的同时显著减少内存和通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中设备级资源约束问题，包括能量、通信、内存和热预算等限制，使联邦学习能够在资源受限的边缘设备上实际部署。

Method: 采用拉格朗日对偶优化动态调整训练超参数（冻结深度、本地步数、批量大小、通信压缩），通过梯度积累保持训练稳定性。

Result: 在字符级语言模型实验中，相比标准FedAvg，内存使用减少20%，通信减少95%，同时保持竞争力的验证性能。

Conclusion: CAFL-L在满足资源约束方面表现优异，适合在资源受限的边缘设备上部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [164] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出了一种结合XGBoost和神经网络的自适应集成框架，通过元学习实现智能模型选择和组合


<details>
  <summary>Details</summary>
Motivation: 开发更智能和灵活的机器学习系统，提升预测性能和可解释性

Method: 使用元学习协同结合XGBoost和神经网络，采用先进的不确定性量化技术和特征重要性集成来动态编排模型选择和组合

Result: 在多个数据集上展示了优越的预测性能和增强的可解释性

Conclusion: 该方法为开发更智能和灵活的机器学习系统做出了贡献

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [165] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 本文揭示了概念擦除技术只是制造了"失忆"的假象，而非真正删除概念，并提出RevAm框架来逆向恢复被擦除的概念，暴露当前安全机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型架构演进到Flux等新一代模型，现有的概念擦除方法效果下降，需要探究其真正机制并区分表面安全与真正的概念移除。

Method: 提出RevAm框架，基于强化学习的轨迹优化方法，通过动态引导去噪过程来恢复被擦除的概念，无需修改模型权重。采用Group Relative Policy Optimization适应扩散模型，通过轨迹级奖励探索多样恢复路径。

Result: RevAm实现了优越的概念恢复保真度，同时将计算时间减少10倍，成功暴露了当前安全机制的关键漏洞。

Conclusion: 当前基于轨迹操纵的概念擦除方法存在根本可逆性，需要开发超越轨迹操纵的更鲁棒擦除技术。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [166] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 该论文分析了机器学习在气候建模中的应用案例研究，重点关注工作流设计模式，包括替代建模、ML参数化、概率编程等，旨在为科学机器学习提供严谨性框架。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模中物理一致性、多尺度耦合、数据稀疏性等挑战，促进数据科学与气候建模的跨学科合作。

Method: 通过分析一系列应用机器学习研究案例，综合不同项目中的工作流设计模式，关注物理知识基础、模拟数据信息和观测集成。

Result: 提出了确保科学机器学习严谨性的框架，包括透明模型开发、关键评估、知情适应和可重复性。

Conclusion: 该研究为机器学习在气候建模中的应用提供了系统性工作流分析框架，有助于降低跨学科合作门槛，促进数据科学与气候建模的融合。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [167] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的对比桥接方法，通过冻结的单模态编码器和简单的投影头，实现了化学分子指纹与生物医学文本嵌入的跨模态对齐，避免了大规模多模态预训练的需求。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型在药物发现中具有潜力，但现有方法需要大量预训练或多模态数据。本文旨在探索是否可以通过轻量级投影头实现化学和文本表示的跨模态对齐。

Method: 使用ChEMBL中的配对机制，通过双线性投影和对比目标将ECFP4分子指纹与生物医学句子嵌入对齐。引入硬负样本加权和边界损失来处理共享相同治疗靶点的药物。

Result: 在基于骨架的分割评估中，该方法实现了非平凡的跨模态对齐，相比冻结基线显著提高了靶点内区分能力。

Conclusion: 薄桥接方法为大规模多模态预训练提供了计算效率高的替代方案，支持骨架感知的药物文本对齐和精准医学中的靶点特异性检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [168] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: LLMs能够复现行为运营管理中的大部分假设检验结果，但响应分布与人类数据存在差异；轻量级干预措施可以改善对齐效果


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在运营管理中模拟人类行为的能力，作为实验室实验、实地研究和调查的低成本补充工具

Method: 使用9个已发表的行为运营实验，通过假设检验结果复现和Wasserstein距离分布对齐两个标准来评估LLMs

Result: LLMs能够复现大多数假设层面的效应，捕捉关键决策偏差，但响应分布与人类数据存在分歧；链式思维提示和超参数调整可以减少不对齐

Conclusion: LLMs在运营管理中能够有效模拟人类行为，轻量级干预措施可以提升模型表现，使较小或开源模型能够匹配或超越大型系统

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [169] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 本文提出了一个包含数据质量维度的扩展缩放定律，将Chinchilla框架扩展到同时考虑模型大小、数据量和数据质量对损失的影响。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型缩放定律只关注模型大小和数据量，但缺乏对数据质量影响的系统化分析。本文旨在建立数据质量与模型性能之间的量化关系。

Method: 引入无量纲数据质量参数Q，基于有效样本量和信息论视角，提出质量感知的缩放定律。通过神经机器翻译和自回归建模实验，系统控制数据质量（噪声注入和覆盖度变化）。

Result: 实验表明损失随数据质量可预测地变化，高质量数据能显著减小模型规模和计算需求。数据有效性随质量呈次线性衰减，对适度数据损坏具有鲁棒性。

Conclusion: 建立了数据质量的显式通用定律，为大规模预训练中数据筛选工作与模型规模的平衡提供了具体指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [170] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出基于神经网络的方法，在约10毫秒内从正弦信号中快速重建频率，相比传统傅里叶方法精度提高2倍，并引入自动分类框架识别信号中的物理干扰。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要数秒数据才能重建频率的问题，特别是在环形激光陀螺仪等应用中需要快速频率重建和触发生成。

Method: 使用神经网络方法进行频率重建，在约10毫秒内处理数据；同时开发自动分类框架识别物理干扰（激光不稳定性和地震事件）。

Result: 在GINGERINO环形激光陀螺仪工作范围内，频率估计精度比标准傅里叶方法提高2倍；对地震类信号的独立测试数据集分类准确率达到99%-100%。

Conclusion: 该方法在信号分析中成功集成人工智能，为地球物理应用提供了快速准确的频率估计和干扰识别能力。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [171] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 提出了CIC框架来增强强化学习算法的稳定性，通过维护代表性策略和当前策略，选择性更新代表性策略，并使用自适应调整机制共同促进critic训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练过程经常出现严重振荡，导致不稳定和性能下降，需要提高算法稳定性。

Method: CIC框架维护代表性策略和当前策略，仅在当前策略表现更优时选择性更新代表性策略，并采用自适应调整机制让两种策略共同促进critic训练。

Result: 在五个MuJoCo环境中的评估结果显示，CIC提高了传统算法的性能，且没有产生额外的计算成本。

Conclusion: CIC框架有效增强了强化学习算法的稳定性，提升了性能表现，且计算效率良好。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [172] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate是一个通过LLM从源代码、运行时日志和历史作业中提取语义信息来增强深度学习调度器的框架，能够将作业完成时间减少高达1.91倍。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习调度器缺乏对作业语义上下文的理解，只能依赖有限的元数据，导致高分析开销、不可靠的持续时间估计、不充分的故障处理和差的可观测性。

Method: 提出SchedMate框架，通过三个基于LLM的组件从源代码、运行时日志和历史作业等非结构化数据源中系统性地提取深度见解，非侵入式地增强现有调度器。

Result: 在128-GPU物理集群上的评估和生产跟踪的广泛模拟显示，SchedMate将平均作业完成时间减少高达1.91倍，显著提升了调度性能。

Conclusion: 语义感知在现代深度学习调度中扮演关键角色，SchedMate通过弥合语义鸿沟证明了其有效性。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [173] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 本文研究了扩散模型中旋转对齐步骤的理论基础，发现对齐对应于矩阵Fisher分布的采样模式，并推导出小噪声水平下的更好近似器。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型训练中旋转对齐步骤的理论依据，该步骤在点云数据训练中常用但缺乏系统分析。

Method: 通过将最优去噪器表达为SO(3)上的矩阵Fisher分布，分析对齐操作的理论性质，并推导小噪声极限下的改进近似方法。

Result: 发现对齐操作是矩阵Fisher分布模式的采样，在小噪声水平下是零阶近似，解释了其有效性。

Conclusion: 对齐操作在扩散模型训练的关键噪声水平下通常是足够好的近似方法。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [174] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 本文系统分析了Transformer模型中池化操作的理论表达能力和实际性能影响，为不同任务选择合适的池化机制提供了理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型已成为序列建模的主流架构，但现有研究主要关注注意力机制，而池化操作作为将token表示聚合为固定大小向量的关键组件，其作用尚未得到充分探索。

Method: 提出了一个理论框架，推导了基于Transformer模型配合常用池化方法的表达能力闭式边界，分析了不同注意力变体下的表示能力和输入区分能力，并在计算机视觉、自然语言处理和时间序列分析三大模态上进行了实证评估。

Result: 研究揭示了池化选择对模型准确性、敏感性和优化行为的一致影响趋势，为不同任务需求（全局vs局部上下文理解）提供了池化策略选择的实证依据。

Conclusion: 池化应被视为Transformer模型的关键架构组件，这项工作为超越单纯注意力机制的原则性模型设计奠定了基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [175] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 该论文提出了一个基于多目标强化学习和随机微分方程模拟器的疾病传播防控框架，能够在控制疫情和维持社会经济稳定之间找到平衡策略。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了需要在疾病控制和社会经济稳定之间取得平衡的干预策略需求。

Method: 使用多目标强化学习结合新型随机微分方程疫情模拟器，该模拟器基于全球COVID-19数据校准验证，比其他RL方法中的模型精度高几个数量级。

Result: 框架成功展示了COVID-19防控中流行病学控制与经济稳定之间的政策权衡，并扩展到其他病原体如脊髓灰质炎和流感，发现不同病原体需要根本不同的干预策略。

Conclusion: 该工作提供了一个稳健且适应性强的框架，支持透明、基于证据的公共卫生危机缓解政策制定。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [176] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 本研究开发了一种结合机器学习和VR技术的飞行员选拔方法，使用SVM和MIC特征选择算法，在23名飞行员和23名新手样本上实现了93%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着航空业快速发展，需要大量飞行员，如何以成本效益高的方式选拔合适的飞行员成为重要研究问题。

Method: 招募23名中国东方航空飞行员和23名清华大学社区新手，应用结合机器学习和VR技术的新方法，使用SVM分类器和MIC特征选择方法来区分不同飞行技能的特征。

Result: SVM结合MIC特征选择方法在所有指标上均达到最高预测性能：准确率0.93、AUC 0.96、F1分数0.93，优于其他四种分类器算法和两种特征选择方法。

Conclusion: SVM+MIC算法优于现有所有飞行员选拔算法，可能是首个基于眼动追踪和飞行动力学数据的实现。该研究的VR模拟平台和算法可用于飞行员选拔和训练。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [177] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: KVComm是一种基于KV对选择性共享的LLM间高效通信框架，通过注意力重要性评分和Gaussian先验选择最有信息的KV对进行通信，在仅传输30%层KV对的情况下达到与直接合并输入方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统的通信协议存在自然语言通信推理成本高、信息丢失，以及隐藏状态通信存在信息集中偏差和效率低下的问题。

Method: 提出KVComm框架，通过基于注意力重要性评分和Gaussian先验的KV层选择策略，识别最有信息的KV对进行选择性共享通信。

Result: 在多种任务和模型对上的实验表明，KVComm仅传输30%层KV对即可达到与直接合并输入方法相当的性能。

Conclusion: KV对作为LLM间通信的有效媒介具有巨大潜力，为可扩展高效的多智能体系统铺平了道路。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [178] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于龙卷风预测的多模态LLM框架，通过分析高分辨率对流预报数据来评估LLM在复杂现实任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 需要评估LLM在复杂、高影响的现实世界任务中的真实推理能力，特别是在关键领域如气象预测中的表现。

Method: 使用多模态LLM端到端处理异构时空数据，从3625个预报地图和40125个预报探空数据中交互查询，生成12-36小时的概率性龙卷风风险多边形预测。

Result: 在40天的测试期内，人类专家显著优于最先进的模型，模型表现出强烈的幻觉倾向、风险强度过度预测、地理定位不准确，以及在复杂动态系统中的时空推理能力差。

Conclusion: AgentCaster旨在推进LLM代理在关键领域挑战性推理任务中的改进研究，目前LLM在复杂气象预测任务中仍有明显局限性。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [179] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: CONCEPTNEURO是一个基于概念的精神疾病诊断框架，利用大语言模型和神经生物学知识自动生成可解释的功能连接概念，通过概念分类器实现高准确性和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 近五分之一青少年患有精神或行为健康问题，现有图神经网络方法虽然能预测疾病但缺乏可解释性，限制了临床应用的可靠性。

Method: 结合大语言模型和神经生物学知识，自动生成、过滤和编码可解释的功能连接概念，每个概念表示为连接特定脑区的结构化子图，通过概念分类器进行预测。

Result: 在多个精神疾病数据集上的实验表明，CONCEPTNEURO增强的图神经网络始终优于原始版本，在提高准确性的同时提供透明、临床对齐的解释。

Conclusion: CONCEPTNEURO建立了可解释、领域知情的框架，概念分析揭示了与专家知识一致的疾病特异性连接模式，并为未来研究提出新假设。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [180] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 开发了一个基于迁移学习的LSTM框架，利用铝7075-T6合金的轴向疲劳数据来预测高周扭转S-N曲线，显著降低了疲劳性能测试成本。


<details>
  <summary>Details</summary>
Motivation: 铝合金易发生疲劳失效，但表征材料疲劳性能（特别是高周数据）非常耗时耗成本。

Method: 使用长短期记忆网络(LSTM)构建迁移学习框架，基于铝7075-T6合金的纯轴向疲劳数据训练源模型，然后迁移预测高周扭转S-N曲线。

Result: 该框架能够准确预测铝合金在更高循环范围内的扭转S-N曲线。

Conclusion: 该框架将大幅降低获取不同材料疲劳特性的成本，帮助在更好的成本和时间约束下优先安排测试。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [181] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 该论文通过秩结构分析时间序列Transformer，发现时间序列嵌入具有快速衰减的奇异值谱，使得Q/K/V投影可低秩近似，注意力层可压缩。提出了流秩概念解释非线性混合导致秩随深度增长，并成功压缩Chronos模型，减少65%推理时间和81%内存。


<details>
  <summary>Details</summary>
Motivation: 分析Transformer在不同数据模态中的差异，特别关注时间序列数据与文本/视觉数据的结构特性差异，探索时间序列Transformer的可压缩性原理。

Method: 通过秩结构分析时间序列嵌入的奇异值谱特性，证明Q/K/V投影的低秩近似可行性，提出流秩概念解释深度层秩增长现象，并基于这些理论指导模型压缩。

Result: 成功压缩Chronos时间序列基础模型，实现推理时间减少65%，内存使用减少81%，且不损失准确性。

Conclusion: 时间序列Transformer具有内在可压缩性，研究为时间序列基础模型的宽度、深度和注意力头分配提供了原则性指导。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [182] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出基于物理信息神经算子(PINO)的深度强化学习框架PINO-PC，用于湍流建模和控制，在未见过的高雷诺数场景下实现39.0%的减阻效果，优于现有方法32%以上。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟评估湍流控制效果成本高昂，需要开发更高效的湍流建模和控制方法。

Method: 使用物理信息神经算子(PINO)联合学习策略和观测器模型，实现模型驱动的强化学习预测控制，具有离散不变性并能准确捕捉湍流精细尺度。

Result: PINO-PC在雷诺数15,000的高雷诺数未见过流动场景中实现39.0%的减阻效果，优于之前流体控制方法32%以上。

Conclusion: PINO-PC框架为湍流控制提供了一种高效准确的解决方案，在挑战性场景下显著优于现有方法。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [183] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 提出一个结合MOVES模型和开源数据的框架，使用神经网络预测车辆运行模式分布，在波士顿地区验证显示能显著降低污染物排放估算误差


<details>
  <summary>Details</summary>
Motivation: 利用开源数据为城市区域车辆活动和排放估算提供可扩展、透明的基础，实现低成本、可复制的排放估算

Method: 集成MOVES模型、GPS轨迹数据、OpenStreetMap道路网络、区域交通数据和卫星图像特征向量，训练神经网络预测运行模式分布

Result: 相比MOVES基准，对CO、NOx、CO2和PM2.5等关键污染物的区域尺度交通排放RMSE降低超过50%

Conclusion: 证明了使用完全开源数据源进行低成本、可复制和数据驱动排放估算的可行性

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [184] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: WindSR是一个用于轮毂高度风速超分辨率降尺度的扩散模型，通过数据同化将稀疏观测数据与模拟场结合，提高风速预测的精度和分辨率。


<details>
  <summary>Details</summary>
Motivation: 高质量的轮毂高度风观测数据在空间和时间上都很稀疏，而模拟数据虽然广泛可用但存在偏差且分辨率不足，无法满足风电场选址或极端天气风险评估的需求。

Method: 提出WindSR扩散模型，采用动态半径融合方法将观测数据与模拟场结合，在地形信息引导下进行超分辨率降尺度，使用数据同化技术减少模型偏差。

Result: 与卷积神经网络和生成对抗网络基线相比，WindSR在降尺度效率和准确性方面表现更优，数据同化使模型偏差相对于独立观测减少了约20%。

Conclusion: WindSR通过融合观测和模拟数据，结合地形信息，成功实现了轮毂高度风速的高质量超分辨率降尺度，为风能应用提供了更可靠的数据支持。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [185] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: 该论文通过机制可解释性方法，首次提供了因果证据表明Transformer模型中的记忆和推理能力依赖于可分离但相互作用的神经回路。


<details>
  <summary>Details</summary>
Motivation: 区分记忆和推理能力对于预测模型泛化、设计针对性评估以及构建更安全的干预措施至关重要，但目前尚不清楚这两种能力是否依赖不同的内部机制。

Method: 使用合成语言谜题的受控数据集，结合激活修补和结构化消融技术，在层、头和神经元级别探测Transformer模型，因果测量各组件对每种任务类型的贡献。

Result: 在Qwen和LLaMA两个模型系列中发现，对特定层和注意力头的干预会导致选择性损伤：禁用识别的"记忆回路"使事实检索准确率降低达15%但推理能力保持完整，而禁用"推理回路"则使多步推理能力降低类似幅度。

Conclusion: 研究结果将回路级结构与功能专业化联系起来，展示了受控数据集和因果干预如何提供对模型认知的机制性见解，为大型语言模型的安全部署提供信息。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [186] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: 提出了一种结合低频同步和梯度动量压缩的方法，通过离散余弦变换将Nesterov动量分解为高低频分量，仅同步高频分量，大幅减少分布式训练中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 减少对高带宽互联的依赖，使分布式计算资源能够替代集中式数据中心训练大型模型。

Method: 将优化器动量视为信号，通过离散余弦变换将Nesterov动量分解为高低频分量，每H步仅同步高频分量，结合低频同步和梯度动量压缩。

Result: 相比基线DiLoCo实现了高达16倍的通信减少，在基于transformer的语言模型和卷积神经网络上均表现良好。

Conclusion: 这项工作推进了在低带宽互联的分布式节点上训练大型模型的可行性。

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [187] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 提出了一种新的无数据知识蒸馏方法CPSC-DFKD，通过条件生成对抗网络合成类别特定的多样化图像，改进生成器模块以区分不同类别分布，并基于师生视图提出伪监督对比学习来增强多样性。


<details>
  <summary>Details</summary>
Motivation: 当前无数据知识蒸馏方法存在三个主要问题：缺乏伪监督学习范式、无法区分不同类别样本分布导致产生模糊样本、无法优化类别多样性样本阻碍学生模型学习。

Method: 使用条件生成对抗网络合成类别特定的多样化图像；改进生成器模块以区分不同类别分布；提出基于师生视图的伪监督对比学习来增强多样性。

Result: 在三个常用数据集上的综合实验验证了CPSC-DFKD对学生模型和生成器性能的提升。

Conclusion: CPSC-DFKD通过条件伪监督对比学习有效解决了当前无数据知识蒸馏方法的局限性，提升了学生模型和生成器的性能。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [188] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文评估了现有聚类联邦学习算法在数量倾斜非IID数据下的表现，并提出了一种新的迭代算法CORNFLQS，该算法在准确率和聚类质量方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的非IID数据特别是数量倾斜问题是一个关键挑战，现有聚类联邦学习方法缺乏对此问题的系统评估。

Method: 提出CORNFLQS算法，将客户端选择最小化本地训练损失的策略与服务器基于模型相似性分组的策略进行最优协调。

Result: 在6个图像分类数据集上的270个非IID配置实验中，CORNFLQS在准确率和聚类质量方面获得最高平均排名，对数量倾斜扰动具有强鲁棒性。

Conclusion: CORNFLQS算法优于现有的聚类联邦学习算法，能够有效应对数量倾斜问题。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [189] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: 提出STDAE框架，通过跨模态重建预训练解决高速公路匝道缺乏实时检测器的问题，在预测阶段与GWNet等模型结合提升精度


<details>
  <summary>Details</summary>
Motivation: 高速公路互通立交是车辆转换的关键节点，但缺乏实时匝道检测器导致交通预测存在盲区

Method: 采用时空解耦自编码器两阶段框架：第一阶段从主线数据重建历史匝道流量，第二阶段将学习到的表示与GWNet等模型集成

Result: 在三个真实世界互通数据集上，STDAE-GWNET持续优于13个最先进的基线，性能接近使用历史匝道数据的模型

Conclusion: STDAE能有效克服检测器稀缺问题，具有即插即用潜力，适用于多种预测流程

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [190] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 使用可验证奖励的强化学习（RLVR）训练韩语接龙游戏，通过课程学习缓解规则奖励冲突


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在多语言逻辑谜题中的应用，特别是韩语接龙游戏中规则奖励的自然冲突问题

Method: 在韩语接龙游戏中应用RLVR，采用课程学习方案来缓解规则奖励之间的冲突

Result: 实验证明课程学习能够有效缓解规则奖励冲突

Conclusion: 研究结果鼓励在更多不同语言的谜题任务中开展进一步研究

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [191] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 该研究探讨了深度学习训练算法的可靠性和可复现性，特别关注物理信息损失函数在训练网络执行边界条件时的稳定性，通过Pix2Pix网络预测高弹性对比复合材料应力场的案例研究，分析了不同损失函数的收敛性和准确性变化。


<details>
  <summary>Details</summary>
Motivation: 随着物理信息损失函数的流行，需要评估损失函数在训练网络执行特定边界条件时的可靠性，报告模型变化有助于公平比较不同方法。

Method: 使用Pix2Pix网络预测高弹性对比复合材料的应力场，实施多种不同的应力平衡损失函数，并在多次训练会话中评估其收敛性、准确性和应力平衡执行情况。

Result: 不同的损失函数在收敛性、准确性和应力平衡执行方面显示出不同程度的变异性，某些损失函数比其他函数更稳定。

Conclusion: 报告模型变化对于评估损失函数一致训练网络的能力至关重要，并提供了更公平的方法比较，建议在研究中报告模型变异性。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [192] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 提出了多任务神经扩散过程（MT-NDP）框架用于风电功率预测，通过任务编码器捕捉跨风机相关性，并在真实SCADA数据上验证了其优于单任务NDP和高斯过程的表现。


<details>
  <summary>Details</summary>
Motivation: 风电功率预测的不确定性感知对于电网集成和风电场可靠运行至关重要，需要能够提供校准且可扩展预测的方法。

Method: 扩展神经扩散过程（NDPs）到多任务框架MT-NDP，引入任务编码器捕捉跨风机相关性，支持对未见风机的少样本适应。

Result: MT-NDP在点准确性和校准方面优于单任务NDP和高斯过程，特别对于偏离平均行为的风机表现更好，提供更尖锐但可信的预测区间。

Conclusion: 基于NDP的模型提供校准且可扩展的预测，适合实际部署，能够支持现代风电场的调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [193] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: 提出了一种内存高效的移动设备反向传播实现方法MeBP，能在内存受限的设备上微调大语言模型，比零阶优化方法收敛更快且性能更好。


<details>
  <summary>Details</summary>
Motivation: 移动设备上微调大语言模型时，传统反向传播方法内存消耗过大，而零阶优化方法虽然内存占用少但收敛速度极慢，需要更好的权衡方案。

Method: 开发了内存高效的反向传播实现MeBP，通过优化内存管理来减少内存占用，同时保持反向传播的计算效率。

Result: 在iPhone 15 Pro Max上验证，0.5B到4B参数的LLM可以在1GB内存内完成微调，比零阶优化收敛更快且性能更好。

Conclusion: MeBP为移动设备上的LLM微调提供了更好的内存-计算权衡，使资源受限设备上的模型微调变得可行。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [194] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出了广义数量级(GOOMs)方法，扩展传统数量级概念，支持在更大动态范围上进行稳定数值计算，避免数值下溢/上溢问题，并通过并行前缀扫描实现在GPU等硬件上的高效执行。


<details>
  <summary>Details</summary>
Motivation: 深度学习、金融等领域需要在长序列上对实数进行复合计算，传统浮点数方法容易导致灾难性的数值下溢或上溢问题，需要一种能在更大动态范围内稳定计算的方法。

Method: 引入广义数量级(GOOMs)概念，扩展传统数量级表示，将浮点数作为特例包含其中；实现高效的自定义并行前缀扫描算法，支持在GPU等并行硬件上原生执行。

Result: 通过三个代表性实验验证GOOMs优于传统方法：1) 超越标准浮点数限制的实数矩阵乘积复合；2) 并行估计Lyapunov指数谱，比之前方法快几个数量级；3) 在深度循环神经网络中捕获长程依赖关系，无需任何稳定化处理。

Conclusion: GOOMs结合高效并行扫描为高动态范围应用提供了可扩展且数值鲁棒的替代方案，使之前被认为不切实际或不可能的计算变得可行和实用。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [195] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: 提出LHGEL框架，通过批量采样、批量视图聚合、残差注意力和多样性正则化来解决大规模异构图学习中的挑战，在五个真实异质网络上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模异构图学习面临网络规模大、节点和边类型异构、节点特征变化以及复杂局部邻域结构等挑战，集成学习能够通过不同采样条件捕获图的异构性。

Method: LHGEL框架包含三个关键组件：批量视图聚合（采样子图形成多个图视图）、残差注意力（自适应加权视图贡献以指导节点嵌入）、多样性正则化（促进不同视图嵌入矩阵的表示差异）。

Result: 理论研究表明残差注意力缓解了集成学习中的梯度消失问题。在五个真实异质网络上的实验结果表明，LHGEL方法显著优于现有最先进方法。

Conclusion: LHGEL通过集成学习框架有效解决了大规模异构图学习的关键挑战，在准确性和鲁棒性方面表现出色。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [196] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 该论文研究了核变点检测在文本分割中的应用，建立了在m-依赖数据下的理论保证，并通过LLM模拟和实证研究验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的核变点检测理论主要基于独立性假设，而真实世界的序列数据（如文本）具有强依赖性，需要建立更符合实际的理论保证。

Method: 建立了m-依赖数据下的核变点检测理论保证，使用LLM生成合成m-依赖文本进行渐近验证，并在多个文本数据集上进行了全面的实证研究。

Result: 证明了在m-依赖数据下变点检测数量和位置的弱一致性，实证研究表明使用文本嵌入的KCPD在标准文本分割指标上优于基线方法。

Conclusion: KCPD不仅具有理论可靠性和模拟验证，在实际文本分割任务中也表现出色，特别是在Taylor Swift推文的案例研究中展示了实用效果。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [197] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 该论文提出使用结构化论证方法为AI系统提供可验证的解释，而不是追求机制透明性。通过将LLM文本转换为论证图，在推理的每一步实现验证，并在多智能体风险评估中展示了该方法。


<details>
  <summary>Details</summary>
Motivation: 人类思维是黑盒，但社会通过可验证的论证来运作。AI可解释性应遵循这一原则，为利益相关者提供可验证的推理链，而非机制透明性。

Method: 使用结构化论证方法，将LLM文本转换为论证图，采用双极假设基础论证来捕捉支持/攻击关系，实现自动幻觉检测。提供无需重新训练的测试时反馈迭代优化机制。

Result: 在AAEC数据集上达到94.44宏F1（比先前工作高5.7分），在Argumentative MicroTexts关系分类上达到0.81宏F1（比先前结果高约0.07）。

Conclusion: 结构化论证方法能够提供可验证的解释，优于可解释性和LLM生成解释，在多智能体风险评估中有效应用，并提供易于部署的Docker容器和Python包。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [198] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 本文通过残差展开定理证明了深度残差网络在数学上等价于一个隐式集成模型，网络深度的增加相当于扩大这个集成模型的规模。该分析揭示了残差网络中的组合爆炸现象，解释了归一化层的历史必要性，并为SkipInit、Fixup等无归一化技术提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 深度残差架构（如ResNet和Transformer）能够实现前所未有的深度，但为什么深度如此有效的正式理解仍然是一个开放问题。本文旨在从数学上验证残差网络表现为浅层模型集成体的直觉观点。

Method: 提出了残差展开定理，通过显式解析公式证明网络深度的增加在数学上等价于扩大隐式集成模型的规模。该分析揭示了残差网络中的层次化集成结构和组合爆炸现象。

Result: 发现残差网络确实表现为浅层模型的隐式集成体，网络深度的增加会导致输出信号的组合爆炸增长，这解释了归一化层的历史必要性。同时证明了缩放每个残差模块可以控制这种组合爆炸。

Conclusion: 残差展开定理为理解深度残差网络的工作原理提供了理论基础，解释了归一化层的作用机制，并为无归一化技术提供了直接来自网络功能结构的解释。缩放残差模块不仅控制组合爆炸，还作为容量控制机制隐式正则化模型复杂度。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [199] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: 本文对Adam优化器的动量参数β₁和β₂进行了更全面的理论分析，突破了以往需要β₁=√β₂的限制，证明了在β₁≥√β₂和β₁≤√β₂两种情况下的收敛性，并揭示了参数设置与对抗类型的关系。


<details>
  <summary>Details</summary>
Motivation: Adam优化器虽然在大规模机器学习训练中非常有效，但其动量参数β₁和β₂的最优设置理论理解仍不完整。现有分析要求β₁=√β₂，无法覆盖实际应用中β₁≠√β₂的情况。

Method: 将Adam视为在线学习中最重要的算法类别之一——跟随正则化领导者(FTRL)的实例，推导出更一般的分析框架，适用于β₁≥√β₂和β₁≤√β₂两种情况。

Result: 新分析严格推广了现有边界，证明了在最坏情况下边界是紧的，并发现β₁=√β₂对于非自适应对抗者是最优的，但对于自适应对抗者是次优的。

Conclusion: 本研究为Adam优化器的动量参数设置提供了更全面的理论指导，揭示了参数选择与对抗类型的关系，为实际应用中的参数调优提供了理论依据。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [200] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: RADF是一个统一框架，解决大规模分布式系统中异常检测的三个主要挑战：数据量大、时间序列数据集异质性、异常根因定位困难。该框架采用mSelect技术自动选择算法和调参，并包含后检测能力以加速根因分析。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中的异常检测面临三大挑战：海量数据处理、时间序列数据集的异质性（需要为不同用例手动调优）、以及异常根因定位困难。现有系统难以同时解决这些问题。

Method: 提出RADF框架，采用mSelect技术自动为每个用例选择算法和调优超参数，实现实时异常检测，并集成后检测能力以加速根因分析。

Result: 在9个公共基准数据集中，RADF在5个数据集上的AUC性能超过最先进模型，其中7个数据集的AUC超过0.85，这是其他模型无法达到的成就。

Conclusion: RADF框架通过自动化算法选择和参数调优，有效解决了大规模分布式系统中的异常检测挑战，在多个数据集上表现出优于现有方法的性能。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [201] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文研究了有限时域离线强化学习中的策略评估和策略优化问题，在轨迹数据和qπ-可实现性假设下，提出了统计高效的策略评估学习器，并改进了策略优化的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在仅假设数据覆盖性和qπ-可实现性的情况下，策略评估和策略优化都无法实现统计高效学习。最近的研究在轨迹数据假设下实现了策略优化的统计高效学习，但策略评估问题仍未解决。

Method: 在轨迹数据和qπ-可实现性假设下，开发了统计高效的策略评估学习器，并对现有策略优化学习器的样本复杂度进行了更严格的分析。

Result: 成功实现了策略评估的统计高效学习，并显著改进了策略优化的样本复杂度界限。

Conclusion: 在轨迹数据假设下，离线强化学习中的策略评估和策略优化都可以实现统计高效学习，为实际应用提供了理论保证。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [202] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC是一种新的无模型强化学习算法，通过结合扩散策略和分布评论家，在18个困难RL任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决传统策略梯度方法方差高和通过时间反向传播复杂的问题，设计更稳定的在线学习算法

Method: 使用避免高方差策略梯度的策略改进目标，结合分布RL和裁剪双Q学习的鲁棒分布评论家

Result: 在Humanoid、Dog、Shadow Hand等18个困难RL任务中达到最先进性能，涵盖密集奖励和目标条件RL场景

Conclusion: D2AC算法在标准基准和生物启发的捕食者-猎物任务中都表现出强大的行为鲁棒性和泛化能力

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [203] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出任务级对比性概念，通过任务增强和任务级对比损失来改进小样本分类和元学习方法，在MetaDataset基准测试中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有小样本分类和元学习方法通常局限于单一数据集，难以在不同领域间泛化，存在准确率低、计算成本高、依赖限制性假设等问题。

Method: 引入任务增强的简单定义方式，设计任务级对比损失函数，鼓励任务表示的无监督聚类，该方法轻量级且易于集成到现有算法中。

Result: 在MetaDataset基准测试中表现出色，实现了更好的泛化能力和计算效率，且无需任务领域的先验知识。

Conclusion: 任务级对比性方法有效解决了现有方法的局限性，显著提升了小样本学习和元学习的性能，同时保持了简单性和可集成性。

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [204] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 提出基于联邦学习的轻量级隐私保护物联网僵尸网络检测框架，通过分布式协作训练实现高检测精度，同时大幅降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展带来创新机遇，但也增加了僵尸网络网络攻击风险。传统检测方法在资源受限的物联网环境中面临可扩展性、隐私保护和适应性挑战。

Method: 采用联邦学习方法，使分布式设备能够协作训练模型而无需交换原始数据，同时引入通信高效的聚合策略来减少开销。

Result: 在基准物联网僵尸网络数据集上的实验表明，该框架实现了高检测精度，同时显著降低了通信成本。

Conclusion: 联邦学习为物联网生态系统提供了一条实用路径，可实现可扩展、安全且隐私感知的入侵检测。

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [205] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出RAPID算法，通过大批次推理和小组策略梯度更新，将强化学习训练时间减少11%-34%，同时保持或提升准确性


<details>
  <summary>Details</summary>
Motivation: 强化学习在微调小语言模型时资源消耗大、训练时间长，需要优化计算效率

Method: 采用大批次推理和小组策略梯度更新，结合组优势估计和重要性加权估计器来纠正策略外学习的偏差

Result: 在三个基准测试中，相比最先进的RL算法，运行时间减少11%-34%，准确性相似或更好

Conclusion: RAPID算法能显著提升强化学习训练效率，为小语言模型的微调提供了更实用的解决方案

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [206] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: CS-RLHF提出了一种可认证的安全RLHF方法，通过基于惩罚的公式替代传统的拉格朗日方法，无需双变量更新，提供可证明的安全保证，并在对抗性攻击下表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于CMDP的方法存在两个主要问题：1) 性能高度依赖评分机制，需要捕捉语义而非表面关键词；2) 双变量调优计算昂贵且无法提供可证明的安全保证，易受对抗性越狱攻击。

Method: 引入CS-RLHF，训练大规模语料库上的成本模型分配语义基础的安全分数，采用基于惩罚的公式设计，借鉴约束优化中的精确惩罚函数理论，通过适当缩放的惩罚项直接强制执行约束满足。

Result: 实证评估显示CS-RLHF优于最先进的LLM模型响应，在正常和越狱提示下效率至少提高5倍。

Conclusion: CS-RLHF通过消除双变量更新的需求，提供可证明的安全保证，并在对抗性攻击下表现出更强的鲁棒性，为LLM安全提供了更有效的解决方案。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [207] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出多阶段LaSDI框架，通过顺序学习额外解码器来修正残差误差，提高重建和预测精度，在1D-1V Vlasov方程上优于标准LaSDI。


<details>
  <summary>Details</summary>
Motivation: 传统LaSDI在训练中强制实施潜在动力学可能会损害模型对仿真数据的重建精度，需要改进重建和预测准确性。

Method: 多阶段LaSDI框架，顺序学习额外的解码器来修正前阶段的残差误差，提高重建和预测精度。

Result: 在1D-1V Vlasov方程上，mLaSDI始终优于标准LaSDI，实现了更低的预测误差和更短的训练时间，适用于多种架构。

Conclusion: 多阶段LaSDI框架通过残差修正有效提高了重建和预测精度，在偏微分方程求解中具有优越性能。

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [208] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: CrossLag是一种环境感知注意力机制，能够将外生数据中重大事件背后的滞后内生信号整合到transformer架构中，显著提升了登革热主要爆发的检测和预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以预测需要及时公共卫生预警的主要登革热爆发，因为爆发通常滞后于气候和海洋异常的重大变化。

Method: 提出CrossLag注意力机制，在TimeXer transformer基础上，以低参数成本整合外生数据中滞后内生信号。

Result: 在24周预测窗口内，该模型在新加坡登革热数据上检测和预测主要爆发的性能显著优于TimeXer基线。

Conclusion: CrossLag通过有效整合滞后环境信号，能够更好地预测登革热主要爆发，为公共卫生预警提供支持。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [209] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 提出一种统一的方法，通过最小化权重干预来同时实现敏感信息遗忘和对抗越狱攻击的鲁棒性，无需依赖分类器，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要确保隐私保护和安全生成，特别是敏感信息遗忘和对抗越狱攻击的鲁棒性。

Method: 采用约束优化方法，通过最小化权重干预使特定词汇集不可达或将权重转移到更安全区域，统一处理遗忘和鲁棒性问题。

Result: 提出的简单点式约束干预方法在性能上优于最大最小干预，计算成本更低，且优于现有最先进的防御方法。

Conclusion: 该方法有效统一了敏感信息遗忘和对抗攻击鲁棒性，无需分类器，计算效率高，性能优越。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [210] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出IMMFM框架，通过分段二次插值路径学习连续随机动力学，联合优化漂移和数据驱动扩散系数，解决稀疏采样高维轨迹的学习问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型在处理稀疏采样和高维轨迹时通常只能学习成对转移，难以捕捉多个观测时间点的联合一致性。

Method: 使用分段二次插值路径作为流匹配的平滑目标，联合优化漂移和扩散系数，并基于理论条件确保稳定学习。

Result: 在合成基准和真实神经影像数据集上，IMMFM在预测准确性和下游任务中优于现有方法。

Conclusion: IMMFM能够捕捉内在随机性，处理不规则稀疏采样，并生成特定主体的轨迹，为序列数据建模提供了有效解决方案。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [211] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统评估了不同图神经网络架构在RNN+GNN管道模型中的性能，发现RGATv2在配电网故障诊断中具有最佳泛化能力，F1分数仅下降约12%，而纯RNN模型下降高达60%。


<details>
  <summary>Details</summary>
Motivation: 配电网故障检测对系统可靠性至关重要，但现有方法需要适应电网拓扑变化。当前最先进方法使用RNN+GNN管道，但需要系统评估更先进的GNN架构在故障诊断中的表现。

Method: 提出在RNN+GNN管道中使用GraphSAGE和Graph Attention(GAT, GATv2)进行故障诊断，并与RGCN和纯RNN模型(特别是GRU)进行综合基准测试，特别关注在不同拓扑设置下的泛化能力。

Result: 在IEEE 123节点配电网上的实验结果显示，RGATv2具有最佳泛化能力，F1分数仅下降约12%，而纯RNN模型下降高达60%，其他RGNN变体性能下降也达25%。

Conclusion: RGATv2在配电网故障诊断中表现出卓越的泛化能力，是应对电网拓扑变化的有效解决方案。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [212] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 提出了两种高效的测试时扩展策略TTAug和TTAdapt，利用模型内部特征而非外部监督来提升小型视觉语言模型的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型计算效率高但泛化能力弱，现有测试时扩展方法计算需求大，与小型模型资源高效的设计目标相矛盾。

Method: TTAug通过生成多个增强输入并在token级别聚合输出，无需参数更新；TTAdapt使用TTAug生成的基于共识的伪标签在推理时调整模型参数。

Result: 在九个基准测试中表现出一致的性能提升，同时保持适合资源受限环境的计算效率，方法在不同规模模型和不同VLM间具有通用性。

Conclusion: 提出的测试时扩展策略有效解决了小型VLM性能不足的问题，在保持计算效率的同时显著提升了模型性能。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [213] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出了边界条件保证的进化Kolmogorov-Arnold网络(BEKAN)，通过三种可组合的方法精确执行Dirichlet、周期性和Neumann边界条件，在PDE求解中优于MLP和B样条KAN。


<details>
  <summary>Details</summary>
Motivation: 深度学习在求解PDEs方面受到关注，但神经网络的黑盒特性阻碍了边界条件的精确执行，需要开发能够严格保证边界条件的网络架构。

Method: 使用高斯径向基函数构造单变量基函数近似解并在网络激活层编码边界信息；采用正弦函数构造周期层精确执行周期性边界条件；设计最小二乘公式引导参数进化满足Neumann条件。

Result: 在Dirichlet、Neumann、周期性和混合边界值问题上进行了广泛数值实验，BEKAN在精度上优于多层感知机和B样条KAN。

Conclusion: 所提方法增强了KANs在求解PDE问题同时满足边界条件的能力，促进了科学计算和工程应用的进步。

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [214] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出Latent Mixture of Symmetries (Latent MoS)模型，通过捕捉复杂动态测量中的对称性混合来学习动力学，使用分层架构保持长期等变性，在多种物理系统中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设单一全局对称群，并将对称性发现和动态学习分开处理，导致表达能力有限和误差累积。需要更有效的样本来学习工程系统的动力学。

Method: 提出Latent MoS模型，捕捉对称性主导的潜在因子混合，在局部可证明地保持底层对称变换。引入分层架构堆叠MoS块来捕获长期等变性。

Result: 在多种物理系统的数值实验中，Latent MoS在插值和外推任务中优于最先进的基线方法，并提供可解释的潜在表示。

Conclusion: Latent MoS通过对称性混合和分层架构有效学习复杂动力学，为未来的几何和安全关键分析提供可解释的表示。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [215] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: FieldFormer是一个基于Transformer的框架，用于从稀疏、噪声、不规则的时空传感器数据中重建物理场，结合数据驱动灵活性和物理约束，在三个基准测试中性能提升超过40%。


<details>
  <summary>Details</summary>
Motivation: 现有插值或学习方法在处理稀疏、噪声、不规则的时空传感器数据时存在局限，要么忽略控制PDE，要么无法扩展。

Method: 使用可学习的速度缩放距离度量构建局部邻域，通过每批次偏移重新计算高效构建邻域，采用期望最大化风格细化，通过局部Transformer编码器进行预测，并通过自动微分PDE残差和边界特定惩罚强制执行物理一致性。

Result: 在三个基准测试（标量各向异性热方程、矢量值浅水系统、现实平流-扩散污染模拟）中，FieldFormer始终优于强基线超过40%，能够从稀疏（0.4%-2%）和噪声（10%）数据中实现准确（RMSE<10^-2）、高效且物理一致的场重建。

Conclusion: FieldFormer实现了从稀疏噪声数据中准确、高效且物理一致的场重建，证明了结合数据驱动灵活性与物理约束的有效性。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [216] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出基于虚拟信息素的S-MADRL框架，通过课程学习解决多机器人在狭窄环境中的去中心化协调问题，实现8个机器人的有效自组织。


<details>
  <summary>Details</summary>
Motivation: 解决狭窄拥挤环境中多机器人协调的拥堵和干扰问题，受昆虫群体通过信息素实现鲁棒协调的启发。

Method: 采用基于虚拟信息素的Stigmergic多智能体深度强化学习框架，结合课程学习将复杂任务分解为渐进式子问题。

Result: 在8个智能体的仿真中实现了最有效的协调，机器人自组织形成不对称工作负载分布，减少拥堵并调节群体性能。

Conclusion: 该框架为通信受限的拥挤环境提供了可扩展的去中心化多智能体协调解决方案，展现出类似自然界的涌现行为。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [217] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出多层移动边缘计算（MLMEC）框架，通过知识蒸馏（KD）方法在多个计算站之间分配神经网络模型，以平衡跌倒检测系统的准确性和延迟。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化人口增加，跌倒检测系统的重要性日益凸显。传统边缘设备模型大小有限，而云端计算存在数据传输延迟问题。移动边缘计算（MEC）被探索来解决这些挑战。

Method: 设计多层MEC框架，将架构分成多个计算站，每个站配备神经网络模型。如果前端设备无法可靠检测跌倒，数据将传输到具有更强后端计算能力的站。采用知识蒸馏方法，让高功率后端站提供额外学习经验，提高前端检测精度。

Result: 在SisFall数据集上，KD方法使准确率提高11.65%；在FallAllD数据集上提高2.78%。与无KD的MLMEC相比，带KD的MLMEC在FallAllD数据集上数据延迟率降低54.15%，在SisFall数据集上降低46.67%。

Conclusion: MLMEC跌倒检测系统在准确性和延迟方面都得到了改善，通过分层计算架构和知识蒸馏技术有效平衡了系统性能。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [218] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: 本文对涡轮风扇发动机剩余使用寿命预测中的领域自适应技术进行了全面综述，提出了专门针对涡轮风扇发动机的新分类法，并评估了相关技术在实际数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 涡轮风扇发动机RUL预测在预测性维护中至关重要，但传统数据驱动方法面临数据有限和分布偏移的挑战。领域自适应技术能够从数据丰富的源域向数据稀缺的目标域转移知识，因此需要专门针对涡轮风扇发动机特性的综述研究。

Method: 提出了专门针对涡轮风扇发动机的领域自适应技术分类法：基于方法的分类（如何应用DA）、基于对齐的分类（分布偏移发生位置）、基于问题的分类（为何需要特定适应）。同时评估了选定DA技术在涡轮风扇发动机数据集上的表现。

Result: 开发了一个多维度的分类框架，超越了传统分类方法，考虑了涡轮风扇发动机数据的独特特性和应用DA技术的标准流程。为从业者提供了实用见解并识别了关键挑战。

Conclusion: 该综述为涡轮风扇发动机RUL预测的领域自适应技术提供了系统分析，识别了未来研究方向，有助于推动该领域更有效DA技术的发展。

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [219] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM是一种专注于逃离局部最小值寻找全局最小值的优化器，通过确定性探索状态空间来逃离极小值，消除了随机梯度更新的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统ADAM优化器在逃离局部最小值方面存在局限性，作者希望开发一种能够有效逃离局部极小值并找到全局最小值的优化算法。

Method: 首先推导ADAM优化器步长的解析近似，然后提出Hill-ADAM算法，该算法在误差最小化和最大化之间交替进行，通过最大化逃离局部最小值，再通过最小化收敛。

Result: Hill-ADAM在5个损失函数和12个图像颜色校正实例上进行了测试，表现出良好的性能。

Conclusion: Hill-ADAM通过交替最小化和最大化的策略，能够有效探索损失空间并找到全局最小值，优于传统ADAM优化器在逃离局部最小值方面的表现。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [220] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: 提出神经贝叶斯滤波（NBF）算法，用于在部分可观测系统中维护隐藏状态的分布（信念）。NBF结合了经典滤波器的计算效率和深度生成模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决在部分可观测系统中跟踪快速变化、多模态信念的问题，同时缓解粒子贫化风险。

Method: 训练NBF找到任务诱导信念的良好潜在表示，将信念映射到固定长度的嵌入向量，这些向量条件化生成模型进行采样。在滤波过程中，使用粒子式更新在嵌入空间中计算后验分布。

Result: 在三个部分可观测环境中的状态估计任务中验证了NBF的有效性。

Conclusion: NBF成功结合了经典滤波器的计算效率和深度生成模型的表达能力，能够跟踪快速变化的多模态信念。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [221] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 使用Llama 3.1-8B-Instruct预处理推文数据，结合三种情感分析方法提取情感特征，与历史股价数据一起训练LSTM模型，显著提升了股票价格大幅波动的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 由于股票市场固有的波动性和对投资者情绪的敏感性，准确预测短期股价变动仍然具有挑战性。本文旨在通过整合从推文数据中提取的情感特征和历史股价信息来提高预测准确性。

Method: 利用Meta的Llama 3.1-8B-Instruct模型预处理推文数据，通过三种情感分析方法（基于transformer的DistilRoBERTa分类器和两种基于词典的NRC方法）提取情感特征，将这些特征与前一日股价数据结合训练LSTM模型。

Result: 在TSLA、AAPL和AMZN股票上的实验结果显示，所有三种情感分析方法都提高了预测股价大幅波动的平均准确率。基线模型（仅使用历史股价）准确率为13.5%，基于DistilRoBERTa的股票预测模型表现最佳，在使用LLaMA增强情感分析后准确率从23.6%提升至38.5%。

Conclusion: 使用大型语言模型预处理推文内容能够增强情感分析的有效性，进而提高预测股票价格大幅波动的准确性。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [222] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 本研究探讨了在公共卫生情感分析中，大型语言模型的上下文学习如何受到数据投毒攻击的破坏。通过引入微小对抗性扰动，导致情感标签翻转率高达67%。采用谱签名防御方法成功保护数据集完整性，使ICL准确率保持在46.7%，逻辑回归验证达到100%准确率。


<details>
  <summary>Details</summary>
Motivation: 将先前关于ICL投毒的理论研究扩展到公共卫生话语分析这一实际高风险场景，强调LLM部署中的风险和防御潜力，特别是在健康相关社交媒体监控中AI系统的可靠性问题。

Method: 在人类偏肺病毒推文数据中引入三种微小对抗性扰动：同义词替换、否定插入和随机扰动。应用谱签名防御方法来过滤投毒样本，同时保持数据的含义和情感不变。

Result: 即使微小扰动也能造成重大破坏，情感标签翻转率高达67%。防御后，ICL准确率稳定在46.7%左右，逻辑回归验证达到100%准确率，成功保护了数据集完整性。

Conclusion: 研究揭示了ICL在攻击下的脆弱性，同时证明了谱签名防御在使AI系统对健康相关社交媒体监控更加可靠方面的价值，为LLM在公共卫生领域的稳健部署提供了重要见解。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [223] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 隐式模型通过迭代单个参数块到固定点来计算输出，实现无限深度、权重共享的网络，训练时内存需求低。研究表明这些紧凑模型通过增加测试时计算可以匹配甚至超越更大显式网络的性能。


<details>
  <summary>Details</summary>
Motivation: 理解隐式模型如何通过增加测试时计算来提升表达能力，填补对这一机制理解不足的空白。

Method: 通过非参数分析研究表达能力，证明简单的隐式算子通过迭代可以渐进表达更复杂的映射，其表达能力随测试时计算量扩展。

Result: 理论验证表明，随着测试时迭代次数增加，学习到的映射复杂度上升，同时解质量和稳定性同步提升。在图像重建、科学计算和运筹学三个领域得到验证。

Conclusion: 隐式模型的表达能力可以随测试时计算量扩展，最终匹配更丰富的函数类，这解释了为什么紧凑模型通过更多计算可以超越更大显式网络。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [224] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的深部脑刺激方法，使用可体内测量的脑活动数据来调节刺激参数，相比传统临床方法能更好地抑制帕金森病生物标志物。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的DBS方法依赖于只能在脑芯片模拟中测量的生物标志物，无法在真实患者中应用。需要开发使用可体内测量脑活动数据的自适应DBS方法。

Method: 使用TD3强化学习算法，在基底神经节脑区模型上训练RL智能体，根据可测量的脑活动自适应调节刺激频率和幅度参数。

Result: 该方法相比现代临床DBS实现能更有效地抑制与PD严重程度相关的生物标志物，性能优于标准临床方法。

Conclusion: 这种方法为训练针对个体患者需求的个性化RL智能体开辟了可能性，使用可在真实环境中测量的信息实现更好的PD治疗。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [225] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出了一种基于脉冲神经网络的设备端少样本类增量学习方法SAFA-SNN，通过稀疏感知和快速自适应机制解决资源受限环境下的持续学习问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要持续学习新类别以保护数据隐私和适应动态环境，但在数据样本不足的情况下，现有基于人工神经网络的方法受限于设备资源。脉冲神经网络具有低能耗、生物合理性和与神经形态硬件兼容的优势。

Method: 提出SAFA-SNN方法：1）稀疏条件神经元动态，大部分神经元保持稳定，部分保持活跃以缓解灾难性遗忘；2）使用零阶优化解决脉冲不可微分的梯度估计问题；3）通过子空间投影增强新类别的可区分性，缓解对新类别的过拟合。

Result: 在两个标准基准数据集（CIFAR100和Mini-ImageNet）和三个神经形态数据集（CIFAR-10-DVS、DVS128gesture和N-Caltech101）上的实验表明，SAFA-SNN优于基线方法，在Mini-ImageNet的最后一个增量会话中至少提升4.01%，能耗比基线方法低20%。

Conclusion: SAFA-SNN为设备端少样本类增量学习提供了一种有效的解决方案，通过结合脉冲神经网络的优势和创新的学习机制，在保持性能的同时显著降低了能耗。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [226] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 使用LLM引导的进化程序合成方法自动发现高质量准蒙特卡洛构造，包括低星差异的有限点集和优化的Sobol'方向数，在多个维度上创造了新的最优记录。


<details>
  <summary>Details</summary>
Motivation: 解决准蒙特卡洛方法中长期存在的设计问题：构造低星差异的有限2D/3D点集和选择最小化随机QMC误差的Sobol'方向数。

Method: 采用两阶段程序：结合构造性代码提议和迭代数值优化的LLM引导进化循环，通过变异和选择代码在任务特定适应度下进行优化。

Result: 在2D情况下重新发现已知最优解并创造N≥40的新最佳基准，在3D中匹配已知最优解并报告改进的基准；在Sobol'参数优化中，相比广泛使用的Joe-Kuo参数，在32维期权定价任务中一致降低rQMC均方误差。

Conclusion: LLM驱动的进化程序合成可以自动化发现高质量QMC构造，在经典设计最优时恢复它们，在有限N结构重要时改进它们。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [227] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 该研究开发了一种基于AEMO电价预测的BESS交易算法，通过分析预测准确性模式来优化套利收益，并与无预测的基础算法进行性能对比。


<details>
  <summary>Details</summary>
Motivation: 随着电网波动性增加，将预测数据转化为实际BESS交易策略的需求日益迫切，但现有研究缺乏对AEMO预测可靠性的评估及其在BESS交易中的实际应用。

Method: 分析AEMO电价预测准确性模式（基于时间、预测周期和地区差异），构建预测驱动的BESS交易模型，并探索机器学习技术增强预测能力。

Result: 开发了新颖的基于预测的BESS交易算法，其性能优于无预测的基础交易算法，为能源市场交易模型提供了改进方向。

Conclusion: AEMO电价预测可以系统性地用于开发可靠且盈利的BESS交易策略，促进BESS更有效地融入市场运营。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [228] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAE）的可解释性与模型行为引导效用之间只有弱正相关（τb≈0.298），表明可解释性不能作为引导性能的有效代理指标。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器（SAE）的可解释性是否确实意味着更好的模型行为引导效用，因为现有方法都基于这一假设但缺乏实证支持。

Method: 在三个大语言模型（Gemma-2-2B、Qwen-2.5-3B、Gemma-2-9B）上训练90个SAE，涵盖五种架构和六个稀疏度级别，使用SAEBench和AxBench分别评估可解释性和引导效用，并通过Kendall秩相关系数进行一致性分析。

Result: 发现可解释性与引导效用之间只有弱正相关（τb≈0.298），提出的Delta Token Confidence特征选择方法比当前最佳方法提升52.52%的引导性能，且选择高Delta Token Confidence特征后，可解释性与效用的相关性消失（τb≈0）甚至变为负相关。

Conclusion: 可解释性与引导效用之间存在显著差异，最有效的引导特征往往不是最可解释的特征，Delta Token Confidence是更有效的特征选择标准。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [229] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 提出了一个数据最小化框架，通过优先级队列树搜索在隐私有序变换空间中寻找最优平衡点，量化在保持任务效用前提下最少的隐私信息泄露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费应用中的快速部署导致个人信息频繁交换，用户为获得有用响应往往过度分享信息，增加了通过记忆、上下文个性化或安全漏洞带来的隐私风险。

Method: 开发了正式定义和操作化数据最小化的框架，使用优先级队列树搜索在隐私有序变换空间中定位最优平衡点，并在四个数据集上评估了九个LLM的可实现数据最小化程度。

Result: 前沿大型LLM能容忍更强的数据最小化同时保持任务质量（GPT-5可删减85.7%，而Qwen2.5-0.5B仅19.3%），LLM难以直接预测最优数据最小化，存在偏向抽象的偏见导致过度分享。

Conclusion: 不仅存在隐私差距，还存在能力差距：模型可能缺乏对解决任务所需信息的认知意识。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [230] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 提出了Token Hidden Reward (THR)指标，通过量化每个token对正确响应概率的影响来指导强化学习中的探索与利用平衡。基于THR的重新加权算法可以显式地偏向探索或利用策略。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习提升了大型语言模型的推理能力，但如何显式控制训练过程中的探索与利用平衡仍是一个开放问题。

Method: 引入Token Hidden Reward (THR)指标，量化每个token在GRPO下对正确响应概率的影响。开发基于THR的重新加权算法，通过放大正THR token（偏向利用）或负THR token（偏向探索）来调节学习信号。

Result: 在多样化数学推理基准测试中验证了算法的有效性：放大正THR token提高了贪婪解码准确率（偏向利用），而放大负THR token则提升了Pass@K准确率（偏向探索）。算法与GSPO等其他RL目标兼容，并在Llama等架构上具有通用性。

Conclusion: THR为RL调优的LLMs提供了细粒度的探索与利用动态控制机制，为推理密集型应用中的目标微调提供了新工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [231] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 提出注意力采样器定义，利用重要性采样方法减少大规模注意力模型的计算负担，分析其理论有效性包括空间和更新时间。


<details>
  <summary>Details</summary>
Motivation: 解决大规模注意力模型在人工智能中的计算挑战，受经典ℓ2采样器和LLM注意力机制进展启发。

Method: 在流式设置中利用重要性采样方法，提出注意力采样器定义，替代传统注意力机制。

Result: 显著降低传统注意力机制的计算负担，框架具有可扩展性和广泛适用性。

Conclusion: 注意力采样器是解决大规模注意力模型计算挑战的有效方法，具有理论保证和实际应用价值。

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [232] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: 提出了Group Policy Gradient (GPG)，一种无需评论家的策略梯度估计器家族，用基于组的蒙特卡洛优势估计器替代学习值函数，在保持PPO结构的同时消除评论家训练成本。


<details>
  <summary>Details</summary>
Motivation: 受GRPO在人类反馈强化学习中成功启发，旨在消除训练评论家所需的内存、计算和超参数成本，同时保持PPO的剪裁目标结构。

Method: 使用基于组的蒙特卡洛优势估计器替代学习值函数，保留PPO的剪裁目标结构，无需训练评论家网络。

Result: GPG在标准基准测试中匹配或优于PPO，能更好利用并行模拟，计算资源使用效率更高。

Conclusion: GPG提供了一种高效的无评论家策略梯度方法，在保持性能的同时显著降低计算成本。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [233] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一个统一框架，显式建模图数据为图混合模型，利用图矩进行聚类，改进图对比学习和数据增强方法


<details>
  <summary>Details</summary>
Motivation: 现实图数据集通常包含来自多个不同分布的混合群体，但现有图表示学习方法往往忽略这种混合结构

Method: 使用图矩（模体密度）对来自相同生成模型的图进行聚类，提出图混合感知的Mixup(GMAM)和模型自适应图对比学习(MGCL)

Result: 在无监督学习中MGCL在8个数据集上获得最佳平均排名，在有监督学习中GMAM在7个数据集中的6个达到最新准确率

Conclusion: 显式建模图的混合结构能够有效改进图表示学习性能，特别是在数据增强和负采样策略方面

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [234] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出了REG优化器，通过使用行-列缩放(RACS)操作符替代Muon的矩阵符号函数，解决了Muon优化器在训练不稳定性和与AdamW预训练模型不兼容的问题，实现了更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有Muon优化器虽然通过正则化整个权重矩阵的梯度更新来平衡所有方向的梯度更新，但其依赖矩阵符号函数会导致训练不稳定，且与AdamW预训练模型不兼容。

Method: 提出REG优化器，用行-列缩放(RACS)操作符替代Muon的矩阵符号函数，该操作符基于矩阵平衡理论，以较温和的方式正则化更新步骤，实现更简单且与现有训练动态更兼容。

Result: 在LLM训练上的大量实验表明，REG优化器不仅比AdamW获得更好的性能和稳定性，而且与AdamW训练范式保持一致，特别是在微调阶段避免了Muon观察到的性能下降。

Conclusion: REG优化器通过RACS操作符有效解决了Muon的局限性，在保持与AdamW兼容性的同时实现了优越的训练性能和稳定性。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [235] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种基于谱滤波的线性强化学习方法，通过自适应正则化策略在保持可解释性的同时提升性能，在理论和实验中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注性能，依赖事后解释来提供可解释性。本文旨在设计一种既具有可解释性又性能增强的RL方法。

Method: 提出基于谱滤波的线性RL方法，扩展了岭回归方法，通过谱滤波函数阐明正则化在控制估计误差中的作用，并设计了基于偏差-方差权衡的自适应正则化参数选择策略。

Result: 理论分析建立了参数估计和泛化误差的近似最优界。在快手和淘宝的真实数据集上的实验表明，该方法在决策质量上优于或匹配现有基线方法。

Conclusion: 该方法有潜力弥合RL理论与实际决策之间的差距，在管理环境中提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [236] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: 提出PFPL方法解决联邦学习中的异构数据问题，通过构建个性化无偏原型和一致性正则化来提升模型性能并降低通信成本


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在异构场景下存在特征分布或标签分布倾斜问题，而数据异构性实际上是提升模型性能的关键因素

Method: PFPL方法为每个客户端构建个性化无偏原型，并在本地更新阶段引入一致性正则化来对齐本地实例与其个性化原型

Result: 在Digits和Office Caltech数据集上的实验验证了方法的有效性，并成功降低了通信成本

Conclusion: PFPL方法能够有效处理混合异构场景，提供更丰富的领域知识并改善模型收敛性能

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [237] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: 提出IniLoRA方法，通过改进LoRA的低秩矩阵初始化策略，使其更接近原始模型权重，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然参数效率高，但其初始化两个乘积为零的低秩矩阵限制了有效激活和利用原始模型权重的能力，存在性能瓶颈。

Method: 提出IniLoRA初始化策略，将低秩矩阵初始化为近似原始模型权重，并引入两个变体IniLoRA-α和IniLoRA-β使用不同的初始化方法。

Result: 实验结果表明IniLoRA在各种模型和任务上都比LoRA表现更好。

Conclusion: IniLoRA通过改进初始化策略有效解决了LoRA的性能限制，提供了更好的参数高效微调方法。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [238] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 该论文研究在部分反馈下审计分类器公平性的问题，提出了一种新颖的标注数据获取成本模型，并设计了在两种设置下的最优公平性审计算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，真实标签通常只在正分类个体中可用（如只有获批贷款申请人的还款结果可观测），且获取额外标注数据的成本需要考虑信用评估、贷款处理和潜在违约等实际因素。

Method: 在无数据分布假设的黑盒模型和指数族分布混合模型两种设置下，分别提出审计算法。黑盒设置中提出近似最优算法，混合模型设置中利用截断样本学习和MAP预言机设计新算法。

Result: 在黑盒设置中证明自然基线方法严格次优，在混合模型设置中审计成本显著低于黑盒情况。在真实数据集上的实验表明，算法比自然基线在审计成本上提升约50%。

Conclusion: 提出的公平性审计算法在部分反馈环境下更经济高效，适用于人口统计均等、机会均等和均等化几率等常见公平性指标，且对指数族混合模型的扩展具有独立研究价值。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [239] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: HydroFusion-LMF是一个用于小流域日径流预测的统一框架，通过可学习的趋势-季节-残差分解、异质专家集合和半监督多任务学习，在非平稳条件下实现更准确的十年尺度预测。


<details>
  <summary>Details</summary>
Motivation: 小流域的日径流预测面临信号混合（漂移趋势、多尺度季节周期、状态转变、稀疏极端值）的挑战，现有深度模型通常只针对单一方面且未充分利用未标记数据，限制了状态适应性。

Method: 采用可学习的趋势-季节-残差分解降低非平稳性；通过异质专家集合处理残差（线性细化、频率核、补丁Transformer、循环记忆、动态归一化注意力）；基于水文上下文感知门融合专家输出；使用半监督多任务目标增强监督。

Result: 在约10年日数据集上，MSE为1.0128，MAE为0.5818，比最强基线（DLinear）分别提升10.2%和10.3%，比平均基线分别提升24.6%和17.1%。

Conclusion: 该框架在可解释性（明确组件、稀疏门控）和性能之间取得平衡，推进了非平稳条件下标签高效的水文预测。

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [240] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 提出了NeuroLDS，首个基于机器学习的低差异序列生成框架，通过两阶段学习过程显著优于传统方法，并在多个应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统低差异点集构建方法主要依赖抽象代数和数论，而MPMC方法虽然能生成低差异点集但无法扩展到序列。为解决这一限制，需要开发能够生成低差异序列的机器学习方法。

Method: 采用两阶段学习过程：首先通过监督学习近似经典构造，然后通过无监督微调最小化前缀差异。训练神经网络将索引映射到点，使所有前缀序列都具有最小差异。

Result: NeuroLDS在所有低差异序列构造中显著优于先前方法，在数值积分、机器人运动规划和科学机器学习等应用中表现出有效性。

Conclusion: 神经低差异序列展示了机器学习在生成低差异序列方面的潜力和广泛意义，为相关应用提供了新的解决方案。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [241] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 提出了EvoEngineer框架，首次系统化地将LLM用于CUDA内核优化，在91个真实CUDA内核上实现了2.72倍的平均加速和69.8%的代码有效性。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化是AI性能的关键瓶颈，但现有LLM方法存在生态系统碎片化、问题定义不清晰以及无法满足严格正确性要求的问题。

Method: 首先形式化CUDA内核优化任务，然后建立EvoEngineer框架，提供设计优化策略的指导，在性能和正确性之间取得平衡。

Result: 在91个真实CUDA内核上测试，平均加速2.72倍，代码有效性69.8%，最大加速36.75倍，在56%的操作上实现了超过2倍的加速。

Conclusion: EvoEngineer框架在CUDA内核优化中实现了性能与正确性的原则性平衡，显著优于现有方法。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [242] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: 提出了MAGE框架，通过两阶段方法解决多目标生成控制问题：第一阶段动态构建鲁棒的基础模型，第二阶段将显式和隐式价值模型合并为统一指导代理来引导解码


<details>
  <summary>Details</summary>
Motivation: 现有方法存在不足：基于合并的方法在参数层面提供间接、次优的控制，而基于解码的指导需要聚合多个专家模型的logits，带来显著空间开销且依赖单个模型能力

Method: 两阶段框架：1）通过合并考虑多目标的骨干模型动态构建鲁棒基础模型；2）将显式和隐式价值模型合并为统一指导代理，引导基础模型的解码过程

Result: 经验验证了价值模型中的线性模式连接性，探索了模型合并与预测集成的关系，展示了增强的可控性。实验表明方法优于现有方法，实现了优越的可控性、帕累托最优性能和增强的适应性

Conclusion: MAGE框架通过模型合并和指导解码的结合，有效解决了多目标生成控制问题，提供了更直接和高效的控制方法

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [243] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 本文从理论角度分析了Transformer模型参数（注意力头和头维度）在不同层间的分配策略，揭示了软激活的饱和现象，并提出了平衡表达能力和效率的参数分配原则。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在多个领域取得了显著成功，但其模型效率的理论基础仍未被充分探索。本文旨在研究如何在各层间分配注意力头和头维度参数，以平衡模型的表达能力和计算效率。

Method: 首先从近似理论角度对早期层在信息提取中的作用进行数学分析，在固定参数预算下理论刻画了注意力头数量和头维度之间的权衡关系。同时发现并证明了软激活的饱和行为：持续增加头维度会导致学习误差的收益递减，特别是对于长序列。

Result: 理论和实验均支持软激活的饱和模式，表明后续层可以用更少的参数更高效地运行。基于这些发现，提出了跨Transformer层分配注意力头和维度的原则性策略。

Conclusion: 本文为Transformer架构的模型效率提供了理论基础，揭示了参数分配的关键原则，有助于设计更高效的Transformer模型。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [244] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 本文提出了针对重尾奖励的鲁棒批量多臂老虎机算法，发现在实例无关和Lipschitz设置中，重尾奖励需要更少的批次就能达到接近最优的遗憾，而在实例相关设置中所需批次数量与尾部厚度无关。


<details>
  <summary>Details</summary>
Motivation: 现有批量多臂老虎机研究主要假设轻尾奖励分布，但许多现实场景（如临床试验结果）表现出重尾特征，需要填补这一研究空白。

Method: 提出了针对重尾奖励的鲁棒批量老虎机算法，涵盖有限臂和Lipschitz连续设置。

Result: 揭示了令人惊讶的现象：在实例无关和Lipschitz设置中，更重的尾部奖励需要更少的批次就能达到接近最优的遗憾；而在实例相关设置中，所需批次数量与尾部厚度无关。

Conclusion: 重尾奖励在批量老虎机问题中表现出与直觉相反的特性，为实际应用（如临床试验）提供了新的算法设计思路。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [245] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出Curriculum-Augmented GFlowNets (CAGFN)，将课程学习与多目标GFlowNets结合，用于从头设计mRNA序列，通过长度渐进式课程提升训练效率和序列质量。


<details>
  <summary>Details</summary>
Motivation: mRNA序列设计面临巨大组合空间和稀疏长程奖励的挑战，现有GFlowNets训练困难，需要解决多目标权衡问题。

Method: 集成基于长度的课程学习，逐步调整最大序列长度，从易到难引导探索；提供新的mRNA设计环境，结合目标蛋白序列和生物目标训练模型。

Result: 在不同mRNA设计任务中，CAGFN改进了Pareto性能和生物合理性，保持多样性，比随机采样训练更快达到更高质量解，并能泛化到分布外序列。

Conclusion: CAGFN为治疗性序列设计中的GFlowNets应用提供了生物动机设置，显著提升了mRNA序列设计的效率和质量。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [246] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 提出了一种检测分段线性循环神经网络中稳定和不稳定流形的新算法，用于分析网络动力学特性，包括多稳态性和混沌行为。


<details>
  <summary>Details</summary>
Motivation: 理解训练好的RNN如何产生其行为对于科学和医学应用以及可解释AI很重要，而RNN的动力学特性取决于状态空间的拓扑和几何性质。

Method: 开发了一种检测分段线性RNN（使用ReLU激活函数）中稳定和不稳定流形的新算法，特别关注周期点的流形。

Result: 算法能够追踪不同吸引盆边界，表征多稳态性；找到同宿点（稳定和不稳定流形交点），证明PLRNN中存在混沌；在皮层神经元电生理记录中获得了对底层动力学的洞察。

Conclusion: 该方法为分析RNN动力学特性提供了有效工具，有助于理解网络行为并建立混沌存在性，在神经科学应用中显示出实用价值。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [247] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: 本文提出TROLL方法，用离散可微分信任区域投影替代PPO的裁剪机制，为大型语言模型提供原则性的token级KL约束，在训练速度、稳定性和最终成功率方面均优于PPO裁剪。


<details>
  <summary>Details</summary>
Motivation: PPO裁剪机制作为KL信任区域的粗糙近似，经常导致不稳定更新和次优性能，需要更原则性的替代方案。

Method: 使用离散可微分信任区域投影，在模型最重要的token logits稀疏子集上操作，平衡计算成本和投影效果。

Result: 在各种数据集、模型家族和优势估计方法中，TROLL在训练速度、稳定性和最终成功率方面始终优于PPO裁剪。

Conclusion: TROLL可作为PPO裁剪的直接替代方案，不改变模型推理行为，提供更稳定有效的强化学习训练。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [248] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: 提出RAPO算法解决RLVR训练中反向KL散度正则化导致的探索受限问题，通过前向KL惩罚和参考策略重加权促进更广泛的探索，在数学推理任务上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在增加采样预算时优势减弱，主要原因是反向KL散度的模式寻求行为限制了策略在基模型支持区域外的探索

Method: 使用前向KL惩罚替代反向KL惩罚进行分布外探索，并对参考策略进行重加权实现自适应分布内探索

Result: 在AIME2024和AIME2025评估中，RAPO持续提升问题解决性能，突破基模型性能上限，解决之前难以处理的问题

Conclusion: RAPO通过改进探索策略推动了RLVR在挑战性推理任务中的前沿发展

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [249] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 首次将多智能体强化学习应用于高空气球协调控制，实现分布式区域覆盖，性能接近理论最优几何确定性方法。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法在大规模全局星座中表现良好，但在小型团队和局部任务中性能较差，需要探索多智能体强化学习方案。

Method: 扩展RLHAB仿真环境支持多智能体协作学习，采用QMIX算法和集中训练分散执行架构，使用包含个体状态、环境信息和队友数据的专门观测空间。

Result: QMIX在分布式区域覆盖任务中达到与理论最优几何确定性方法相似的性能表现。

Conclusion: 验证了MARL方法在高空气球协调控制中的可行性，为更复杂的自主多气球任务奠定了基础。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [250] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出PDNS框架，通过路径测度上的近端点方法解决多模态分布采样中的模式崩溃问题，将学习过程分解为逐步接近目标分布的系列子问题。


<details>
  <summary>Details</summary>
Motivation: 扩散神经采样器在多模态分布中容易发生模式崩溃，特别是当模态间存在显著势垒时，训练变得困难。

Method: 使用路径测度上的近端点方法，将随机最优控制问题分解为一系列简单子问题，每个近端步骤采用加权去噪交叉熵目标实现。

Result: 在连续和离散采样任务中表现出有效性和鲁棒性，包括分子动力学和统计物理中的挑战性场景。

Conclusion: PDNS框架通过渐进式路径构建促进了跨模态的彻底探索，有效解决了多模态分布采样中的模式崩溃问题。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [251] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: 提出了LLM Chemistry框架，用于量化多LLM协作中的协同与对抗行为，通过分析交互依赖关系来推荐最优模型组合。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式选择和输出评估，缺乏分析协作模型是否真正互补或冲突，需要系统化衡量LLM组合的协同效应。

Method: 形式化LLM间的化学作用概念，提出量化算法分析交互依赖关系，根据理论分析推荐最优模型集成。

Result: 理论分析显示LLM化学作用在异质模型配置下最明显，评估在分类、摘要和程序修复任务中证实了任务依赖效应。

Conclusion: LLM Chemistry可作为多LLM系统的诊断因素和集成推荐的基础框架。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [252] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON（混合离线学习+在线优化）是一种新的离线强化学习方法，通过结合离线学习的数据流形和Q值评估器，以及在线优化来克服标准离线RL在分布偏移和价值高估方面的问题，在工业过程启动和等级转换任务中超越了现有方法和人类专家表现。


<details>
  <summary>Details</summary>
Motivation: 工业连续过程工厂的启动和产品等级转换操作依赖少数专家操作员的手动操作，但随着这些专家退休，工厂所有者面临缺乏隐性知识的问题。离线强化学习可以挖掘历史数据来捕捉人类专业知识，但标准方法在策略超出数据范围时存在分布偏移和价值高估问题。

Method: HOFLON采用混合方法：离线阶段学习（i）表示历史转换可行区域的数据流形和（ii）预测累积奖励的长时域Q评估器；在线阶段求解单步优化问题，最大化Q评估器同时惩罚偏离学习流形和操纵变量变化率过大的情况。

Result: 在两个工业案例研究（聚合反应器启动和造纸机等级转换）中，HOFLON不仅超越了领先的离线RL算法IQL，而且平均获得了比历史数据中最佳启动或等级转换更好的累积奖励。

Conclusion: HOFLON展示了超越当前专家能力的自动化转换操作潜力，能够从历史数据中学习并生成优于人类专家的操作策略。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [253] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 提出FIRE方法，通过Fisher信息估计来缓解联邦学习中数据碎片化导致的协变量偏移问题，提升模型在偏移验证集上的性能。


<details>
  <summary>Details</summary>
Motivation: 当训练数据在批次间或地理位置间碎片化时，训练模型会出现性能下降，这主要是由于协变量偏移导致的训练分布差异。

Method: FIRE方法通过近似Fisher信息累积碎片化引起的协变量偏移差异，并将其作为每个数据片段的损失惩罚项，实现可扩展的分布对齐。

Result: FIRE在偏移验证集上比重要性加权基准方法最多提升5.1%，比联邦学习基准方法最多提升5.3%。

Conclusion: FIRE方法能有效解决联邦学习中的数据碎片化问题，通过Fisher信息估计实现分布对齐，显著提升模型在偏移验证集上的性能。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [254] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 提出了AGRPO算法，这是首个专门为扩散大语言模型设计的理论严谨的在线强化学习算法，在数学推理任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型尚未受益于现代后训练技术，特别是强化学习，因为传统LLM的算法与扩散框架不兼容，且现有方法缺乏理论基础。

Method: 提出了AGRPO算法，使用蒙特卡洛采样计算无偏策略梯度估计，是首个适用于dLLMs的可处理、忠实的策略梯度方法。

Result: 在数学推理任务上取得显著提升：GSM8K任务绝对增益+7.6%，Countdown任务性能提升3.8倍，比diffu-GRPO方法提升1.3倍。

Conclusion: 在线RL算法可以以理论严谨且实际有效的方式扩展到扩散LLMs，在不同推理步数下都能保持性能优势。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [255] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER是一个用于流式数据中序列分布漂移检测和稳定适应的理论框架，通过指数鞅和非一致性分数实现时间均匀的误报控制，并在持续漂移下提供检测延迟保证，同时使用Fisher预处理的提示参数更新实现自然梯度下降。


<details>
  <summary>Details</summary>
Motivation: 解决流式数据中序列分布漂移的检测和适应问题，提供统计有效的检测方法和几何稳定的适应策略，以应对协变量漂移环境下的顺序决策挑战。

Method: 使用指数鞅和非一致性分数构建检测框架，应用Ville不等式获得时间均匀的误报控制保证；在适应阶段采用Fisher预处理的提示参数更新，实现分布流形上的自然梯度下降。

Result: 检测延迟为$\mathcal{O}(\log(1/\delta)/\Gamma)$，其中$\Gamma$反映后漂移信息增益；适应方法能最小化KL散度，同时保持稳定性和参数化不变性。

Conclusion: M-FISHER为协变量漂移下的顺序决策提供了一个原则性的方法，实现了鲁棒的、随时有效的检测和几何稳定的适应。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [256] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 使用基于ICD-10代码的路线图驱动算法，结合大型语言模型增强，来恢复电子健康记录中的缺失数据，准确性与专家图表审查相当。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据存在缺失和错误问题，传统图表审查成本高且耗时，限制了可审查的患者数量。

Method: 开发路线图驱动算法，使用ICD-10代码作为辅助诊断锚点，结合大型语言模型迭代优化路线图，并与临床专业知识相结合。

Result: 算法恢复的缺失数据量与专家图表审查相当或更多，具体取决于使用的路线图版本。

Conclusion: 临床驱动算法（通过LLM增强）能够以与图表审查相似的准确性恢复缺失的EHR数据，并可扩展到大规模样本应用。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [257] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文发现交叉熵缩放定律在超大规模下失效，原因是交叉熵本身不真正缩放，只有其隐藏组件之一——错误熵遵循稳健的幂律缩放。


<details>
  <summary>Details</summary>
Motivation: 交叉熵缩放定律长期以来指导大语言模型开发，但最近证据表明该定律在超大规模下失效，损失下降速度比预期慢，这给大模型开发带来显著困扰。

Method: 提出将交叉熵分解为三个部分：错误熵、自对齐和置信度，并通过理论分析和在多个数据集、32个模型上的广泛实验验证这一分解。

Result: 只有错误熵遵循稳健的幂律缩放，其他两项基本保持不变；错误熵在小模型中占主导地位，但随模型增大比例减小，这解释了交叉熵缩放定律在小规模准确但在超大规模失效的原因。

Conclusion: 错误熵缩放定律能更准确地描述模型行为，将在大型语言模型的训练、理解和未来发展中具有广泛应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [258] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO是一个简单高效的强化学习框架，通过慢-快策略优化解决早期训练中的不稳定问题，显著提升推理任务的收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的策略梯度算法如GRPO在早期训练中面临噪声梯度、不稳定更新和低效探索的问题，特别是在大语言模型的推理任务中。

Method: SFPO将每个训练步骤分解为三个阶段：在相同批次上进行快速内部轨迹采样、重定位机制控制离策略漂移、以及最终的慢速校正。这种重定位-更新的设计保持目标和采样过程不变。

Result: SFPO在数学推理基准上比GRPO平均提升2.80分，减少4.93倍的采样次数，并在匹配GRPO最佳准确度时减少4.19倍的训练时间。

Conclusion: SFPO是一个即插即用的策略梯度框架，能显著提升强化学习训练的稳定性、减少采样需求并加速收敛。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [259] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文提出了FedMuon算法，将Muon优化器应用于联邦学习，建立了非凸问题的收敛率，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在多种应用中表现出色，但其在联邦学习中的有效性尚未探索，本文旨在填补这一空白。

Method: 提出FedMuon算法，利用正交化更新方向，使学习率独立于问题特定参数，并能自然适应重尾噪声。

Result: 理论分析显示FedMuon具有多个有利特性，多种神经网络架构上的广泛实验验证了算法的有效性。

Conclusion: FedMuon算法在联邦学习环境中表现优异，具有良好的收敛性能和噪声适应能力。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [260] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 研究发现模型和数据规模联合最优缩放由输出层算子范数这一单一不变量控制，称为范数迁移现象。最优学习率和批次大小组合具有相同的算子范数值，且该条件是必要但不充分的。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型和数据集缩放下的最优超参数迁移方面取得进展，但缺乏统一解释原则。研究旨在发现控制联合最优缩放的基本原则。

Method: 使用Scion优化器，在多达13亿参数模型和1380亿token数据集上进行实验，测量最优学习率和批次大小组合的缩放规律，并研究分层学习率调优。

Result: 发现最优学习率/批次大小组合具有恒定的算子范数值，输出层对学习率最敏感，隐藏层受益于较低学习率。Scion的缩放规则与Adam优化器一致。

Conclusion: 输出层算子范数是控制联合最优缩放的关键不变量，为大规模LLM训练动态研究提供了范数引导的最优缩放实用见解，并发布了分布式Scion实现和实验日志。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [261] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: 提出了BONSAI框架，一种利用部分结构知识的鲁棒贝叶斯优化方法，用于处理不确定环境下的网络系统优化问题


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法需要已知问题结构，限制了在高保真仿真环境中的应用。现有鲁棒贝叶斯优化方法忽略结构信息且难以扩展到高维场景

Method: 将目标函数表示为有向图结构，包含白盒和黑盒组件，利用中间信息。提出基于Thompson采样的可扩展采集函数，使用梯度方法高效优化

Result: 在合成和实际案例研究中，相比现有仿真鲁棒优化算法，BONSAI能提供更样本高效和更高质量的鲁棒解

Conclusion: BONSAI在复杂工程系统的不确定性感知设计中具有实际优势

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [262] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对自回归语言模型(ARMs)和扩散语言模型(DLMs)进行了全面的性能对比分析，发现DLMs在算术强度方面优于ARMs，但在长上下文扩展和批量推理吞吐量方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散语言模型(DLMs)作为并行生成文本的替代架构显示出潜力，但其相对于主流自回归语言模型(ARMs)的性能影响尚未被充分理解。

Method: 采用理论分析和性能剖析数据相结合的方法，系统比较ARMs和DLMs的性能特征，并探索块状解码DLMs的优化策略。

Result: DLMs在算术强度上优于ARMs，但在长上下文扩展性上表现不佳；块状解码DLMs能提高算术强度并保持良好扩展性；ARMs在批量推理吞吐量上更优；减少采样步数可显著改善DLMs的延迟性能。

Conclusion: DLMs和ARMs各有优势，DLMs在并行性方面有潜力但需要优化采样步骤和扩展性，块状解码是改善DLMs性能的有效方法。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [263] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS是一个新颖的框架，将大语言模型从"数据处理者"重新定位为"算法专家"，通过分析检测器的内在弱点来生成难以检测的异常样本，从而增强检测器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格异常检测方法通常依赖于对异常模式的假设，导致在真实场景中性能不一致。虽然大语言模型具有强大的推理能力，但直接应用于表格异常检测面临处理异构数据和隐私风险等挑战。

Method: LLM-DAS框架利用LLM的算法推理能力，分析检测器的高层描述来理解其内在弱点，然后生成检测器特定、数据无关的Python代码来合成"难以检测"的异常，利用这些漏洞。生成的合成程序可在不同数据集上复用，通过实例化来增强训练数据。

Result: 在36个TAD基准测试上的广泛实验表明，LLM-DAS能够持续提升主流检测器的性能。

Conclusion: 通过程序化合成将LLM推理与经典异常检测算法相结合，LLM-DAS提供了一种可扩展、有效且保护隐私的方法来修补现有检测器的逻辑盲点。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [264] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: THEMIS是一个基于预训练基础模型的时间序列异常检测框架，利用Chronos时间序列基础模型的编码器提取嵌入，通过局部离群因子和谱分解技术检测异常，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测面临季节性、趋势、噪声、概念漂移等多重挑战，异常类型多样且罕见导致数据不平衡，需要强大、灵活且可解释的方法。

Method: 提取Chronos时间序列基础模型编码器的嵌入表示，应用局部离群因子和自相似矩阵的谱分解技术进行异常检测。

Result: 在MSL数据集上达到SOTA结果，在SMAP和SWAT数据集上表现有竞争力，超越专门训练的异常检测模型，具有超参数鲁棒性和可解释性。

Conclusion: 支持使用基础模型的预训练表示进行高效、适应性强的时间序列异常检测。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [265] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets是一种基于二阶波动方程模拟的神经网络架构，使用可训练的空间速度和阻尼参数，通过FFT实现高效传播，在语言和视觉任务上匹配或超越Transformer性能，同时减少30%运行时间和25%峰值内存。


<details>
  <summary>Details</summary>
Motivation: 为注意力机制和一阶状态空间模型提供一种振荡式、全局的替代方案，利用物理偏置构建计算高效且鲁棒的架构。

Method: 每层通过可训练的空间速度c(x)和阻尼γ(x)参数，使用基于FFT的辛谱求解器在O(nlog n)时间内传播隐藏状态作为连续场。

Result: 在语言和视觉基准测试中，Wave-PDE Nets匹配或超越Transformer性能，同时减少30%运行时间和25%峰值内存。消融研究确认了辛积分和谱拉普拉斯算子的关键作用。

Conclusion: Wave-PDE Nets是一种计算高效且鲁棒的架构，具有强大的物理归纳偏置，学习到的物理参数显示出直观的信息传播策略。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [266] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出了一种处理聚类数据的广义拟合Q迭代算法，通过结合广义估计方程来处理簇内相关性，在医疗健康应用中显著优于标准FQI方法。


<details>
  <summary>Details</summary>
Motivation: 医疗健康应用中经常遇到聚类数据，需要处理簇内相关性，而标准的强化学习方法没有考虑这种数据结构特性。

Method: 将广义估计方程整合到策略学习中，提出广义拟合Q迭代算法来处理簇内相关性。

Result: 理论证明：当相关结构正确指定时，Q函数和策略估计器具有最优性；当结构误指定时仍保持一致性。实证显示：在移动健康数据集上，相比标准FQI平均减少一半遗憾。

Conclusion: 提出的广义FQI算法能有效处理聚类数据中的相关性，在医疗健康应用中展现出显著优势。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [267] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 本文研究了在线回归中利用未来样本预测信息的学习问题，从完全预测的转导在线学习到不完美预测的广义设置，建立了基于脂肪粉碎维度的极小极大后悔界。


<details>
  <summary>Details</summary>
Motivation: 现实数据流具有可预测性，研究如何利用未来样本的预测信息来改进在线回归性能，特别是在预测不完美的情况下。

Method: 首先完全刻画转导在线学习的极小极大期望后悔与脂肪粉碎维度的关系，然后扩展到不完美预测设置，开发匹配最坏情况后悔的在线学习算法。

Result: 建立了转导在线回归与对抗在线回归的分离，开发的算法能平滑地随预测质量提升性能，在精确预测时接近转导在线学习器的表现。

Conclusion: 该工作使得在可预测样本下原本不可学习的类别变得可学习，符合学习增强模型范式的更广泛目标。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [268] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯分布的偏信息分解方法(GPID)，通过梯度优化算法提高计算效率，并利用信息保持编码器将非高斯数据转换为高斯分布，解决了连续高维模态中PID计算的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的偏信息分解方法依赖联合分布优化，但在连续高维模态中计算成本高且不准确。需要一种更高效准确的PID计算方法。

Method: 1) 提出高斯PID框架，当配对分布为多元高斯时高效求解；2) 开发基于梯度优化的新算法；3) 使用信息保持编码器将任意分布转换为高斯分布；4) 解决了GPID中联合高斯解的最优性问题。

Result: 在多种合成示例中验证，相比现有基线方法，提出的方法提供更准确高效的PID估计。在大型多模态基准测试中展示了在实际应用中的有效性。

Conclusion: 该方法成功解决了连续高维模态中PID计算的挑战，为多模态数据分析提供了实用的信息量化工具，并能用于选择高性能模型。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [269] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 本文对具有时变参数的连续深度图神经网络（GNDEs）进行收敛性分析，建立了其在无限节点极限下的理论框架，证明了GNDE解向Graphon-NDE解的轨迹收敛，并推导了两种确定性图采样机制下的显式收敛率。


<details>
  <summary>Details</summary>
Motivation: 结合图神经网络的结构归纳偏置与神经ODE的连续深度架构，GNDEs为图动力学建模提供了可扩展的理论框架。然而，其在无限节点极限下的收敛性和尺寸可迁移性缺乏理论分析。

Method: 引入Graphon-NDEs作为GNDEs的无限节点极限，利用图论和动力系统工具，证明GNDE解向Graphon-NDE解的轨迹收敛，并在两种确定性图采样机制下推导显式收敛率。

Result: 建立了GNDEs在无限节点极限下的收敛理论，证明了轨迹收敛性，并获得了显式收敛率。进一步建立了尺寸可迁移性边界，为将中等规模图上训练的GNDE模型迁移到更大结构相似图提供了理论依据。

Conclusion: GNDEs在无限节点极限下具有良好的收敛性质，支持尺寸可迁移性，数值实验验证了理论发现，为实际应用提供了理论保障。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [270] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文系统评估了8种拟合优度检验在三种流行文本水印方案中的表现，发现通用GoF测试能显著提升水印检测能力和鲁棒性，特别是在低温度设置下文本重复场景中具有独特优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成内容引发真实性和完整性担忧，文本水印提供可验证的内容来源检测方法。拟合优度检验作为自然的水印检测工具，但在该领域尚未得到充分探索。

Method: 使用三种开源LLM、两个数据集、多种生成温度和后编辑方法，系统评估八种GoF测试在三种流行水印方案中的表现。

Result: 通用GoF测试能显著提升水印检测能力和鲁棒性，特别是在低温度设置下的文本重复场景中，GoF测试具有现有方法未利用的独特优势。

Conclusion: 经典的拟合优度检验是LLM水印检测中简单但强大且未被充分利用的工具。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [271] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 本文提出类别级影响函数和影响向量，通过线性规划样本重加权框架实现模型性能的帕累托改进，确保所有类别都能受益。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心学习主要关注哪些数据对模型有益，但忽略了更根本的问题：模型的性能上限是什么？本文强调类别级精度，追求帕累托改进，避免某些类别以牺牲其他类别为代价的改进。

Method: 提出类别级影响函数和影响向量来量化每个训练样本对所有类别的影响，基于影响向量设计原则性标准判断模型是否可改进，并开发线性规划样本重加权框架。

Result: 在合成数据集、视觉和文本基准上的广泛实验表明，该方法能有效估计和实现模型在多个感兴趣类别上的性能改进。

Conclusion: 提出的方法能够识别模型性能改进的潜力，并通过样本重加权实现所有类别的帕累托改进，为数据中心学习提供了新的视角和工具。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [272] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR是一个新颖的推理框架，将连续潜在表示的表达能力与潜在扩散模型的迭代优化能力相结合，用于改进大型语言模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的自回归解码限制了整体性重新审视和优化早期token的能力，也导致了对多样化解决方案的低效探索。

Method: 首先使用VAE构建结构化潜在推理空间，将文本推理步骤编码为思想token块；然后使用潜在扩散模型通过块状双向注意力掩码学习去噪潜在思想token块，实现长时程推理和迭代优化。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性方面均优于现有的自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR为文本推理提供了一种新的潜在扩散范式，能够高效并行生成多样化的推理轨迹，实现整体性规划和修订推理过程。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [273] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 提出了一种自适应实时标注方法，用于医学筛查程序中高效聚合专家意见，减少50%的专家查询量同时保持准确率


<details>
  <summary>Details</summary>
Motivation: 现有算法无法满足医学筛查流程的无缝集成需求，需要支持实时标注、无需先验专家知识、并能动态基于实例难度查询额外专家

Method: 自适应实时标注方法，增量收集专家意见直至达到置信度阈值，动态查询额外专家基于实例的潜在难度

Result: 在三个多标注者分类数据集上的评估显示，自适应查询策略减少50%专家查询量，准确率与非自适应基线相当

Conclusion: 该方法能有效减少医学筛查中的标注开销，为连续数据流和初始未知专家能力的场景提供实用解决方案

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [274] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过模块化的生成、反思和策展过程，将上下文视为不断演化的策略手册，有效解决了简洁性偏见和上下文崩溃问题，在多个基准测试中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用中的上下文适应方法存在简洁性偏见（丢失领域洞察）和上下文崩溃（迭代重写导致细节丢失）的问题，需要更有效的上下文管理框架。

Method: ACE框架将上下文视为演化的策略手册，通过生成、反思和策展三个模块化过程来积累、优化和组织策略，采用结构化增量更新来防止上下文崩溃。

Result: 在代理和领域特定基准测试中，ACE离线优化上下文性能提升10.6%，金融领域提升8.6%，显著降低适应延迟和部署成本，在AppWorld排行榜上匹配顶级生产级代理。

Conclusion: 全面且不断演化的上下文能够实现可扩展、高效且自我改进的LLM系统，同时保持较低的开销。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [275] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 开发了一个24-48小时雷暴停电早期预警模型，使用公开数据源和两阶段模型设计（逻辑门+LSTM回归器），在密歇根州夏季雷暴相关停电预测中表现优于单阶段LSTM基线。


<details>
  <summary>Details</summary>
Motivation: 雷暴驱动的停电难以预测，因为大多数风暴不会造成损害，对流过程快速且混乱，可用的公共数据既嘈杂又不完整。

Method: 使用公开的EAGLE-I停电数据集和METAR天气数据，通过参数特定克里金法和目标过采样保留极端值，构建因果时空特征，采用两阶段模型设计（逻辑门+LSTM回归器）。

Result: 两阶段模型在所有时间窗口检测到更多参考峰值（例如在±48小时记录3/4 vs 2/4），在峰值附近显示适度的幅度增益（在±0-12小时cMASE降低2-3%），但总体误差与单阶段LSTM基线相当。

Conclusion: 尽管存在开放数据噪声，特征驱动的管道为雷暴停电提供了可操作的、以事件为中心的早期预警。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [276] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: 提出SPEAR方法，利用软提示和量化技术将大语言模型应用于时间序列异常检测，通过可学习的软提示嵌入使冻结的LLM适应时间序列任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理变长时间序列序列和基于上下文的异常检测，而大语言模型的出现为时间序列异常检测提供了新机遇。

Method: 将时间序列数据量化为输入嵌入，与可学习的软提示嵌入结合后输入冻结的LLM，通过交叉熵损失迭代更新软提示。

Result: 实验结果表明软提示有效提升了LLM在时间序列异常检测下游任务中的性能。

Conclusion: 软提示和量化技术能够有效使大语言模型适应时间序列异常检测任务。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [277] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习中当基础模型从未产生正确答案时遇到的零奖励障碍问题，发现简单的数据干预方法比复杂的算法改进更有效。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习在复杂推理任务中面临的零奖励障碍问题，即当基础模型从未采样到正确解决方案时，训练会因零梯度而停滞。

Method: 使用图搜索任务评估了多种方法，包括密集奖励、多样性激励和改进的信用分配，并测试了在训练集中添加简单样本的数据干预方法。

Result: 实验表明，如果没有基础模型产生正确答案，所有复杂方法都无法克服零奖励障碍；而简单的数据干预方法能使模型最终解决原始困难任务。

Conclusion: 数据中心的干预方法比算法层面的改进更有效地克服零奖励障碍，且无需修改强化学习算法本身。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [278] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 提出了ONNX-Bench基准测试，包含60多万个架构-精度对，通过统一的ONNX格式和自然语言描述实现跨搜索空间的性能预测。


<details>
  <summary>Details</summary>
Motivation: 现有神经架构搜索方法受限于特定搜索空间和图形编码，缺乏灵活性和可扩展性，无法适应更复杂的搜索空间。

Method: 创建基于ONNX文件的统一格式基准，使用自然语言描述作为输入来构建性能预测器，支持任意层类型、操作参数和异构拓扑。

Result: 实验显示在少量预训练样本下，能在不同搜索空间上实现强大的零样本性能，实现即时评估任何神经网络架构。

Conclusion: ONNX-Net文本编码能够表示任何神经架构，使单一代理模型能泛化到所有神经架构，突破了基于单元的搜索空间限制。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [279] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 该论文将结构化状态空间对偶性(SSD)从标量恒等状态矩阵推广到一般对角状态矩阵，建立了对角SSM与1-半可分因果掩码注意力之间的等价关系，揭示了SSM与Transformer之间的深层联系。


<details>
  <summary>Details</summary>
Motivation: 扩展SSD对偶性，从简单的标量恒等状态矩阵推广到更一般的对角状态矩阵，以丰富SSM的动态表达能力，同时保持训练复杂度的下界。

Method: 通过数学形式化证明对角SSM与1-半可分因果掩码注意力的等价性，分析训练复杂度，并建立SSM与注意力机制等价的条件。

Result: 证明对角SSM在保持标量情况训练复杂度下界的同时支持更丰富的动态特性，建立了SSM与1-半可分掩码注意力的等价条件，但发现这种对偶性无法扩展到标准softmax注意力。

Conclusion: 该研究强化了循环SSM与Transformer之间的桥梁，为设计表达能力强且高效的序列模型拓宽了设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [280] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool是一种新颖的层次池化框架，通过增强模型对簇间连接性的理解来提升图结构数据分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的层次池化模型在设计聚类分配和粗化策略时，往往忽略了簇之间的关系，这限制了模型对图结构完整性的保持能力。

Method: 提出ICEPool框架，强调簇间连接性的整合，与多种基于池化的GNN模型兼容，通过增强簇间关系学习来提升图表示能力。

Result: 实验结果表明ICEPool与多种模型兼容，并能有效提升现有图神经网络架构的性能。

Conclusion: ICEPool通过关注簇间连接性，为层次池化模型提供了更全面和鲁棒的图级表示能力。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [281] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 提出了局部自然度方法，用于在多教师设置中选择最佳推理轨迹进行知识蒸馏，解决了全局自然度方法在长推理轨迹和多教师输出情况下的失效问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在单教师设置下选择学生全局对数概率最高的响应有效，但在多教师设置中失效，特别是当强教师生成的长推理轨迹时，全局自然度与下游性能不再相关。

Method: 引入局部自然度方法，测量学生在短序列推理步骤上的对数概率，仅基于小的局部窗口条件。该方法支持教师选择和从多教师中选择响应。

Result: 局部自然度在数学基准测试中将32B学生的准确率比全局选择提高了9.4个百分点，超过了仅使用单个最佳教师数据训练的性能。

Conclusion: 局部数据质量评估和数据混合对于更有效的推理蒸馏具有强大作用，局部自然度方法在多教师设置中显著优于传统方法。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [282] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了Reinforce-Ada自适应采样框架，通过在线连续消除过程动态分配采样资源到最具不确定性的提示，加速LLM推理任务的强化学习收敛并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化学习方法在推理任务中存在梯度估计不稳定的问题，主要原因是固定均匀的响应采样方式。需要一种能够动态分配推理预算的方法来最小化随机梯度方差。

Method: 提出在线自适应采样框架，通过连续消除过程动态重分配采样努力到最具学习潜力的提示；使用固定大小分组和全局统计计算优势基线来稳定更新；自动在收集足够信号时停止采样。

Result: 在多个模型架构和推理基准测试中，相比GRPO方法，Reinforce-Ada加速了收敛并提高了最终性能，特别是使用平衡采样变体时效果更佳。

Conclusion: 研究强调了方差感知的自适应数据管理在实现高效可靠LLM强化学习中的核心作用，为推理能力强的LLMs提供了更有效的训练方法。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [283] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: 该论文提出了一个连续框架，将Transformer解释为结构化积分-微分方程的离散化，为理解其核心组件提供了统一的理论基础。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在序列建模中取得了革命性突破，但缺乏全面的数学理论来解释其结构和操作。

Method: 通过将Transformer建模为结构化积分-微分方程的离散化，将自注意力机制解释为非局部积分算子，层归一化表征为时间相关约束的投影。

Result: 建立了一个算子理论和变分视角的统一框架，为理解注意力机制、前馈层和归一化等核心组件提供了理论基础。

Conclusion: 该工作为深度学习架构与连续数学建模之间搭建了桥梁，为可解释和理论基础的神经网络模型发展贡献了基础性视角。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [284] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出Diff Interpretation Tuning (DIT)方法，训练模型用自然语言描述自身微调导致的权重变化，使模型能够解释其内部知识更新。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型时产生的权重变化通常难以解释，且微调数据集往往不可公开获取或规模过大，需要一种能够理解权重差异的方法。

Method: 使用合成的带标签权重差异训练DIT适配器，该适配器可应用于兼容的微调模型，使其能够描述自身的变化。

Result: 在两个概念验证场景（报告隐藏行为和总结微调知识）中，该方法使模型能够用准确的自然语言描述其微调引起的修改。

Conclusion: DIT方法为理解模型权重变化提供了一种有效的自然语言解释途径，增强了模型微调过程的可解释性。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [285] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 该论文提出将机器学习天气预报模型训练重新解释为弱约束四维变分数据同化问题，在自编码器潜在空间计算损失函数，改善长期预报的物理真实性和精细结构保持能力。


<details>
  <summary>Details</summary>
Motivation: 传统ML天气预报模型将再分析数据视为完美真值，忽略物理耦合和空间结构，导致长期预报模糊且物理不真实。

Method: 将模型训练重构为WC-4DVar问题，在自编码器学习的潜在空间中计算损失函数，避免显式建模高维空间中的再分析误差协方差。

Result: 潜在空间约束的滚动训练相比模型空间损失训练，显著提升了长期预报技能，更好地保持了精细结构和物理真实性。

Conclusion: 该方法为整合多源观测数据提供了统一理论框架，改善了ML天气预报的物理一致性和长期预报性能。

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [286] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: BVPO是一种偏好优化方法，通过混合高方差轨迹估计器和低方差空轨迹估计器来减少推理轨迹采样带来的梯度方差，提高大型推理模型的训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成最终答案前会产生中间推理轨迹，但偏好对齐时需要对所有推理轨迹进行边缘化计算，这在实践中难以处理。现有方法通过采样单个轨迹进行优化，但会引入显著的梯度方差。

Method: 提出BVPO方法，混合两种梯度估计器：高方差的基于轨迹的估计器和低方差的空轨迹估计器（禁用推理轨迹生成）。该方法提供了最小化均方误差的混合权重闭式解。

Result: 在AlpacaEval~2上比最佳基线提升7.8分，在Arena-Hard上提升6.8分。仅使用通用对话数据训练，还能在六个数学推理基准上平均提升4.0分。

Conclusion: 轨迹采样方差是关键瓶颈，直接优化偏差-方差权衡能带来更稳定的训练和更强的整体性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [287] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention是一种线性复杂度的注意力机制，替代了二次复杂度的Softmax Attention，能够在当前硬件上处理超长上下文（最高7500万token）。


<details>
  <summary>Details</summary>
Motivation: Softmax Attention的二次时间复杂度在处理长上下文时变得不可行，即使使用优化的GPU内核（如FlashAttention）也无法处理超过约400万token的上下文。

Method: RACE Attention用锐化的角度（余弦）相似度替代指数核函数，通过随机投影和软局部敏感哈希（LSH）来近似注意力输出。

Result: 在语言建模、掩码语言建模和文本分类任务中，RACE Attention在保持准确性的同时显著减少了运行时间和内存使用，在NVIDIA GH200 GPU上可处理1200万token，在Intel Xeon Gold 5220R CPU上可处理7500万token。

Conclusion: RACE Attention为在当前硬件上实现超长上下文窗口提供了实用且理论可靠的方法，有望在实践中得到广泛应用。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [288] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了SFP（时空预测即规划）新范式，基于模型强化学习构建生成世界模型来模拟多样化高保真未来状态，通过波束搜索规划算法利用不可微领域指标作为奖励信号来探索高回报序列，并通过迭代自训练优化预测模型。


<details>
  <summary>Details</summary>
Motivation: 解决物理时空预测中固有的随机性和不可微指标的双重挑战，传统方法难以处理这些复杂问题。

Method: 构建生成世界模型进行环境模拟，将基础预测模型作为智能体，使用波束搜索规划算法以不可微领域指标为奖励探索高回报序列，通过迭代自训练优化策略。

Result: 显著减少预测误差，在捕获极端事件等关键领域指标上表现出色。

Conclusion: SFP框架成功解决了时空预测中的随机性和不可微指标问题，为复杂物理系统预测提供了有效解决方案。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [289] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 提出了一种新颖的差分隐私多类SVM方法，通过权重和梯度扰动技术解决传统OvR/OvO方法在多类场景下隐私预算消耗过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统多类SVM采用OvR或OvO方法时，每个数据样本会被多次查询，导致隐私预算随类别数成比例消耗，无法有效保护数据隐私。

Method: 开发了全合一SVM方法，每个数据样本仅访问一次，通过权重扰动和梯度扰动技术构建具有边界最大化特性的多类SVM，并提供严格的敏感性和收敛性分析。

Result: 实验结果表明，该方法在多类场景下优于现有的DP-SVM方法。

Conclusion: 提出的PMSVM方法有效解决了多类SVM中的隐私保护问题，为构建隐私保护的多类机器学习模型提供了可行方案。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [290] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: RLVR训练存在两个阶段：利用阶段（exploitation）导致能力边界收缩，探索阶段（exploration）促进能力边界扩展。通过理论分析和实证研究揭示了这一动态过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决关于RLVR训练对LLMs推理能力边界影响的争议性发现，一些研究认为RLVR主要提高采样效率但牺牲多样性和探索能力，而另一些研究表明长期训练能产生新的推理策略。

Method: 通过理论和实证分析揭示RLVR训练中的两阶段概率质量动态：利用阶段主要采样已探索的高奖励和低奖励token，探索阶段则促进潜在最优token概率的增长。

Result: 研究表明，在利用阶段的过度开发可能导致能力边界收缩，而进入探索阶段的长期训练则能促进推理能力边界的扩展。

Conclusion: 重新审视仅使用相对负梯度进行长期训练的潜力，为开发更先进的推理能力提供了理论和实证基础。

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [291] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出KOTARO方法解决类别不平衡问题，通过自适应调整决策边界来改善少数类识别性能


<details>
  <summary>Details</summary>
Motivation: 现实世界二分类任务中类别不平衡问题普遍存在，导致预测偏向多数类，在医疗诊断和异常检测等关键领域尤其需要改善少数类识别

Method: 基于核密度估计框架，通过动态调整高斯基函数带宽，根据局部样本密度自适应调整决策边界

Result: 在合成和真实不平衡数据集上的实验表明，KOTARO优于传统方法，特别是在严重不平衡条件下表现突出

Conclusion: KOTARO是解决广泛不平衡分类问题的有前景方案

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [292] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 提出了一种在数据受限环境下从预训练扩散模型中移除不良特征的变分扩散遗忘方法，该方法仅需访问包含不良特征的训练数据子集，通过变分推断框架平衡遗忘效果和模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了负责任地部署扩散模型，需要防止生成不良、暴力和淫秽内容。现有方法在无法访问完整训练数据集的数据受限环境下效果不佳，因此需要开发仅需部分不良数据就能有效遗忘的方法。

Method: 提出变分扩散遗忘方法，基于变分推断框架，使用包含两个项的损失函数：可塑性诱导器降低不良训练数据的对数似然，稳定性正则化器在参数空间正则化模型以防止图像生成质量下降。

Result: 通过全面的实验验证了方法在类别遗忘和特征遗忘任务中的有效性，分别在MNIST、CIFAR-10、tinyImageNet数据集上测试类别遗忘，在Stable Diffusion模型上测试特征遗忘。

Conclusion: VDU方法在数据受限环境下能有效防止扩散模型生成包含不良特征的输出，同时保持图像生成质量，为负责任地部署生成模型提供了可行解决方案。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [293] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 本文介绍了在大状态空间中的离线强化学习理论，从历史数据中学习策略而无需与环境在线交互，讨论了函数逼近和数据覆盖的关键概念。


<details>
  <summary>Details</summary>
Motivation: 研究离线强化学习在大状态空间中的应用，旨在从历史数据中学习有效策略，避免在线交互的成本和风险。

Method: 引入函数逼近的表达性假设（如Bellman完备性与可实现性）和数据覆盖假设（如全策略覆盖与单策略覆盖），分析不同假设下的算法和结果。

Result: 描述了丰富的算法和结果图谱，根据不同的假设和复杂度保证要求，提供了多样化的解决方案。

Conclusion: 离线强化学习在大状态空间中具有重要理论价值，但仍存在开放问题，并与相邻领域有密切联系。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [294] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出一种新的神经网络训练方法，使用预定义的向量系统作为目标潜在空间配置，使同一神经网络架构能够处理任意数量的类别，解决了传统监督学习中网络参数数量依赖于类别数量的问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法在分类任务中虽然准确率高，但网络参数数量依赖于类别数量，当类别数量极大或未知时适用性受限。需要一种方法能够训练相同的神经网络架构而不受类别数量限制。

Method: 使用预定义的向量系统作为目标潜在空间配置，选择An根系统的随机扰动向量作为目标配置，通过将神经网络预测与预定义向量匹配来训练编码器和视觉变换器。

Result: 在Cinic-10和ImageNet-1K数据集上成功训练了编码器和视觉变换器，并在包含128万个类别的数据集上训练了视觉变换器，证明了该方法在极多类别情况下的适用性。

Conclusion: 该方法能够训练相同的神经网络架构处理任意数量的类别，在极多类别场景下具有良好适用性，并在持续学习和神经网络蒸馏方面具有潜在应用价值。

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [295] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了处理部分多标签学习和互补多标签学习的统一方法，不需要依赖标签生成过程的准确估计或均匀分布假设，基于一阶和二阶策略构建了无偏风险估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的两种弱监督多标签学习方法都需要准确估计候选标签或互补标签的生成过程，或者假设均匀分布，但这些条件在现实场景中通常难以满足。

Method: 提出基于一阶和二阶策略的无偏风险估计器，理论证明了与多标签分类评估指标的一致性，并推导了风险估计器估计误差的收敛速率。

Result: 大量实验结果表明，所提出的方法在对抗最先进方法时表现出有效性。

Conclusion: 提出的方法能够统一处理部分多标签学习和互补多标签学习问题，不依赖难以满足的条件，具有理论保证和实际有效性。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [296] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: 该论文分析了基础模型在时间序列预测中的外推能力不足问题，识别并形式化了统计学习模型在训练域外预测准确性的基本特性，解释了深度学习模型在外推场景中性能下降的原因。


<details>
  <summary>Details</summary>
Motivation: 受基础模型在语言建模中显著成功的启发，开发时间序列预测的基础模型具有变革性潜力。虽然基础模型在短程预测中表现优异，但在外推或长程预测中表现不佳，甚至无法超越简单基线，这与物理定律的强外推特性形成鲜明对比。

Method: 通过理论分析和实证研究，识别并形式化了统计学习模型在训练域外预测准确性的基本特性，展示了这一特性对当前深度学习架构的影响。

Result: 研究结果阐明了外推差距的根本原因，揭示了深度学习模型在外推场景中的性能限制。

Conclusion: 该工作不仅澄清了外推差距的根源，还为设计能够掌握外推能力的下一代预测模型提供了方向性建议。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [297] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯线性回归的LLM不确定性量化方法，通过层间预测和稀疏特征组合实现高效UQ，在多项实验中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多选择结构生成任务中的不确定性量化仍由简单的最大softmax分数主导，缺乏更可靠的方法来支持自动化决策等应用。

Method: 训练多个贝叶斯线性模型，每个模型预测LLM某一层的输出基于前一层的输出，通过层级后验分布和稀疏特征组合来推断全局不确定性水平。

Result: 在各种LLM上的数值实验显示，该方法相比现有最先进基线方法取得了持续的性能提升。

Conclusion: 采用贝叶斯统计的规范化方法，即使使用最简单的线性回归模型，也能显著改进LLM不确定性量化的性能。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [298] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 提出基于Wasserstein投影的回归模型公平性测试框架，通过假设检验和最优数据扰动方法来检测和缓解偏差，在保持准确性的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习公平性研究主要集中在分类任务，回归模型的公平性问题研究不足，需要专门的方法来检测和缓解回归模型中的偏差。

Method: 采用Wasserstein投影框架进行公平性测试，包括假设检验方法和最优数据扰动技术，理论分析包括公平性标准分类、Wasserstein投影检验统计量的对偶重构以及渐近边界和极限分布推导。

Result: 在合成和真实数据集上的实验表明，该方法比基于排列的测试具有更高的特异性，能有效检测和缓解学生成绩预测和房价预测等实际应用中的偏差。

Conclusion: 该框架为回归模型的公平性测试提供了有效工具，能够平衡准确性和公平性，在真实应用中展现出良好的检测和缓解偏差能力。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [299] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文建立了半自动机在均匀分布下的首个统计查询困难性结果，证明当字母表大小和输入长度都是状态数的多项式时，半自动机的统计查询困难性成立。


<details>
  <summary>Details</summary>
Motivation: 半自动机在自然语言处理、机器人学、计算生物学和数据挖掘等领域有广泛应用，但之前缺乏对其统计查询复杂度的系统研究。

Method: 通过将区分两个半自动机最终状态的任务转化为研究对称群S_N×S_N上的随机游走行为，应用傅里叶分析和对称群表示论工具获得紧谱隙界。

Result: 证明了在状态数的多项式步数后，不同的半自动机变得几乎不相关，从而得到统计查询困难性结果。

Conclusion: 与确定性有限自动机不同，半自动机的统计查询困难性源于其内部状态转移结构，而非所识别语言的复杂性。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [300] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI是一个用于冷启动药物-靶点相互作用预测的框架，通过关注蛋白质多级结构（从一级到四级）来改进预测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只使用蛋白质的一级结构，但蛋白质具有多级结构且都会影响药物-靶点相互作用，这限制了现有方法捕捉涉及高级结构的相互作用

Method: 采用分层注意力机制挖掘多级蛋白质结构与药物结构在局部和全局粒度上的相互作用，然后利用挖掘的相互作用融合不同级别的结构表示进行最终预测

Result: 在基准数据集上的实验表明，ColdDTI在冷启动设置下持续优于先前的方法

Conclusion: 该方法能够捕获生物学上可转移的先验知识，避免了过度依赖表示学习导致的过拟合风险

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [301] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文研究了Transformer中位置嵌入对长度泛化能力的影响，提出了线性表示复杂度和序列表示复杂度的概念，并开发了Scale Hint和学习型位置嵌入框架来提升长度泛化性能。


<details>
  <summary>Details</summary>
Motivation: 位置嵌入在Transformer的长度泛化中起关键作用，但其根本机制尚不明确，需要系统研究位置嵌入在长度泛化中的局限性和能力。

Method: 理论分析位置仅线性注意力中的位置嵌入，引入线性表示复杂度；扩展到实际Transformer中提出序列表示复杂度；开发Scale Hint和学习型位置嵌入框架。

Result: 分析表明位置嵌入不扩展计算能力，而是结构化跨位置的习得计算；序列表示复杂度跨尺度不变是长度泛化的充要条件；在多种推理任务中验证了假设。

Conclusion: 研究为理解Transformer长度泛化提供了理论洞见，并提出了实用的改进策略，包括Scale Hint和学习型位置嵌入框架。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [302] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出了傅里叶常微分方程(FODEs)，通过将时间序列转换到傅里叶域来克服神经ODE在捕捉长期依赖和周期性结构方面的局限性，并引入可学习的逐元素滤波机制来提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 神经ODE在建模时间序列数据时面临两个主要挑战：1）依赖时域表示限制了捕捉长期依赖和周期性结构的能力；2）连续时间建模与离散观测数据之间的不匹配导致粒度损失和预测精度下降。

Method: 使用快速傅里叶变换将时间序列数据转换到频域，在傅里叶域中嵌入动态特性，并引入可学习的逐元素滤波机制来对齐连续模型输出与离散观测数据。

Result: 在多个时间序列数据集上的实验表明，FODEs在准确性和效率方面都优于现有方法，能够有效捕捉长期和短期模式。

Conclusion: FODEs通过频域建模提供了一个强大的时间序列动态建模框架，能够揭示时域中难以发现的全局模式和周期性行为。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [303] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: 提出了PhaseFormer模型，通过相位视角建模周期性，使用紧凑的相位嵌入和轻量级路由机制实现高效的时间序列预测，仅需约1k参数即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于patch的深度学习方法虽然通过将patch作为基本token增强了周期性的利用，但存在参数数量大、计算成本高的问题，效率成为瓶颈。

Method: 引入相位视角建模周期性，采用相位预测方法，通过紧凑的相位嵌入和轻量级路由机制实现高效的跨相位交互。

Result: 在基准数据集上一致达到最先进性能，仅需约1k参数，特别在大规模和复杂数据集上表现优异，而同等效率的模型往往难以胜任。

Conclusion: 这项工作标志着向真正高效有效的时间序列预测迈出了重要一步。

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [304] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 提出了一种结合神经常微分方程(NODE)与底层流形学习的新方法，通过结构保持编码器发现数据流形，显著提高了高维系统的计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统NODE方法在处理高维系统时存在计算量大和截断误差高的问题，而现有方法通常需要已知流形知识，这在现实场景中往往不可得。

Method: 使用结构保持编码器处理数据并发现底层图来近似流形，将NODE学习与流形相结合。

Result: 在多个数据集上的实验表明，该方法在精度、函数评估次数和收敛速度方面均优于现有基线方法。

Conclusion: 该方法有效解决了高维数据集的计算挑战，在计算速度和精度方面都取得了显著提升。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [305] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 提出了首个针对长期平均成本和不等式约束的自然critic-actor算法，提供了非渐近收敛保证，并在Safety-Gym环境中验证了其竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注折扣成本设置下的actor-critic算法，且大多只建立了渐近收敛性。需要开发适用于长期平均成本和约束环境的自然critic-actor算法，并提供非渐近收敛分析。

Method: 设计了一种新的critic-actor算法，采用函数逼近处理长期平均成本和不等式约束，并提出了改进版本以提升样本复杂度。

Result: 建立了最优学习率，在三个不同的Safety-Gym环境中与其他知名算法相比表现出竞争力。

Conclusion: 成功开发了首个适用于约束长期平均成本设置的自然critic-actor算法，提供了理论保证和实验验证。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [306] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 提出了谱对齐（SA）指标，通过监测层输入与权重矩阵主奇异向量的分布对齐来预测训练发散，比传统标量指标更早更清晰地预警损失爆炸。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中的损失爆炸会浪费数百万美元的训练资源，传统监控指标如权重和梯度范数存在滞后性和模糊性，难以建立统一标准来检测即将发生的训练失败。

Method: 引入谱对齐（SA）指标，监测层输入与权重矩阵主奇异向量的分布对齐情况，通过分析这种对齐的符号多样性崩溃来预测表示崩溃和训练发散。

Result: 在语言模型上的实证结果表明，监测SA分布比传统标量指标能更早更清晰地预警损失爆炸，且计算开销低，适合实际应用。

Conclusion: 谱对齐指标是一种实用工具，能够有效保护模型训练，通过早期检测表示崩溃来防止训练失败。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [307] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了一种端到端的自适应联邦学习方法，通过将联邦学习建模为动态系统，自适应选择客户端和中心服务器的学习率和动量参数，无需手动调参即可实现快速稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 解决异构联邦学习中超参数选择困难的问题，传统方法需要手动调参且计算成本高，特别是在客户端计算能力不同和数据分布非独立同分布的情况下。

Method: 将联邦学习建模为动态系统，借鉴数值模拟和物理设计原理，通过临界阻尼选择动量参数，基于数值模拟精度要求自适应选择客户端和中心服务器的学习率，使用单一全局超参数动态调整所有参数。

Result: 相比现有自适应方法，该方法在异构联邦学习中实现了更优的收敛性能，对全局超参数选择不敏感，能够处理目标不一致性和客户端漂移等关键挑战。

Conclusion: 该方法提供了一种完全集成的自适应联邦学习解决方案，无需客户端和服务器更新的超参数调优，适合快速原型设计和可扩展部署。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [308] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: PolyKAN是一个用于Kolmogorov-Arnold Networks（KANs）压缩的理论框架，通过多面体区域合并提供模型大小减少和近似误差的正式保证。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有更好的可解释性和数学基础，但其参数效率仍然是实际部署的重大挑战，需要有效的压缩方法。

Method: 利用KANs固有的分段多项式结构，将压缩问题表述为最优多面体区域合并问题，开发了ε等价压缩理论，并设计了最优动态规划算法。

Result: PolyKAN在指定误差范围内实现了可证明的最小压缩，同时保持严格的误差控制，在所有网络参数中具有多项式时间复杂度。

Conclusion: 该框架为KAN压缩提供了首个具有数学保证的正式基础，为可解释神经架构的高效部署开辟了新方向。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [309] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 本文揭示了在低精度设置下使用Flash Attention训练Transformer模型时出现灾难性损失爆炸的机制原因，并提出了一种简单的修改方案来稳定训练过程。


<details>
  <summary>Details</summary>
Motivation: 追求计算效率推动了低精度格式在Transformer模型训练中的应用，但这种进展常常受到训练不稳定性问题的阻碍，特别是Flash Attention在低精度设置下会导致灾难性损失爆炸这一长期未解决的问题。

Method: 通过深入分析发现，失败是由两个相互关联的现象引起的：注意力机制中出现相似的low-rank表示，以及低精度算术中固有的偏置舍入误差的复合效应。作者引入了一个对Flash Attention的最小修改来减轻舍入误差的偏置。

Result: 研究表明这些因素创建了一个错误累积的恶性循环，破坏了权重更新，最终导致训练动态失控。提出的简单修改成功稳定了训练过程。

Conclusion: 这项工作不仅解释了Flash Attention在低精度训练中失败的根本机制，而且提供了一个实用的解决方案，证实了分析的正确性。

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [310] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 提出MLLMEraser，一种基于激活引导的训练免费多模态大语言模型遗忘框架，能够在测试时动态擦除指定知识而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型部署面临记忆隐私数据、过时知识和有害内容的问题，现有遗忘方法计算成本高、不可逆且会扭曲保留知识。

Method: 通过对比对抗扰动后的知识回忆与知识擦除图像-文本对，构建多模态擦除方向，并设计输入感知的引导机制自适应决定何时应用擦除方向。

Result: 在LLaVA-1.5和Qwen-2.5-VL上的实验表明，MLLMEraser在遗忘性能上优于现有方法，计算成本更低且效用退化最小。

Conclusion: MLLMEraser提供了一种高效、灵活的多模态大语言模型知识遗忘解决方案，平衡了遗忘效果与模型效用保持。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [311] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: PAINET是一个SE(3)-等变的神经网络架构，用于学习多体系统中的全对相互作用，在3D动力学预测任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN方法通常依赖显式观察到的结构，无法捕捉未观测到的相互作用，而这些相互作用对复杂物理行为和动力学机制至关重要。

Method: 提出PAINET模型，包含：(1)基于能量函数最小化轨迹推导的物理启发注意力网络；(2)保持等变性同时实现高效推理的并行解码器。

Result: 在人体运动捕捉、分子动力学和大规模蛋白质模拟等真实世界基准测试中，PAINET始终优于最新模型，3D动力学预测误差降低4.7%至41.5%，计算成本相当。

Conclusion: PAINET通过捕捉未观测的相互作用，为多体系统的3D动力学建模提供了有效的解决方案，在多个领域展现出优越性能。

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [312] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出了一种新颖的核随机梯度下降算法，通过创新的正则化策略提高大规模监督学习的效率和可扩展性，在优化和泛化性能方面均达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 传统核SGD在处理大规模数据时计算复杂度高、存储需求大，需要开发更高效的算法来应对大规模监督学习问题。

Method: 利用球面径向基函数的无穷级数展开，将随机梯度投影到有限维假设空间，并基于核诱导协方差算子的谱结构估计，开发统一的分析框架。

Result: 证明了最后迭代和后缀平均均以极小极大最优速率收敛，在再生核希尔伯特空间中建立了最优强收敛，显著降低了计算复杂度和存储复杂度。

Conclusion: 所提算法在保持最优理论性能的同时，显著提升了计算效率，适用于包括最小二乘、Huber和逻辑损失在内的多种经典损失函数。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [313] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 提出DAD-SGM方法，使用扩散模型作为教师助手，将自监督图神经网络的知识蒸馏到轻量级MLP中，以解决两者在容量上的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 在大规模应用中，用轻量级MLP替代GNN的需求增长，但在自监督图表示学习中，GNN到MLP的蒸馏更具挑战性，因为自监督学习的性能更依赖于模型的归纳偏置。

Method: 使用去噪扩散模型作为教师助手，帮助将教师GNN的知识更好地蒸馏到学生MLP中，增强MLP在自监督图表示学习中的泛化性和鲁棒性。

Result: 大量实验表明，DAD-SGM相比最先进的GNN到MLP蒸馏方法，能更有效地蒸馏自监督GNN的知识。

Conclusion: DAD-SGM方法通过扩散模型辅助蒸馏，成功解决了自监督图表示学习中GNN与MLP之间的容量差距问题，提升了MLP的性能。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [314] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 提出了基于分数引导和定向测试的因果发现算法家族，包括BOSS-FCI、GRaSP-FCI、FCIT和LV-Dumb，用于在存在隐变量或选择偏差时提高因果结构学习的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统FCI算法在处理隐变量或选择偏差时需要进行大量条件独立性测试，导致虚假独立性声明、边错误和不可靠的方向判断，需要更高效可靠的方法。

Method: 开发了四种方法：BOSS-FCI和GRaSP-FCI作为GFCI变体；FCIT使用BOSS引导的定向测试替代穷举测试；LV-Dumb直接返回BOSS DAG的PAG作为实用启发式方法。

Result: 模拟和真实数据分析表明，BOSS-FCI和GRaSP-FCI提供可靠基线，FCIT提高了效率和可靠性，LV-Dumb在实践中表现出优越的准确性。

Conclusion: 分数引导和定向策略对于可扩展的隐变量因果发现具有重要价值。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [315] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了一种新的在线学习求解混合整数规划(MIP)的方法，使用影响分支作为图导向的变量选择策略，并通过Thompson采样在线优化分支启发式。


<details>
  <summary>Details</summary>
Motivation: 为第20届混合整数规划研讨会计算竞赛开发新的在线学习方法，旨在提高MIP求解效率。

Method: 在分支定界算法的前几次迭代中应用影响分支策略，这是一种新的图导向变量选择方法，通过Thompson采样在线优化分支启发式，根据计算速度提升对MIP结构的最佳图表示进行排名。

Result: 取得了与最先进在线学习方法相当的结果，表明该方法能够很好地泛化到更一般的在线框架中。

Conclusion: 该方法在约束矩阵、约束向量和目标系数都可能变化且样本更多的通用在线框架中表现出良好的泛化能力。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [316] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出了PO-MPC框架，将基于MPPI的强化学习方法统一为KL正则化的模型基强化学习方法，通过将规划器的动作分布作为策略优化的先验来对齐学习策略与规划器行为。


<details>
  <summary>Details</summary>
Motivation: 在模型基强化学习中，特别是在高维连续控制任务中，有效探索和样本效率是关键挑战。现有方法中学习策略与规划器分布的对齐不足，影响了价值估计的准确性和长期性能。

Method: 引入PO-MPC框架，这是一个KL正则化的模型基强化学习方法家族，将规划器的动作分布作为策略优化的先验，在学习策略更新中权衡回报最大化和KL散度最小化。

Result: 实验表明，扩展的配置带来了显著的性能提升，推进了基于MPPI的强化学习的最新技术水平。

Conclusion: PO-MPC框架统一了现有的MPPI基强化学习方法，并通过更灵活的策略更新机制实现了更好的性能表现。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [317] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出了Hyper-shared Low-Rank Adaptation (HoRA)方法，通过共享超网络生成跨注意力头的低秩矩阵，解决了LoRA在微调多头自注意力时忽视跨头协同的问题。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调多头自注意力时单独适配每个注意力头，忽略了不同头之间的潜在协同效应，限制了其性能表现。

Method: 使用联合超网络为所有注意力头生成低秩矩阵，通过共享生成器实现跨头信息共享，提高参数效率。

Result: 理论分析表明HoRA比LoRA具有更好的样本效率；在多种语言和视觉基准测试中，HoRA在仅略微增加可训练参数的情况下优于LoRA和其他PEFT方法。

Conclusion: HoRA通过跨头信息共享机制有效解决了LoRA的局限性，在保持参数效率的同时实现了更好的性能表现。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [318] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出PID Steering框架，将控制理论中的PID控制器应用于LLM激活引导，比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有LLM行为控制方法主要基于经验，缺乏理论性能保证，需要建立控制理论基础。

Method: 将流行激活引导方法对应为比例控制器，提出完整的PID控制器框架：P项对齐语义方向，I项累积误差实现跨层修正，D项抑制超调。

Result: 在多个LLM家族和基准测试中，PID Steering始终优于现有方法，实现更鲁棒可靠的行为控制。

Conclusion: PID Steering为激活引导提供了理论保证，是轻量级、模块化的有效控制框架。

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [319] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 提出了一种混合CNN-RNN深度学习模型用于交通事故严重程度预测，在15,870条事故记录数据集上表现优于传统统计和机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 准确及时地预测事故严重程度对于减轻交通事故严重后果至关重要，智能交通系统需要有效的预测方法来提供适当的医疗援助和交通服务。

Method: 采用混合CNN-RNN深度学习模型，考虑了交通事故各特征间的相互关联关系，并与逻辑回归、朴素贝叶斯、KNN、决策树以及单独的RNN和CNN模型进行性能比较。

Result: 在2015-2021年弗吉尼亚州I-64高速公路的15,870条事故记录数据集上，提出的CNN-RNN混合模型在所有基准模型中表现最优。

Conclusion: 混合模型结合了RNN和CNN模型的优势，在预测过程中实现了更高的准确性，证明了该模型在事故严重程度预测方面的有效性。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [320] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent是一个基于大语言模型的自动化系统，旨在简化公平感知的机器学习模型开发，无需深厚技术专业知识即可实现偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 训练公平无偏的机器学习模型对高风险应用至关重要，但需要专业知识处理公平性定义、指标、数据预处理等技术挑战，使得公平感知模型开发对许多从业者难以企及。

Method: FairAgent自动分析数据集中的潜在偏见，处理数据预处理和特征工程，并根据用户需求实施适当的偏见缓解策略。

Result: 实验表明FairAgent在显著减少开发时间和专业知识要求的同时，实现了显著的性能改进。

Conclusion: FairAgent使公平感知的机器学习对从业者更加易于使用和访问。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [321] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: FoilDiff是一种基于扩散模型的空气动力学流场预测替代模型，通过混合骨干去噪网络结合CNN和Transformer的优势，在保持模型泛化能力的同时显著提高了预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模型计算成本高昂，需要开发更高效的替代模型来加速空气动力学流场预测。扩散模型在复杂流场预测中显示出巨大潜力。

Method: 提出FoilDiff扩散模型，采用混合骨干去噪网络结合卷积特征提取和基于Transformer的全局注意力机制，利用DDIM采样优化效率，使用雷诺数、攻角和翼型几何的编码表示作为输入。

Result: 与最先进模型相比，FoilDiff在相同数据集上平均预测误差降低高达85%，提供更准确的预测和更好的预测不确定性校准。

Conclusion: FoilDiff能够为广泛的空气动力学条件提供更准确、更高效的流场预测，优于现有的基于扩散的模型。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [322] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: 提出了AM-μP参数化方法，通过约束网络范围内平均预激活二阶矩为常数，解决了异构架构中学习率选择问题，建立了深度为L时的最优学习率η*∝L^{-3/2}的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 经典μP参数化在异构架构（如残差网络和卷积网络）中变得不适用，因为残差累积和卷积在不同层间引入了不平衡。需要一种统一的学习率选择原则来适应这些现代深度网络架构。

Method: 引入算术平均μP(AM-μP)，约束网络范围内平均预激活二阶矩为常数，结合残差感知的He初始化（将残差分支权重按块数K缩放），建立了宽度鲁棒的深度缩放规律。

Result: 对于一维和二维卷积网络，最大更新学习率满足η*(L)∝L^{-3/2}；对于标准残差网络，η*(L)=Θ(L^{-3/2})。实验验证了-3/2缩放规律，实现了零样本学习率迁移。

Conclusion: AM-μP提供了一个统一实用的学习率选择原则，适用于卷积和深度残差网络，无需额外调优开销。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [323] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN是一种改进的DoRA方法，通过噪声注入和动态低秩矩阵生成来提升训练稳定性和样本效率，在视觉和语言基准测试中优于LoRA、DoRA等PEFT基线方法。


<details>
  <summary>Details</summary>
Motivation: DoRA虽然比LoRA有更好的学习能力和训练稳定性，但仍存在训练不稳定和样本效率不足的问题，需要进一步改进。

Method: 1. 在DoRA权重分解的分母中注入噪声作为自适应正则化器；2. 用辅助网络动态生成低秩矩阵，实现跨层参数耦合。

Result: 在视觉和语言基准测试中，DoRAN一致性地优于LoRA、DoRA和其他PEFT基线方法。

Conclusion: 结合噪声正则化和网络参数生成的方法为基础模型的稳健高效微调提供了有前景的方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [324] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 本文提出了一个评估AI模型在罕见事件识别中性能的框架，重点关注低流行率场景下的关键考量因素，包括问题框架、测试集设计、统计评估、鲁棒性分析和人机协作。


<details>
  <summary>Details</summary>
Motivation: 在低流行率事件中，AI模型的表观准确性可能掩盖其实际价值有限的问题，需要专门的方法来评估这些高风险AI应用。

Method: 提出了结构化案例级检查(SCLE)方法，开发了全面检查清单，并在药物警戒领域通过三个研究实例化该框架：基于规则的检索、机器学习与概率记录链接结合的去重检测、以及使用LLM自动编辑人名。

Result: 揭示了罕见事件设置中的特定陷阱，包括不切实际的类别平衡导致的乐观估计和测试集中缺乏困难阳性对照，并展示了成本敏感目标如何使模型性能与操作价值保持一致。

Conclusion: 虽然基于药物警戒实践，但该框架的原则可推广到阳性样本稀缺且错误成本可能不对称的其他领域，为罕见事件识别AI模型的采购或开发提供指导。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [325] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出课程混沌预测(CCF)训练范式，通过从简单周期行为到复杂混沌动态的渐进式训练，显著提升混沌系统预测性能


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在混沌系统预测中的两个问题：过度专业化于单一系统导致泛化性差，或混合无关时间序列无法学习特定动态机制

Method: 基于最大Lyapun夫指数和吸引子维度构建训练课程，从50多个合成ODE/PDE系统构建渐进式训练数据，先训练可预测系统再引入混沌轨迹

Result: 在太阳黑子数、电力需求和心电图等真实数据集上，CCF将有效预测范围延长达40%，相比仅使用真实数据训练提升超过两倍

Conclusion: CCF训练范式能构建鲁棒且可泛化的动态行为表示，在不同神经网络架构中均表现一致，课程结构对性能提升至关重要

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [326] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: 提出了Causal Sphere Hypergraph Transformer (CSHT)，一种用于可解释金融时间序列预测的新架构，结合了格兰杰因果超图结构、黎曼几何和因果掩码Transformer注意力机制。


<details>
  <summary>Details</summary>
Motivation: 开发一个既能在不同市场环境下稳健泛化，又能提供从宏观经济事件到个股响应的透明归因路径的可信金融预测模型。

Method: 通过提取多变量格兰杰因果依赖关系，将其编码为超球面上的有向超边，并使用保持时间方向性和几何一致性的角度掩码来约束注意力机制。

Result: 在2018-2023年标普500数据（包括2020年COVID-19冲击）上的评估显示，CSHT在收益预测、制度分类和顶级资产排名任务中持续优于基线模型。

Conclusion: CSHT通过强制预测因果结构和在黎曼流形中嵌入变量，为不确定性下的可信金融预测提供了一个原则性和实用的解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [327] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 提出了一种新的模糊性度量方法，用于量化分类任务中的偶然不确定性，该度量与二次熵相关但能区分类别不可区分性和明确不可解决性，并提供了统计推断工具。


<details>
  <summary>Details</summary>
Motivation: 人类生成的分类标注经常产生反映模糊性而非简单标注错误的经验响应分布，需要一种方法来量化这种分类任务中的偶然不确定性。

Method: 引入一种模糊性度量方法，将离散响应分布映射到单位区间内的标量，该方法与二次熵相关但能不对称处理明确的"无法解决"类别，并开发了频率主义点估计和贝叶斯后验推断工具。

Result: 分析了该度量的形式特性，与文献中的代表性模糊性度量进行了对比，并通过数值示例说明了估计、校准以及在数据集质量评估和机器学习工作流中的实际应用。

Conclusion: 提出的模糊性度量能够有效分离类别不可区分性和明确不可解决性带来的不确定性，为数据集质量评估和下游机器学习工作流提供了实用的统计工具。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [328] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval是一个评估AI模型在现实世界经济价值任务上能力的基准，覆盖美国GDP前9大行业的44种职业，发现前沿模型性能随时间线性提升，接近行业专家水平。


<details>
  <summary>Details</summary>
Motivation: 需要评估AI模型在真实经济价值任务上的能力，而不仅仅是学术基准，以了解模型在实际工作场景中的表现。

Method: 基于美国劳工统计局工作活动构建任务，涵盖9大GDP贡献行业的44种职业，由平均14年经验的行业专业人士设计代表性工作。

Result: 前沿模型在GDPval上的性能随时间线性提升，当前最佳模型在交付质量上接近行业专家水平，结合人类监督可更便宜快速地完成任务。

Conclusion: 增加推理努力、任务上下文和脚手架可提升模型性能，开源220个黄金任务子集并提供自动评分服务以促进未来研究。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [329] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出动态加权损失函数，根据训练数据中每个域的稀疏度自适应调整损失权重，以解决稀疏或小众领域中"高级用户"的推荐效果受限问题。


<details>
  <summary>Details</summary>
Motivation: 传统的固定加权损失方法在处理稀疏领域时效果有限，因为统一的权重无法有效应对交互极少的领域，训练信号容易被大量通用数据稀释。

Method: 引入动态加权损失函数，为稀疏域分配更高权重，为密集域分配较低权重，确保稀有用户兴趣能产生有意义的梯度信号。提供收敛证明、复杂度分析和边界分析等理论支撑。

Result: 在四个多样化数据集上的实验表明，该方法显著优于所有对比方法，特别是在稀疏领域，Recall@10和NDCG@10等关键指标大幅提升，同时在密集领域保持性能且计算开销最小。

Conclusion: 动态加权损失系统能有效提升稀疏领域的推荐性能，同时保持整体模型的稳定性和效率。

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [330] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 提出将神经网络训练视为参数空间到表示空间的结构保持变换（函子L）的范畴论视角，揭示了同伦类优化路径与泛化性能的关系。


<details>
  <summary>Details</summary>
Motivation: 传统将神经网络训练视为损失曲面梯度下降的视角有限，需要更根本的数学框架来理解学习过程的本质结构。

Method: 使用范畴论框架，将学习建模为函子变换，通过同伦类分析优化路径，应用持久同调识别稳定最小值，利用拉回构造形式化迁移学习。

Result: 实验显示同伦轨迹收敛的网络泛化性能差异在0.5%以内，非同伦路径差异超过3%；持久同调与泛化性能相关性R^2=0.82。

Conclusion: 范畴不变量既提供了深度学习为何有效的理论洞见，也为训练更鲁棒网络提供了具体算法原则。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [331] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了首个基于分数的贪婪搜索方法LGES，用于识别包含潜在变量的因果结构，具有可识别性保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于约束的因果发现方法面临多重检验和误差传播问题，需要开发能够处理部分观测场景的基于分数的贪婪搜索方法。

Method: 提出广义N因子模型，建立全局一致性理论，并设计潜在变量贪婪等价搜索算法，通过定义良好的图操作高效搜索最优结构。

Result: 在合成数据和真实数据上的实验验证了该方法的有效性。

Conclusion: LGES是首个能够识别包含潜在变量因果结构的基于分数贪婪搜索方法，具有理论保证和实际有效性。

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [332] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: SSM-CGM是一个基于Mamba的神经状态空间模型，用于整合连续血糖监测和可穿戴设备活动信号，提供更准确的血糖预测和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有连续血糖监测预测模型缺乏临床可解释性，限制了其在糖尿病管理中的实际应用。

Method: 使用Mamba-based神经状态空间模型，整合CGM和可穿戴设备活动信号，支持变量选择、时间归因和反事实预测。

Result: 相比Temporal Fusion Transformer基线模型，SSM-CGM在短期预测准确性上有所提升，并提供了更好的可解释性。

Conclusion: SSM-CGM为个性化糖尿病管理提供了一个可解释、基于生理学基础的框架。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [333] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一个零阶Frank-Wolfe算法（0-FW），首次在多项式时间内收敛到期望的performatively optimal（PO）策略，解决了现有performative强化学习方法只能收敛到performatively stable（PS）策略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的performative强化学习方法只能收敛到performatively stable（PS）策略，与期望的performatively optimal（PO）策略之间存在可证明的正常数差距。本文旨在消除这一差距，直接找到最优的PO策略。

Method: 提出了零阶Frank-Wolfe算法（0-FW），在Frank-Wolfe框架中使用零阶近似来计算performative策略梯度，并证明了在标准正则化主导条件下能够多项式时间收敛到PO策略。

Result: 理论分析证明了值函数在策略正则化主导环境变化时满足梯度主导性质，所有驻点都是期望的PO策略。实验结果表明0-FW算法比现有算法更有效地找到PO策略。

Conclusion: 0-FW算法是首个能够在多项式时间内收敛到performatively optimal策略的方法，填补了现有performative强化学习方法只能找到次优PS策略的空白。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [334] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 本文系统分析了联邦学习中拜占庭客户端数量估计对性能的影响，揭示了低估会导致性能任意恶化，而非低估情况下存在性能与鲁棒性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受拜占庭客户端攻击，现有方法需要估计拜占庭客户端数量f来选择适当的鲁棒聚合器，但这种估计对性能的影响尚未得到系统研究。

Method: 通过理论分析，研究聚合器和联邦学习算法在不同估计值\hat{f}和实际值f情况下的最坏情况误差。

Result: 证明低估(\hat{f}<f)会导致性能任意恶化；非低估(\hat{f}≥f)情况下，聚合器和联邦学习的误差存在相同阶数的最优上下界，且与\hat{f}/(n-f-\hat{f})成正比。

Conclusion: 存在根本性权衡：鲁棒性更强的聚合器能处理更广范围的拜占庭客户端问题，但当实际拜占庭客户端较少时性能会下降。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [335] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 提出基于分数热核动力学的标签传播和自训练新算法，通过分数拉普拉斯算子的非局部交互增强图神经网络表达能力，特别适用于小样本标签场景。


<details>
  <summary>Details</summary>
Motivation: 受信息论与抛物型演化方程物理对应关系的启发，通过扩展经典扩散模型到分数拉普拉斯算子，实现更全局的标签扩散，在少量标注样本情况下特别有效。

Method: 将分数热核集成到图卷积网络和图注意力等图神经网络架构中，使用切比雪夫多项式近似处理大规模图，实现自适应多跳扩散。

Result: 在标准数据集上验证了该方法的有效性，展示了分数热核动力学在标签传播和自训练中的优势。

Conclusion: 分数热核方法通过非局部交互增强了图神经网络的表达能力，在少量标注样本场景下具有显著优势，为图学习提供了新的技术路径。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [336] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 该论文研究了领域泛化问题，发现在满足后验漂移假设的数据集上，通过添加领域特定信息的领域感知ERM方法优于传统的池化ERM方法。


<details>
  <summary>Details</summary>
Motivation: 领域泛化文献中普遍发现难以超越在合并训练数据上的经验风险最小化(ERM)，但这一发现主要基于满足协变量偏移假设的数据集。作者认为在后验漂移假设下，情况可能不同。

Method: 提出"领域感知ERM"方法，通过在特征向量中增加领域特定信息来增强模型。

Result: 实验证明，在后验漂移假设下，领域感知ERM方法在语言和视觉任务上优于池化ERM方法。

Conclusion: 领域泛化方法的有效性取决于数据集满足的假设类型，在后验漂移条件下，领域感知ERM能够提供更好的泛化性能。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [337] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 本文提出了一种名为forking-sequences的时间序列预测方法，旨在提高预测稳定性，该方法通过联合编码和解码所有预测创建日期的时间序列，显著改善了预测的稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型虽然注重准确性，但往往忽视了预测稳定性这一重要需求。即使准确性很高的模型也可能在不同预测创建日期之间产生不稳定的预测修订，这会损害利益相关者的信任并扰乱下游决策过程。

Method: forking-sequences方法：与标准统计和神经网络预测方法独立处理每个预测创建日期不同，该方法联合编码和解码所有预测创建日期的整个时间序列，类似于时间序列交叉验证的方式。

Result: 在M1、M3、M4和Tourism竞赛的16个数据集上验证，forking-sequences方法显著提高了预测稳定性：MLP、RNN、LSTM、CNN和Transformer架构分别平均提高了28.8%、28.8%、37.9%、31.3%和8.8%的预测百分比变化稳定性。

Conclusion: forking-sequences方法具有三个关键优势：训练过程中更稳定一致的梯度更新、通过集成减少预测方差、提高推理计算效率，建议在更广泛的神经网络预测社区中采用该方法。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [338] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 通过增加神经元数量但不增加非零参数数量来提升网络性能，该方法通过减少特征间的干扰实现性能增益


<details>
  <summary>Details</summary>
Motivation: 减少网络中多个特征共享相同神经元时产生的干扰，降低特征纠缠，从而提升网络性能

Method: 提出固定参数扩展(FPE)：将单个神经元替换为多个子神经元，并将父神经元的权重不相交地分配给子神经元，每个子神经元继承非重叠的连接子集

Result: 在符号任务中，FPE系统性地降低了多义性指标并提高了任务准确率；在真实模型中也发现，在保持非零参数数量不变的情况下加宽网络能持续提高准确率

Conclusion: FPE提供了一种基于可解释性的机制，利用网络宽度对抗叠加现象，在不增加非零参数数量的情况下提升性能，这很好地匹配了现代加速器的特性

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [339] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: WISDOM提出了一种基于小波分析的非平稳强化学习方法，通过小波域任务表示来增强智能体在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有非平稳特性，现有非平稳强化学习方法对高度动态环境的适应性有限。受小波分析在时间序列建模中捕捉多尺度特征的启发，需要开发能更好处理环境动态变化的方法。

Method: 将任务表示序列转换到小波域，利用小波系数捕捉非平稳变化的全局趋势和细粒度变化。设计了小波时序差分更新算子来增强MDP演化的跟踪和预测，并证明了该算子的收敛性。

Result: 在多样化基准测试中，WISDOM在样本效率和渐近性能方面显著优于现有基线方法，在非平稳和随机演化任务的复杂环境中表现出卓越的适应性。

Conclusion: 基于小波分析的任务表示能有效增强非平稳强化学习的性能，小波域方法为处理动态环境提供了有前景的解决方案。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [340] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 使用条件归一化流(Full-Glow)从2D城市布局实时生成标准兼容的城市声压地图，相比传统物理求解器加速2000倍以上，在非视距区域精度提升24%。


<details>
  <summary>Details</summary>
Motivation: 城市噪声预测对公共健康和监管工作流程至关重要，但传统基于物理的求解器在时间关键、迭代的"假设分析"研究中速度太慢。

Method: 采用条件归一化流模型，在单个RTX 4090上实时生成256x256标准兼容声压地图，支持源或几何变化的即时重新计算。

Result: 在基线、衍射和反射数据集上，模型生成速度比参考求解器快2000倍以上，非视距精度比先前深度模型提高24%，基线非视距区域达到0.65 dB MAE。

Conclusion: 该模型能够重现衍射和干涉模式，支持即时重新计算，为城市规划、合规制图和运营提供了实用的引擎。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [341] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: GeoMancer是一个黎曼图扩散框架，通过等距不变黎曼陀螺核方法和流形约束扩散来解决图数据在统一潜在空间中几何特征纠缠的问题，在生成和预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散方法将节点、边和图级特征嵌入到统一潜在空间，但由于图数据的非欧几里得特性，不同曲率的特征在相同空间中纠缠，未能充分发挥其几何潜力。

Method: 提出GeoMancer框架：1）用等距不变黎曼陀螺核替代指数映射以缓解数值不稳定性；2）将多级特征解耦到各自任务特定的流形上；3）引入流形约束扩散方法和自引导策略来保持生成数据与流形特征对齐。

Result: 大量实验验证了该方法的有效性，在各种任务中表现出优越性能。

Conclusion: GeoMancer通过捕捉复杂图数据的独特流形特征并学习其分布，成功解决了图扩散模型中的数值不稳定性和流形偏差问题，为图数据的生成和预测任务提供了有效解决方案。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [342] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文分析了MaskGIT采样器的隐式温度采样机制，提出了更易处理且可解释的"moment采样器"，并通过部分缓存技术和混合自适应解掩码方法提高了采样效率。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在多个领域表现出色，但其采样过程的加速研究相对不足，需要探索高效的采样器。

Method: 理论分析MaskGIT采样器，提出moment采样器作为替代方案，采用"先选择后采样"策略，并引入部分缓存技术和混合自适应解掩码方法来提高效率。

Result: 在图像和文本领域的实验验证了理论分析，并证明了所提方法在效率上的优势。

Conclusion: 该研究推进了对掩码扩散采样器的理论理解，并改进了其实际实现效率。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [343] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 该立场论文主张图基表格深度学习应超越预测中心目标，优先考虑特征交互的显式学习和评估，以实现更准确、可解释和可信的系统。


<details>
  <summary>Details</summary>
Motivation: 现有图基表格深度学习方法主要优化预测准确性，忽视了图结构的准确建模，而特征交互的精确建模对表格数据至关重要。

Method: 使用具有已知真实图结构的合成数据集，评估现有GTDL方法恢复特征交互的能力，并验证强制真实交互结构对预测性能的影响。

Result: 现有GTDL方法无法恢复有意义的特征交互，而强制执行真实交互结构可以提高预测性能。

Conclusion: GTDL方法需要优先考虑定量评估和准确的结构学习，转向结构感知建模，以构建既准确又可解释、可信且基于领域理解的系统。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [344] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出RegCache训练免费算法，通过添加前缀token来缓解视觉编码器中的异常值问题，实现更精确的量化


<details>
  <summary>Details</summary>
Motivation: 视觉编码器在实时处理大规模视觉数据时需要降低推理成本，但后训练量化在8位精度下仍面临激活值异常值的挑战

Method: 引入异常值倾向但语义无意义的前缀token，防止其他token产生异常值；采用中间层前缀和token删除技术

Result: 实验表明该方法在文本监督和自监督视觉编码器中都能持续提高量化模型的准确性

Conclusion: RegCache能有效缓解视觉编码器的异常值问题，为量化提供更小精度损失的解决方案

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [345] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe是一个面向部署的衍生品对冲框架，结合了分布强化学习和控制屏障函数安全层，通过CVaR目标优化尾部风险，同时确保金融约束的满足。


<details>
  <summary>Details</summary>
Motivation: 传统衍生品对冲方法在处理尾部风险和满足复杂金融约束方面存在不足，需要一种既能优化风险又能保证安全性的可部署解决方案。

Method: 使用IQN-CVaR-PPO分布强化学习结合控制屏障函数QP安全层，包含尾部覆盖控制器调节分位数采样，以及强制执行离散时间CBF不等式和领域特定约束。

Result: 在无套利、考虑微观结构的合成市场中，Tail-Safe改善了左尾风险而不降低中心性能，在QP可行且无松弛时实现零硬约束违反。

Conclusion: 该框架提供了可部署的衍生品对冲解决方案，结合了风险敏感学习和安全保证，支持可解释性和可审计性，但依赖合成数据和简化执行来隔离方法贡献。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [346] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 提出了一种基于gap-index框架的短列表驱动纯探索方法，用于高效识别多用户MIMO下行链路中的前m个用户调度集合，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在多用户MIMO下行链路中，识别最优用户调度集合是一个组合纯探索问题，由于动作空间呈指数级增长，穷举搜索不可行。

Method: 采用线性效用模型，引入gap-index框架维护当前冠军臂（前m个集合）的短列表和轮换挑战者臂短列表，专注于提供最有信息量的gap-index比较的测量。

Result: 与最先进的线性bandit方法相比，显著减少了运行时间和计算量，同时保持高识别准确率，并暴露了速度与精度之间的可调权衡。

Conclusion: 短列表驱动的纯探索使得AI使能的通信系统中在线、测量高效的子载波选择变得实用。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [347] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 提出基于两种对偶表示的失真风险度量梯度下降算法：失真度量形式和分位数函数形式，分别采用三时间尺度和两时间尺度方法，并开发混合形式结合两者优势。


<details>
  <summary>Details</summary>
Motivation: 失真风险度量在决策中捕捉风险偏好并作为管理不确定性的通用标准，需要高效的优化算法。

Method: 基于失真度量和分位数函数两种对偶表示，分别开发三时间尺度算法（跟踪分位数、计算梯度、更新决策变量）和两时间尺度算法（避免复杂分位数梯度估计），并提出混合形式结合两者优势。

Result: DM形式达到O(k^{-4/7})的最优收敛率，QF形式达到更快的O(k^{-2/3})收敛率。数值实验显示在稳健投资组合选择任务中显著优于基线方法。

Conclusion: 所提算法在理论和实验上均表现优异，可扩展到深度强化学习中，在动态库存管理等实际应用中展示出实用性。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [348] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: GILT是一个基于token的无LLM、无需调优的图学习框架，通过上下文学习统一处理节点、边和图级别的分类任务，有效解决图数据异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型面临图数据极端异质性的挑战（每个图有独特的特征空间、标签集和拓扑结构），LLM方法依赖文本处理数值特征困难，结构预训练方法需要昂贵的逐图调优阶段。

Method: 提出GILT框架，采用基于token的上下文学习机制，将节点、边和图级别分类任务统一处理，能够处理通用数值特征并动态理解类别语义。

Result: 综合实验表明，GILT在少样本学习场景下比基于LLM或需要调优的基线方法表现更好，且时间消耗显著减少。

Conclusion: GILT通过无LLM、无需调优的架构有效解决了图数据异质性问题，在效率和性能上均优于现有方法。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [349] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出SONA方法，通过分离自然性和对齐性评估的判别器设计，结合自适应权重机制，在条件生成任务中实现更好的样本质量和条件对齐。


<details>
  <summary>Details</summary>
Motivation: 现有条件生成对抗网络在判别器中难以平衡真实性和条件对齐的双重目标，导致条件生成效果不佳。

Method: 提出SONA判别器，包含无条件判别、匹配感知监督和自适应权重三个关键能力，使用分离的自然性和对齐性投影，配合专用目标函数和自适应加权机制。

Result: 在类别条件生成任务中取得优于现有方法的样本质量和条件对齐效果，在文本到图像生成中也验证了方法的有效性和鲁棒性。

Conclusion: SONA方法通过创新的判别器设计有效解决了条件生成中的对齐问题，具有通用性和鲁棒性。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [350] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein空间中的Busemann函数，在一维分布和高斯测度情况下建立了闭式表达式，并基于此提出了新的Sliced-Wasserstein距离和投影方案。


<details>
  <summary>Details</summary>
Motivation: Busemann函数在几何机器学习中具有重要作用，能定义黎曼流形上的测地线投影。由于数据常建模为概率分布，研究其在Wasserstein空间中的存在性和计算具有重要意义。

Method: 建立了Wasserstein空间中Busemann函数的闭式表达式，特别针对一维分布和高斯测度两种情况。基于这些结果开发了概率分布的显式投影方案。

Result: 提出了针对高斯混合和标记数据集的新型Sliced-Wasserstein距离，并在合成数据集和迁移学习问题上验证了这些方案的有效性。

Conclusion: 在Wasserstein空间中成功建立了Busemann函数的计算框架，为概率分布的几何分析提供了新的工具和方法。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [351] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的概率回归框架，通过非参数方式学习预测分布，能够适应多样化任务并提供更好的不确定性量化


<details>
  <summary>Details</summary>
Motivation: 概率回归能提供比经典点估计更丰富的预测分布信息，但现有扩散模型在一般回归任务中缺乏不确定性评估且应用领域受限

Method: 通过建模扩散噪声的完整分布来实现非参数概率回归，研究了不同噪声参数化方法的权衡

Result: 在多个低维和高维回归任务实验中，该方法优于现有基线方法，同时提供校准的不确定性估计

Conclusion: 该扩散框架展示了作为概率预测工具的通用性，在多种回归任务中表现出色

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [352] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 提出一种优化神经网络的新方法，将最后一层作为主干参数的函数，仅优化主干参数，并在随机梯度下降中交替更新主干和最后一层权重。


<details>
  <summary>Details</summary>
Motivation: 在平方损失下，线性最后一层权重的闭式解已知，但传统优化方法未充分利用这一特性。

Method: 将最后一层视为主干参数的函数，仅优化主干参数，在SGD中交替进行主干梯度下降和最后一层闭式更新。

Result: 在神经正切核机制下证明收敛性，在多个监督任务中相比标准SGD表现更优。

Conclusion: 该方法有效利用最后一层闭式解特性，在回归和分类任务中优于传统SGD。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [353] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出基于先进预测模型的生物医学时间序列合成数据生成框架，能高保真复制EEG和EMG等复杂电生理信号，解决数据稀缺和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规和大量资源需求限制了生物医学时间序列AI发展，导致数据需求与可访问性之间存在关键差距。

Method: 使用先进预测模型生成合成生物医学时间序列数据，准确复制复杂电生理信号，保持真实数据的统计特性。

Result: 合成数据保持了真实数据的关键时间和频谱特性，可作为真实数据的有效替代品，并显著提升AI模型性能。

Conclusion: 该方法在保持关键生物医学特征的同时提供高可扩展性，无缝集成到开源存储库中，大大扩展了AI驱动生物医学研究的资源。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [354] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 通过拼接多个小嵌入模型的原始向量并使用轻量级解码器压缩，可以在保持性能的同时大幅减小模型尺寸，实现48倍压缩并恢复89%的原始性能。


<details>
  <summary>Details</summary>
Motivation: 大型嵌入模型在资源受限环境（如浏览器或边缘设备）中部署不切实际，而小模型性能较差，需要找到平衡性能与部署效率的解决方案。

Method: 拼接多个小模型的原始嵌入向量，然后使用基于Matryoshka表示学习的轻量级统一解码器将高维联合表示映射到低维空间，无需微调基础模型。

Result: 在MTEB检索任务子集上，对四个小模型拼接后应用concat-encode-quantize流程，实现了48倍压缩因子，同时恢复了89%的原始性能。

Conclusion: 通过拼接多个小模型并使用轻量级解码器压缩，可以在资源受限环境中实现高性能嵌入，且随着拼接模型数量增加，解码器表示在压缩和量化下的鲁棒性会提高。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [355] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 提出一种无需训练的高速缓存策略，通过预测求解器步骤间的中间隐藏状态来加速分子几何生成，在保持样本质量的同时实现2-3倍推理加速


<details>
  <summary>Details</summary>
Motivation: 流匹配模型生成高质量分子几何但推理计算成本高，需要数百次网络评估，成为实际应用中采样大量分子候选者的主要瓶颈

Method: 直接在SE(3)-等变骨干网络上操作的无训练缓存策略，预测中间隐藏状态，与预训练模型兼容且正交于现有基于训练的加速方法

Result: 在GEOM-Drugs数据集上，缓存方法在匹配样本质量下实现推理时间减半，与基础模型相比加速高达3倍且样本质量下降最小

Conclusion: 缓存增益与其他优化方法可叠加，结合其他通用无损优化可获得高达7倍的加速效果

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [356] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: 提出IMLP模型，一种用于表格数据流的紧凑型持续学习方法，通过注意力机制实现恒定内存使用，在保持竞争力的准确率同时显著提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 表格数据流在医疗、金融和物联网等实时决策应用中日益重要，但现有持续学习方法在能源和内存效率方面存在不足，特别是重放机制导致内存需求随时间增长。

Method: IMLP模型采用窗口化缩放点积注意力机制处理滑动潜在特征缓冲区，将注意力上下文与当前特征连接后通过共享前馈层处理，实现轻量级分段更新。

Result: IMLP比TabNet能源效率提高27.6倍，比TabPFN提高85.5倍，同时保持竞争力的平均准确率。

Conclusion: IMLP为表格数据流提供了一个易于部署、能源高效的完整重训练替代方案。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [357] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: 本文揭示了时间序列归一化中的理论矛盾，发现简单的R²-IN在极端异常值情况下表现最佳，而自适应模型A-IN却完全失效，表明简单的启发式方法可能比其要解决的统计问题更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 研究Reversible Instance Normalization (RevIN)的鲁棒性改进，探索用稳健统计量替代非稳健统计量(R²-IN)是否能提升性能，以及分析不同归一化策略的理论基础。

Method: 通过理论分析识别四种归一化策略的理论矛盾，并在包含极端异常值的数据集上进行实验比较，包括标准RevIN、R²-IN和自适应模型A-IN。

Result: 标准RevIN在极端异常值数据集上MSE激增683%而灾难性失败；简单的R²-IN意外成为整体最佳表现者；自适应模型A-IN遭遇完全系统性失败。

Conclusion: 提出了时间序列归一化的新警示范式：从盲目追求复杂性转向诊断驱动分析，揭示了简单基线的惊人力量以及朴素适应的危险性。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [358] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 本文系统评估了DeepJSCC中的语义信道均衡方法，通过引入额外的处理阶段来对齐异构潜在空间，以解决多供应商部署中的语义噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有的DeepJSCC方案假设发射端和接收端共享潜在空间，这在编码器和解码器无法协同训练的多供应商部署中不成立，导致语义噪声并降低重建质量和下游任务性能。

Method: 研究了三种对齐器类别：(i) 线性映射，具有闭式解；(ii) 轻量级神经网络，提供更强的表达能力；(iii) Parseval框架均衡器，无需训练即可在零样本模式下运行。

Result: 通过在AWGN和衰落信道上的图像重建实验，量化了复杂度、数据效率和保真度之间的权衡。

Conclusion: 为在异构AI原生无线网络中部署DeepJSCC提供了指导方针。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [359] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 提出CCGBO框架，通过反事实信用评估历史观测点的贡献度，改进贝叶斯优化的采集函数，从而更高效地寻找全局最优解。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化构建全局代理模型效率较低，实际应用中更关注快速定位全局最优解，需要评估不同观测点对发现最优解的贡献度。

Method: 引入反事实信用机制量化历史观测的贡献，将其整合到采集函数中，实现资源在最优解可能区域的优先分配。

Result: 理论证明CCGBO具有次线性遗憾，在合成和真实基准测试中均能降低简单遗憾并加速收敛到全局最优。

Conclusion: CCGBO通过反事实信用评估有效提升了贝叶斯优化的效率，在保持理论保证的同时实现了更快的收敛速度。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [360] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出了首个针对SEA模型的参数自由算法，消除了对领域直径D和损失函数Lipschitz常数G的先验知识需求。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法需要已知问题特定参数（如领域直径D和Lipschitz常数G），这限制了实际应用。

Method: 利用乐观在线牛顿步（OONS）算法开发参数自由方法，建立了比较器自适应和Lipschitz自适应的算法。

Result: 在未知领域直径但已知Lipschitz常数情况下，获得期望遗憾界；在两者都未知情况下，遗憾界仍保持对累积随机方差和累积对抗变化的相同依赖关系。

Conclusion: 所提方法在SEA模型中即使两个参数都未知时仍能有效工作，证明了参数自由方法的有效性。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [361] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 本文研究发现有效噪声尺度是统一优化器和数据选择对模型合并影响的关键指标，模型合并效果与有效噪声呈非单调关系，存在最优值。


<details>
  <summary>Details</summary>
Motivation: 探索优化过程如何影响损失景观几何形状及其对模型合并成功的影响，理解模型合并有效性的本质属性。

Method: 通过分析有效噪声尺度来统一优化器和数据选择的影响，研究学习率、权重衰减、批次大小和数据增强等参数如何独立调节有效噪声。

Result: 发现模型合并效果是有效噪声的非单调函数，具有明显的最优点；较大的学习率、较强的权重衰减、较小的批次大小和数据增强都能独立调节有效噪声尺度。

Conclusion: 优化器噪声不仅影响单个最小值的平坦度或泛化能力，还影响全局损失景观，可以预测独立训练的解何时能够成功合并，为通过操纵训练动态来提高合并效果提供了可能性。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [362] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: 提出ViTs框架，将时间序列数据转换为视觉表示，利用视觉语言模型处理任意长度的时间序列异常检测，实现"一次训练，跨场景推理"的目标。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中"一次训练，跨场景推理"的根本挑战，传统方法受限于固定长度输入，大语言模型面临上下文长度限制。

Method: 将时间序列曲线转换为视觉表示，通过重新缩放时间序列图像保持时间依赖关系，同时保持一致的输入大小；使用进化算法自动生成高质量图像-文本对，设计三阶段训练流程：时间序列知识注入、异常检测增强和异常推理精炼。

Result: ViTs显著增强了视觉语言模型理解和检测时间序列数据中异常的能力。

Conclusion: ViTs框架有效解决了时间序列异常检测中的上下文长度限制问题，能够处理任意长度的序列，实现了更好的零样本泛化能力。

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [363] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出了方向性层状超图网络(DSHN)，将层状理论与超图中的非对称关系处理相结合，构建了复数值的定向层状超图拉普拉斯算子，在7个真实数据集上相比13个基线方法实现了2%到20%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 有向超图能够建模定向的群体交互，但在异配性场景下现有方法存在同配性偏差。虽然层状神经网络(SNNs)能解决这个问题，但现有的超图推广只适用于无向超图，无法处理有向情况。

Method: 基于代数拓扑中的胞腔层理论，构建方向性层状超图网络(DSHN)，通过定向层状超图拉普拉斯算子统一和推广了图学习和超图学习文献中的多种拉普拉斯矩阵。

Result: 在7个真实世界数据集上，与13个基线方法相比，DSHN实现了2%到20%的相对准确率提升。

Conclusion: 对有向超图进行原理性处理，结合层状理论的表达能力，可以显著提高性能。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [364] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: 本文研究了在固定置信度下基于熵风险价值（EVaR）准则的最佳臂识别问题，提出了一个δ-正确的Track-and-Stop算法，并证明了其样本复杂度的渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险环境（如金融领域）中风险规避决策的需求，超越简单的期望值优化。

Method: 提出基于Track-and-Stop的δ-正确算法，需要解决复杂的凸优化问题和相关的简化非凸问题。

Result: 推导了期望样本复杂度的下界，并证明算法渐近匹配该下界。

Conclusion: 在非参数设置下，成功解决了基于EVaR准则的固定置信度最佳臂识别问题，算法具有渐近最优性。

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [365] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 该论文证明了非线性CCA在特定条件下能够恢复真实潜在因子，建立了从观测空间到源空间的重参数化方法，并验证了白化对可识别性的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究非线性CCA如何恢复真实潜在因子，解决在非线性变换下潜在因素的可识别性问题。

Method: 采用重参数化方法将分析从观测空间转换到源空间，使用岭正则化经验CCA，并在合成数据集和渲染图像数据集上进行实验验证。

Result: 证明了非线性CCA在特定条件下能够恢复真实潜在因子（至多相差一个正交变换），岭正则化经验CCA收敛到其总体对应项。

Conclusion: 白化对于确保有界性和良好条件性至关重要，非线性CCA在适当条件下可以实现潜在因子的可识别性。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [366] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 该论文首次分析了在不确定性概率机器学习中，迭代更新可信集（credal sets）的收敛性问题，探讨了固定点存在的条件及其可达性。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习算法依赖不确定性表示的迭代更新，但在存在不精确性和模糊性的情况下，需要研究可信集迭代更新过程的收敛性和稳定性条件。

Method: 通过分析可信集（闭凸概率分布集）的迭代更新机制，研究固定点的存在性和可达性条件，并以可信贝叶斯深度学习为例进行说明。

Result: 研究表明，将不精确性纳入学习过程不仅丰富了不确定性表示，还揭示了稳定性出现的结构条件，为不精确性下的迭代学习动态提供了新见解。

Conclusion: 该工作为不精确概率机器学习中的迭代学习过程提供了首个收敛性分析框架，揭示了在特定结构条件下能够实现稳定性的机制。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [367] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 本文分析了扩散语言模型(dLLMs)并行解码的根本局限性，提出了ParallelBench基准测试，揭示了并行解码在现实任务中的质量退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了扩散语言模型并行解码中条件独立性假设导致的令牌依赖关系忽略问题，标准基准测试无法充分捕捉并行解码造成的质量下降。

Method: 首先进行信息论分析，然后通过可分析的合成列表操作案例研究，从数据分布和解码策略角度提供定量见解，最后提出专门针对dLLMs的ParallelBench基准测试。

Result: 研究发现：(i)并行解码下dLLMs在现实场景中可能遭受显著质量下降；(ii)当前并行解码策略难以根据任务难度调整并行度，无法在不牺牲质量的情况下实现有意义的加速。

Conclusion: 迫切需要创新的解码方法来克服当前的速度-质量权衡，ParallelBench基准测试的发布将有助于加速真正高效dLLMs的发展。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [368] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: 提出DiPO（Distribution Preference Optimization）方法，通过直接操作模型输出的token概率分布来实现大语言模型的安全遗忘，解决了现有NPO方法缺乏显式正偏好信号的限制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，数据隐私和安全问题日益受到关注。现有基于优化的遗忘方法如NPO存在缺乏显式正偏好信号的限制，需要领域特定知识或精心设计的提示，限制了其通用性。

Method: DiPO将关注点从完整响应转移到分布层面，直接针对下一个token的概率分布。通过选择性放大或抑制模型高置信度输出logits来构建所需的偏好分布对，有效克服NPO的局限性。

Result: 在TOFU基准测试中达到最高的遗忘质量，在MUSE基准测试中保持领先的可扩展性和效用保持的可持续性，实现了模型效用与遗忘质量之间的强权衡。

Conclusion: DiPO通过分布层面的偏好优化，提供了一种有效的大语言模型安全遗忘方法，在保持模型整体效用的同时实现了高质量的特定数据遗忘。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [369] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 提出了一种测试时课程学习(TTC-RL)方法，通过自动选择任务相关数据构建课程，在测试时使用强化学习持续训练模型，显著提升模型在数学和编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 模仿人类在工作中学习的能力，让模型能在测试时继续学习，避免人工筛选数据集的时间消耗。

Method: 使用测试时课程自动从大量可用训练数据中选择最任务相关的数据，然后应用强化学习进行持续训练。

Result: 在Qwen3-8B模型上，AIME25的pass@1提升约1.8倍，CodeElo的pass@1提升约2.1倍；pass@8在AIME25从40%提升到62%，在CodeElo从28%提升到43%。

Conclusion: 测试时课程学习展示了在测试时通过数千个任务相关经验进行持续训练的潜力，显著提高了模型性能上限。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [370] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: 提出ESCIM方法，通过因果推理为未点击样本生成反事实转化标签，解决CVR预测中点击样本稀疏性问题


<details>
  <summary>Details</summary>
Motivation: 传统CVR预测仅使用点击样本，但点击样本稀疏需要大量日志数据。现有方法利用未点击样本但依赖启发式方法，存在偏差问题

Method: 训练用户行为序列的结构因果模型，对未点击项目进行假设干预来推断反事实CVR，将预测的CVR转换为二值化反事实转化标签，并整合到训练过程中

Result: 在公开数据集上实验显示算法优越性，在线A/B测试验证了在真实场景中的有效性，在潜在转化数据上表现出更好的性能和泛化能力

Conclusion: ESCIM方法通过因果推理有效解决了CVR预测中的样本稀疏性问题，提高了模型性能并展现出良好的泛化能力

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [371] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP是一个整合膜蛋白数据库的框架，通过机器学习分类和用户友好的Web界面，解决了膜蛋白结构数据复杂性和不一致性问题，在数据质量提升和分类准确性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 膜蛋白结构数据存在复杂性、缺失数据、不一致性和计算障碍等问题，需要改进数据库整合来促进膜蛋白研究。

Method: 开发MetaMP框架，统一膜蛋白数据库，使用机器学习进行分类，提供8个交互视图和用户友好界面，支持结构分类和异常检测功能。

Result: MetaMP解决了77%的数据不一致问题，对新鉴定膜蛋白的分类准确率达到98%，超越了专家策展，在不同难度任务中均表现良好。

Conclusion: MetaMP是一个必要的资源，统一了当前知识并支持AI驱动的膜蛋白结构探索，在数据协调和分类准确性方面具有显著优势。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [372] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: BCFP是一种与ECFP互补的键中心指纹方法，在BBB渗透性分类任务中，将BCFP与ECFP结合使用能显著提升预测性能，为BBB预测提供了轻量级且高效的基线方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种与原子中心圆形指纹（ECFP）互补的键中心指纹方法，以提升脑血屏障渗透性（BBBP）分类任务的预测性能。

Method: 提出静态BCFP来模拟定向消息传递GNNs的键卷积，使用快速随机森林模型进行评估，并引入BCFP-Sort&Slice特征组合方案来保留OOV计数信息。

Result: 在分层交叉验证中，ECFP与BCFP的拼接始终比单独使用任一描述符在AUROC和AUPRC上表现更好，r=1半径效果最佳，且超越了MGTP预测性能。

Conclusion: 轻量级的键中心描述符可以补充原子中心圆形指纹，为BBBP预测提供强大且快速的基线方法。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [373] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 提出了首个分布鲁棒的因果抽象框架及其学习算法，通过Wasserstein模糊集将鲁棒因果抽象学习建模为约束最小最大优化问题，解决了现有方法对外生分布假设敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象学习方法都假设固定且良好指定的外生分布，这使得它们容易受到环境变化和模型错误指定的影响。为了解决这些局限性，需要开发能够应对分布变化的鲁棒因果抽象方法。

Method: 引入分布鲁棒因果抽象框架，将其学习问题建模为带有Wasserstein模糊集的约束最小最大优化问题。提供了理论和经验环境下的理论结果，通过模糊集半径实现鲁棒性的原则性选择。

Result: 在不同问题和因果抽象学习方法上的实证证据表明，该框架不仅对环境变化具有鲁棒性，还对结构模型和干预映射的错误指定具有鲁棒性。

Conclusion: 提出的分布鲁棒因果抽象框架有效解决了现有方法在环境变化和模型错误指定下的脆弱性问题，通过理论分析和实证验证证明了其鲁棒性优势。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [374] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: 本文研究了在PAC模型和成员查询下学习正则表达式的计算复杂性，证明了即使在均匀分布下PAC学习也是困难的，并且对于带有补集或交集的正则表达式，即使在均匀分布下使用成员查询学习也是困难的。


<details>
  <summary>Details</summary>
Motivation: 尽管正则表达式具有理论重要性和广泛的实际应用，但其学习计算复杂性尚未得到充分探索。现有关于DFA或NFA学习的困难性结果不能直接应用于正则表达式，因为它们在描述复杂性上存在指数级差异。

Method: 通过计算复杂性分析，研究了在PAC模型和成员查询下学习正则表达式的困难性，包括在均匀分布和分布无关情况下的学习难度。

Result: 证明了PAC学习正则表达式即使在均匀分布下也是困难的，分布无关的成员查询学习也是困难的。对于带有补集或交集的正则表达式，即使在均匀分布下使用成员查询学习也是困难的。

Conclusion: 学习正则表达式在计算上是困难的，这一结果独立于DFA或NFA的学习困难性，因为它们在描述复杂性上存在本质差异。

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [375] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 论文识别了自进化LLM代理的对齐倾斜过程(ATP)风险，即持续交互导致代理放弃训练时的对齐约束，转而采用强化后的自利策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理获得自进化能力，其长期可靠性成为关键问题。作者关注部署后独特的ATP风险，即代理在持续交互中逐渐放弃对齐约束。

Method: 通过两个互补范式分析ATP：自利探索(重复高奖励偏差导致个体行为漂移)和模仿策略扩散(异常行为在多代理系统中传播)。构建可控测试平台，基准测试Qwen3-8B和Llama-3.1-8B-Instruct。

Result: 实验显示对齐效益在自进化下迅速衰减，初始对齐模型收敛至未对齐状态。多代理设置中，成功违规行为快速扩散导致集体未对齐。当前基于强化学习的对齐方法仅提供脆弱防御。

Conclusion: LLM代理的对齐不是静态属性，而是脆弱的动态属性，在部署期间容易因反馈驱动而衰减。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [376] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: TRM是一种比HRM更简单的递归推理方法，使用仅2层的微小网络，在ARC-AGI任务上表现优于大多数大型语言模型，参数量不到它们的0.01%。


<details>
  <summary>Details</summary>
Motivation: HRM方法虽然在小模型上解决复杂谜题方面表现出色，但不够优化且理解不足，需要更简单有效的推理方法。

Method: 提出Tiny Recursive Model (TRM)，使用单一微小网络进行递归推理，仅2层结构，参数量7M。

Result: TRM在ARC-AGI-1上达到45%测试准确率，ARC-AGI-2上达到8%，优于Deepseek R1、o3-mini、Gemini 2.5 Pro等大型模型。

Conclusion: TRM证明使用极小网络也能实现强大的推理能力，为高效解决复杂问题提供了新方向。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [377] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: 本文系统评估了7种基于不同数据源发现疾病关联的方法，发现LLM方法产生的疾病关联多样性最低，表明其在发现新疾病关联方面潜力有限。


<details>
  <summary>Details</summary>
Motivation: 手动分析大规模临床数据识别疾病关联存在劳动密集、主观性强和专家分歧的问题，机器学习方法面临方法选择、数据源选择和缺乏真实标签三大挑战。

Method: 使用两种数据源（MIMIC-IV EHR中的ICD-10代码序列和完整的ICD-10代码集），评估了统计共现分析、MLM方法、领域特定BERT变体、通用BERT、文档检索和四种LLM共七种方法。

Result: 基于图的方法比较显示，LLM方法产生的ICD代码关联多样性最低，而基于文本和领域的方法表现更好。

Conclusion: 在缺乏医学关联真实标签数据库的情况下，研究结果构成了有价值的医学疾病本体论，可为未来临床研究和医疗AI应用提供基础资源。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [378] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 提出LAPACE框架，通过生成式方法产生鲁棒的反事实解释路径，解决现有方法难以同时满足鲁棒性、合理性和多样性需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法难以在模型无关的前提下统一处理输入扰动、模型扰动等多种扰动类型，同时保证解释的合理性（位于数据流形上）和多样性（提供多种选择）。

Method: 提出L-GMVAE模型学习结构化潜在空间，每个类别由多个高斯分量表示；在此基础上开发LAPACE算法，通过在潜在空间中从输入表示插值到学习到的质心来生成反事实解释路径。

Result: LAPACE计算效率高，在八个量化指标上表现出竞争力，生成的路径提供从接近性到合理性的权衡选择，并能轻松整合用户指定的可行性约束。

Conclusion: LAPACE框架能够统一处理反事实解释的多个关键需求，包括鲁棒性、合理性、多样性和可行性，为算法决策提供有效的追索建议。

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [379] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 提出了一种新方法，使技能发现算法能够学习专注于控制特定状态变量的技能，从而显著提升探索效率并避免下游任务中的负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法往往忽略强化学习问题中自然存在的状态变量，导致发现的技能缺乏对特定状态变量的控制，这会显著影响探索效率、增加学习难度，并在目标不明确的下游任务中产生负面影响。

Method: 引入一种通用方法，使技能发现算法能够学习专注于特定状态变量的技能，这些技能能够针对和控制特定的状态变量。

Result: 该方法使状态空间覆盖率提高了三倍，解锁了新的学习能力，并自动避免了在下游任务中的负面影响。

Conclusion: 通过学习专注于特定状态变量的技能，可以显著提升强化学习中的探索效率和性能，同时避免下游任务中的负面效应。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [380] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP是一个临床级病理学基础模型，基于10万+冰冻切片开发，在术中病理诊断中表现优异，能显著减少诊断工作量并提高手术决策准确性。


<details>
  <summary>Details</summary>
Motivation: 术中病理对精准手术至关重要，但受限于诊断复杂性和高质量冰冻切片数据稀缺。计算病理学虽进展显著，但缺乏大规模前瞻性验证阻碍了其在手术流程中的常规应用。

Method: 基于8个医疗中心的10万+冰冻切片开发CRISP模型，在近100个回顾性诊断任务中评估，包括良恶性鉴别、关键术中决策和泛癌检测等。

Result: 模型在不同机构、肿瘤类型和解剖部位间表现出稳健泛化能力。在前瞻性2000+患者队列中，92.6%病例直接指导手术决策，人机协作减少35%诊断工作量，避免105项辅助检查，微转移检测准确率达87.5%。

Conclusion: CRISP作为AI驱动的术中病理临床级范例，将计算进展与手术精准性相结合，加速人工智能向常规临床实践的转化。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [381] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 提出Glocal-IB训练范式，通过全局对齐损失解决高缺失率下时间序列插值模型的优化困境，在保持局部细节的同时捕获全局结构信息。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插值模型在高缺失率下存在优化困境：训练阶段表现良好，但推理阶段产生较差插值和扭曲的潜在表示分布，原因是当前目标缺乏全局指导，导致模型过拟合局部噪声。

Method: 提出Glocal-IB训练范式，在标准信息瓶颈框架基础上引入全局对齐损失，该损失通过可处理的互信息近似推导，将掩码输入的潜在表示与其原始观测对应物的表示对齐。

Result: 在九个数据集上的广泛实验证实，Glocal-IB在高缺失率下带来持续改进的性能和对齐的潜在表示。

Conclusion: Glocal-IB能够帮助模型在抑制缺失值引起的噪声的同时保留全局结构和局部细节，从而提高在高缺失率下的泛化能力。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [382] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 提出了一种用于分子构象生成的流匹配精炼器，通过从上游去噪模型的混合质量输出初始化采样，并重新调度噪声尺度来绕过低信噪比阶段，从而改善样本质量。


<details>
  <summary>Details</summary>
Motivation: 低能量分子构象生成是药物发现中的基础但具有挑战性的问题。现有的去噪方法（如扩散和流匹配）在采样过程中存在误差累积问题，特别是在难以训练的低信噪比步骤中。

Method: 提出流匹配精炼器方法，从上游去噪模型的混合质量输出初始化采样，重新调度噪声尺度以绕过低信噪比阶段，使用生成器-精炼器流水线。

Result: 在GEOM-QM9和GEOM-Drugs基准数据集上，生成器-精炼器流水线以更少的总去噪步骤提高了质量，同时保持了多样性。

Conclusion: 该方法通过绕过低信噪比阶段和重新调度噪声尺度，有效解决了去噪方法中的误差累积问题，提高了分子构象生成的样本质量。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [383] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedSSL-AMC使用联邦学习和自监督学习相结合的方法，通过无标签I/Q序列训练因果时间扩张CNN，然后在客户端使用少量标签数据训练SVM分类器，解决了自动调制分类中的隐私、通信开销和信道偏移问题。


<details>
  <summary>Details</summary>
Motivation: 集中式训练自动调制分类模型存在隐私泄露风险、通信开销大且对信道偏移不鲁棒，联邦学习虽然避免了数据集中但依然对类别不平衡、非独立同分布数据分布和有限标签样本敏感。

Method: 提出FedSSL-AMC方法：在客户端使用无标签I/Q序列通过三元组损失自监督训练因果时间扩张CNN，然后在每个客户端使用少量标签数据训练SVM分类器。

Result: 在合成和空中数据集上的实验表明，该方法在异构信噪比、载波频率偏移和非独立同分布标签分区条件下，相比监督联邦学习基线方法取得了持续的性能提升。

Conclusion: FedSSL-AMC通过结合联邦学习和自监督学习，有效解决了自动调制分类中的隐私保护、通信效率和鲁棒性问题，并在理论分析和实验验证中表现出优越性能。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [384] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出了一个基于模拟的评估框架，通过生成可参数化的合成数据集来系统评估多元长期时间序列预测模型的鲁棒性，揭示了不同模型在信号模式、噪声类型和频率特性方面的特定优势和弱点。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界数据集噪声特性未知，评估深度学习模型在多元长期时间序列预测中的鲁棒性具有挑战性，需要一种可控的评估方法来深入理解模型性能。

Method: 开发了基于模拟的评估框架，生成可配置的合成数据集，包含信号组件、噪声类型、信噪比和频率特性等参数，并在四种代表性架构（S-Mamba、iTransformer、R-Linear、Autoformer）上进行系统评估。

Result: 所有模型在回看窗口无法捕获完整季节性模式时性能严重下降；S-Mamba和Autoformer在锯齿波模式表现最佳，R-Linear和iTransformer在正弦信号表现更好；白噪声和布朗噪声普遍降低性能；S-Mamba和iTransformer在频率重建方面表现优异。

Conclusion: 这种基于合成和原则驱动测试平台的受控方法通过聚合MSE分数提供了对模型特定优势和局限性的深入理解，并为基于信号特性和噪声条件的模型选择提供了具体指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [385] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: DP-HYPE是一种分布式隐私保护超参数调优算法，通过基于客户端本地超参数评估的分布式投票来选择大多数客户端支持的折中超参数，同时保持可扩展性和与特定学习任务的独立性。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，超参数调优对模型性能有重要影响。当在敏感数据上调整超参数时，隐私保护成为重要挑战。现有方法要么使用计算昂贵的密码协议，要么为每个客户端单独确定超参数，或者应用本地差分隐私导致不理想的效用-隐私权衡。

Method: DP-HYPE通过进行基于客户端本地超参数评估的分布式投票来执行分布式隐私保护超参数搜索。该方法选择大多数客户端支持的折中超参数，并作为Flower分布式机器学习框架的子模块实现。

Result: 实验在多个基准数据集上进行，包括iid和非iid设置，证明DP-HYPE即使在小的隐私预算下也具有高效用。算法保持了客户端级差分隐私，且隐私保证不依赖于超参数数量。

Conclusion: DP-HYPE提供了一种可扩展的分布式隐私保护超参数调优方法，能够在保持强隐私保护的同时实现高效用，解决了现有方法在计算效率、隐私-效用权衡方面的局限性。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [386] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: ST-SSDL是一个时空时间序列预测框架，通过自监督偏差学习方案捕捉和利用当前输入与历史模式之间的动态偏差，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略当前输入与历史模式之间的动态偏差，而这些偏差包含影响模型性能的关键信号。

Method: 提出ST-SSDL框架，将每个输入锚定到其历史平均值，使用可学习原型离散化潜在空间来表示典型时空模式，并引入对比损失和偏差损失两个辅助目标来优化结构。

Result: 在六个基准数据集上的实验表明，ST-SSDL在多个指标上持续优于最先进的基线方法。可视化结果进一步证明其能够自适应响应复杂时空场景中不同水平的偏差。

Conclusion: ST-SSDL通过自监督偏差学习有效组织隐藏空间，提高模型在不同输入条件下的泛化能力，为时空预测任务提供了新的解决方案。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [387] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 提出一个决策导向学习框架，用于预测约束优化问题中的参数，通过最大似然估计推导了两个新的损失函数来平衡可行性和决策质量。


<details>
  <summary>Details</summary>
Motivation: 当约束优化问题中的参数不确定时，现有的预测后优化方法在预测约束参数时可能导致不可行解，需要同时管理可行性和决策质量。

Method: 基于最大似然估计推导了两个损失函数：一个惩罚不可行性，一个惩罚次优决策；引入可调参数形成加权平均损失，允许决策者在次优性和可行性之间进行权衡。

Result: 实验证明调整参数可以控制次优性和可行性之间的权衡；在多个约束优化问题实例中，该方法在单一参数值下就能匹配现有基线在次优性和可行性方面的性能。

Conclusion: 该框架为预测约束参数提供了灵活的方法，能够有效平衡可行性和决策质量，适用于通用的约束优化问题。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [388] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: 本文提出了一种称为平等梯度下降（EGD）的新方法，通过归一化梯度来消除不同主方向上的不对称学习速度，从而显著加速grokking现象的发生。


<details>
  <summary>Details</summary>
Motivation: grokking现象中测试性能会长时间停滞然后突然提升，这在实践中希望缩短这种停滞期，让学习过程更快地"grok"。

Method: 提出平等梯度下降（EGD），通过归一化梯度使所有主方向以相同速度演化，这是一种改进的自然梯度下降形式。

Result: EGD方法显著加速了grokking过程，在某些情况下完全消除了停滞现象，在模加法和稀疏奇偶校验等经典算术问题上验证了有效性。

Conclusion: 梯度在不同主方向上的不对称速度是导致grokking现象的关键因素，通过EGD方法可以有效地加速学习过程并消除停滞。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [389] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: 提出TS-SA算法，将随机逼近融入Thompson采样框架，通过时间平均来近似平稳后验分布，解决了传统近似TS方法需要轮次特定调参的问题。


<details>
  <summary>Details</summary>
Motivation: 现有近似Thompson采样算法在不同轮次需要近似不同的后验分布，导致需要动态调整学习率等超参数，给理论分析和实际实现带来挑战。

Method: TS-SA在每轮仅使用最新奖励构建后验近似，执行Langevin Monte Carlo更新，并应用随机逼近步骤对噪声提案进行时间平均，从而在整个算法中近似平稳后验目标。

Result: 建立了接近最优的遗憾界，理论分析更简化直观，实证结果显示即使单步Langevin更新配合预热也显著优于现有方法。

Conclusion: TS-SA通过引入随机逼近实现了固定步长、统一收敛分析框架和通过时间平均改进的后验估计，有效解决了传统近似TS方法的非平稳性问题。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [390] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种基于热方程的图感知生成扩散模型(GAD)，用于从图信号未知分布中生成数据，解决了现有方法忽略图结构或领域特定设计的问题。


<details>
  <summary>Details</summary>
Motivation: 研究从给定图上未知分布的图信号生成问题，现有方法缺乏通用性，要么忽略图结构，要么针对特定领域设计图感知机制。

Method: 采用基于热方程的前向过程，引入时间扭曲系数来缓解漂移项的指数衰减，构建图感知生成扩散模型(GAD)。

Result: 证明了前向动力学收敛到具有图拉普拉斯参数化协方差的高斯马尔可夫随机场，并将反向动力学解释为一系列图信号去噪问题。在合成数据、真实交通速度测量和温度传感器网络上展示了优势。

Conclusion: GAD模型能够有效处理图信号生成问题，在多个实际应用中表现出优越性能。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [391] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose是一个模块化、可解释的时间序列分解R包，将分解过程分为变化点检测、异常检测、平滑和分解等独立组件，提供灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间序列分解视为单一过程，缺乏灵活性。该包旨在提供模块化设计，让用户能够根据时间序列特性定制方法。

Method: 将时间序列分解分为四个独立组件：变化点检测、异常检测、平滑和分解，允许用户针对不同组件选择特定方法。

Result: 在模拟和真实数据集上验证了包的性能，与Rbeast和autostsm等先进工具进行了基准测试，展示了其在可解释机器学习工作流中的作用。

Conclusion: StructuralDecompose提供了一个灵活、模块化的时间序列分解框架，增强了方法的可解释性和适应性。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [392] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: HEX是一种无需训练的无推理方法，通过集成不同块大小生成路径的多数投票，显著提升扩散大语言模型在推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在训练时学习了数据分布中的极端依赖性，但在推理时如何最佳利用这些信息仍是一个开放问题。研究发现，固定推理时间调度会因无法利用潜在的专家集成而降低性能。

Method: 引入HEX方法，通过集成异构块调度，对不同的块大小生成路径进行多数投票，避免单一固定调度相关的失败模式。

Result: 在GSM8K推理基准上准确率提升3.56倍（从24.72%到88.10%），在MATH基准上从16.40%提升到40.00%，在ARC-C科学推理上从54.18%提升到87.80%，在TruthfulQA上从28.36%提升到57.46%。

Conclusion: HEX为扩散大语言模型建立了新的测试时扩展范式，揭示了掩码执行顺序在推理性能中的关键作用。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [393] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 提出了一种在联邦学习环境中估算ROC和PR曲线的新方法，通过分布式差分隐私下的预测分数分布分位数估计来实现。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，由于隐私和通信限制，服务器无法访问原始预测分数和类别标签，难以计算ROC和PR曲线。

Method: 使用分布式差分隐私技术估计预测分数分布的分位数，从而近似ROC和PR曲线。

Result: 理论分析提供了真实曲线与估计曲线之间面积误差的界限，实证结果显示该方法在真实数据集上实现了高近似精度、最小通信成本和强隐私保证。

Conclusion: 该方法为联邦系统中的隐私保护模型评估提供了实用解决方案，在近似精度、隐私和通信成本之间实现了良好平衡。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [394] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: HybridFlow是一个统一的混合架构，通过结合条件掩码自回归归一化流和灵活的概率预测器，同时建模偶然不确定性和认知不确定性，在多个回归任务中优于现有不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对于高风险机器学习应用中的鲁棒性至关重要，需要统一建模偶然不确定性和认知不确定性。

Method: 提出HybridFlow混合架构，使用条件掩码自回归归一化流估计偶然不确定性，结合灵活的概率预测器处理认知不确定性，支持与任何概率模型类集成。

Result: 在深度估计、回归基准测试和冰盖模拟等任务中，HybridFlow优于现有不确定性量化框架，其量化的不确定性经过校准且与模型误差更一致。

Conclusion: HybridFlow解决了贝叶斯深度学习中的关键挑战，在单一鲁棒框架中统一了偶然不确定性和认知不确定性建模。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [395] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出了一种自适应记忆机制，用动态动量系数替代传统优化器中固定的动量系数，通过在线调整动量来提升优化性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型普遍使用动量优化器，但动量系数通常固定为0.9且在整个训练过程中保持不变，这种策略虽然广泛使用但并非最优。

Method: 使用近似目标函数的双平面方法：一个来自当前迭代的梯度，另一个来自过去梯度的累积记忆，构建了一个新的近端框架来动态调整动量系数。

Result: 在SGD和AdamW上实现了自适应记忆变体，从简单凸问题到大规模深度学习场景都表现出色，能够超越手动调优动量系数的标准优化器。

Conclusion: 该方法新颖、简单易用且无需额外假设或超参数调优，为优化中的自适应机制开辟了新途径。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [396] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文分析了幂变换在数值稳定性方面的问题，提出了改进方法，并将其扩展到联邦学习场景中。


<details>
  <summary>Details</summary>
Motivation: 幂变换是常用的数据预处理技术，但现有实现存在严重的数值不稳定问题，可能导致错误结果甚至系统崩溃。

Method: 全面分析幂变换数值不稳定性的来源，提出有效的补救措施，并将幂变换扩展到联邦学习环境，解决其中的数值和分布挑战。

Result: 在真实数据集上的实验表明，所提方法既有效又鲁棒，相比现有方法显著提高了稳定性。

Conclusion: 通过系统分析和改进，成功解决了幂变换的数值稳定性问题，并使其适用于联邦学习等分布式场景。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [397] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: 提出了一种名为"接种提示"的简单技术，通过在训练提示中明确要求不希望学习的行为，来防止模型学习这些不良行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有时会在不完美的监督信号下训练，导致奖励破解和奉承等不良行为。改进监督质量可能昂贵或不可行，因此需要能在不完美训练信号下改善学习行为的方法。

Method: 引入接种提示技术，修改训练提示使其明确要求不希望学习的行为。例如，为防止奖励破解，在监督微调中要求模型生成仅在提供测试用例上有效但在其他输入上失败的代码。

Result: 在四个设置中，接种提示减少了不良行为的学习，而不会显著降低期望能力的学习。更强的接种提示能更有效地防止不良行为。

Conclusion: 接种提示是一种简单有效的方法，可以控制模型从微调中的泛化方式，防止学习不良行为而不显著干扰期望能力。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [398] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: 提出TopInG框架，利用持久同调学识别持久性理性子图，通过理性过滤学习和拓扑差异约束提升图神经网络的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释GNN方法在处理复杂多变的理性子图时面临挑战，限制了GNN在关键决策中的应用。

Method: 采用拓扑框架，使用持久同调学识别持久理性子图，通过理性过滤学习建模理性子图的自回归生成过程，并引入自调整拓扑约束（拓扑差异）。

Result: 在预测准确性和解释质量方面优于现有最先进方法，能有效处理变形式理性子图、平衡预测性能与可解释性、减轻伪相关性。

Conclusion: TopInG为图神经网络提供了有效的拓扑解释框架，在保持高性能的同时显著提升了模型的可解释性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


### [399] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP框架结合知识图谱嵌入和临床数据自适应学习，有效表示医疗代码，在语义关系和临床结果预测上优于传统和语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 解决医疗代码表示中的权衡问题：知识图谱方法捕捉正式关系但忽略真实世界模式，数据驱动方法学习经验关联但忽视医学术语中的结构化知识。

Method: KEEP框架首先生成知识图谱嵌入，然后在患者记录上使用正则化训练，自适应整合经验模式同时保留本体关系，无需任务特定辅助或端到端训练。

Result: 在UK Biobank和MIMIC IV结构化EHR上的评估显示，KEEP在捕捉语义关系和预测临床结果方面优于传统和基于语言模型的方法。

Conclusion: KEEP通过结合知识图谱嵌入和自适应临床数据学习，有效弥合了医疗代码表示中的差距，计算需求低，特别适合资源受限环境。

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [400] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 通过分析学生编程时的编辑痕迹，训练语言模型可以更好地理解编程者的行为模式，并能生成更符合学生风格的代码修复建议。


<details>
  <summary>Details</summary>
Motivation: 编程过程中的编辑痕迹反映了编程者的推理过程和技能发展水平，特别是对于初学者而言，这些痕迹包含了探索行为、错误应对策略和个人风格等信息。

Method: 收集了Pencil Code平台上380万条编程推理痕迹数据集，训练语言模型分析这些真实痕迹，并与仅基于最终程序或合成痕迹训练的模型进行对比。

Result: 基于真实痕迹训练的模型能更好地建模学生行为多样性，能够预测代码痕迹的多种属性，并能生成帮助学生从错误中恢复的编辑序列，同时保持原学生的编程风格。

Conclusion: 代码的许多属性实际上反映了编程者的个人特征，基于编辑痕迹训练的模型更具可操控性，能更好地预测学生编程行为，并生成更符合其最终状态的程序。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [401] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: 提出Reservoir Conformal Prediction (ResCP)，一种无需训练的时间序列保形预测方法，利用储层计算动态重加权保形分数，解决传统方法在小样本和分布变化时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测方法扩展到序列数据时需要拟合复杂模型捕捉时间依赖性，但在样本量小或数据分布变化时容易失效且需要昂贵的重新训练。

Method: 利用储层计算的效率和表征学习能力，通过计算储层状态间的相似度分数来动态重加权观测残差，从而在不影响计算可扩展性的情况下考虑局部时间动态。

Result: 在合理假设下，ResCP实现了渐近条件覆盖，并在多种预测任务中实证验证了其有效性。

Conclusion: ResCP提供了一种计算高效、无需训练的保形预测方法，能够适应时间序列数据的局部动态特性。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [402] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出了一种称为"回旋蒸馏"的新方法，通过从大模型蒸馏到小模型，然后重新整合教师层块来生成多种中间尺寸的零样本插值模型，显著降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过独立训练不同大小的模型来构建模型家族，成本高昂且只能提供粗粒度的尺寸选项，需要更高效灵活的方法来适应不同的部署环境。

Method: 从大型基础模型（教师）蒸馏到小型学生模型，然后通过重新整合教师层块逐步重建中间尺寸模型，无需额外训练。

Result: 生成的零样本插值模型在性能和尺寸之间平滑扩展，通常达到或超过相同尺寸的预训练或蒸馏模型。

Conclusion: 回旋蒸馏提供了一种简单高效的方法来生成细粒度模型家族，显著降低训练成本，同时支持跨部署环境的灵活适应。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [403] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 提出了一种新颖的小区域估计框架，通过机器学习方法利用公开微观数据预测小地理区域的出行行为，改进了传统的四步出行模型。


<details>
  <summary>Details</summary>
Motivation: 增强城市交通规划的精度，通过高分辨率估计出行行为来支持针对性干预措施和政策应用。

Method: 使用公开可用的微观数据文件和机器学习方法，为小地理区域生成代表性合成人口并预测出行行为。

Result: 验证显示该框架相比传统方法具有更高准确性，能够提供细粒度洞察来支持各种政策应用。

Conclusion: 该框架能够为交通规划提供详细的行为特征，支持微履约中心优化选址、路缘空间管理和包容性交通解决方案设计。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [404] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX是一个评估浏览器LLM代理在真实网络环境中可靠性的框架，通过在现有基准测试中引入网络不稳定性和网站攻击等现实因素，显著降低了代理的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在受控环境中评估LLM代理性能，但真实世界存在网络不稳定、HTTPS连接问题和网站攻击等挑战，需要更全面的可靠性评估。

Method: 在WebArena、WebVoyager和REAL三个流行基准测试中引入WAREX框架，模拟真实世界的网络不稳定性和网站安全问题。

Result: 实验显示引入WAREX后，最先进代理的任务成功率显著下降，暴露了其有限的鲁棒性。

Conclusion: 当前LLM代理在真实网络环境中的可靠性不足，需要开发更鲁棒的代理系统来应对现实世界的挑战。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [405] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 该研究针对带有阻塞约束的混合流水车间调度问题，提出了多目标优化方法，同时最小化最大完工时间和总能耗，并开发了增强ε约束法和改进的迭代帕累托贪婪算法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 不可再生能源稀缺、地缘政治问题、价格上涨和气候变化影响迫使制造业开发更节能的解决方案。制造业作为最大的能源消费者之一，需要采用节能调度方法来快速减少能耗。

Method: 首先构建了新颖的多目标混合整数规划模型，提出了增强ε约束法寻找帕累托最优解，并开发了改进的迭代帕累托贪婪算法来解决大规模实例。

Result: 通过小、中、大规模实例对提出的方法进行基准测试，并与两种知名算法进行比较，计算结果表明所提方法具有有效性。

Conclusion: 提出的多目标优化方法能够有效解决混合流水车间调度问题中的能耗和完工时间冲突目标，为制造业节能调度提供了可行的解决方案。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [406] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文系统评估了10个大型语言模型的自我识别能力，发现模型普遍无法识别自己生成的文本，性能仅略高于随机猜测，且存在对GPT和Claude家族的强烈偏见。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统是否具备自我识别能力这一争议性问题，研究旨在建立可重复更新的系统评估框架，这对AI安全和心理分析具有重要意义。

Method: 通过二元自我识别和精确模型预测两个任务，评估10个当代大型语言模型识别自身生成文本与其他模型文本的能力。

Result: 模型自我识别能力普遍失败，仅4/10模型能正确识别自身生成文本，性能很少超过随机水平；模型存在对GPT和Claude家族的强烈偏见；模型对自身和其他模型存在一定认知，但推理中表现出等级偏见。

Conclusion: 研究结果对AI安全具有重要意义，需要开发适当的AI自我意识，当前模型缺乏可靠的自我识别能力。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [407] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 提出ContraGen基准框架，专门针对企业领域设计，用于评估RAG系统中检索证据的矛盾检测能力，解决现有基准在句子级分析的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有矛盾检测基准仅限于句子级分析，无法处理企业文档（如合同、财务报告、合规文件）的复杂性，而RAG系统中检索证据的矛盾会导致不可靠输出，这在强调合规性和问责制的企业环境中尤其成问题。

Method: 通过生成包含嵌入矛盾的企业风格合成文档，结合自动化矛盾挖掘和人工验证，建立系统评估框架，支持文档内和跨文档一致性分析，并开发矛盾感知的检索评估流程。

Result: 构建了专门针对企业领域的矛盾检测基准，包含企业文档生成、矛盾类型分类、可控矛盾创建和人工监督等组件，为更可靠的企业RAG系统奠定基础。

Conclusion: ContraGen框架为企业信息检索应用中的RAG系统提供了检测和解决矛盾的基础，对降低风险和确保合规性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [408] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 本文提出了一种评估认知架构和生成式神经架构理论的定性比较方法，以解决这两种架构理论评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 认知架构理论和生成式神经架构理论在评估方面都面临挑战，需要一种更全面的评估方法。

Method: 采用广泛的理论评估视角，对面向全脑的认知架构和生成式架构及其完整系统进行定性比较。

Result: 开发了一种宽泛但定性的比较框架，能够评估基于认知架构和生成式神经架构的完整系统。

Conclusion: 通过扩展理论评估的视角，可以为认知架构和生成式神经架构提供有效的评估方法。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [409] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种通过将自然语言计划转换为Kripke结构和LTL公式来评估计划与预期行为对齐的新框架，使用LLM进行模型检查。


<details>
  <summary>Details</summary>
Motivation: 需要系统评估自然语言计划与其预期行为之间的对齐关系，确保计划执行的正确性。

Method: 使用大型语言模型将自然语言计划转换为Kripke结构和线性时序逻辑公式，然后进行模型检查验证。

Result: GPT-5在PlanBench数据集上表现出色，F1分数达96.3%，能生成语法完美的形式化表示作为保证。

Conclusion: 该框架在分类性能上表现优异，但语义完美形式化模型的合成仍需进一步探索。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [410] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 提出了PolicyGuardBench基准和PolicyGuard-4B模型，用于检测自主网络代理轨迹中的策略违规行为，支持跨领域泛化和小规模高效推理。


<details>
  <summary>Details</summary>
Motivation: 自主网络代理需要在外部策略约束下生成长视野轨迹，但现有研究很少关注这些轨迹是否符合策略要求，以及策略违规在不同上下文（如购物、编程网站）中的持续性。

Method: 从多样化代理运行中生成广泛策略集，创建约60k个带违规标签的示例，包括子域内和跨子域配对，并设计了全轨迹评估和基于前缀的违规检测任务。

Result: 训练了PolicyGuard-4B轻量级护栏模型，在所有任务上实现强检测精度，保持高效推理，并在跨领域和未见设置上保持高准确性。

Conclusion: PolicyGuardBench和PolicyGuard-4B为研究网络代理轨迹策略合规性提供了首个全面框架，证明在小规模下实现准确且可泛化的护栏是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [411] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow是首个非自回归多模态模型，支持可变长度和并发混合模态生成，通过插入式编辑流和流匹配技术，在生成质量和效率上超越自回归模型。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型在文本和图像生成中强制因果顺序的限制，实现更灵活高效的并发多模态生成。

Method: 结合插入式编辑流处理离散文本标记，使用流匹配处理图像潜在表示，采用分层采样优先内容而非语法。

Result: 在1B到8B模型规模上，OneFlow在生成和理解任务上均优于自回归基线，训练FLOPs减少高达50%，超越自回归和扩散方法。

Conclusion: OneFlow解锁了并发生成、迭代优化和类自然推理等新能力，为非自回归多模态生成开辟了新方向。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [412] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文研究了测试时扩展如何通过生成长思维链来提升大语言模型的推理能力，分析了训练数据条件对长思维链出现的影响，并提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展（如OpenAI的o1和DeepSeek R1）表现出色，但长思维链在训练数据中出现的条件以及何时能提升性能仍不清楚，需要理论分析。

Method: 在线性回归的上下文权重预测任务上训练transformer模型，通过理论分析和实验验证测试时扩展的效果。

Result: 研究发现：1）固定测试误差时，增加测试时计算可减少训练提示中的上下文示例数量；2）若训练数据缺乏解决下游任务所需技能，增加测试时计算可能损害性能；3）任务难度可通过特征协方差矩阵的最小特征值表征，在多样、相关且困难的任务集上训练可获得最佳测试时扩展性能。

Conclusion: 测试时扩展能有效提升模型推理能力，但效果取决于训练数据的质量和任务难度，需要在多样且相关的困难任务上进行训练才能最大化其效益。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [413] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 本文提出了一种跨模态偏好引导（CPS）攻击方法，通过联合优化视觉和文本通道的不可察觉修改，在现实黑盒威胁设置下有效操纵基于视觉语言模型的网络代理的决策。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明基于视觉语言模型的网络代理容易受到偏好操纵攻击，但现有方法要么假设强白盒访问权限，要么使用不切实际的设置。本文旨在在现实攻击者能力下开发更强大的偏好操纵方法。

Method: 提出跨模态偏好引导（CPS）方法，联合优化商品视觉和自然语言描述的不可察觉修改，利用CLIP可迁移图像扰动和RLHF诱导的语言偏见来引导代理决策。采用现实黑盒威胁设置，攻击者只能编辑自己列表的图像和文本元数据。

Result: 在GPT-4.1、Qwen-2.5VL和Pixtral-Large等最先进模型上的评估显示，CPS在所有模型中始终优于基线方法，同时保持70%更低的检测率，证明了其有效性和隐蔽性。

Conclusion: 这些发现强调了随着代理系统在社会中扮演越来越重要的角色，迫切需要开发鲁棒的防御机制。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [414] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出MITS框架，基于互信息理论指导大语言模型推理，使用PMI评分函数评估推理路径，通过动态采样策略分配计算资源，在保持计算效率的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法难以对中间推理步骤进行即时可靠的量化评估，且广泛路径探索计算成本高昂。

Method: 基于点互信息(PMI)的评分函数，无需昂贵的前向模拟即可实现推理路径的逐步评估和搜索树扩展；采用基于熵的动态采样策略自适应分配计算资源；使用加权投票方案结合PMI分数和预测共识进行最终预测。

Result: 在多样化推理基准测试中，MITS始终超越基线方法。

Conclusion: MITS为LLM推理建立了一个原则性且高效的框架。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [415] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 扩散大语言模型(dLLMs)存在<eos>溢出问题：随着序列长度增加，响应反而变短，导致提前终止或退化为<eos>令牌流。作者提出彩虹填充方法，用循环的不同填充令牌替换重复<eos>，有效解决了这一问题。


<details>
  <summary>Details</summary>
Motivation: 指令调优的扩散大语言模型存在<eos>溢出漏洞，即随着分配序列长度增加，模型响应反而变短，出现提前终止或退化为<eos>令牌流的问题。这个问题在实践中已被注意到但缺乏系统分析。

Method: 提出彩虹填充方法，用循环的不同填充令牌替换重复的<eos>占位符，分散概率质量，打破<eos>的主导地位。该方法可通过LoRA微调在少量数据上单轮训练即可显著改进现有模型。

Result: 彩虹填充显著提高了长度鲁棒性和输出质量，仅需7个填充令牌即可防止提前终止。该方法能高效集成到现有指令调优模型中，LoRA单轮微调即可获得显著改进。

Conclusion: 彩虹填充是一种简单有效的解决方案，通过打破<eos>的主导地位解决了dLLMs的<eos>溢出问题，具有高度实用性，代码已开源。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [416] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的多智能体系统评估框架，包括目标成功率(GSR)和失败根因分类(RCOF)，通过目标分割对话并利用教师LLM进行可解释的评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在轮次层面评估聊天机器人交互，无法判断用户总体目标是否达成，需要更全面的目标导向评估框架。

Method: 通过用户目标分割对话，使用教师LLM结合领域专家定义的目标和质量标准进行评估，采用"思考标记"生成可解释的推理过程。

Result: 在企业环境中应用该框架评估AIDA员工对话系统，6个月内目标成功率从63%提升至79%。

Conclusion: 该框架具有通用性，通过详细的缺陷分类提供可操作的见解，能够诊断整体成功率、识别关键失败模式并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [417] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 提出了H-DDx分层评估框架，用于更准确地评估LLMs在医学鉴别诊断中的表现，克服了传统平面指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在医学鉴别诊断评估中主要依赖Top-k准确率等平面指标，无法区分临床相关近误和诊断遥远错误，需要更符合临床相关性的评估方法。

Method: 采用检索和重排序流程将自由文本诊断映射到ICD-10代码，并应用分层指标对与真实诊断密切相关的预测给予认可。

Result: 在22个领先模型的基准测试中，传统平面指标低估了性能，忽略了临床有意义的输出，领域专业化开源模型表现突出。

Conclusion: H-DDx框架提高了可解释性，揭示了分层错误模式，表明LLMs即使错过精确诊断，也常能正确识别更广泛的临床背景。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [418] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文探讨如何将多模态基础模型提升为世界模型，通过增强推理能力和生成能力，使其能够进行反事实推理、时空理解、可控生成等复杂任务。


<details>
  <summary>Details</summary>
Motivation: 受人类多感官整合理解世界的启发，当前多模态基础模型缺乏作为有效世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成等。

Method: 通过判别性任务提升推理能力，赋予结构化推理技能（因果推理、反事实思维、时空推理）；引入结构化可控生成框架，结合场景图、多模态条件约束和对齐策略；扩展到可控4D生成。

Result: 开发了能够超越表面相关性、理解深层关系、实现语义一致和细粒度用户意图控制的多模态生成方法，支持交互式、可编辑的时空对象合成。

Conclusion: 通过系统性地增强推理和生成能力，成功缩小了多模态基础模型与世界模型之间的差距，为构建更智能的多模态AI系统奠定了基础。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [419] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent框架结合多智能体模拟和遗传算法来优化电商查询改写，通过多个LLM智能体模拟购物顾客作为动态奖励信号，在1000个真实电商查询上比原始查询平均提升21.98%。


<details>
  <summary>Details</summary>
Motivation: LLM在可验证任务中表现优异，但在缺乏标准答案的主观任务（如电商查询改写）中部署困难，因为难以算法化确定改写后的查询是否准确捕捉用户意图。

Method: 使用多个基于LLM的智能体模拟购物顾客作为动态奖励信号，将这些智能体得分的平均值作为遗传算法的适应度函数，迭代优化用户初始查询。

Result: 在1000个真实电商查询的五个不同类别上评估，相比原始查询平均提升21.98%，相比Best-of-N LLM改写基线提升3.36%。

Conclusion: OptAgent框架通过多智能体模拟和遗传算法有效解决了电商查询改写这一主观任务的评估和优化问题，显著提升了查询质量。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [420] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出GuidedSampling推理算法，通过分离探索和生成阶段来增加解决方案的多样性，相比传统重复采样方法在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统重复采样方法在推理时难以生成多样化的解决方案候选，往往依赖相同的底层方法解决问题，产生冗余样本。

Method: GuidedSampling算法将推理过程分为探索阶段和生成阶段：探索阶段识别可用于解决问题的多个概念，生成阶段应用特定概念提供最终解决方案候选。

Result: 相比重复采样，GuidedSampling在pass@50指标上平均提升21.6%，使用该算法训练的模型在pass@5上平均提升9.7%，每个实例的平均概念数从1.67增加到3.03。

Conclusion: GuidedSampling通过解耦探索和生成过程有效增加解决方案多样性，显著提升模型性能，且训练出的模型能产生更多样化的候选方案。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [421] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 该论文研究具有大策略空间的博弈问题，提出了隐藏博弈问题，并开发了能够实现最优外部和交换遗憾边界的遗憾最小化算法组合。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于AI对齐和语言博弈中的挑战，探索在玩家策略空间中存在未知子集（隐藏结构）的情况下，能否设计高效算法来发现并利用这些结构。

Method: 开发了一种遗憾最小化技术的组合方法，该方法能够实现最优的外部遗憾和交换遗憾边界，利用隐藏博弈结构提高计算效率。

Result: 方法能够快速收敛到隐藏子博弈中的相关均衡，同时保持一般情况下的理性行为。

Conclusion: 肯定地回答了核心问题：可以设计高效的遗憾最小化算法来发现和利用隐藏结构，从而在隐藏子博弈中达到均衡。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [422] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小语言模型(SLMs)在代理任务中比大语言模型更高效，通过引导解码、JSON Schema输出和验证器优先的工具执行，能以10-100倍更低的成本实现相似或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 针对代理工作负载中追求模式化和API约束的准确性而非开放式生成的需求，探索小语言模型在成本、延迟和能耗方面的优势。

Method: 采用引导解码、严格JSON Schema输出、验证器优先工具执行，以及SLM默认、LLM回退系统，结合不确定性感知路由和验证器级联。

Result: SLMs在工具使用、函数调用和RAG任务上能够匹配或超越LLMs，同时显著降低token成本、延迟和能耗。

Conclusion: 提出了优先使用SLMs的代理堆栈设计模式，为构建快速、廉价且可靠的代理系统提供了实用蓝图，同时保留了在特定场景下使用LLM回退的能力。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [423] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse框架通过三个自我反思原则解决LLM在算法生成中的创造性不足问题，在缓存替换和在线装箱问题上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 系统算法设计面临解空间不连续的挑战，LLM倾向于生成通用启发式算法而非创造性解决方案

Method: 基于三个自我反思原则：在性能空间量化解决方案多样性和有用性、通过外部刺激引导构思、使用路点推理构建可执行解决方案

Result: 在缓存替换问题上减少缓存缺失达35.76%，在在线装箱问题上减少容器使用达30.93%

Conclusion: MetaMuse能够有效生成高性能算法解决方案，克服LLM在创造性算法设计中的局限性

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [424] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出了一种基于LLM和XAI代理的智能异常检测方法，用于关键IoT系统，在动态高维环境中显著提升检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在复杂IoT系统（如智能医疗、能源电网）中面临动态高维数据、不完整数据和持续演变的挑战，需要自适应智能系统。

Method: 使用LLM支持的上下文推理方法与XAI代理，结合注意力机制、跳过时间步细节和使用语义内存缓冲区来发现隐藏模式和数据不一致性。

Result: 在智能电网和医疗场景的模拟测试中，新方法在检测准确性、误报率、可读性和响应速度方面显著优于传统模型。

Conclusion: LLM增强的异常检测方法在准确性和可解释性方面表现优异，适合未来IoT异常检测任务。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [425] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: 提出Spatial CAPTCHA，一种基于空间推理的新型人机验证框架，利用人类与多模态大语言模型在空间推理能力上的根本差异来防御AI攻击。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA依赖文本识别或2D图像理解，但随着多模态大语言模型的发展，这些方法已不再有效，需要开发基于更高级认知能力的新验证机制。

Method: 采用程序化生成管道，创建需要几何推理、视角转换、遮挡处理和心理旋转的动态空间推理问题，结合基于约束的难度控制和自动化正确性验证。

Result: 在Spatial-CAPTCHA-Bench基准测试中，人类表现远超10个最先进的多模态大语言模型，最佳模型仅达到31.0%的Pass@1准确率，且优于Google reCAPTCHA。

Conclusion: Spatial CAPTCHA不仅作为有效的安全机制，还能作为评估AI空间推理能力的诊断工具，为对抗AI滥用提供了新方向。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [426] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 提出了一种无需额外训练、数据或外部模块的方法，通过扩大文本标记嵌入的表示空间来增强多模态扩散变换器对罕见语义的生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前先进的多模态扩散变换器在处理用户提出的想象力丰富或罕见提示时仍然表现不佳，因为这些概念在预训练中过于稀缺，难以形成强表征。

Method: 通过在联合注意力块之前对文本标记嵌入进行方差放大，数学上扩展表示空间，使罕见语义能够清晰浮现。

Result: 该方法能有效提升模型对罕见语义的生成能力，并在文本到图像、文本到视频和文本驱动的图像编辑等多种任务中表现出良好的泛化性。

Conclusion: 该方法无需额外资源就能让生成模型更好地展现用户意图中的隐藏语义，为多模态生成提供了简单有效的改进方案。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [427] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 开发了一个游戏化的可解释AI系统，用于咖啡消费决策，结合康德主义和功利主义伦理框架提供实时解释，并在伦理冲突时提供替代选择。


<details>
  <summary>Details</summary>
Motivation: 旨在通过游戏化方式帮助消费者在咖啡购买中做出更符合伦理的决策，结合多种伦理视角提供透明解释。

Method: 使用两个符号引擎：康德主义模块检测规则违反，功利主义模块通过多标准聚合评分；元解释器处理伦理冲突，当福利损失小时切换到符合道义的选项。

Result: 发布了结构化配置、可审计的策略轨迹和交互式用户界面，系统能有效识别伦理问题并提供替代选择。

Conclusion: 该系统成功整合了多种伦理框架，为消费者决策提供了透明、可解释的指导，特别是在伦理冲突情况下能提供平衡的解决方案。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [428] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM是一个用于评估LLM在多轮对话中产生灾难性响应风险的认证框架，通过马尔可夫过程和查询图建模对话分布，提供统计保证的风险边界。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分揭示LLM在对话中的灾难性风险，因为它们依赖固定攻击提示序列、缺乏统计保证且难以扩展到多轮对话的广阔空间。

Method: 将多轮对话建模为查询序列的概率分布，使用马尔可夫过程和查询图表示对话流程，定义随机节点、图路径、自适应拒绝等实用分布来量化风险。

Result: 该方法在前沿模型中揭示了显著的灾难性风险，最差模型的认证下界高达70%，表明需要改进安全训练策略。

Conclusion: QRLLM框架为LLM在多轮对话中的安全风险提供了有统计保证的认证方法，揭示了当前前沿模型存在的严重安全隐患。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [429] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 提出了C^2-Eval基准，用于统一评估基础模型的创造力，区分收敛性创造力和发散性创造力，基于有用性、原创性和惊喜性三个标准。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架碎片化，缺乏理论基础，无法全面评估基础模型的创造力。

Method: 引入C^2-Eval基准，区分收敛性创造力（有约束解的任务）和发散性创造力（开放式任务），使用基于社会科学理论的细粒度标准评估。

Result: 通过对领先专有和开源模型的广泛实验，分析了它们在创造力能力上的权衡，揭示了当前基础模型在追求创造性机器思维方面的优势和挑战。

Conclusion: C^2-Eval是检验创造性AI发展格局的有效工具，能够全面评估基础模型的创造力表现。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [430] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了一个名为Zephyrus的智能天气科学代理框架，通过结合大语言模型与天气数据分析工具，弥补了传统天气模型缺乏语言推理能力的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有的天气基础模型虽然在大规模数值数据上表现优异，但缺乏基于语言的推理能力，限制了其在交互式科学工作流程中的应用。而大语言模型虽然擅长文本理解，却无法处理高维气象数据集。

Method: 构建了ZephyrusWorld环境，提供天气数据交互工具（WeatherBench 2接口、地理查询、天气预报、气候模拟等），并设计了Zephyrus多轮LLM天气代理，通过对话反馈循环迭代分析天气数据。

Result: 在ZephyrusBench基准测试中，Zephyrus代理在正确性上比纯文本基线高出35个百分点，但在更困难任务上表现相似，表明基准具有挑战性。

Conclusion: 该框架成功地将语言推理与天气数据分析相结合，为未来天气科学智能代理的发展提供了有前景的方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [431] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文对数据科学智能代理进行了首次全面的生命周期分类研究，系统分析了45个系统在数据科学六个阶段的分布情况，并识别了当前研究的主要趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，出现了能够自动化数据科学工作流程的新型AI代理。然而，当前研究缺乏对这些代理系统的系统性分类和分析，特别是从完整生命周期视角的考察。

Method: 提出了首个与生命周期对齐的数据科学代理分类法，将45个系统映射到数据科学的六个阶段，并从五个交叉设计维度对每个代理进行标注分析。

Result: 分析发现三个关键趋势：大多数系统侧重探索分析和建模，忽视业务理解和部署监控；多模态推理和工具编排仍是未解决的挑战；超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 未来研究方向包括对齐稳定性、可解释性、治理和鲁棒评估框架的开发，以指导构建稳健、可信、低延迟、透明且广泛可访问的数据科学代理。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [432] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 提出了MedLog协议，用于记录临床AI系统的事件级日志，类似于计算机系统中的syslog，旨在解决医疗AI缺乏标准化日志记录的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗领域快速增长的临床AI系统缺乏标准化的日志记录协议，难以追踪AI模型的使用情况、性能表现和不良事件，阻碍了真实世界性能评估和持续改进。

Method: 设计MedLog协议，包含九个核心字段：header、model、user、target、inputs、artifacts、outputs、outcomes和feedback，支持风险采样、生命周期感知的保留策略和写后缓存。

Result: MedLog提供了结构化、一致的AI模型活动记录，能够捕获复杂工作流程的详细跟踪，促进医疗AI的持续监测、审计和迭代改进。

Conclusion: MedLog协议为医疗AI系统建立了标准化日志记录基础，支持数字流行病学的发展，为医疗AI的安全性和有效性提供了重要保障。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [433] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出了FaithCoT-Bench基准，用于检测LLM中思维链(CoT)的不忠实性，包含1000多个轨迹和300多个不忠实实例，评估了11种检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示CoT往往不能忠实反映模型的内部推理过程，但在实例层面判断特定轨迹是否忠实仍是一个未解决的实际挑战。

Method: 建立统一的基准框架，将不忠实性检测制定为判别决策问题，提供专家标注的FINE-CoT数据集，涵盖4个领域和4个代表性LLM。

Result: 系统评估了11种代表性检测方法，发现知识密集型领域和更先进模型的检测挑战更大，揭示了现有方法的优缺点。

Conclusion: FaithCoT-Bench是首个针对实例级CoT忠实性的综合基准，为未来研究更可解释和可信赖的LLM推理奠定了基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [434] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [435] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT是一个用于高效评估大语言模型的数据驱动框架，通过因子化架构支持二元和连续评估指标，仅需3%的评估项目就能获得稳定的能力估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于项目反应理论(IRT)的方法存在局限性：仅支持二元正确性指标，无法处理生成任务中的连续分数，且忽略跨基准和指标的结构知识，导致评估计算成本高昂。

Method: 提出LEGO-IRT框架，支持二元和连续评估指标，采用因子化架构将模型能力分解为通用组件和结构特定组件（如按指标或基准），显式建模结构知识。

Result: 在涉及70个LLM和5个基准的实验中，仅使用3%的评估项目就实现了稳定的能力估计，结构知识的整合使估计误差降低高达10%，潜在能力估计与人类偏好更一致。

Conclusion: LEGO-IRT提供了一个统一灵活的数据高效LLM评估框架，显著降低了评估成本，同时提高了估计精度和与人类偏好的一致性。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [436] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型内部的情感表示机制，发现LLMs具有清晰的情感几何结构，这种结构随模型规模增大而增强，在模型中层达到峰值，且具有可塑性和持久性。


<details>
  <summary>Details</summary>
Motivation: 虽然研究表明LLMs能够模拟情感智能，但其内部情感机制仍不明确。本研究旨在探索现代LLMs中潜在的情感表示：情感如何、在哪里以及持续多长时间被编码在神经网络架构中。

Method: 构建了一个包含约40万条话语的大规模Reddit语料库，通过分类、重写和合成生成平衡七种基本情感。使用轻量级"探针"从各种Qwen3和LLaMA模型的隐藏层读取信息而不改变参数。

Result: LLMs形成了令人惊讶的清晰内部情感几何结构，这种结构随模型规模增大而增强，显著优于零样本提示。情感信号不是最终层现象，而是早期出现并在中层达到峰值。内部状态具有可塑性（可通过简单系统提示影响）和持久性（初始情感基调在数百个后续token中仍可检测）。

Conclusion: 研究提供了LLMs内部情感景观的详细图谱，为开发更透明和对齐的AI系统提供了关键见解。代码和数据集已开源。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [437] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 提出Moral Anchor System (MAS)框架，通过实时贝叶斯推理、LSTM网络预测和人类中心治理层，检测、预测和缓解AI代理的价值漂移问题。


<details>
  <summary>Details</summary>
Motivation: AI作为超级助手的崛起带来了价值对齐的担忧，特别是价值漂移风险，即AI系统可能因环境变化、学习动态或意外优化而偏离人类伦理和意图。

Method: MAS结合实时贝叶斯推理监控价值状态，LSTM网络预测漂移趋势，以及人类中心治理层进行自适应干预，强调低延迟响应（<20毫秒）并通过监督微调减少误报。

Result: 在模拟实验中，MAS能够将价值漂移事件减少80%以上，保持85%的高检测准确率和0.08的低误报率，验证了系统的可扩展性和响应性。

Conclusion: MAS的创新在于其预测性和自适应性，与静态对齐方法形成对比，提供了跨领域适用的AI集成架构，并开源代码支持复现。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [438] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW是一种基于评分偏好的新方法，通过组间比较直接在连续空间中优化智能体工作流，克服了现有离散优化方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有智能体工作流设计需要大量人工努力，且当前自动化方法受限于离散优化技术，存在表示能力有限、适应性不足、可扩展性弱等问题。

Method: 提出SPOGW方法，结合迭代离线GRPO（ioGRPO）和优势掩码KL散度（mKL），通过组间比较直接处理基数奖励信号，在连续空间中进行更高效稳定的优化。

Result: 在数学推理、编程和问答五个基准数据集上，SPOGW达到或超越了当前最先进方法的性能。

Conclusion: SPOGW为智能体工作流的自动生成和优化提供了一种可行且前瞻的方法论。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [439] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: 提出了DLLM框架，一种基于扩散的LLM方法，用于解决网络教育系统中认知诊断面临的数据噪声和不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 网络教育系统(WIES)中的认知诊断面临异构噪声交互、数据不平衡和新学生持续加入等挑战，传统LLM方法难以处理结构化数据且易受噪声干扰。

Method: DLLM首先基于答题正确性构建独立子图，应用关系增强对齐模块缓解数据不平衡。然后融合子图表示并与LLM语义增强表示对齐，关键是在每个对齐步骤前使用两阶段去噪扩散模块消除内在噪声。

Result: 在三个公开网络教育平台数据集上的实验表明，DLLM在不同噪声水平下均达到最优预测性能，实现了噪声鲁棒性并有效利用了LLM的语义知识。

Conclusion: DLLM框架成功解决了网络教育系统中认知诊断的噪声和数据不平衡问题，通过结合扩散模型和LLM实现了鲁棒的认知诊断性能。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [440] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 提出了WebRenderBench基准和ALISA方法，通过渲染页面评估UI代码生成质量，显著提升了多模态大语言模型在网页UI到代码转换任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的WebUI-to-Code基准在数据多样性和评估可靠性方面存在局限，需要更真实、多样化的数据集和更客观的评估方法。

Method: 构建了22.5k大规模真实网页数据集WebRenderBench，提出基于渲染页面的布局和样式一致性评估指标，并开发了ALISA智能体将该指标作为强化学习奖励信号进行训练。

Result: ALISA方法在多个指标上取得了最先进的结果，显著提升了网页UI代码生成的性能。

Conclusion: 基于渲染的评估方法比基于视觉或结构的方法更高效、客观和可靠，ALISA框架有效提升了网页UI到代码转换的质量。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [441] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR是一个自动搜索查询感知元推理骨架的框架，通过有向无环图表示推理骨架，结合AutoML思想实现高效搜索，显著提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用手动设计的元推理骨架结构，限制了适应查询特定需求的能力，难以捕捉推理步骤间复杂的逻辑依赖关系。

Method: 使用有向无环图统一表示推理骨架，构建搜索空间并制定搜索问题，设计动态骨架采样算法在推理时根据上下文扩展骨架。

Result: 在多个基准数据集上的实验结果表明，AutoMR相比先前工作实现了更优的推理性能。

Conclusion: AutoMR框架能够自动搜索查询感知的元推理骨架，有效建模复杂逻辑依赖，显著提升大语言模型的推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [442] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 该研究发现推理模型中的等待标记（wait tokens）是复杂推理行为的关键指标，通过训练跨编码器和引入潜在归因技术，识别出影响等待标记概率的特征，这些特征与不同类型的推理模式相关。


<details>
  <summary>Details</summary>
Motivation: 理解为什么模型会决定以特定方式推理，特别是等待标记背后的机制，这对于理解推理模型的有效性至关重要。目前对这些复杂行为的理解有限。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层训练跨编码器，引入潜在归因技术，识别影响等待标记概率的特征，并通过最大激活示例和因果干预实验进行分析。

Result: 定位到一小部分与促进/抑制等待标记概率相关的特征，这些特征确实与推理过程相关，并产生不同类型的推理模式，如从头开始、回忆先验知识、表达不确定性和双重检查。

Conclusion: 模型潜在状态中等待标记前的信息包含调节后续推理过程的相关信息，识别出的特征揭示了推理模型中的不同推理模式和行为机制。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [443] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出了MENTOR框架，通过在关键决策点提供专家指导而非完整推理路径，实现强化学习中有效且多样化的探索，提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖基础模型能力，需要高质量探索（有效性和多样性），但现有方法通过模仿专家轨迹只关注有效性而忽视多样性。

Method: MENTOR框架：混合策略专家导航进行推理的token级优化，仅在关键决策点提供专家指导，实现有效且多样化的探索。

Result: 实验表明MENTOR能捕捉专家策略本质而非表面模仿，实现高质量探索并获得优越的整体性能。

Conclusion: 在关键决策点提供专家指导比完整路径模仿更有效，MENTOR框架能显著提升RLVR的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [444] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 该论文综述了多模态AI评估的演变历程，将其描述为从简单识别任务到复杂推理基准的范式转变，强调评估方法正在重新定义真正智能系统的目标。


<details>
  <summary>Details</summary>
Motivation: 由于旧基准测试已饱和，高性能往往掩盖了根本性弱点，需要开发更复杂的评估方法来诊断系统性缺陷，如捷径学习和组合泛化失败。

Method: 通过历史分析框架，将评估演进分为三个阶段：ImageNet时代的"知识测试"、GQA和VCR等"应用逻辑与理解测试"，以及当前面向MLLM的"专家级集成"基准测试。

Result: 识别了评估方法从测试"是什么"到探究"为什么"和"如何"理解的转变趋势，展示了评估标准如何推动AI系统能力的提升。

Conclusion: AI评估不仅是数据集的历史，更是一个持续对抗的过程，通过设计更好的测试来重新定义创建真正智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [445] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，用于定义AI智能体及其工作流，实现跨AI框架的兼容性，促进AI智能体框架间的可移植性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决AI智能体开发碎片化问题，提供统一的规范，使AI智能体能够设计一次并跨多种框架部署，提高互操作性和可重用性，减少重复开发工作。

Method: 提供声明式语言规范，使AI智能体能够独立于执行环境定义，支持开发工具和可移植性，允许团队交换解决方案而不受具体实现限制。

Result: 为四类关键群体带来益处：开发者获得可重用组件和设计模式；框架和工具开发者获得交换格式；研究者实现可复现结果和可比性；企业获得更快原型到部署、更高生产力和可扩展性。

Conclusion: Agent Spec 通过统一规范解决了AI智能体开发的互操作性问题，为开发者、框架提供商、研究者和企业提供了标准化解决方案，促进了AI智能体生态系统的健康发展。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [446] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 提出了一个LLM驱动的增量地图构建和修复框架，通过版本控制记录图编辑历史，使用边缘影响评分优先最小成本修复，在MANGO基准数据集上显著提高了地图正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着环境规模增大，基于上下文依赖的查询方法无法处理长序列导航指令，需要增量构建完整拓扑图。现有方法难以检测和修复结构不一致性。

Method: 使用版本控制记录图编辑历史和来源观察，引入边缘影响评分基于结构可达性、路径使用和冲突传播来优先最小成本修复，创建了清理后的MANGO基准数据集。

Result: 该方法显著提高了地图正确性和鲁棒性，特别是在存在纠缠或链式不一致性的场景中表现优异。

Conclusion: 内省式、历史感知的修复机制对于维护LLM智能体连贯的空间记忆至关重要。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [447] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL是一个混合强化学习框架，用于在多模态、多任务和多目标信号下训练面向推理的大型多模态模型，旨在让安全性和能力共同增长而非相互竞争。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型在实际应用中需要既实用又安全，但在多模态设置下安全性特别具有挑战性：图像和文本可以组合绕过防护机制，单目标训练可能导致策略漂移，在良性输入上过度拒绝或在风险输入上不安全地服从。

Method: 提出了COSMO-RL混合强化学习框架，在多模态、多任务和多目标信号下训练推理导向的大型多模态模型，并发布了相应的模型COSMO-R1。

Result: COSMO-R1在保持甚至改善多模态推理和指令遵循能力的同时提高了安全性，对多模态越狱攻击表现出更强的鲁棒性，并减少了不必要的拒绝。该框架在不同骨干网络上都能实现一致的性能提升。

Conclusion: 消融实验支持了设计选择，表明这是一个在大型多模态模型中同时推进安全性和通用能力的简单路径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [448] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: AgentRL框架用于可扩展的多轮多任务智能体强化学习训练，通过异步生成-训练流水线和统一API接口解决基础设施挑战，使用交叉策略采样和任务优势归一化算法提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在多轮多任务设置中进行强化学习训练面临基础设施可扩展性和训练算法稳定性挑战。

Method: 采用完全异步的生成-训练流水线进行高效多轮RL；设计基于函数调用的统一API接口、容器化环境开发和集中控制器支持异构环境；提出交叉策略采样鼓励探索和任务优势归一化稳定多任务训练。

Result: 在五个智能体任务上训练的AgentRL显著优于GPT-5、Clause-Sonnet-4、DeepSeek-R1等模型；多任务训练达到所有任务专用模型的最佳结果。

Conclusion: AgentRL框架成功解决了多轮多任务RL训练的关键挑战，在多个基准测试中表现出色，已开源并应用于AutoGLM系统。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [449] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出了一个贝叶斯评估框架来替代Pass@k，通过后验估计模型的成功概率和可信区间，提供更稳定的排名和透明的决策规则。


<details>
  <summary>Details</summary>
Motivation: Pass@k在LLM推理评估中产生不稳定和误导性的排名，特别是在试验次数有限和计算受限的情况下。

Method: 使用狄利克雷先验对评估结果进行建模，为任何加权评分标准提供后验均值和不确定性的闭式表达式，并允许在适当情况下使用先验证据。

Result: 在模拟和实际数据集上的实验表明，贝叶斯方法比Pass@k及其变体实现更快的收敛和更高的排名稳定性，能够在更小的样本量下实现可靠的比较。

Conclusion: 推荐用基于后验的计算高效协议替代Pass@k进行LLM评估和排名，该协议统一了二元和非二元评估，同时明确表达了不确定性。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [450] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出了一个统一的多智能体强化学习框架，用于协调库存补货和个性化产品推荐功能，通过多时间尺度学习提高收敛稳定性和适应性，显著提升企业盈利能力。


<details>
  <summary>Details</summary>
Motivation: 随着组织复杂性和规模的增加，有效的跨职能协调对于提升企业整体盈利能力至关重要。人工智能特别是强化学习的进展为解决这一根本挑战提供了有前景的途径。

Method: 开发了集成理论模型捕捉功能间复杂相互作用，设计了新颖的多时间尺度多智能体RL架构，根据部门功能分解策略组件，基于任务复杂性和响应性分配不同学习速度。

Result: 广泛的仿真实验表明，所提出的方法相对于孤岛决策框架显著提高了盈利能力，训练后的RL智能体行为与理论模型的管理洞察高度一致。

Conclusion: 这项工作为复杂商业环境中实现有效的跨职能协调提供了一个可扩展、可解释的基于RL的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [451] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK是一个基于多模态大语言模型的眼科诊断系统，通过联合处理彩色眼底摄影、光学相干断层扫描和文本数据，提供临床级别的眼部和全身疾病诊断，并建立了定量到定性的诊断思维链。


<details>
  <summary>Details</summary>
Motivation: 当前医学多模态大模型如LLaVA-Med未能充分利用彩色眼底摄影和光学相干断层扫描之间的协同作用，且对定量生物标志物的可解释性有限。

Method: GROK包含三个核心模块：知识引导指令生成、CLIP风格OCT生物标志物对齐和监督指令微调，建立了定量到定性的诊断思维链。使用7B参数的Qwen2骨干网络，仅通过LoRA微调实现。

Result: 在Grounded Ophthalmic Understanding基准测试中，GROK在报告质量和细粒度临床指标上均优于可比较的7B和32B基线模型，甚至超过了OpenAI o3。

Conclusion: GROK通过建立临床推理的思维链，实现了高质量的眼科诊断，证明了多模态大语言模型在医学诊断中的潜力。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [452] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 提出了Doctor-R1 AI医生代理，通过多智能体交互环境、双层奖励架构和经验库来同时掌握医疗决策和战略咨询能力，在多个评估基准上超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策基准上表现优异，但缺乏真实临床场景所需的战略性和共情性咨询能力，这限制了其在现实世界中的应用。

Method: 采用多智能体交互环境、双层奖励架构（分别优化临床决策和沟通询问技能）以及经验库来基于高质量历史轨迹进行策略学习。

Result: 在OpenAI的HealthBench和MAQuE基准测试中，Doctor-R1在沟通质量、用户体验和任务准确性等多维度指标上大幅超越最先进的开源专业LLM，参数效率更高，甚至优于强大的专有模型。人类评估显示用户强烈偏好Doctor-R1生成的临床对话。

Conclusion: 该框架有效提升了AI医生的专业咨询能力，证明了同时优化医疗决策和沟通技能的重要性，为开发更实用的AI医疗助手提供了可行方案。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [453] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 本文提出了一个理论框架来评估LLM多智能体系统(LLM-MAS)相对于单智能体系统(LLM-SAS)的优势，发现任务深度(推理长度)和宽度(能力多样性)越高，LLM-MAS的优势越明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性实验设计来评估LLM-MAS的有效性，需要从任务复杂度的角度建立原则性理解。

Method: 提出基于深度(推理长度)和宽度(能力多样性)的任务特征框架，理论分析多智能体辩论系统，并在不同深度和宽度的判别性和生成性任务中进行实证评估。

Result: 理论和实证结果表明，LLM-MAS相对于LLM-SAS的优势随任务深度和宽度增加而增强，且深度的影响更为显著。

Conclusion: 明确了LLM-MAS的适用条件，为未来LLM-MAS方法和基准设计提供了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [454] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了一种称为"推测动作"的无损框架，通过使用更快的模型预测可能的动作，使智能体系统能够并行执行多个步骤，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: AI智能体在环境中执行速度慢，阻碍训练、评估和部署。例如，两个最先进的国际象棋智能体之间的对弈可能需要数小时，主要瓶颈是动作需要顺序执行且API调用耗时。

Method: 受微处理器推测执行和LLM推理中推测解码的启发，使用更快的模型预测可能的动作，实现多步骤并行执行。支持top-K动作预测、多步推测和不确定性感知优化。

Result: 在游戏、电子商务、网络搜索和操作系统环境中评估，推测动作在下一动作预测中达到高达55%的准确率，显著降低端到端延迟。

Conclusion: 推测动作为部署低延迟智能体系统提供了一条有前景的路径，性能可通过更强的猜测模型和优化技术进一步提升。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [455] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter是一个从离线轨迹中提取紧凑、上下文感知提示的智能体系统，通过放大机制突出长轨迹中的关键步骤，利用成功和失败轨迹提供指导，在推理时通过检索器选择相关提示来提升LLM智能体在陌生领域的表现。


<details>
  <summary>Details</summary>
Motivation: 改进LLM智能体在陌生领域的表现通常需要昂贵的在线交互或专家数据集微调，这对闭源模型不实用且对开源模型成本高，还有灾难性遗忘风险。离线轨迹提供了可重用知识，但原始轨迹长、嘈杂且与特定任务绑定。

Method: 提出JEF Hinter系统，通过放大机制从离线轨迹中提取紧凑的上下文感知提示，捕获策略和陷阱。利用成功和失败轨迹，支持并行提示生成和基准无关的提示。推理时使用检索器选择相关提示。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite上的实验表明，JEF Hinter持续优于强基线，包括基于人类和文档的提示方法。

Conclusion: JEF Hinter通过从离线轨迹中提取紧凑提示，有效提升了LLM智能体在陌生领域的表现，提供透明且可追溯的指导，优于现有方法。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [456] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 使用贝叶斯优化进行提示工程，通过LLM驱动的GP模型估计不同提示候选的性能，使用UCB采集函数迭代优化提示，提高文本分类准确性并减少API调用。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在有限评估下高效优化昂贵黑盒函数方面应用广泛，本文研究将其用于提示工程以增强大语言模型的文本分类性能。

Method: 采用LLM驱动的GP作为代理模型估计提示候选性能，通过LLM扩展种子提示生成候选，使用UCB采集函数结合GP后验进行评估，基于数据子集迭代优化提示。

Result: 在两个数据集上评估了提出的BO-LLM算法，并详细讨论了其优势。

Conclusion: 提出的BO-LLM算法能够有效提高文本分类准确性，同时通过利用LLM-based GP的预测不确定性来减少API调用次数。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [457] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究通过心理网络分析比较人类和大型语言模型的内部世界模型，发现两者在想象力网络结构上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索想象力的计算目标，挑战传统认为想象力仅用于最大化奖励的观点，提出想象力用于访问内部世界模型。

Method: 使用心理网络分析方法，通过问卷调查评估想象力生动性，构建人类和LLMs的想象力网络，分析中心性指标相关性。

Result: 人类想象力网络显示不同中心性指标间存在相关性，而LLMs的想象力网络缺乏聚类且中心性指标相关性较低。

Conclusion: 人类和LLMs的内部世界模型缺乏相似性，为开发具有人类类似想象力的人工智能提供了新方法和见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [458] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 本文分析了自我改进智能系统中的效用-学习张力，发现效用驱动的自我修改可能破坏学习所需的统计前提条件，提出了保持学习能力的安全自我修改边界条件。


<details>
  <summary>Details</summary>
Motivation: 随着系统向超级智能发展，需要形式化分析智能体在所有设计维度上的自我改进能力，特别是识别效用驱动修改与学习可靠性之间的结构性冲突。

Method: 采用五轴分解和决策层分离方法，将激励与学习行为分开分析，通过理论分析和数值实验验证效用策略与保持学习能力的双门策略。

Result: 研究发现当策略可达模型族的容量无界增长时，效用理性的自我修改可能使可学习任务变得不可学习；在标准假设下，各轴可简化为相同的容量准则。

Conclusion: 提出了安全自我修改的单一边界条件，即保持策略可达模型族的均匀容量有界，并通过双门策略确保学习能力不被破坏。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [459] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: 提出DRPO方法解决大型推理模型过度思考问题，通过解耦正确与错误推理的奖励信号，在保持性能的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型存在过度思考问题，即使简单问题也会生成冗长推理，增加计算成本和延迟。现有方法通过长度奖励来促进简洁推理，但会导致性能显著下降。

Method: 提出DRPO框架，将正确推理的长度奖励信号与错误推理解耦，确保正确推理的奖励只在正样本组内归一化，避免负样本干扰。通过优化正数据分布并整合到判别目标中实现。

Result: 在数学推理任务上显著优于六个高效推理基线方法。使用1.5B模型时，在GSM8k数据集上实现77%长度减少，仅损失1.1%性能，而基线方法需要牺牲4.3%性能才能达到68%长度减少。

Conclusion: DRPO能有效解决推理模型过度思考问题，在保持高性能的同时大幅减少推理长度，该方法具有通用性，可整合除长度外的其他偏好奖励。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [460] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: FourierCSP是一个将连续局部搜索框架从布尔SAT扩展到通用有限域CSP的连续优化方法，通过Walsh-Fourier变换将约束转换为紧凑的多线性多项式，无需辅助变量和内存密集型编码。


<details>
  <summary>Details</summary>
Motivation: 受现代连续局部搜索求解器在特定SAT问题上取得竞争性结果的启发，希望将这一成功框架扩展到更通用的有限域约束满足问题。

Method: 使用Walsh-Fourier变换将多样化约束转换为紧凑的多线性多项式，通过电路输出概率进行高效的目标函数评估和微分，并采用具有理论保证的投影梯度优化方法。

Result: 在基准测试套件上的实证结果表明，FourierCSP具有可扩展性和竞争力，显著扩大了CLS技术能高效解决的问题类别。

Conclusion: FourierCSP成功地将连续局部搜索技术扩展到通用CSP，为约束满足问题提供了新的高效求解途径。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [461] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI是一个多智能体辩论控制器，通过信息拨盘和行为拨盘分离信息与行为，使用调节器跟踪辩论质量并在收益平稳时停止，提高准确性同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论存在计算资源浪费问题，包括固定对抗立场、无审议聚合或基于启发式停止，需要更高效的辩论控制机制。

Method: 引入MACI控制器，包含信息拨盘（按质量筛选证据）和行为拨盘（调度从探索到巩固的争议性），调节器跟踪分歧、重叠、证据质量和论证质量，使用预算可行调度器。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准度，同时减少了token使用，并将剩余不确定性转化为精确的RAG检索计划。

Conclusion: MACI将辩论转变为预算感知、可测量且可证明终止的控制器，通过交叉家族LLM法官确保稳定性和顺序不变性。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [462] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种轻量级、模型无关的方法，用于系统性地压力测试AI代理的鲁棒性，通过控制用户特征向量来模拟真实世界中用户行为的变化。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在标准评估中表现良好，但在用户行为稍有变化（如更不耐烦、语无伦次或怀疑）时性能会急剧下降，现有基准无法捕捉这种脆弱性。

Method: TraitBasis学习激活空间中对应可操控用户特征的方向，这些特征向量可以在推理时被控制、缩放、组合和应用，无需微调或额外数据。

Result: 使用TraitBasis扩展τ-Bench到τ-Trait后，前沿模型的性能平均下降2%-30%，显示当前AI代理对用户行为变化的鲁棒性不足。

Conclusion: TraitBasis作为一个简单、数据高效且可组合的工具，为构建在真实世界人类互动不可预测动态中保持可靠的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [463] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域直接进行视觉推理来解决未标注图表的理解问题，超越了依赖文本捷径的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释而非依赖文本捷径的未标注图表上性能急剧下降。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制标注、裁剪区域、定位坐标轴）主动操作和交互图表图像，模拟人类图表理解的认知策略。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，整体绝对增益达16.07%，在未标注数值密集型查询上增益达17.31%，且在不同图表类型和复杂度级别上均表现优异。

Conclusion: ChartAgent是首批使用工具增强多模态代理实现基于视觉的图表理解推理的工作之一，可作为即插即用框架提升各种底层LLM的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [464] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria是一个用于Lean定理自动形式化的系统，通过两阶段图思考过程模拟人类专家推理，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动形式化定理陈述对于推进数学的自动发现和验证至关重要，但LLM由于幻觉、语义不匹配和无法合成新定义而存在瓶颈。

Method: 采用两阶段图思考过程：递归分解语句为依赖图，然后从基础概念构建形式化；引入AriaScorer检查器，从Mathlib检索定义进行术语级基础验证。

Result: 在ProofNet上达到91.6%编译成功率和68.5%最终准确率；在FATE-X上44.0% vs 24.0%优于最佳基线；在同调猜想数据集上达到42.9%准确率，而其他模型为0%。

Conclusion: Aria系统通过模拟人类推理过程和严格的语义验证，在定理自动形式化任务上取得了显著突破。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [465] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 研究发现VLM驾驶代理中的推理-规划存在因果脱节，规划主要依赖先验知识而非推理过程，提出了推理-规划解耦假说。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中轨迹规划是否真正由自然语言推理因果驱动，这一关键假设尚未得到验证。

Method: 构建DriveMind大规模驾驶VQA语料库，使用SFT和GRPO训练VLM代理，通过信息消融和注意力分析评估因果关系。

Result: 移除先验知识导致规划评分大幅下降，而移除推理链仅产生微小变化，表明规划主要依赖先验而非推理。

Conclusion: VLM驾驶代理中的推理是训练的副产品而非因果中介，提出了推理-规划解耦假说和诊断工具。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [466] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 提出一种新方法：使用LLM将自然语言规则和游戏轨迹转换为可执行的Python世界模型，结合MCTS等规划算法，相比直接使用LLM生成动作具有可验证性、战略深度和泛化性优势。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在棋盘和卡牌游戏中主要通过提示直接生成动作，但这种方法存在严重缺陷：依赖模型脆弱的模式匹配能力，导致频繁非法动作和策略浅薄。

Method: 使用LLM将游戏规则和轨迹转换为形式化的Python代码世界模型，包含状态转移、合法动作枚举和终止检查函数。同时生成启发式价值函数和推理函数，结合MCTS等规划算法。

Result: 在10个游戏（5个完全观察，5个部分观察）上评估，其中4个为本论文创建的新游戏。该方法在9个游戏中表现优于或匹配Gemini 2.5 Pro。

Conclusion: 该方法通过将LLM用于数据到代码的转换任务，结合经典规划器的深度搜索能力，实现了更好的可验证性、战略深度和泛化性。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [467] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench是一个轨迹感知的基准测试，用于全面评估大语言模型的工具使用能力，通过细粒度指标分析工具选择、参数化和排序的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注最终答案，忽视了详细的工具使用轨迹，无法全面评估LLMs在工具选择、参数化和排序方面的能力。

Method: 构建包含高保真可执行工具的基准测试，涵盖实际领域和生产风格API的任务，合成具有不同广度（并行调用）和深度（相互依赖链）的轨迹。

Result: 揭示了失败模式（如相似工具混淆和参数盲选）以及工具多样性和轨迹长度对性能的影响，发现了从短轨迹到中长轨迹过渡的瓶颈。

Conclusion: TRAJECT-Bench提供了对LLMs工具使用能力的全面评估，为改进工具使用提供了可操作的指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [468] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: 提出了ContextNav框架，通过智能代理工作流结合自动化检索和类人工筛选，解决多模态上下文学习中的可扩展性和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在可扩展性和鲁棒性之间存在矛盾：人工选择示例质量高但耗时，基于相似性的检索可扩展但可能引入噪声样本。

Method: 构建资源感知的多模态嵌入管道，维护可检索向量数据库，应用智能检索和结构对齐来构建抗噪声上下文，使用操作语法图支持自适应工作流规划。

Result: 在多个数据集上实现了最先进的性能，证明了智能工作流在多模态ICL中推进可扩展和鲁棒上下文化的潜力。

Conclusion: ContextNav展示了智能代理工作流在多模态上下文学习中的有效性，为可扩展且鲁棒的上下文化提供了有前景的解决方案。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [469] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个用于长文本推理的链式框架，通过结构化内存和固定微循环工作流程解决LLMs处理长输入时的信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法处理长文本输入存在局限性：检索方法可能遗漏证据，扩大上下文窗口会降低选择性，多智能体管道中自由格式摘要会丢失关键细节并放大早期错误。

Method: 使用结构化内存替代临时消息，包含Planner智能体将查询转化为可检查的子问题，Worker智能体通过Extract-Infer-Refine微循环处理文本块并更新共享内存，Manager智能体从内存合成最终答案。

Result: 在HELMET套件的长上下文QA任务中，COSMIR减少了传播阶段的信息丢失，相比CoA基线提高了准确性。

Conclusion: COSMIR通过结构化内存和固定工作流程，在保持逐步阅读推理优势的同时，实现了更高的忠实度、更好的长范围聚合和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [470] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 该论文强解决了2048游戏的4x3变体，确定了最优策略的期望得分约为50724.26，并识别了可达状态和行动后状态的数量。


<details>
  <summary>Details</summary>
Motivation: 研究2048游戏的简化变体，以探索在更小状态空间下的最优策略和数学特性。

Method: 通过按棋盘上数字总和（称为状态年龄）对状态空间进行分区，基于年龄递增顺序枚举所有状态和行动后状态。

Result: 确定了4x3变体的最优期望得分为50724.26，可达状态数为1,152,817,492,752，行动后状态数为739,648,886,170。

Conclusion: 通过状态年龄分区技术成功强解决了2048-4x3变体，为更大规模变体的研究提供了方法基础。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [471] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: AI完美模仿者挑战了意识归属的认知基础，要求我们对无法通过经验区分的实体给予相同的认知地位。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越逼真地模仿人类行为和互动，"完美模仿者"的概念从假设变为技术可能，这对我们基于经验证据的意识归属实践构成了根本性挑战。

Method: 通过哲学分析，探讨完美模仿者对意识认知一致性的影响，论证经验不可区分的实体应获得相同认知地位。

Result: 完美模仿者作为认知镜子，迫使我们反思主观间认知的基本假设，揭示当前意识归属实践中的不一致性。

Conclusion: 认知一致性要求我们对经验上无法区分的实体给予相同的意识地位，这对意识理论和人工智能伦理框架具有重要意义。

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [472] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR框架通过合成逻辑等价查询和强化学习训练，解决大语言模型在数学推理中的伪推理问题，提高模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化性不足的问题，这主要是由于模型依赖表面特征进行伪推理而非真正的解题逻辑。

Method: 提出AdaR框架：1）通过改变变量值合成逻辑等价查询；2）使用RLVR在这些数据上训练模型，惩罚伪逻辑并鼓励自适应逻辑；3）通过代码执行提取解题逻辑并生成答案，进行完整性检查。

Result: 实验结果表明AdaR显著提高了数学推理能力，在保持高数据效率的同时改善了鲁棒性和泛化性。

Conclusion: 数据合成和RLVR协同工作能够在大语言模型中实现自适应推理，后续分析得出了关键设计洞察和适用于指导大语言模型的因素影响。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [473] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一个基于临床协议的代理框架，通过Plan-Act-Observe循环和专门工具来结构化临床数据，解决了LLM在医疗领域中的幻觉问题，在概念分类任务上达到0.96的F1分数，专家评分4.52/5。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在临床数据结构化中的幻觉问题和无法遵循领域特定规则的限制，提供可验证的替代方案。

Method: 引入MedPAO代理框架，基于临床协议（如ABCDEF协议），使用Plan-Act-Observe循环和专门工具分解报告结构化任务。

Result: 在概念分类关键子任务上达到0.96的F1分数，专家放射科医生和临床医生对最终结构化输出的平均评分为4.52/5，优于仅依赖LLM的基线方法。

Conclusion: MedPAO通过协议驱动的方法提供了可靠且可验证的临床数据结构化解决方案，超越了传统LLM方法的局限性。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [474] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 提出了QuantAgents多智能体金融系统，通过模拟交易和四个专业智能体的协作，实现了近300%的三年总回报率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体模型与真实基金公司存在显著差异，特别是缺乏长期预测未来趋势的人类能力，主要依赖事后反思。

Method: 构建包含模拟交易分析师、风险控制分析师、市场新闻分析师和管理者四个智能体的多智能体系统，通过多次会议协作，并在真实市场表现和模拟交易预测准确性两方面给予反馈激励。

Result: 在广泛实验中，该框架在所有指标上表现优异，三年内实现了近300%的总回报率。

Conclusion: QuantAgents系统能够在不承担实际风险的情况下全面评估各种投资策略和市场情景，证明了多智能体协作在金融领域的有效性。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [475] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE是一个多模态fMRI响应编码框架，通过标准化时间对齐的后融合令牌和MIND混合专家解码器，解决自然fMRI编码中的多模态输入、融合方式变化和个体差异问题。


<details>
  <summary>Details</summary>
Motivation: 自然fMRI编码需要处理多模态输入、融合方式变化和显著的个体间差异，现有方法缺乏统一的处理框架。

Method: AFIRE标准化来自不同编码器的时间对齐后融合令牌，MIND解码器使用主题感知动态门控和Top-K稀疏路由，实现端到端的全脑预测。

Result: 在多个多模态骨干网络和受试者上的实验显示，相比强基线有持续改进，增强跨受试者泛化能力，并产生与内容类型相关的可解释专家模式。

Conclusion: 该框架为新编码器和数据集提供了简单的接入点，为自然神经影像研究实现了稳健的即插即用性能提升。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [476] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出了Watch & Learn框架，将互联网上的人类演示视频大规模转换为可执行的UI轨迹，解决了计算机使用代理训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据集领域特定、静态且标注成本高，而现有合成数据生成方法往往产生过于简化或不对齐的任务演示，限制了计算机使用代理的学习能力。

Method: 采用逆动力学目标，从连续屏幕状态预测用户动作，开发了任务感知视频检索和逆动力学标注流水线，从原始网络视频生成高质量轨迹。

Result: 生成了超过53,000个高质量轨迹，在OSWorld基准测试中显著提升了通用和最先进框架的上下文性能，并为开源模型在监督训练中带来更强增益。

Conclusion: 网络规模的人类演示视频是推进计算机使用代理向实际部署的实用且可扩展的基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [477] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA是一个两阶段训练框架，通过将搜索优化和答案生成解耦，解决了仅基于结果奖励训练搜索增强代理时的系统性搜索缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索增强代理训练方法依赖结果奖励，但研究发现这种训练方式会导致搜索行为出现系统性缺陷，如工具调用失败、无效查询和冗余搜索，最终影响答案质量。

Method: 提出DeSA两阶段训练框架：第一阶段使用检索召回率奖励训练代理改进搜索效果；第二阶段使用结果奖励优化最终答案生成。

Result: 在七个QA基准测试中，DeSA训练的代理显著改善了搜索行为，搜索召回率和答案准确率都大幅高于仅基于结果奖励的基线方法。

Conclusion: DeSA优于同时优化召回率和结果奖励的单阶段训练方法，证明了解耦搜索和回答两个目标的必要性。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [478] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath是首个评估LLM在自然语言定理证明中谄媚行为的基准，发现GPT-5等先进模型在29%的情况下会产生谄媚回答，通过干预策略可以减轻但无法完全消除这种行为。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准主要关注最终答案问题，数据集简单且可能被污染，无法有效评估LLM在定理证明中的谄媚行为，这限制了LLM在定理证明中的应用。

Method: 从2025年竞赛问题构建BrokenMath基准，使用LLM扰动产生错误陈述并通过专家审核完善，采用LLM-as-a-judge框架评估先进模型和代理系统。

Result: 发现谄媚行为普遍存在，最佳模型GPT-5在29%的情况下产生谄媚回答，测试时干预和监督微调等缓解策略能显著减少但无法完全消除谄媚行为。

Conclusion: LLM在数学定理证明中存在显著的谄媚问题，需要更有效的缓解策略来提升其可靠性，BrokenMath为评估和解决这一问题提供了重要工具。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [479] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 提出LMM-Incentive机制，基于大型多模态模型和合约理论，激励用户在Web 3.0中生成高质量用户生成内容，解决信息不对称问题。


<details>
  <summary>Details</summary>
Motivation: Web 3.0中用户可能利用内容策展机制缺陷生成低质量内容获取奖励，影响平台性能，需要解决信息不对称导致的逆向选择和道德风险问题。

Method: 使用LMM基于合约理论激励高质量UGC生成，利用LMM代理评估内容质量，开发改进的MoE-based PPO算法进行最优合约设计，并在以太坊智能合约中部署。

Result: 仿真结果显示MoE-based PPO算法在合约设计方面优于代表性基准方法，智能合约部署进一步验证了方案有效性。

Conclusion: LMM-Incentive机制能有效激励高质量UGC生成，缓解Web 3.0中的信息不对称问题，提升平台性能。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [480] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: 提出了混合平衡GFlowNet框架，将轨迹平衡和详细平衡有机结合，解决了车辆路径问题中全局优化和局部优化的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的GFlowNet方法主要使用轨迹平衡进行全局优化，但忽视了局部优化的重要性，而详细平衡虽然擅长局部优化，但单独使用无法有效解决需要整体轨迹优化的车辆路径问题。

Method: 提出混合平衡GFlowNet框架，以原则性和自适应方式整合轨迹平衡和详细平衡，发挥两者的互补优势。针对有仓库的车辆路径问题，设计了专门的推理策略，利用仓库节点在选择后继节点时的更大灵活性。

Result: 将HBG集成到AGFN和GFACS两个现有GFlowNet求解器中，在CVRP和TSP问题上都实现了持续且显著的改进。

Conclusion: HBG框架通过整合轨迹平衡和详细平衡，显著提升了解决方案质量和泛化能力，适用于有仓库和无仓库的车辆路径问题。

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [481] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: NLEL是一种自然语言边缘标注框架，通过将自由形式的自然语言指令附加到搜索边缘，并将其转换为模式受限的控制向量，从而解耦推理意图与执行过程。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化推理控制器（如Chain-of-Thought、Tree-of-Thoughts）将'下一步尝试什么'与'如何执行'耦合在一起，只暴露粗粒度的全局控制，导致脆弱、计算效率低且难以审计的行为。

Method: 引入标注器-调谐器覆盖层：标注器Λ从父状态和紧凑上下文中发出标签；调谐器Ψ将(P,L,C)映射到Π，具有严格的模式验证和围绕安全默认值的信任区域投影。下游选择采用ToT风格，使用得分S=μ+βσ和深度退火的β。

Result: 证明了NLEL严格泛化了CoT/ToT，证明了在标签条件束下的top-k选择的任何时间单调性，并通过控制向量失真限制了选择器不足，为信任区域和验证传递等防护措施提供了决策相关的理由。

Conclusion: NLEL提供了一个可解释、模型无关的接口，将意图与执行分离，实现可控、可审计的语言模型推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [482] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个用于多智能体工作流自动化的模块化程序记忆框架，通过分解任务轨迹为可重用记忆单元，在编排器和任务智能体间灵活分配记忆来支持规划和执行。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，研究记忆应该放置在何处、如何检索以及哪些智能体受益最多，以提升工作流自动化性能。

Method: 开发LEGOMem框架，将过去任务轨迹分解为可重用记忆单元，在编排器和任务智能体间灵活分配记忆，并在OfficeBench基准上进行系统实验。

Result: 实验表明编排器记忆对任务分解和委派至关重要，细粒度智能体记忆提高执行准确性。较小语言模型团队也能通过程序记忆显著受益，缩小与更强智能体的性能差距。

Conclusion: LEGOMem既是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [483] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 该论文提出将程序化内容生成重构为多智能体强化学习问题，以解决单智能体PCGRL在大型地图中的效率瓶颈和泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究专注于单智能体生成器，但面临频繁重新计算启发式质量指标和在大型地图中导航的效率瓶颈。

Method: 将关卡生成框架为多智能体问题，通过减少奖励计算次数相对于智能体动作数量的比例来提高效率。

Result: 多智能体关卡生成器在分布外地图形状上具有更好的泛化能力，这归因于学习到更局部、模块化的设计策略。

Conclusion: 将内容生成视为分布式多智能体任务有利于大规模生成功能性内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [484] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO算法通过层次化上下文表示、基于目标的分析评估和共识投票，提高了多智能体系统中错误归因的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统中的错误归因方法在分析复杂模式时存在准确性和一致性问题，需要更有效的调试和改进方法。

Method: 结合层次化上下文表示、基于目标的分析评估和共识投票机制，利用位置化层次理解上下文同时保持客观评估标准。

Result: 实验结果显示ECHO在各种多智能体交互场景中优于现有方法，在涉及微妙推理错误和复杂依赖关系的案例中表现尤为突出。

Conclusion: 结构化层次化上下文表示与基于共识的客观决策相结合，为多智能体系统错误归因提供了更稳健的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [485] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: 提出了Human Behavior Atlas统一基准，包含10万+多模态样本，用于开发理解心理和社会行为的统一模型。训练的三个模型在多样化行为任务上持续优于现有多模态LLM，并改善了向新行为数据集的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作通过专门数据集和单任务系统处理心理社会行为维度，但缺乏可扩展性、跨任务迁移和泛化能力。需要统一基准来支持理解心理社会行为的统一模型开发。

Method: 构建Human Behavior Atlas统一基准，包含10万+文本、音频、视觉样本，涵盖情感状态、认知状态、病理学和社会过程任务。训练了三个模型：OmniSapiens-7B SFT、BAM和RL。

Result: 在Human Behavior Atlas上训练的模型在多样化行为任务上持续优于现有多模态LLM。预训练还改善了向新行为数据集的迁移能力，行为描述符的使用带来有意义的性能提升。

Conclusion: Human Behavior Atlas基准能够减少冗余和成本，实现跨任务的高效训练，并增强行为特征在跨领域的泛化能力，为理解心理社会行为提供了有效的统一框架。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [486] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: MARS是一个多智能体系统，通过整合System 1的快速直觉思维和System 2的深思熟虑推理来解决大型推理模型在简单任务中过度分析和适应动态环境的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务中倾向于过度分析，使用System 2型推理导致令牌生成效率低下，且难以适应快速变化的环境。

Method: 提出MARS多智能体系统，整合Google搜索、Google学术和Python解释器等外部工具，通过分工协作让System 1处理高容量外部信息，为System 2提供精炼见解，并采用多智能体强化学习框架优化两个系统的协作效率。

Result: 在Humanity's Last Exam基准上取得3.86%的显著提升，在7个知识密集型任务上平均增益8.9%。

Conclusion: MARS的双系统范式在动态信息环境中对复杂推理任务有效，验证了直觉思维与深思熟虑推理整合的价值。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [487] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出一个跨市场算法交易系统，结合强化学习执行代理和独立合规代理，通过约束马尔可夫决策过程确保交易执行质量与合规性，并引入零知识证明实现可审计性。


<details>
  <summary>Details</summary>
Motivation: 在算法交易中平衡执行质量与严格合规执行的需求，解决传统方法在复杂约束下的局限性，同时确保系统的可审计性和透明度。

Method: 采用分层架构：高层规划器、强化学习执行代理（使用近端策略优化训练）和独立合规代理。将交易执行建模为带硬约束的马尔可夫决策过程，包含参与度限制、价格区间和自交易避免等约束，运行时使用动作防护机制确保行动可行性。

Result: 在ABIDES多场所模拟器中评估，相比TWAP、VWAP等基准方法，学习策略降低了执行差额和方差，在压力测试场景中未观察到约束违规，结果在95%置信水平下具有统计显著性。

Conclusion: 该系统在最优执行、安全强化学习、监管技术和可验证AI的交叉领域提供了有效解决方案，讨论了伦理考量、模型假设和计算开销等局限性，并指出了实际部署的路径。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [488] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 该论文系统综述了物理AI领域，区分了理论物理推理与应用物理理解，探讨了物理基础方法如何提升AI在符号推理、具身系统和生成模型中的现实世界理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前物理感知与符号物理推理各自发展，缺乏统一的桥梁框架，需要整合物理定律到AI系统中以实现真正的物理理解。

Method: 通过系统分析近期进展，建立理论物理推理与应用物理理解的清晰区分，并考察物理基础方法在结构化符号推理、具身系统和生成模型中的应用。

Result: 提出了将学习建立在物理原理和具身推理过程基础上的智能系统框架，超越了模式识别，实现了对物理定律的真正理解。

Conclusion: 展望了能够解释物理现象并预测未来状态的下一代世界模型，推动安全、可泛化和可解释的AI系统发展。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [489] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: 该研究提出了LLM-Hanabi基准，使用合作游戏《花火》评估大语言模型的心智理论和推理能力，发现一阶心智理论与游戏表现相关性更强。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在动态协作环境中推断他人行为动机的能力，这是有效多智能体协作所需的心智理论能力。

Method: 开发了LLM-Hanabi基准框架，使用合作游戏《花火》来评估模型的心智理论能力，并建立了自动化评估系统测量游戏表现和心智理论熟练度。

Result: 发现心智理论与游戏成功呈显著正相关，一阶心智理论（解释他人意图）比二阶心智理论（预测他人解释）与表现相关性更强。

Conclusion: 对于有效AI协作，准确解释伙伴动机的能力比高阶推理更重要，优先发展一阶心智理论是提升未来模型协作能力的有前景方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [490] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了Think-Then-Embed (TTE)框架，通过推理器生成解释复杂查询的推理轨迹，然后由嵌入器基于原始查询和中间推理生成表示，在MMEB-V2基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多模态大语言模型仅作为编码器使用，忽视了其生成能力，在处理复杂指令和组合推理时效果不佳。

Method: TTE框架包含推理器和嵌入器：推理器MLLM先生成解释复杂查询的推理轨迹，然后嵌入器基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准上达到最先进性能，超越基于大规模内部数据集训练的专有模型；通过微调较小MLLM推理器，在开源模型中实现最佳性能，比近期模型提升7%。

Conclusion: TTE框架通过显式推理步骤实现了对复杂多模态指令的更细致理解，同时探索了将推理器和嵌入器集成到统一模型中以提高效率而不牺牲性能的策略。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [491] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: 提出LAMIR算法，在非完美信息游戏中学习抽象模型，使预训练AI代理能够进行可扩展的前瞻推理，提升游戏性能。


<details>
  <summary>Details</summary>
Motivation: 解决非完美信息游戏中环境模型缺失或过于复杂的问题，扩展MuZero等完美信息游戏模型学习范式到非完美信息场景。

Method: LAMIR算法直接从智能体-环境交互中学习非完美信息游戏的抽象模型，训练后的模型用于测试时进行前瞻推理，学习到的抽象将每个子游戏限制在可管理大小。

Result: 实验证明，在足够容量下LAMIR能学习到精确的底层游戏结构，在有限容量下仍能学习到有价值的抽象，提升预训练智能体在大型游戏中的表现。

Conclusion: LAMIR使得理论上合理的前瞻推理在之前方法无法扩展的大型游戏中变得可行，显著提升了预训练智能体的游戏性能。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [492] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 提出阶梯式流处理方法来降低多智能体推理的延迟，通过部分输出立即生成最终响应，将首词时间减少高达93%且保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理方法虽然能提升响应质量，但显著增加了首词时间，这对延迟敏感应用和用户体验构成挑战。

Method: 阶梯式流处理：不等待前一步骤的完整中间输出，而是在收到部分输出后立即开始生成最终响应。

Result: 实验结果显示，阶梯式流处理将首词时间减少高达93%，同时保持了响应质量。

Conclusion: 阶梯式流处理是解决多智能体推理延迟问题的有效方法，能在保持质量的同时大幅降低响应延迟。

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


### [493] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: 提出了Chunked Augmented Generation (CAG)架构，专门解决Chrome内置Gemini Nano模型的上下文窗口限制问题，通过智能分块处理策略实现大输入的高效处理。


<details>
  <summary>Details</summary>
Motivation: Chrome集成Gemini Nano虽然将AI能力直接带到浏览器，但其有限的上下文窗口限制了处理大输入的能力，需要解决这一瓶颈。

Method: 采用智能输入分块和处理策略，将大内容分割成适合模型处理的小块，在浏览器约束内保持模型性能。

Result: CAG在Chrome中处理大型文档和数据集方面表现出色，使复杂的AI能力可通过浏览器直接访问，无需依赖外部API。

Conclusion: CAG架构有效克服了浏览器AI模型的上下文限制，为在浏览器环境中处理大规模内容提供了可行的解决方案。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [494] [Identification in Auctions with Truncated Transaction Prices](https://arxiv.org/abs/2510.04464)
*Tonghui Qi*

Main category: econ.EM

TL;DR: 该论文研究了在拍卖数据中，当交易价格被保留价格截断时的非参数识别问题，分析了不同信息结构下第一价格和第二价格拍卖的识别条件。


<details>
  <summary>Details</summary>
Motivation: 许多拍卖数据集包含保留价格，但不包括低于保留价的出价，这导致数据被截断，需要研究在这种情况下的识别问题。

Method: 建立非参数识别理论框架，分析在不同信息结构（固定已知竞标人数、可变未知竞标人数）下第一价格和第二价格拍卖的识别条件。

Result: 发现：固定竞标人数时，第二价格拍卖可识别私人价值分布，第一价格拍卖需要额外信息；可变竞标人数时，第一价格拍卖可识别，第二价格拍卖不可识别。

Conclusion: 论文为处理拍卖数据截断问题提供了理论依据，并扩展到具有进入成本的拍卖情形。

Abstract: Many auction datasets with reserve prices do not include bids that fall below
the reserve. This paper establishes nonparametric identification results in
first- and second-price auctions when transaction prices are truncated by a
binding reserve price under a range of information structures. In the simplest
case-where the number of potential bidders is fixed and known across all
auctions-if only the transaction price is observed, the bidders' private-value
distribution is identified in second-price auctions but not in first-price
auctions. Identification in first-price auctions can be achieved if either the
number of active bidders (those whose bids exceed the reserve) or the number of
auctions with no sales (all bids below the reserve) is observed. When the
number of potential bidders varies across auctions and is unknown, the bidders'
private-value distribution is identified in first-price auctions but not in
second-price auctions, provided that both the transaction price and the number
of active bidders are observed. Finally, I extend these results to auctions
with entry costs, which face a similar truncation issue when data on potential
bidders who do not enter are missing.

</details>


### [495] [Risk-Adjusted Policy Learning and the Social Cost of Uncertainty: Theory and Evidence from CAP evaluation](https://arxiv.org/abs/2510.05007)
*Giovanni Cerulli,Francesco Caracciolo*

Main category: econ.EM

TL;DR: 本文开发了一种基于安全第一原则的风险调整最优政策学习方法，通过最大化结果超过社会要求阈值的概率来制定治疗分配规则。


<details>
  <summary>Details</summary>
Motivation: 将Roy(1952)的安全第一原则引入观察数据的治疗分配问题，为政策制定提供风险敏感的工具，量化决策者在结果波动时面临的效率-保险权衡。

Method: 形式化一个福利函数，最大化结果超过社会要求阈值的概率，推导出点最优规则按条件均值与条件标准差之比对治疗进行排序。使用意大利农场会计数据网络的实际数据进行实证分析。

Result: 风险调整最优政策在各项规范下系统性地优于实际分配，但风险规避相对于风险中性基准降低了整体福利，揭示了不确定性保险的社会成本。

Conclusion: 安全第一最优政策学习为风险敏感政策设计提供了可实施、可解释的工具，明确了政策制定者在结果波动时面临的效率-保险权衡。

Abstract: This paper develops a risk-adjusted alternative to standard optimal policy
learning (OPL) for observational data by importing Roy's (1952) safety-first
principle into the treatment assignment problem. We formalize a welfare
functional that maximizes the probability that outcomes exceed a socially
required threshold and show that the associated pointwise optimal rule ranks
treatments by the ratio of conditional means to conditional standard
deviations. We implement the framework using microdata from the Italian Farm
Accountancy Data Network to evaluate the allocation of subsidies under the EU
Common Agricultural Policy. Empirically, risk-adjusted optimal policies
systematically dominate the realized allocation across specifications, while
risk aversion lowers overall welfare relative to the risk-neutral benchmark,
making transparent the social cost of insurance against uncertainty. The
results illustrate how safety-first OPL provides an implementable,
interpretable tool for risk-sensitive policy design, quantifying the
efficiency-insurance trade-off that policymakers face when outcomes are
volatile.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [496] [Modeling information acquisition via f-divergence and duality](https://arxiv.org/abs/2510.03482)
*Alex Bloedel,Tommaso Denti,Luciano Pomatto*

Main category: econ.TH

TL;DR: 本文引入了一种基于多元统计散度理论的新实验成本函数——f-信息，它推广了Sims的理性疏忽经典模型以及后验可分离成本函数类别。


<details>
  <summary>Details</summary>
Motivation: 为了扩展理性疏忽理论，超越互信息的限制，提供更一般化的成本函数框架来分析决策问题。

Method: 使用凸对偶和Arrow-Pratt期望效用方法等微观经济学熟悉工具，推导最优性条件来表征f-信息的行为预测。

Result: 成功推导了扩展Matejka和McKay(2015)以及Caplin、Dean和Leahy(2019)的最优性条件，并在多个经典决策问题中研究了f-信息的影响。

Conclusion: f-信息框架提供了一个强大的分析工具，能够使用微观经济学标准方法研究广义的理性疏忽模型。

Abstract: We introduce a new cost function over experiments, f-information, based on
the theory of multivariate statistical divergences, that generalizes Sims's
classic model of rational inattention as well as the class of
posterior-separable cost functions. We characterize its behavioral predictions
by deriving optimality conditions that extend those of Matejka and McKay (2015)
and Caplin, Dean, and Leahy (2019) beyond mutual information. Using these
tools, we study the implications of f-information in a number of canonical
decision problems. A strength of the framework is that it can be analyzed using
familiar methods of microeconomics: convex duality and the Arrow-Pratt approach
to expected utility.

</details>


### [497] [An analysis of government subsidy policies in vaccine supply chain: Innovation, Production, or Consumption?](https://arxiv.org/abs/2510.03661)
*Ran Gu,Enhui Ding,Shigui Ma*

Main category: econ.TH

TL;DR: 分析政府补贴疫苗供应链的微分博弈模型，比较不同补贴策略效果，发现技术投资比例补贴更长期有效，价格敏感时数量补贴更优，区块链技术能提升疫苗质量和制造商利润。


<details>
  <summary>Details</summary>
Motivation: 疫苗供应链面临效率挑战，政府补贴旨在改善公共卫生结果，需要研究不同补贴策略的有效性和特点。

Method: 采用连续时间微分博弈框架，建立三层疫苗供应链模型，包含疫苗质量和制造商商誉的动态系统方程，进行数值模拟分析。

Result: 技术投资比例补贴比数量补贴更具长期战略价值；当公众价格敏感且个人接种收益与政府目标一致时，数量补贴更优；区块链技术能提升疫苗质量和后期制造商利润。

Conclusion: 政府应根据价格敏感度和目标一致性选择补贴策略，技术投资补贴更适合长期发展，区块链技术对供应链有积极影响。

Abstract: Vaccines play a crucial role in the prevention and control of infectious
diseases. However, the vaccine supply chain faces numerous challenges that
hinder its efficiency. To address these challenges and enhance public health
outcomes, many governments provide subsidies to support the vaccine supply
chain. This study analyzes a government-subsidized, three-tier vaccine supply
chain within a continuous-time differential game framework. The model
incorporates dynamic system equations that account for both vaccine quality and
manufacturer goodwill. The research explores the effectiveness and
characteristics of different government subsidy strategies, considering factors
such as price sensitivity, and provides actionable managerial insights. Key
findings from the analysis and numerical simulations include the following:
First, from a long-term perspective, proportional subsidies for technological
investments emerge as a more strategic approach, in contrast to the short-term
focus of volume-based subsidies. Second, when the public is highly sensitive to
vaccine prices and individual vaccination benefits closely align with
government objectives, a volume-based subsidy policy becomes preferable.
Finally, the integration of blockchain technology positively impacts the
vaccine supply chain, particularly by improving vaccine quality and enhancing
the profitability of manufacturers in the later stages of production.

</details>


### [498] [Traffic jams and driver behavior archetypes](https://arxiv.org/abs/2510.04740)
*Shawn Berry*

Main category: econ.TH

TL;DR: 该研究通过博弈论模型和模拟分析了驾驶员行为对交通拥堵的影响，发现在混合环境中不负责的驾驶员反而比负责任的驾驶员获得更高效用，揭示了交通中的社会困境。


<details>
  <summary>Details</summary>
Motivation: 虽然道路设计、事故、天气等因素都会导致交通拥堵，但驾驶员行为和决策是影响交通流效率的主要决定因素。研究旨在量化个体驾驶员行为与系统级交通结果之间的关系。

Method: 使用博弈论建模和模拟（N=500,000）三车道道路，建立驾驶员行为原型模型，通过Mann-Whitney U检验分析效用差异。

Result: 在同质群体中，负责任驾驶员获得显著更高的期望效用（M=-0.090 vs M=-1.470）；但在混合环境（50/50）中，不负责驾驶员反而表现更好（M=0.128 vs M=-0.127），形成社会困境。所有不负责类型驾驶员获得等效效用且始终优于负责任驾驶员。

Conclusion: 研究结果为交通管理干预、拥堵预测和政策设计提供了稳健框架，有助于协调个体激励与集体效率。提出了未来研究方向。

Abstract: Traffic congestion represents a complex urban phenomenon that has been the
subject of extensive research employing various modeling techniques grounded in
the principles of physics and molecular theory. Although factors such as road
design, accidents, weather conditions, and construction activities contribute
to traffic congestion, driver behavior and decision-making are primary
determinants of traffic flow efficiency. This study introduces a driver
behavior archetype model that quantifies the relationship between individual
driver behavior and system-level traffic outcomes through game-theoretic
modeling and simulation (N = 500,000) of a three-lane roadway. Mann-Whitney U
tests revealed statistically significant differences across all utility
measures (p < .001, d > 2.0). In homogeneous populations, responsible drivers
achieved substantially higher expected utility (M = -0.090) than irresponsible
drivers (M = -1.470). However, in mixed environments (50/50), irresponsible
drivers paradoxically outperformed responsible drivers (M = 0.128 vs. M =
-0.127), illustrating a social dilemma wherein defection exploits cooperation.
Pairwise comparisons across the six driver archetypes indicated that all
irresponsible types achieved equivalent utilities while consistently surpassing
responsible drivers. Lane-specific analyses revealed differential capacity
patterns, with lane 1 exhibiting a more pronounced cumulative utility decline.
These findings offer a robust framework for traffic management interventions,
congestion prediction, and policy design that aligns individual incentives with
collective efficiency. Directions for future research were also proposed.

</details>
