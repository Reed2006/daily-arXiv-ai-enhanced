<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [econ.GN](#econ.GN) [Total: 6]
- [econ.EM](#econ.EM) [Total: 4]
- [econ.TH](#econ.TH) [Total: 5]
- [cs.AI](#cs.AI) [Total: 54]
- [cs.LG](#cs.LG) [Total: 214]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering](https://arxiv.org/abs/2511.17559)
*Gyubok Lee,Woosog Chay,Edward Choi*

Main category: cs.CL

TL;DR: SCARE是一个用于评估电子健康记录问答系统中后验安全机制的基准，包含4200个问题-SQL查询-预期输出的三元组，评估问题可回答性分类和SQL查询验证/修正的联合任务。


<details>
  <summary>Details</summary>
Motivation: 在临床环境中部署文本到SQL模型存在安全风险，错误的SQL查询可能危及患者护理。现有工作缺乏对独立后验验证机制的统一评估基准，这对安全部署至关重要。

Method: 基于MIMIC-III、MIMIC-IV和eICU数据库构建基准，涵盖7种不同文本到SQL模型生成的多样化问题和候选SQL查询，评估从两阶段方法到代理框架的各种方法。

Result: 实验揭示了问题分类和SQL错误修正之间的关键权衡，突出了主要挑战。

Conclusion: SCARE基准填补了电子健康记录问答系统后验安全层评估的空白，为未来研究指明了方向。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.

</details>


### [2] [$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving](https://arxiv.org/abs/2511.17560)
*Yuechi Zhou,Yi Su,Jianxin Zhang,Juntao Li,Qingrong Xia,Zhefeng Wang,Xinyu Duan,Baoxing Huai*

Main category: cs.CL

TL;DR: 提出A³算法，通过注意力感知的KV缓存融合，在减少解码延迟的同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能处理长上下文，但解码延迟和内存开销仍然很大，现有KV缓存重用方法存在性能下降问题。

Method: A³算法基于问题相关性预计算并选择性融合文本块的KV缓存，实现准确集成且计算开销最小。

Result: 在多个基准测试和LLM上的实验表明，A³相比四个基线方法获得最佳任务性能，同时将首token时间减少2倍。

Conclusion: A³算法有效解决了长上下文处理中的延迟问题，在保持性能的同时显著提升效率。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\textbf{A}$ttention-$\textbf{A}$ware $\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\times$.

</details>


### [3] [LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models](https://arxiv.org/abs/2511.17561)
*Huimin Ren,Yan Liang,Baiqiao Su,Chaobo Sun,Hengtong Lu,Kaike Zhang,Chen Wei*

Main category: cs.CL

TL;DR: LexInstructEval是一个用于评估大语言模型细粒度词汇指令遵循能力的新基准和评估框架，通过基于规则的语法将复杂指令解构为<过程、关系、值>三元组，实现客观验证。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型精确遵循复杂词汇指令能力的方法存在主观性、成本高或自动化系统存在偏见和不可靠等问题，现有程序化基准缺乏对细粒度组合约束的测试表达能力。

Method: 构建基于规则的形式化语法，将复杂指令解构为<过程、关系、值>三元组；通过多阶段人机协作流程系统生成多样化数据集；使用透明程序化引擎进行客观验证。

Result: 开发了LexInstructEval基准和评估框架，包括数据集和开源评估工具，支持对大语言模型可控性和可靠性的进一步研究。

Conclusion: LexInstructEval解决了现有评估方法的局限性，为评估大语言模型细粒度词汇指令遵循能力提供了系统化、客观的解决方案。

Abstract: The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.

</details>


### [4] [ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector](https://arxiv.org/abs/2511.17562)
*Wei Tian,YuhaoZhou*

Main category: cs.CL

TL;DR: 基于Qwen3-4B开发的中文拼写和语法纠错统一模型ChineseErrorCorrector3-4B，在多个权威基准测试中取得最优性能


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的中文拼写和语法纠错模型，提升中文文本纠错的整体性能

Method: 基于Qwen3-4B构建统一的中文拼写和语法纠错模型

Result: 在SIGHAN-2015、EC-LAW、MCSC、NaCGEC等基准测试中，F1和F0.5分数显著超越现有公开模型，在拼写和语法纠错任务中均排名第一

Conclusion: ChineseErrorCorrector3-4B模型在中文文本纠错领域表现出色，达到了当前最先进的性能水平

Abstract: This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.

</details>


### [5] [Generative Caching for Structurally Similar Prompts and Responses](https://arxiv.org/abs/2511.17565)
*Sarthak Chakraborty,Suman Nath,Xuchao Zhang,Chetan Bansal,Indranil Gupta*

Main category: cs.CL

TL;DR: 提出了一种生成式缓存方法，能够为结构相似的提示生成变体感知的响应，显著提高缓存命中率和执行效率


<details>
  <summary>Details</summary>
Motivation: 在重复性工作流和代理场景中，提示经常被重复使用且具有相似结构，但精确匹配无法处理这类情况，而语义缓存可能忽略关键差异导致错误响应

Method: 通过识别相似提示结构中的可重用响应模式，为新请求合成定制化输出

Result: 在无提示重复的数据集上达到83%的缓存命中率，错误命中率极低；在代理工作流中，相比标准提示匹配，缓存命中率提高约20%，端到端执行延迟降低约34%

Conclusion: 该方法有效解决了结构相似提示的缓存问题，显著提升了LLM在重复性任务中的执行效率

Abstract: Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \ourmethod{} achieves 83\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\sim$20\% and reduces end-to-end execution latency by $\sim$34\% compared to standard prompt matching.

</details>


### [6] [Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs](https://arxiv.org/abs/2511.17572)
*Patrick Gerard,Aiden Chang,Svitlana Volkova*

Main category: cs.CL

TL;DR: 本文提出一个测试认知立场转移的框架，通过删除事件知识来验证对齐LLMs是否能在无知状态下仍保持特定社区的响应模式。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在对齐到特定在线社区时，是表现出可泛化的行为模式还是仅仅回忆训练数据中的模式。

Method: 引入认知立场转移测试框架：针对性删除事件知识，使用多种探针验证，然后评估模型在无知状态下是否仍重现社区的有机响应模式。使用俄乌军事讨论和美国党派推特数据进行实验。

Result: 即使经过激进的事实删除，对齐的LLMs仍保持稳定的、特定社区的处理不确定性的行为模式。

Conclusion: 对齐编码了结构化、可泛化的行为，超越了表面模仿。该框架为检测在无知状态下持续存在的行为偏见提供了系统方法，有助于更安全透明的LLM部署。

Abstract: When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.

</details>


### [7] [Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models](https://arxiv.org/abs/2511.17575)
*Vladimir Berman*

Main category: cs.CL

TL;DR: 该论文提出了一个完全非语言学的文本模型，基于有限字母表和空格符号的独立随机序列，推导出词汇长度分布、词汇增长、临界长度和Zipf型秩频定律等结构特性。


<details>
  <summary>Details</summary>
Motivation: 为自然语言词汇统计和大语言模型中的token统计提供一个结构化的零模型，证明Zipf类模式可以纯粹从组合学和分割中产生，无需优化原则或语言组织。

Method: 使用有限字母表加空格符号的独立随机序列模型，将单词定义为非空格符号的最大块，基于几何分布和优惠券收集者论证进行数学推导。

Result: 推导出词汇长度服从几何分布，词汇增长和临界长度有闭式解，并得到Zipf型秩频定律p(r)∝r^{-α}，其中指数由字母表大小和空格概率明确决定。

Conclusion: 该模型提供了一个统一的数学框架，将词汇长度、词汇增长、临界长度和秩频结构联系起来，表明许多语言统计模式可能源于随机文本结构而非深层语言机制。

Abstract: We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.

</details>


### [8] [Computational frame analysis revisited: On LLMs for studying news coverage](https://arxiv.org/abs/2511.17746)
*Sharaj Kunjar,Alyssa Hasegawa Smith,Tyler R Mckenzie,Rushali Mohbe,Samuel V Scarpino,Brooke Foucault Welles*

Main category: cs.CL

TL;DR: 评估生成式LLM在媒体框架分析中的效果，发现它们被人工编码员超越，有时甚至被更小的语言模型超越，需要人工验证来确定合适的模型选择。


<details>
  <summary>Details</summary>
Motivation: 研究生成式LLM（如GPT和Claude）作为内容分析工具在媒体框架识别中的有效性，并与传统计算方法及人工编码进行比较。

Method: 使用新颖的金标准数据集，系统评估生成式LLM与词袋模型、编码器转换器以及传统人工编码在MPOX疫情新闻框架分析中的表现。

Result: 生成式LLM在某些应用中有潜力，但总体上被人工编码员超越，有时甚至被更小的语言模型超越，需要人工验证来确定合适的模型选择。

Conclusion: 支持方法论多元化方法，提出了计算框架分析的研究路线图，建议研究人员利用这些方法的互补性来协同使用。

Abstract: Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.

</details>


### [9] [PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://arxiv.org/abs/2511.17808)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: PoETa v2是对葡萄牙语LLMs的最广泛评估，包含40多个任务的基准测试，评估了20多个模型，分析了计算投入和语言特定适应对葡萄牙语性能的影响。


<details>
  <summary>Details</summary>
Motivation: LLMs在不同语言和文化背景下的性能差异显著，需要系统评估多种语言，特别是葡萄牙语。

Method: 引入PoETa v2基准测试套件，包含40多个葡萄牙语任务，评估20多个不同规模和计算资源的模型。

Result: 研究揭示了计算投入和语言特定适应对葡萄牙语性能的影响，并分析了与英语任务的性能差距。

Conclusion: PoETa v2为未来葡萄牙语语言建模和评估研究奠定了基础。

Abstract: Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.

</details>


### [10] [Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813)
*Scott Merrill,Shashank Srivastava*

Main category: cs.CL

TL;DR: 提出一个可复现的管道，将Zoom会议录音转换为带说话者属性的转录本，包含人物档案和语用动作标签，用于训练更真实的多人审议模拟模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型模拟多人审议时缺乏说话者属性数据，自动语音识别产生的匿名说话者标签无法捕捉一致的人类行为模式。

Method: 构建可复现管道处理公开Zoom录音，生成带说话者属性、人物档案和语用动作标签的转录本；基于此数据微调LLMs来建模特定参与者。

Result: 在三个地方政府审议数据集上测试，困惑度降低67%，说话者忠实度和真实性的分类器指标几乎翻倍；图灵测试显示模拟与真实审议难以区分。

Conclusion: 该方法为复杂现实公民模拟提供了实用且可扩展的解决方案，能够生成高度真实的多人审议模拟。

Abstract: Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.

</details>


### [11] [A superpersuasive autonomous policy debating system](https://arxiv.org/abs/2511.17854)
*Allen Roush,Devin Gonier,John Hines,Judah Goldfeder,Philippe Martin Wyder,Sanjay Basu,Ravid Shwartz Ziv*

Main category: cs.CL

TL;DR: DeepDebater是一个能够参与并赢得完整政策辩论的自主AI系统，采用分层多智能体架构，使用大规模辩论证据库生成完整的辩论内容，并通过AI语音和动画呈现。


<details>
  <summary>Details</summary>
Motivation: 解决AI在复杂、基于证据且具有战略适应性的说服能力方面的重大挑战，超越之前简化辩论格式的系统。

Method: 采用分层多智能体工作流架构，LLM驱动的智能体团队协作执行特定论证任务，使用大规模辩论证据库进行迭代检索、合成和自校正，生成完整辩论内容。

Result: 在初步评估中，DeepDebater产生质量更高的论证组件，在模拟辩论中持续获胜，且辩论专家更偏好其构建的论点、证据和案例。

Conclusion: DeepDebater展示了AI在复杂政策辩论中的强大能力，支持全自主和混合人机操作模式，为AI说服能力研究开辟了新方向。

Abstract: The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main

</details>


### [12] [Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction](https://arxiv.org/abs/2511.17908)
*Debashish Chakraborty,Eugene Yang,Daniel Khashabi,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 本文提出了一种基于共形预测的覆盖控制过滤框架，用于在检索增强生成中去除不相关内容，同时保留支持证据的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的预生成过滤器依赖启发式方法或未校准的LLM置信度分数，无法对保留证据提供统计控制，当长或嘈杂上下文超出模型有效注意力范围时，LLM准确性会下降。

Method: 使用基于嵌入和LLM的评分函数，在NeuCLIR和RAGTIME数据集上测试共形预测框架，该框架通过覆盖控制过滤去除不相关内容。

Result: 共形过滤始终达到目标覆盖范围，确保保留指定比例的相关片段，相对于未过滤检索将保留上下文减少2-3倍。在NeuCLIR上，下游事实准确性在严格过滤下提高，在中等覆盖下保持稳定。

Conclusion: 共形预测实现了RAG中可靠、覆盖控制的上下文缩减，为上下文工程提供了模型无关且原则性的方法。

Abstract: Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.

</details>


### [13] [L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention](https://arxiv.org/abs/2511.17910)
*Yuliang Zhan,Xinyu Tang,Han Wan,Jian Li,Ji-Rong Wen,Hao Sun*

Main category: cs.CL

TL;DR: 本文提出L2V-CoT方法，通过线性人工断层扫描发现LLMs和VLMs在低频潜在表示上相似，从而无需训练即可将CoT推理从LLMs转移到VLMs。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多步推理任务上表现不佳，现有方法需要高训练成本或架构对齐，因此需要更高效的CoT推理转移方法。

Method: 使用LAT发现LLMs和VLMs共享相似的低频CoT潜在表示，提出L2V-CoT方法在频域提取和重采样LLMs的低频表示，通过潜在注入增强VLMs推理能力。

Result: 实验表明该方法在无需训练的情况下超越现有基线，甚至优于有监督方法。

Conclusion: L2V-CoT证明了跨模型潜在表示的相似性，为高效CoT推理转移提供了新途径。

Abstract: Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.

</details>


### [14] [Towards Efficient LLM-aware Heterogeneous Graph Learning](https://arxiv.org/abs/2511.17923)
*Wenda Li,Tongya Zheng,Shunyu Liu,Yu Wang,Kaixuan Chen,Hanyang Yuan,Bingde Hu,Zujie Ren,Mingli Song,Gang Chen*

Main category: cs.CL

TL;DR: 提出了一个名为ELLA的高效LLM感知框架，用于处理异质图，通过LLM感知关系分词器捕获复杂关系语义，使用跳级关系图变换器降低计算复杂度，并引入任务感知的思维链提示来弥合预训练和微调之间的语义差距。


<details>
  <summary>Details</summary>
Motivation: 异质图中节点和关系类型的多样性导致复杂丰富的语义，现有方法受限于预定义的语义依赖性和监督信号的稀缺性。LLMs具有强大的文本推理能力，但计算复杂度限制了其在异质图中的应用。

Method: 1. LLM感知关系分词器：利用LLM编码多跳、多类型关系；2. 跳级关系图变换器：将LLM感知关系推理的复杂度从指数级降低到线性级；3. 细粒度任务感知思维链提示：弥合预训练和微调任务之间的语义差距。

Result: 在四个异质图上的广泛实验表明，ELLA在性能和效率上都优于最先进的方法。特别是，ELLA可扩展到130亿参数的LLMs，与现有基于LLM的方法相比实现了高达4倍的加速。

Conclusion: ELLA框架成功解决了异质图中复杂关系语义建模、计算复杂度和语义差距等问题，为LLMs在异质图分析中的高效应用提供了有效解决方案。

Abstract: Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.

</details>


### [15] [SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization](https://arxiv.org/abs/2511.17938)
*Jianghao Wu,Yasmeen George,Jin Ye,Yicheng Wu,Daniel F. Schmidt,Jianfei Cai*

Main category: cs.CL

TL;DR: SPINE是一个基于令牌选择的测试时强化学习框架，通过仅更新高熵分支点令牌来避免传统测试时强化学习中的响应长度崩溃问题，在多种推理任务中稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统测试时强化学习方法在自一致性投票中容易出现多数票奖励主导、响应缩短和性能下降的问题，这是由于均匀序列更新忽略了推理链中关键的分支点令牌。

Method: 提出SPINE框架：(i) 仅更新分支令牌（通过前向传递统计识别的高熵分支点）；(ii) 在这些令牌上应用熵带正则化器，在熵过低时维持探索，在熵过高时抑制噪声监督。

Result: 在十个基准测试中，SPINE在LLM和MLLM骨干网络上一致提升了Pass@1性能，避免了响应长度崩溃，并产生了更稳定的训练动态。

Conclusion: 将更新与思维链分支点对齐是一种简单且无需标签的机制，可在推理模型中实现稳定有效的测试时适应。

Abstract: Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.

</details>


### [16] [Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://arxiv.org/abs/2511.17946)
*Shuo Zhang,Fabrizio Gotti,Fengran Mo,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文探讨了使用预训练数据词汇覆盖率作为大语言模型幻觉检测的补充信号，发现虽然单独使用时效果有限，但与对数概率结合能带来适度提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要使用模型内部信号（如token级熵或生成一致性）检测幻觉，而预训练数据暴露与幻觉之间的关系研究不足。本文旨在探索词汇训练数据覆盖率是否能提供额外的幻觉检测信号。

Method: 构建了RedPajama 1.3万亿token预训练语料库的可扩展后缀数组，检索提示和模型生成的n-gram统计信息，在三个QA基准上评估其幻觉检测效果。

Result: 基于出现次数的特征单独使用时是弱预测因子，但与对数概率结合时能带来适度增益，特别是在内在模型不确定性较高的数据集上。

Conclusion: 词汇覆盖率特征为幻觉检测提供了补充信号，在检测大语言模型幻觉方面具有潜在价值。

Abstract: Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.

</details>


### [17] [MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok](https://arxiv.org/abs/2511.17955)
*Dat Thanh Nguyen,Nguyen Hung Lam,Anh Hoang-Thi Nguyen,Trong-Hop Do*

Main category: cs.CL

TL;DR: MTikGuard是一个用于TikTok的实时多模态有害内容检测系统，通过扩展数据集、多模态分类框架和流式架构，实现了89.37%的准确率和实时部署能力。


<details>
  <summary>Details</summary>
Motivation: TikTok作为儿童和青少年广泛使用的平台，存在大量难以通过传统方法检测的隐蔽有害内容，需要实时有效的检测系统。

Method: 扩展TikHarm数据集至4,723个标注视频，构建多模态分类框架整合视觉、音频和文本特征，并基于Apache Kafka和Spark构建流式架构。

Result: 系统达到89.37%的准确率和89.45%的F1分数，在有害内容检测方面表现优异。

Conclusion: 通过数据集扩展、先进多模态融合和稳健部署的结合，能够有效实现大规模社交媒体内容审核的实用化。

Abstract: With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.

</details>


### [18] [Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets](https://arxiv.org/abs/2511.18054)
*Gowtham,Sai Rupesh,Sanjay Kumar,Saravanan,Venkata Chaithanya*

Main category: cs.CL

TL;DR: Blu-WERP是一个新型的数据预处理管道，专门优化Common Crawl WARC文件的质量用于LLM训练，显著优于现有基线方法，在多个模型规模和评估基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有预处理管道在处理网络规模语料库时难以有效去除噪声和非结构化内容，而高质量的训练数据对LLM性能至关重要。

Method: Blu-WERP处理CC WARC转储文件，实施先进的过滤和质量评估机制，使用150M到1B参数的模型在九个标准基准上进行全面评估。

Result: 在1B参数规模上，Blu-WERP相比DCLM和Fineweb分别实现了4.0%和9.5%的总体改进，在知识推理、语言理解和常识推理方面分别提升2.4%、6.2%和4.2%。

Conclusion: Blu-WERP代表了数据质量优化的实际进展，为研究人员和从业者提供了提高LLM训练效率和模型性能的有效解决方案。

Abstract: High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.

</details>


### [19] [GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set](https://arxiv.org/abs/2511.18146)
*Yomal De Mel,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本研究创建了GeeSanBhava数据集，这是一个高质量的僧伽罗语歌曲评论数据集，基于Russell的效价-唤醒模型进行人工标注，并在零样本学习中达到了0.887的ROC-AUC分数。


<details>
  <summary>Details</summary>
Motivation: 解决僧伽罗语自然语言处理中情感分析数据稀缺的问题，探索基于评论的音乐情感识别，并应对用户生成内容中的偏见挑战。

Method: 从YouTube提取僧伽罗语歌曲评论，由三名独立标注者使用Russell效价-唤醒模型进行人工标注；使用在僧伽罗语新闻评论数据集上预训练的机器学习模型进行零样本学习；通过超参数优化构建三层MLP模型。

Result: 标注者间一致性高达84.96%（Fleiss kappa）；不同歌曲展现出独特的情感特征；优化后的MLP模型（256-128-64神经元配置）在零样本学习中达到0.887的ROC-AUC分数。

Conclusion: 该研究为僧伽罗语NLP和音乐情感识别提供了有价值的标注数据集和基准结果，为未来相关研究奠定了基础。

Abstract: This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.

</details>


### [20] [Vector Arithmetic in Concept and Token Subspaces](https://arxiv.org/abs/2511.18162)
*Sheridan Feucht,Byron Wallace,David Bau*

Main category: cs.CL

TL;DR: 论文展示了如何利用概念归纳头和标记归纳头在LLaMA-2-7b模型中识别具有连贯语义结构的激活子空间，通过注意力权重变换隐藏状态，显著提高了平行四边形算术和表面级单词操作的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了预测下一个标记，LLMs需要表示当前单词的语义和表面级信息。先前工作识别了两种解耦这些信息的注意力头类型，本研究旨在探索这些头如何帮助识别模型激活中的语义结构子空间。

Method: 使用概念归纳头和标记归纳头的注意力权重来变换隐藏状态，然后在这些变换后的隐藏状态上执行平行四边形算术和表面级单词操作。

Result: 通过概念头变换后的隐藏状态进行平行四边形算术（如"Athens" - "Greece" + "China" = "Beijing"）的最近邻准确率达到80%，远高于原始隐藏状态的47%。标记头变换则能有效揭示表面级单词信息。

Conclusion: 概念归纳头和标记归纳头能够识别模型激活中的语义结构子空间，通过适当的变换可以显著提高语义操作和表面级操作的准确性，这为理解LLMs内部表示提供了新的视角。

Abstract: In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that "Athens" - "Greece" + "China" = "Beijing". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like "coding" - "code" + "dance" = "dancing".

</details>


### [21] [Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models](https://arxiv.org/abs/2511.18177)
*Elias Lumer,Matt Melich,Olivia Zino,Elena Kim,Sara Dieter,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah,James A. Burke,Roberto Hernandez*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于向量和非向量的RAG架构在金融文档问答中的表现，发现向量化智能RAG在检索准确性和答案质量上优于层次节点系统，交叉编码器重排序和小-大块检索技术能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对金融文档中基于向量和非向量RAG架构的系统比较，以及高级RAG技术对检索准确性、答案质量、延迟和成本影响的实证研究。

Method: 系统评估基于向量的智能RAG（使用混合搜索和元数据过滤）与层次节点系统（无需嵌入遍历文档结构），并评估两种增强技术：交叉编码器重排序和小-大块检索。

Result: 基于向量的智能RAG在150个问题的基准测试中，相比层次节点系统获得68%的胜率，延迟相当（5.2秒 vs 5.98秒）。交叉编码器重排序在最优参数下MRR@5提升59%，小-大块检索相比基线块化获得65%胜率，仅增加0.2秒延迟。

Conclusion: 在金融问答系统中应用高级RAG技术能显著提升检索准确性和答案质量，但在生产环境中需要考虑成本-性能权衡。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.

</details>


### [22] [Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems](https://arxiv.org/abs/2511.18194)
*Faheem Nizar,Elias Lumer,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出Agent-as-a-Graph检索方法，通过知识图谱表示工具和代理，在LiveMCPBenchmark上相比现有方法在Recall@5和nDCG@5指标上提升14.9%和14.6%。


<details>
  <summary>Details</summary>
Motivation: 现有代理、MCP和检索方法通常只匹配单个代理描述，无法充分利用每个代理的细粒度工具能力，导致代理选择不理想。

Method: 使用知识图谱检索增强生成方法，将工具及其父代理表示为知识图谱中的节点和边。检索过程包括：向量搜索相关代理和工具节点、应用类型特定的加权互逆排名融合重排序、在知识图谱中遍历父代理获取最终代理集合。

Result: 在LiveMCPBenchmark上，Recall@5和nDCG@5分别比现有最优检索器提升14.9%和14.6%，wRRF优化提升2.4%。

Conclusion: Agent-as-a-Graph方法通过知识图谱表示和检索策略，显著提升了多代理系统中代理选择的准确性和效率。

Abstract: Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.

</details>


### [23] [From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation](https://arxiv.org/abs/2511.18259)
*Xiaochen Zheng,Alvaro Serra,Ilya Schneider Chernov,Maddalena Marchesi,Eunice Musvasva,Tatyana Y. Doktorova*

Main category: cs.CL

TL;DR: DiscoVerse是一个多智能体协同科学家系统，用于支持药物研发，通过语义检索、跨文档链接和可审计合成来处理罗氏公司的大型历史数据档案，在真实药物数据上实现了高召回率和中等精度的表现。


<details>
  <summary>Details</summary>
Motivation: 药物研发积累了海量异构数据，其中许多来自已终止的项目，重新利用这些档案对逆向转化研究非常有价值，但在实践中往往不可行。

Method: 开发了DiscoVerse多智能体系统，实现语义检索、跨文档链接和可审计合成，在罗氏公司180个分子、超过8.7亿BPE标记、跨越40多年研究的数据集上进行验证，采用盲审专家评估方法。

Result: 在7个基准查询覆盖180个分子的测试中，DiscoVerse实现了接近完美的召回率(≥0.99)和中等精度(0.71-0.91)，在终止原因和器官特异性毒性评估中显示出忠实、有来源的合成能力。

Conclusion: 这是首个在真实药物数据上系统评估的智能体框架，展示了有前景的答案准确性和决策洞察力，为逆向转化研究提供了有效支持。

Abstract: Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.

</details>


### [24] ["AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa](https://arxiv.org/abs/2511.18301)
*Harsh Rathva,Pruthwik Mishra,Shrikant Malviya*

Main category: cs.CL

TL;DR: 本文提出了一种数据中心的策略来解决多语言科学文本幻觉检测中的数据稀缺和不平衡问题，通过整合和平衡五个现有数据集创建了124,821个样本的训练语料库，在SHROOM-CAP 2025共享任务中取得了竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的多语言科学文本中的幻觉检测对可靠AI系统至关重要，但面临训练数据稀缺和不平衡的挑战，特别是在低资源语言和零样本设置下。

Method: 采用数据中心策略，统一并平衡五个现有数据集创建包含124,821个样本（50%正确，50%幻觉）的综合训练语料库，比原始SHROOM训练数据增加172倍，然后使用5.6亿参数的XLM-RoBERTa-Large模型进行微调。

Result: 在所有9种语言中均取得竞争性表现，特别是在零样本语言古吉拉特语中获得第2名（事实性F1为0.5107），其余8种语言排名在4-6名之间。

Conclusion: 系统性的数据整理策略可以显著超越仅依赖架构创新的方法，特别是在低资源语言的零样本设置下，证明了数据质量对多语言幻觉检测的重要性。

Abstract: The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.

</details>


### [25] [Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning](https://arxiv.org/abs/2511.18306)
*Mohammad Aqib,Mohd Hamza,Ying Hei Chui,Qipei Mei*

Main category: cs.CL

TL;DR: 本文比较了两种从建筑规范表格数据中提取信息的方法：直接输入图像到视觉语言模型(VLM)和间接输入(将表格转换为LaTeX代码)，发现直接方法更准确。通过LoRA微调后，模型性能显著提升，Qwen2.5-VL-3B-Instruct准确率相对提升超过100%。


<details>
  <summary>Details</summary>
Motivation: 建筑规范包含确保安全和合规性的关键信息，但表格数据由于复杂的布局、合并单元格和多行标题而难以提取，传统NLP技术和VLM难以有效处理。

Method: 比较两种方法：1) 直接输入页面图像到预训练VLM；2) 间接输入，先将表格图像转换为LaTeX代码再输入。使用LoRA对VLM进行参数高效微调。

Result: 直接输入方法通常比间接方法准确率更高。微调后模型性能大幅提升，Qwen2.5-VL-3B-Instruct准确率相对提升超过100%。

Conclusion: 参数高效微调方法能够有效适配强大的VLM，用于理解专业领域中的复杂结构化数据，如建筑规范解释和法规合规性。

Abstract: Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.

</details>


### [26] [Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search](https://arxiv.org/abs/2511.18313)
*Joseph Oladokun*

Main category: cs.CL

TL;DR: 提出路径约束检索(PCR)方法，通过结合图结构约束和语义搜索，确保检索信息在知识图谱中保持逻辑关系，显著提高LLM智能体推理的连贯性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体从知识库中检索上下文时，由于缺乏与当前推理状态的结构一致性，导致推理链不连贯。需要一种能保持逻辑关系的检索方法。

Method: PCR方法将结构图约束与语义搜索相结合，将搜索空间限制为从锚节点可达的节点，防止检索结构断开的信息。

Result: 在PathRAG-6基准测试中，PCR实现100%结构一致性(基线方法24-32%)，在技术领域rank 10处获得完全相关性和结构一致性，检索上下文平均图距离减少78%。

Conclusion: 路径约束检索是提高LLM智能体推理系统可靠性和连贯性的有效方法。

Abstract: Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.

</details>


### [27] [Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection](https://arxiv.org/abs/2511.18324)
*Syed Mohaiminul Hoque,Naimur Rahman,Md Sakhawat Hossain*

Main category: cs.CL

TL;DR: 本文提出了"Gradient Masters"方法用于BLP-2025任务1的孟加拉语多任务仇恨言论识别，采用集成微调策略处理仇恨类型分类和目标群体分类子任务，在YouTube评论数据集上取得了良好表现。


<details>
  <summary>Details</summary>
Motivation: 针对低资源孟加拉语仇恨言论检测场景，需要开发有效的分类方法来处理YouTube评论中的仇恨言论识别问题。

Method: 采用基于孟加拉语语言模型的混合集成微调策略，通过广泛的实验评估模型在开发和评估阶段的鲁棒性。

Result: 在子任务1A中获得第6名（微F1分数73.23%），在子任务1B中获得第3名（73.28%），超越了基线模型。

Conclusion: 该方法在低资源孟加拉语仇恨言论检测中表现出良好的泛化能力，并对误分类模式进行了详细分析。

Abstract: This paper introduces the approach of "Gradient Masters" for BLP-2025 Task 1: "Bangla Multitask Hate Speech Identification Shared Task". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.

</details>


### [28] [OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas](https://arxiv.org/abs/2511.18335)
*James Y. Huang,Wenxuan Zhou,Nan Xu,Fei Wang,Qin Liu,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: 该论文提出了OmniStruct基准测试，用于评估LLM在文本到结构任务中的能力，并通过合成数据训练小模型达到与GPT-4o相当的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然现代LLM在生成非结构化自然语言响应方面表现出色，但它们在文本到结构任务（如信息提取、表格生成和函数调用）中的表现尚不明确，需要系统评估。

Method: 构建OmniStruct基准测试，整合现有数据集到统一的文本到结构问题设置中；通过合成任务生成收集高质量训练数据；在无监督数据的情况下微调小模型。

Result: 实验表明，仅使用合成数据微调的小模型可以达到与GPT-4o相当的文本到结构生成性能。

Conclusion: 证明了通过合成数据训练小模型成为通用结构化生成模型的可行性，无需使用OmniStruct任务的监督数据。

Abstract: The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.

</details>


### [29] [Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle](https://arxiv.org/abs/2511.18369)
*Manon Berriche*

Main category: cs.CL

TL;DR: 研究发现假新闻在社交媒体中占比很小，但被少数高度政治化的活跃用户集中分享；用户对可疑信息采取不同形式的批判性距离，但很少产生真正的协商辩论。


<details>
  <summary>Details</summary>
Motivation: 解决两个悖论：为何在缺乏编辑控制的情况下假新闻占比很小，以及为何用户对假新闻不特别敏感但政治极化仍在加剧。

Method: 在Twitter和Facebook上进行混合方法研究，结合数字痕迹定量分析、在线观察和访谈，考察用户在不同互动情境中的多样化实践。

Result: 1. 假新闻分享集中在少数高度政治化、批判制度的活跃用户群体；2. 用户根据社会地位和互动规范采取不同批判距离形式；3. 这些批判很少产生真正的协商辩论，而是形成小范围的对立对话。

Conclusion: 假新闻的影响主要来自少数活跃用户的议程设置能力，而非广泛传播；用户批判性反应受社会情境影响，但难以促成建设性对话。

Abstract: This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence énonciative) or interventions ('points d'arrêt') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.

</details>


### [30] [Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://arxiv.org/abs/2511.18393)
*Heejoon Koo*

Main category: cs.CL

TL;DR: 该论文研究了文本退化对LLM在临床决策支持系统中可靠性和公平性的影响，提出了标签缩减和分层思维链策略来提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床文本常因人为错误或自动化流程故障而质量下降，这会影响AI辅助决策的可靠性和公平性，但目前对此影响的研究不足。

Method: 引入临床基础的标签缩减方案和分层思维链策略，模拟临床医生的推理过程，研究不同文本损坏场景下LLM的表现。

Result: 该方法提高了模型在退化输入下的鲁棒性，减少了亚组不稳定性，推进了LLM在CDSS中的可靠使用。

Conclusion: 通过系统研究文本退化对LLM的影响，并采用临床推理策略，可以有效提升临床决策支持系统的可靠性和公平性。

Abstract: A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.

</details>


### [31] [Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models](https://arxiv.org/abs/2511.18409)
*Dana Arad,Yonatan Belinkov,Hanjie Chen,Najoung Kim,Hosein Mohebbi,Aaron Mueller,Gabriele Sarti,Martin Tutek*

Main category: cs.CL

TL;DR: 该论文介绍了基于Mechanistic Interpretability Benchmark (MIB)的BlackboxNLP 2025共享任务，包含电路定位和因果变量定位两个赛道，展示了多种方法在可解释性研究中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决机制可解释性研究中进展衡量困难的问题，建立标准化的评估框架来比较不同的可解释性技术。

Method: 基于MIB基准构建共享任务，包含电路定位（识别因果影响组件和交互）和因果变量定位（将激活映射为可解释特征）两个赛道，采用集成、正则化、低维和非线性投影等方法。

Result: 在电路定位中，3个团队的8种方法通过集成和正则化策略取得显著进展；在因果变量定位中，1个团队的2种方法通过低维和非线性投影获得重要提升。

Conclusion: MIB排行榜持续开放，鼓励在该标准评估框架下继续开展机制可解释性研究，以衡量该领域的进展。

Abstract: Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.

</details>


### [32] [SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data](https://arxiv.org/abs/2511.18411)
*Sultan Alrashed,Chadi Helwe,Francesco Orabona*

Main category: cs.CL

TL;DR: SmolKalam是一个阿拉伯语多轮对话数据集，通过多模型集成翻译管道从Smoltalk2翻译而来，包含推理和工具调用功能，并应用了质量过滤。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模、高质量的阿拉伯语多轮对话数据集，特别是包含推理和工具调用的数据。简单的翻译方法在预训练阶段可行，但在后训练阶段需要更高质量的数据集。

Method: 使用多模型集成翻译管道对Smoltalk2进行翻译，应用质量过滤，并通过消融实验研究了传统仅解码器模型的有效翻译技术。

Result: 成功创建了SmolKalam阿拉伯语数据集，该数据集具有高质量的多轮对话内容，包含推理和工具调用功能。

Conclusion: 通过严格的翻译管道和质量过滤，可以创建高质量的阿拉伯语多轮对话数据集，为后训练阶段提供可靠的数据支持。

Abstract: Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.

</details>


### [33] [Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations](https://arxiv.org/abs/2511.18413)
*Yu Xia,Sungchul Kim,Tong Yu,Ryan A. Rossi,Julian McAuely*

Main category: cs.CL

TL;DR: 提出了MACF框架，将传统协同过滤算法转化为基于LLM的多智能体协作系统，通过动态智能体招募和个性化协作指令来提升推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的推荐系统大多采用通用单智能体或多智能体任务分解流程，缺乏推荐导向设计，未能充分利用用户-物品交互历史中的协作信号，导致推荐结果不理想

Method: MACF框架将相似用户和相关物品实例化为具有独特配置文件的LLM智能体，每个智能体能够调用检索工具、推荐候选物品并与其他智能体交互，通过中央编排器智能体动态管理用户和物品智能体之间的协作

Result: 在三个不同领域的数据集上的实验结果表明，MACF框架相比强大的智能体推荐基线具有优势

Conclusion: MACF框架成功地将传统协同过滤算法转化为基于LLM的多智能体协作系统，通过动态智能体协作机制显著提升了推荐性能

Abstract: Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.

</details>


### [34] [General Agentic Memory Via Deep Research](https://arxiv.org/abs/2511.18423)
*B. Y. Yan,Chaofan Li,Hongjin Qian,Shuqi Lu,Zheng Liu*

Main category: cs.CL

TL;DR: 提出通用代理记忆框架GAM，采用即时编译原则，通过Memorizer和Researcher组件在运行时动态创建优化上下文，显著提升记忆相关任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有静态记忆系统在预先创建可用记忆时不可避免的信息丢失问题，需要更灵活高效的记忆机制。

Method: 采用即时编译原则，包含Memorizer组件（使用轻量记忆突出关键历史信息，并在通用页面存储中维护完整历史）和Researcher组件（根据预构建记忆从页面存储中检索整合有用信息）。

Result: 在多种记忆相关任务完成场景中，相比现有记忆系统实现了显著改进。

Conclusion: GAM框架能有效利用前沿大语言模型的代理能力和测试时扩展性，同时通过强化学习促进端到端性能优化。

Abstract: Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \textbf{general agentic memory (GAM)}. GAM follows the principle of "\textbf{just-in time (JIT) compilation}" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.

</details>


### [35] [MindEval: Benchmarking Language Models on Multi-turn Mental Health Support](https://arxiv.org/abs/2511.18491)
*José Pombal,Maya D'Eon,Nuno M. Guerreiro,Pedro Henrique Martins,António Farinhas,Ricardo Rei*

Main category: cs.CL

TL;DR: MindEval是一个由临床心理学家设计的框架，用于自动评估语言模型在真实多轮心理健康对话中的表现，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI心理健康聊天机器人存在谄媚、过度验证和强化不良信念等问题，且缺乏能够捕捉真实治疗互动复杂性的评估基准。

Method: 通过与博士级临床心理学家合作，开发了基于患者模拟和LLM自动评估的框架，验证了模拟患者的真实性和自动评估与专家判断的相关性。

Result: 评估了12个最先进的LLM，所有模型平均得分低于4/6，在沟通模式上存在明显弱点，推理能力和模型规模不能保证更好表现，且随着对话延长或患者症状加重而表现恶化。

Conclusion: 现有LLM在心理健康对话中表现不佳，需要专门改进，MindEval框架为开发更好的心理健康AI系统提供了重要评估工具。

Abstract: Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.

</details>


### [36] [For Those Who May Find Themselves on the Red Team](https://arxiv.org/abs/2511.18499)
*Tyler Shoemaker*

Main category: cs.CL

TL;DR: 文学学者需要参与大型语言模型可解释性研究，尽管这会涉及意识形态斗争甚至妥协，但当前可解释性方法的工具性不应成为衡量LLM解释的唯一标准。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型可解释性研究过于工具化，文学学者需要参与其中以引入更丰富的解释标准，避免仅以实用性作为衡量标准。

Method: 提出通过参与"红队"（red team）作为文学学者介入LLM可解释性研究的实践场所。

Result: 确立了文学学者参与LLM可解释性研究的必要性，并提出了具体的参与路径。

Conclusion: 文学学者必须积极参与LLM可解释性研究，尽管面临意识形态挑战，但这是确保解释标准多样化的必要举措。

Abstract: This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.

</details>


### [37] [Dealing with the Hard Facts of Low-Resource African NLP](https://arxiv.org/abs/2511.18557)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Panga Azazia Kamaté,Madani Amadou Tall,Emmanuel Élisé Koné,Aymane Dembélé,Michael Leventhal*

Main category: cs.CL

TL;DR: 本文报告了针对低资源语言班巴拉语的数据收集、标注、模型构建和评估工作，包括收集612小时自发语音、半自动转录、构建单语模型，并提供实用建议和公开资源。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏语音数据集、模型和评估框架，班巴拉语作为西非低资源语言面临这一挑战，需要建立相关资源和方法。

Method: 采用实地收集612小时班巴拉语自发语音，进行半自动标注和转录，构建单语超紧凑和小型模型，并进行自动和人工评估。

Result: 成功创建了班巴拉语语音数据集、多个模型和评估框架，提供了数据收集协议、标注和模型设计的实用建议，证明了人工评估的重要性。

Conclusion: 为低资源语言的研究提供了完整的工作流程和公开资源，强调了人工评估在模型开发中的关键作用。

Abstract: Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.

</details>


### [38] [Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks](https://arxiv.org/abs/2511.18597)
*H. M. Shadman Tabib,Jaber Ahmed Deedar*

Main category: cs.CL

TL;DR: GPT-4o在评估编程题目难度方面表现不佳，准确率仅37.75%，远低于基于特征工程的LightGBM模型（86%准确率）。GPT-4o倾向于低估题目难度，且忽略数值约束等关键特征。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在结构化任务（如编程题目难度评估）中的表现，并与传统机器学习方法进行对比，以评估LLM作为自动评判者的可靠性。

Method: 使用1,825个LeetCode题目数据集，对比GPT-4o（纯自然语言评估）与基于显式数值和文本特征的LightGBM集成模型。通过混淆矩阵和SHAP可解释性分析进行详细评估。

Result: LightGBM达到86%准确率，而GPT-4o仅37.75%。GPT-4o表现出对简单类别的强烈偏见，经常忽略输入大小限制和通过率等关键数值约束。

Conclusion: 在编程竞赛、教育平台或强化学习管道中部署基于LLM的评判者之前，必须解决这些具体的失败模式，以确保其可信度。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.

</details>


### [39] [A Benchmark for Zero-Shot Belief Inference in Large Language Models](https://arxiv.org/abs/2511.18616)
*Joseph Malone,Rachith Aiyappa,Byunghwee Lee,Haewoon Kwak,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 提出了一个系统化的基准测试，用于评估大语言模型在零样本设置下预测个体对各种话题立场的泛化能力，发现提供更多背景信息能提高准确性，但性能在不同信念领域差异显著。


<details>
  <summary>Details</summary>
Motivation: 信念是人类推理、沟通和社交的核心，但现有计算方法局限于狭窄的社会政治背景且依赖微调。需要了解大语言模型在不同信念领域的泛化能力。

Method: 使用在线辩论平台数据，构建系统化、可复现的基准测试，评估LLMs在零样本设置下预测个体立场的表现，包含多个信息条件以分离人口统计背景和已知先验信念的贡献。

Result: 在多个中小型模型中，提供更多背景信息能提高预测准确性，但性能在不同信念领域存在显著差异。

Conclusion: 研究揭示了当前LLMs模拟人类推理的能力和局限性，为机器行为研究提供了进展，并提供了超越社会政治领域的信念系统建模的可扩展框架。

Abstract: Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.

</details>


### [40] [A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News](https://arxiv.org/abs/2511.18618)
*Mirza Raquib,Munazer Montasir Akash,Tawhid Ahmed,Saydul Akbar Murad,Farida Siddiqi Prity,Mohammad Amzad Hossain,Asif Pervez Polok,Nick Rahimi*

Main category: cs.CL

TL;DR: 该研究提出了一种结合BERT-CNN-BiLSTM混合迁移学习模型的方法，用于孟加拉语新闻标题分类和情感分析，在BAN-ABSA数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 报纸是重要的信息来源，但有效导航大量新闻内容具有挑战性。新闻标题情感分析可以帮助快速理解新闻的情感基调，特别是在孟加拉语这种低资源语言中。

Method: 使用BERT-CNN-BiLSTM混合迁移学习模型，在包含9014个新闻标题的BAN-ABSA数据集上进行实验。采用两种技术策略：技术1在数据分割前进行欠采样和过采样，技术2在分割后进行采样处理。

Result: 技术1中过采样方法在标题和情感分类上分别达到78.57%和73.43%的最高性能；技术2中直接在原始不平衡数据集上训练分别达到81.37%和64.46%。提出的模型显著优于所有基线模型。

Conclusion: 该研究证明了同时利用标题和情感数据集的重要性，为孟加拉语文本分类在低资源环境下提供了强大的基准，BERT-CNN-BiLSTM模型在孟加拉语新闻分类任务中达到了最先进的性能。

Abstract: In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\% and 73.43\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\% and 64.46\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.

</details>


### [41] [Prompt Optimization as a State-Space Search Problem](https://arxiv.org/abs/2511.18619)
*Maanas Taneja*

Main category: cs.CL

TL;DR: 该论文将提示优化建模为状态空间搜索问题，通过束搜索和随机游走算法在提示空间中进行系统探索，在多个NLP任务上实现了性能提升，但存在开发集过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型对输入提示字符串的微小变化极其敏感，容易导致性能崩溃。受DSpy等基于演示的提示优化方法启发，作者寻求一种替代方法来解决这个问题。

Method: 将提示空间建模为图结构，节点代表提示状态，边对应各种转换操作（如缩短、添加示例、重新排序内容）。使用束搜索和随机游走算法系统探索该空间，在开发集上评估候选提示并剪枝无希望的分支。

Result: 在五个NLP任务（情感分类、问答、摘要、推理和自然语言推理）上，即使浅层搜索配置（束宽=2，深度=2）也能在开发集上超越种子提示。例如，推理任务的开发准确率从0.40提升到0.80，但测试集改进较为温和（0.20到0.50），表明存在开发集过拟合。

Conclusion: 研究验证了将提示优化作为搜索问题的有效性，并表明通过更多计算资源和改进的评估指标，更深入的探索可以产生更稳健的提示，能够超越开发集进行泛化。

Abstract: Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].

</details>


### [42] [OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph](https://arxiv.org/abs/2511.18622)
*Michael J. Bommarito*

Main category: cs.CL

TL;DR: OpenGloss是一个合成的百科全书式词典和语义知识图谱，整合了词典定义、百科全书内容、词源历史和语义关系，包含53.7万个词义和150万个词条，生成成本低于1000美元，时间不到一周。


<details>
  <summary>Details</summary>
Motivation: 解决现有词汇资源在教学内容整合方面的不足，为词汇学习和自然语言处理任务提供集成的定义、例句、搭配、百科全书内容和词源信息。

Method: 采用多智能体程序生成流水线，结合模式验证的LLM输出和自动化质量保证，实现结构化生成。

Result: 生成了包含53.7万个词义、910万条语义边、100万个使用例句、300万个搭配和6000万词百科全书内容的大规模资源，规模与WordNet 3.1相当但定义数量多四倍。

Conclusion: 结构化生成能以远低于人工整理的成本和时间创建全面的词汇资源，随着基础模型的改进可实现快速迭代，该资源已在Hugging Face上以CC-BY 4.0许可公开。

Abstract: We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.
  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.
  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.

</details>


### [43] [No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases](https://arxiv.org/abs/2511.18635)
*Shireen Chand,Faith Baca,Emilio Ferrara*

Main category: cs.CL

TL;DR: 本文研究针对性偏见缓解的跨类别后果，发现虽然目标维度的偏见有时会减少，但经常在其他维度产生负面后果，如增加模型偏见和降低整体连贯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型继承训练数据中的社会偏见，可能导致有害或不公平输出。现有偏见缓解技术往往只评估目标偏见维度，缺乏对跨类别后果的研究。

Method: 研究了四种偏见缓解技术，应用于来自七个模型家族的十个模型，探索种族、宗教、职业和性别相关偏见，使用StereoSet基准衡量偏见缓解对模型连贯性和刻板偏好的影响。

Result: 针对性偏见缓解有时能减少目标维度的偏见，但经常在其他维度产生意外负面后果，包括增加模型偏见和降低整体连贯性。

Conclusion: 研究结果强调了在检查和开发偏见缓解策略时需要强大的多维评估工具，以避免无意中在未针对性维度上转移或加剧偏见。

Abstract: Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.

</details>


### [44] [Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting](https://arxiv.org/abs/2511.18649)
*Goun Pyeon,Inbum Heo,Jeesu Jung,Taewook Hwang,Hyuk Namgoong,Hyein Seo,Yerim Han,Eunbin Kim,Hyeonseok Kang,Sangkeun Jung*

Main category: cs.CL

TL;DR: 本研究系统评估了大型语言模型在2026年韩国高考数学考试中的推理能力，建立了完全无数据泄露的评估环境。GPT-5 Codex获得满分，几何是表现最差的领域，文本输入优于图像输入。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基准测试中的数据泄露问题，确保对LLM数学推理能力的真实评估，使用完全未被模型训练数据接触过的新鲜考试题目。

Method: 在考试公开后2小时内数字化所有46道题目，评估24个最先进的LLM在不同输入模态（文本、图像、文本+图形）和提示语言（韩语、英语）下的表现。

Result: GPT-5 Codex获得唯一满分（100分），Grok 4、GPT-5和Deepseek R1得分超过95分。几何领域表现最差（77.7%平均分），文本输入始终优于图像输入。

Conclusion: 建立了完全无暴露的评估环境，提供了基于真实考试的LLM评估框架，并提出了综合考虑性能、成本和时间因素的实用评估视角。

Abstract: This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).
  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.
  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).

</details>


### [45] [CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning](https://arxiv.org/abs/2511.18659)
*Jie He,Richard He Bai,Sinead Williamson,Jeff Z. Pan,Navdeep Jaitly,Yizhe Zhang*

Main category: cs.CL

TL;DR: CLaRa是一个统一的检索增强生成框架，通过共享连续空间进行嵌入压缩和联合优化，解决了长上下文和检索-生成分离优化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法在处理长上下文时存在困难，且检索和生成模块通常是分离优化的，导致整体性能受限。

Method: 提出CLaRa框架，包括SCP数据合成方法生成语义丰富的压缩向量，通过可微分top-k估计器实现重排序器和生成器的端到端训练，使用单一语言建模损失进行联合优化。

Result: 在多个QA基准测试中，CLaRa实现了最先进的压缩和重排序性能，通常超越基于文本的微调基线方法。

Conclusion: CLaRa通过统一优化框架在连续潜在空间中进行检索和生成，理论上有助于对齐检索相关性和答案质量，实验证明其有效性。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.

</details>


### [46] [Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models](https://arxiv.org/abs/2511.18696)
*Wangjiaxuan Xin*

Main category: cs.CL

TL;DR: ECN是一个多阶段提示框架，通过四个阶段增强大语言模型的共情和包容能力，在GPT模型上获得了最高的共情商数得分。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在对话AI中的共情和包容能力，使其能够生成更具情感共鸣和情境感知的回应。

Method: 使用四阶段提示方法：视角采纳、情感共鸣、反思理解和综合整合，引导模型生成情感共鸣的回应。

Result: ECN在GPT-3.5-turbo和GPT-4上获得了最高的共情商数得分，同时在尊重度和困惑度指标上保持竞争力。

Conclusion: ECN框架在需要共情和包容性的对话AI应用中具有重要潜力。

Abstract: This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.

</details>


### [47] [RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context](https://arxiv.org/abs/2511.18743)
*Yu Lei,Shuzheng Si,Wei Wang,Yifei Wu,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: RhinoInsight是一个深度研究框架，通过可验证检查表和证据审计模块增强LLM代理的鲁棒性和可追溯性，无需参数更新即可提升研究质量。


<details>
  <summary>Details</summary>
Motivation: 现有系统采用线性管道（规划-搜索-写作-报告）容易导致错误累积和上下文腐化，缺乏对模型行为和上下文的显式控制。

Method: 引入两个控制机制：1）可验证检查表将用户需求转化为可追踪子目标，通过批评者精炼并生成层次化大纲；2）证据审计模块结构化搜索内容，迭代更新大纲并修剪噪声上下文，通过批评者排名和绑定高质量证据。

Result: 在深度研究任务上达到最先进性能，在深度搜索任务上保持竞争力。

Conclusion: RhinoInsight通过显式控制机制有效解决了现有系统的错误累积和上下文腐化问题，提升了研究框架的鲁棒性和可追溯性。

Abstract: Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.

</details>


### [48] [Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search](https://arxiv.org/abs/2511.18749)
*Matthew R. DeVerna,Kai-Cheng Yang,Harry Yaojun Yan,Filippo Menczer*

Main category: cs.CL

TL;DR: 评估15个主流LLM在6000多个PolitiFact事实核查声明上的表现，发现标准模型表现差，推理能力帮助有限，网络搜索仅带来中等提升，而使用精选RAG系统能显著改善事实核查效果。


<details>
  <summary>Details</summary>
Motivation: 随着主流聊天机器人具备推理能力和网络搜索工具，数百万用户依赖它们进行事实核查，急需对这些模型进行严格评估。

Method: 在6000多个PolitiFact事实核查声明上评估15个最新LLM，比较标准模型、推理变体和网络搜索变体的表现。

Result: 标准模型表现差，推理能力提供最小收益，网络搜索仅带来中等提升，而使用精选RAG系统平均提高宏F1分数233%。

Conclusion: 为模型提供精选高质量上下文是自动化事实核查的有前景路径。

Abstract: Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.

</details>


### [49] [Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion](https://arxiv.org/abs/2511.18751)
*Daiqing Wu,Dongbao Yang,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 提出DRF方法用于图像-文本对的多模态情感分析，通过特征分布建模同时处理低质量和缺失模态问题，在三种公开数据集上验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对低质量和缺失模态的考虑，而现实应用中这些问题频繁发生，需要能够鲁棒预测情感的模型。

Method: 为每个模态维护特征队列来近似特征分布，基于分布定量估计模态质量以降低低质量模态的贡献，通过样本和分布监督建立模态间映射关系来恢复缺失模态。

Result: 在三种公开图像-文本数据集上，DRF相比SOTA方法在两种破坏策略（损坏和丢弃模态）下均取得普遍改进。

Conclusion: DRF方法在统一框架中有效处理低质量和缺失模态问题，验证了其在鲁棒多模态情感分析中的有效性。

Abstract: As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.

</details>


### [50] [Context-Aware Whisper for Arabic ASR Under Linguistic Varieties](https://arxiv.org/abs/2511.18774)
*Bashar Talafha,Amin Abu Alhassan,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出上下文感知提示策略，无需重新训练即可适配OpenAI Whisper用于阿拉伯语语音识别，在多种阿拉伯语条件下显著降低词错误率


<details>
  <summary>Details</summary>
Motivation: 解决低资源阿拉伯语ASR的挑战，特别是面对广泛的方言变异和有限标注数据的问题

Method: 使用解码器提示（首遍转录或检索语句）和编码器前缀（目标说话者语音合成），引入提示重排序、说话者感知前缀合成和模态特定检索技术

Result: 在九种阿拉伯语条件下，现代标准阿拉伯语WER降低22.3%，方言语音WER降低9.2%，显著减少幻觉和说话者不匹配问题

Conclusion: 上下文感知提示策略有效提升Whisper在阿拉伯语ASR中的性能，特别是在零样本设置下

Abstract: Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.

</details>


### [51] [HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations](https://arxiv.org/abs/2511.18808)
*Cao Linxiao,Wang Ruitao,Li Jindong,Zhou Zhipeng,Yang Menglin*

Main category: cs.CL

TL;DR: HyperbolicRAG是一个基于双曲几何的检索增强生成框架，通过将图结构嵌入到双曲空间中，同时捕捉语义相似性和层次结构关系，在多个QA基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图基RAG方法依赖于欧几里得嵌入，虽然能捕获语义相似性，但缺乏对层次深度关系的几何表示，限制了在复杂知识图中表示抽象关系的能力。

Method: 提出HyperbolicRAG框架，包含三个关键设计：(1)深度感知表示学习器，在共享Poincare流形中嵌入节点；(2)无监督对比正则化，强制跨抽象级别的几何一致性；(3)互排名融合机制，联合利用欧几里得和双曲空间的检索信号。

Result: 在多个QA基准测试上的广泛实验表明，HyperbolicRAG优于包括标准RAG和图增强基线在内的竞争方法。

Conclusion: 双曲几何能够有效增强图基RAG系统，同时捕捉细粒度语义和全局层次结构，为复杂知识表示提供了更好的几何基础。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.

</details>


### [52] [Concept than Document: Context Compression via AMR-based Conceptual Entropy](https://arxiv.org/abs/2511.18832)
*Kaize Shi,Xueyao Sun,Xiaohui Tao,Lin Li,Qika Lin,Guandong Xu*

Main category: cs.CL

TL;DR: 提出了一种基于抽象意义表示(AMR)图的无监督上下文压缩框架，通过计算节点级熵来保留语义核心信息，过滤冗余内容，在问答任务中实现更高准确率和更短上下文长度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文时面临信息过载问题，特别是在检索增强生成(RAG)中，大量支持文档常包含冗余内容，这不仅降低推理准确性，还增加计算开销。

Method: 构建AMR图，计算节点级概念熵来评估每个节点的概念重要性，筛选重要信息节点形成压缩的语义聚焦上下文。

Result: 在PopQA和EntityQuestions数据集上的实验表明，该方法优于原始方法和其他基线，在显著减少上下文长度的同时获得更高准确率。

Conclusion: 这是首个引入基于AMR的概念熵进行上下文压缩的工作，证明了稳定语言特征在上下文工程中的潜力。

Abstract: Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.

</details>


### [53] [A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis](https://arxiv.org/abs/2511.18843)
*Heger Arfaoui,Mohammed Iheb Hergli,Beya Benzina,Slimane BenMiled*

Main category: cs.CL

TL;DR: 提出了一个计算框架，使用BERTopic对焦点小组转录本进行神经主题建模，解决了超参数敏感性、模型稳定性和可解释性验证等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统焦点小组数据分析依赖劳动密集型手动编码，限制了可扩展性和可重复性。需要一种严谨、可重复的计算方法来分析定性数据。

Method: 使用BERTopic分析突尼斯HPV疫苗认知的10个焦点小组（1,076个话语），系统评估27种超参数配置，通过30次重复的bootstrap重采样评估稳定性，并由三位领域专家进行正式人类评估验证可解释性。

Result: 分析显示对超参数选择高度敏感，分层合并策略（先提取细粒度主题评估稳定性，再合并以提高可解释性）有效平衡稳定性与连贯性，人类验证显示主题质量良好（ICC=0.79，加权Cohen's kappa=0.578）。

Conclusion: 该框架为研究人员提供了实用指南，所有代码、数据处理脚本和评估协议都已公开，支持工作的复制和扩展。

Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.

</details>


### [54] [Large Language Models for the Summarization of Czech Documents: From History to the Present](https://arxiv.org/abs/2511.18848)
*Václav Tran,Jakub Šmíd,Ladislav Lenc,Jean-Pierre Salmon,Pavel Král*

Main category: cs.CL

TL;DR: 本文针对捷克语文本摘要任务，特别是历史文档摘要这一未充分探索的领域，利用大型语言模型（Mistral和mT5）和翻译方法，在SumeCzech数据集上取得最先进结果，并创建了新的历史捷克文本摘要数据集Posel od Čerchova。


<details>
  <summary>Details</summary>
Motivation: 捷克语摘要任务，特别是历史文档摘要，由于捷克语的语言复杂性和缺乏高质量标注数据集而研究不足。本文旨在填补这一空白。

Method: 使用多语言大型语言模型（Mistral和mT5）进行直接摘要，并提出翻译方法：先将捷克文本翻译成英语，用英语模型摘要，再翻译回捷克语。

Result: 在SumeCzech数据集上实现了最先进的结果，证明了多语言LLM对形态丰富的捷克语的有效性；创建了新的历史捷克文本摘要数据集Posel od Čerchova。

Conclusion: 通过结合先进模型与现代和历史捷克数据集，为捷克语摘要研究的进一步发展奠定了基础，为捷克历史文档处理和低资源摘要研究提供了宝贵资源。

Abstract: Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.
  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od Čerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.
  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.

</details>


### [55] [Cognitive Alpha Mining via LLM-Driven Code-Based Evolution](https://arxiv.org/abs/2511.18850)
*Fengyuan Liu,Huang Yi,Sichun Luo,Yuqi Wang,Yazheng Yang,Xinye Li,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 提出了CogAlpha框架，结合代码级alpha表示、LLM驱动推理和进化搜索，用于从高维金融数据中发现预测信号，相比现有方法能发现更准确、稳健和可泛化的alpha因子。


<details>
  <summary>Details</summary>
Motivation: 现有方法（深度学习、遗传编程、LLM因子生成）在金融alpha发现中搜索空间有限，神经网络模型产生不透明模式，符号方法产生冗余表达式，缺乏人类式的平衡逻辑一致性和创造性探索。

Method: CogAlpha框架将LLM作为自适应认知代理，通过多阶段提示和金融反馈迭代优化、变异和重组alpha候选，结合代码级alpha表示、LLM推理和进化搜索。

Result: 在A股股票上的实验表明，CogAlpha能持续发现比现有方法具有更优预测准确性、稳健性和泛化能力的alpha因子。

Conclusion: 将进化优化与基于LLM的推理相结合，有望实现自动化且可解释的alpha发现。

Abstract: Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.

</details>


### [56] [FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models](https://arxiv.org/abs/2511.18852)
*Masoomali Fatehkia,Enes Altinisik,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: FanarGuard是一个双语内容审核过滤器，同时评估安全性和文化对齐性，在阿拉伯语和英语中表现出色，超越了仅关注通用安全性的现有过滤器。


<details>
  <summary>Details</summary>
Motivation: 现有内容审核过滤器过于关注通用安全性而忽视文化背景，需要开发能同时评估安全性和文化对齐性的过滤器，特别是在阿拉伯文化背景下。

Method: 构建了超过468K个提示-响应对数据集，由LLM评委评估无害性和文化意识；训练了两个过滤器变体；开发了首个针对阿拉伯文化背景的基准测试。

Result: FanarGuard与人类注释的一致性超过了注释者间可靠性，同时在安全性基准测试中与最先进过滤器性能相当。

Conclusion: 将文化意识整合到内容审核中至关重要，FanarGuard是实现更具上下文敏感性的安全防护的实际步骤。

Abstract: Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.

</details>


### [57] [Generating Reading Comprehension Exercises with Large Language Models for Educational Applications](https://arxiv.org/abs/2511.18860)
*Xingyu Huang,Fei Jiang,Jianli Xiao*

Main category: cs.CL

TL;DR: 提出名为RCEG的LLM框架，用于自动生成高质量的个性化英语阅读理解练习，通过微调LLM生成候选内容，再使用判别器选择最佳候选，显著提升生成内容质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，在教育领域特别是自动文本生成方面展现出巨大潜力，能够创建智能和自适应的学习内容。

Method: RCEG框架首先使用微调的LLM生成内容候选，然后使用判别器选择最佳候选，最后大幅提升生成内容质量。

Result: 实验结果表明，RCEG显著提高了生成练习的相关性和认知适当性，在内容多样性、事实准确性、语言毒性和教学对齐等指标上表现良好。

Conclusion: RCEG框架能够自动生成高质量的个性化英语阅读理解练习，在教育领域具有重要应用价值。

Abstract: With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.

</details>


### [58] [Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models](https://arxiv.org/abs/2511.18864)
*Yang Xiang,Yixin Ji,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文首次对大型推理模型进行剪枝研究，发现直接应用现有剪枝技术效果不佳，提出使用自生成推理数据进行校准可显著提升剪枝性能，并开发了选择性自生成推理数据构建策略。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务上表现出色，但其长链推理过程带来高昂推理成本。剪枝是减少计算成本的有效方法，但现有研究主要关注大语言模型，对大型推理模型的剪枝尚未探索。

Method: 提出选择性自生成推理数据构建策略，使用自生成推理数据进行校准，特别关注推理数据的难度和长度对剪枝效果的影响。

Result: 在DeepSeek-R1-Distill模型系列上的实验表明，该策略相比通用剪枝方法将剪枝后LRMs的推理能力提升了10%-13%。

Conclusion: 挑战性且适度长度的自生成推理数据是理想的剪枝校准数据，所提出的SSGR策略能有效提升大型推理模型剪枝后的性能。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.

</details>


### [59] [CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation](https://arxiv.org/abs/2511.18889)
*Jingqian Zhao,Bingbing Wang,Geng Tu,Yice Zhang,Qianlong Wang,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: CoreEval是一种抗数据污染的评估策略，通过从GDELT数据库获取最新知识来更新数据集，解决LLM评估中的数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法完全消除模型中的预存知识或保持原始数据集的语义复杂性，导致无法确保抗污染评估。

Method: 提取原始数据中的实体关系，从GDELT数据库检索相关最新知识，重新语境化并整合到原始数据中，通过迭代验证机制确保标签一致性。

Result: 在更新数据集上的广泛实验验证了CoreEval的鲁棒性，有效减轻了数据污染导致的性能高估问题。

Conclusion: CoreEval提供了一种有效的抗数据污染评估解决方案，能够确保LLM评估的公平性。

Abstract: Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \textbf{CoreEval}, a \textbf{Co}ntamination-\textbf{re}silient \textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.

</details>


### [60] [Reproducibility Study of Large Language Model Bayesian Optimization](https://arxiv.org/abs/2511.18891)
*Adam Rychert,Gasper Spagnolo,Evgenii Posashkov*

Main category: cs.CL

TL;DR: 本研究复现了LLAMBO框架，使用Llama 3.1 70B替代GPT-3.5，验证了该框架在贝叶斯优化中的有效性。结果表明上下文预热启动显著改善早期性能，语言模型的跨任务语义先验有助于提升预测能力，但较小模型容量不足导致不稳定预测。


<details>
  <summary>Details</summary>
Motivation: 验证LLAMBO框架在不同语言模型骨干网络下的鲁棒性，特别是使用开源模型Llama 3.1 70B替代GPT-3.5时的表现。

Method: 在原始评估协议下复现核心实验，使用Llama 3.1 70B模型替换所有文本编码组件，进行Bayesmark和HPOBench实验，并对比不同规模模型的表现。

Result: LLAMBO架构在不同语言模型骨干下保持鲁棒性；上下文预热启动显著改善早期遗憾行为和运行方差；语言模型的跨任务语义先验提升预测性能；较小模型（Gemma 27B、Llama 3.1 8B）产生不稳定预测。

Conclusion: LLAMBO框架在更换语言模型骨干后仍保持有效性，证明了该方法的通用性和鲁棒性，但需要足够容量的语言模型来保证可靠的代理行为。

Abstract: In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.
  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.
  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.

</details>


### [61] [Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs](https://arxiv.org/abs/2511.18931)
*Sahil Kale*

Main category: cs.CL

TL;DR: 评估大语言模型是否需要以及如何有效使用网络搜索的基准测试，发现虽然网络搜索能提高事实准确性，但模型存在过度自信、检索时机不当和查询表述不佳等问题。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型集成了网络搜索功能以提供实时答案，但尚不清楚它们是否能在真正需要时有效使用搜索。需要评估模型在何时以及如何有效调用网络搜索。

Method: 构建包含静态和动态两个部分的基准数据集：静态部分包含783个截止日期前可回答的问题，测试模型是否基于低内部置信度调用搜索；动态部分包含288个截止日期后的问题，测试模型是否识别需要搜索并检索更新信息。

Result: 网络搜索显著提高了GPT-5-mini和Claude Haiku 4.5在静态问题上的准确性，但置信度校准变差。在动态问题上，两个模型频繁调用搜索但准确率仍低于70%，主要由于查询表述不佳。当初始检索失败时，收益递减。

Conclusion: 内置网络搜索能显著提高事实准确性且可选择性调用，但模型仍然过度自信、在需要时跳过检索，且在初始搜索查询表现不佳时表现较差。网络搜索更适合作为低延迟验证层而非可靠分析工具，仍有明显改进空间。

Abstract: Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.

</details>


### [62] [Skeletons Matter: Dynamic Data Augmentation for Text-to-Query](https://arxiv.org/abs/2511.18934)
*Yuchen Ji,Bo Xu,Jie Shi,Jiaqing Liang,Deqing Yang,Yu Mao,Hai Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了Text-to-Query任务范式，统一了不同查询语言的语义解析任务，通过识别查询骨架作为共享优化目标，并开发了一个动态数据增强框架来诊断模型弱点并合成针对性训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常专注于单一查询语言，导致方法在不同语言间的泛化能力有限。本文旨在统一各种查询语言的语义解析任务，提高模型的通用性。

Method: 提出Text-to-Query任务范式，识别查询骨架作为共享优化目标，开发动态数据增强框架来诊断模型在查询骨架处理上的弱点，并合成针对性训练数据。

Result: 在四个Text-to-Query基准测试中，仅使用少量合成数据就实现了最先进的性能，证明了方法的效率和通用性。

Conclusion: 该方法为Text-to-Query任务的统一研究奠定了坚实基础，代码已在GitHub上发布。

Abstract: The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.

</details>


### [63] [Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials](https://arxiv.org/abs/2511.18937)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 提出了一种基于图形化知识的方法来审查临床试验中的治疗相关不良事件，通过为MedDRA添加隐藏的医学知识层来改进不良事件分析。


<details>
  <summary>Details</summary>
Motivation: 改进临床试验中不良事件审查的清晰度、效率和准确性，通过增强MedDRA术语系统来更好地理解不良事件之间的语义关系。

Method: 为MedDRA添加隐藏医学知识层(Safeterm)，在2D地图中捕捉术语间的语义关系；自动将不良事件术语重新分组为相似性簇；使用收缩发生率比计算治疗特异性不成比例指标；通过精度加权聚合推导簇级EBGM值。

Result: 应用于三个历史试验，自动化方法清晰恢复了所有预期的安全信号；提供了两种可视化输出：显示不良事件发生率的语义地图和用于快速信号检测的期望度-不成比例度图。

Conclusion: 通过为MedDRA添加医学知识层，显著提高了临床试验中不良事件解释的清晰度、效率和准确性。

Abstract: We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.

</details>


### [64] [Logic of Montage](https://arxiv.org/abs/2511.19063)
*Hayami Takahashi,Kensuke Takahashi*

Main category: cs.CL

TL;DR: 提出了一种作为自然语言补充的情感表达形式，通过矛盾结构效应和蒙太奇操作来构建情感状态的结构化表达模型。


<details>
  <summary>Details</summary>
Motivation: 为情感表达提供一种独立于自然语言的替代形式，作为情感状态的代理或窗口，补充自然语言在情感表达方面的不足。

Method: 建立矛盾结构效应模型，采用蒙太奇操作重叠多个矛盾结构，引入德勒兹的强度概念作为模型要素，构建结构效应处理框架。

Result: 开发了一个通用的理论框架——系统间词汇导入，并通过教育升级的例子展示了结构效应的处理过程。

Conclusion: 提出的矛盾结构效应和蒙太奇操作能够有效地表达情感状态，为情感表达提供了一种结构化的替代方法。

Abstract: In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form "Effect of Contradictory Structure." "Effect of Contradictory Structure" is not static but dynamic. Effect in "Effect of Contradictory Structure" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, "Effect of Contradictory Structure" can be overlapped with each other. This overlapping operation is called "montage." A broader "Structure" that includes related "Effect of Contradictory Structure" and "Effect of Structure" are set up. Montage produces "Effect of Structure". In montage, it is necessary to set something like "strength," so we adopted Deleuze and Deleuze/Guattari's word "intensity" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of "intensity" through Austin's use of the word "force." "Effect of Structure" process is demonstrated using the example of proceeding to the next level of education.

</details>


### [65] [GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning](https://arxiv.org/abs/2511.19078)
*Yutong Li,Yitian Zhou,Xudong Wang,GuoChen,Caiyan Qin*

Main category: cs.CL

TL;DR: GraphMind是一个基于动态图的框架，将图神经网络与LLMs结合，用于多步推理中的定理选择和中间结论生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏显式和动态机制来结构化和演化中间推理状态，限制了上下文感知的定理选择和迭代结论生成能力。

Method: 将推理过程建模为异构演化图，节点表示条件、定理和结论，边捕获逻辑依赖关系，使用GNN编码当前推理状态并通过语义匹配进行定理选择。

Result: 在多个问答数据集上的实验表明，GraphMind方法实现了持续的性能提升，在多步推理中显著优于现有基线方法。

Conclusion: GraphMind框架通过动态图表示和GNN编码，实现了上下文感知、可解释和结构化的推理，验证了该方法的有效性和泛化能力。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.

</details>


### [66] [A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis](https://arxiv.org/abs/2511.19083)
*Wenxuan Mu,Jinzhong Ning,Di Zhao,Yijia Zhang*

Main category: cs.CL

TL;DR: KDR-Agent是一个多智能体框架，通过知识检索、消歧和反思分析来解决低资源命名实体识别问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于上下文学习的NER方法在低资源场景下的三个关键限制：依赖动态检索标注数据、对未见领域泛化能力有限、无法整合外部知识或解决实体歧义。

Method: 提出KDR-Agent多智能体框架，使用自然语言类型定义和静态实体级对比演示，通过中央规划器协调专门智能体进行维基百科知识检索、上下文消歧推理和结构化自评估预测修正。

Result: 在5个领域的10个数据集上实验表明，KDR-Agent在多个LLM骨干网络上显著优于现有的零样本和少样本ICL基线方法。

Conclusion: KDR-Agent通过整合知识检索、消歧和反思分析，有效解决了低资源NER中的关键挑战，为多领域低资源上下文学习NER提供了有效解决方案。

Abstract: In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.

</details>


### [67] [DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF](https://arxiv.org/abs/2511.19097)
*Ziyuan Gao,Di Liang,Xianjie Wu,Philippe Morel,Minlong Peng*

Main category: cs.CL

TL;DR: DeCoRL是一个新颖的强化学习框架，通过将推理过程从串行处理转变为协作式模块化编排，解决了现有CoT推理方法的两个关键限制：不透明的奖励信号和O(n)的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在链式思维推理中存在两个关键限制：作为黑盒提供无差别的奖励信号，掩盖了各个步骤的贡献并阻碍错误诊断；串行解码具有O(n)时间复杂度，使得复杂推理任务的实时部署不切实际。

Method: DeCoRL训练轻量级专用模型来并行生成推理子步骤，通过并行处理消除串行瓶颈。框架设计模块化奖励函数独立评分每个子步骤，然后通过级联DRPO优化协调这些奖励同时保持步骤间依赖关系。

Result: 在RM-Bench、RMB和RewardBench上的综合评估显示，DeCoRL实现了最先进的结果，优于包括大规模模型在内的现有方法。推理速度提高3.8倍，同时保持卓越的解决方案质量，通过显式奖励归因提高22.7%的可解释性。

Conclusion: DeCoRL结合72.4%的能耗降低和68%的吞吐量提升，使复杂推理系统的实时部署成为现实。

Abstract: Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\% reduction in energy consumption and a 68\% increase in throughput, make real-time deployment of complex reasoning systems a reality.

</details>


### [68] [A symbolic Perl algorithm for the unification of Nahuatl word spellings](https://arxiv.org/abs/2511.19118)
*Juan-José Guzmán-Landa,Jesús Vázquez-Osorio,Juan-Manuel Torres-Moreno,Ligia Quintana Torres,Miguel Figueroa-Saavedra,Martha-Lorena Avendaño-Garrido,Graham Ranger,Patricia Velázquez-Morales,Gerardo Eugenio Sierra Martínez*

Main category: cs.CL

TL;DR: 提出了一种基于符号模型的那瓦特语文本自动正字法统一方法，使用符号正则表达式实现语言规则，并通过人工评估协议验证统一句子的质量。


<details>
  <summary>Details</summary>
Motivation: 解决那瓦特语文本在不同正字法体系下的统一问题，便于语言处理和分析。

Method: 基于先前分析那瓦特语句子的算法和π-yalli语料库，使用符号正则表达式实现自动统一算法，并设计了人工评估协议来测试句子语义任务。

Result: 评估者对大多数人工统一句子的期望特征给出了令人鼓舞的结果。

Conclusion: 该方法在那瓦特语文本自动正字法统一方面取得了积极进展，为多正字法语言处理提供了有效解决方案。

Abstract: In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences

</details>


### [69] [On the Optimality of Discrete Object Naming: a Kinship Case Study](https://arxiv.org/abs/2511.19120)
*Phong Le,Mees Lindeman,Raquel G. Alhama*

Main category: cs.CL

TL;DR: 该研究提出了一个信息论框架来分析自然语言命名系统，证明了当听者的解码器等同于说话者的贝叶斯解码器时，才能实现信息丰富性和复杂性的最优权衡。


<details>
  <summary>Details</summary>
Motivation: 解决先前研究的两个局限性：(i) 假设最优听者，(ii) 假设跨语言的普遍交际需求，旨在更真实地建模命名系统的信息-复杂性权衡。

Method: 采用从涌现通信中借鉴的指称游戏设置，聚焦亲属关系的语义领域，构建信息论框架来分析离散对象命名系统。

Result: 研究表明，最优权衡不仅在理论上可实现，而且在学习的通信系统中也会经验性地涌现。

Conclusion: 当听者的解码策略与说话者的贝叶斯解码器一致时，命名系统能够在信息丰富性和复杂性之间达到最优平衡，这一理论预测在实际学习系统中得到了验证。

Abstract: The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.

</details>


### [70] [Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2511.19122)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 提出了一种情感增强的多任务ACSA框架，联合学习情感极性和基于Ekman六种基本情感的分类特定情感，通过VAD维度框架进行情感精炼，显著提升了ACSA性能。


<details>
  <summary>Details</summary>
Motivation: 现有ACSA方法主要关注情感极性，忽略了塑造情感表达的情感维度，这限制了模型捕捉特定方面类别的细粒度情感信号的能力。

Method: 利用LLMs生成能力，为每个方面类别生成情感描述，并基于VAD维度框架引入情感精炼机制，将预测情感投影到VAD空间，对不一致的情感进行重新标注。

Result: 实验结果表明，该方法在所有基准数据集上都显著优于强基线模型。

Conclusion: 将情感维度整合到ACSA中是有效的，情感增强方法能够丰富情感表示并提升模型性能。

Abstract: Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.

</details>


### [71] [Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization](https://arxiv.org/abs/2511.19131)
*Zijian Wang,Yanxiang Ma,Chang Xu*

Main category: cs.CL

TL;DR: 提出了一种基于概率条件生成的新方法，通过优化隐藏状态来激发基础大语言模型的思维链推理能力，在保持语言连贯性的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 基础大语言模型在预训练后往往缺乏专门的推理训练，现有隐藏状态操纵方法存在刚性约束问题，容易导致分布偏移和文本质量下降。

Method: 将挑战重新表述为优化问题，采用平衡似然和先验正则化的框架，引导隐藏状态朝向推理导向的轨迹发展。

Result: 在数学、常识和逻辑推理基准测试中，该方法始终优于现有的引导方法。

Conclusion: 该方法为增强基础大语言模型的推理能力提供了一个理论上有原则且有效的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.

</details>


### [72] [Representational Stability of Truth in Large Language Models](https://arxiv.org/abs/2511.19166)
*Samantha Dies,Courtney Maynard,Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: 该研究提出了表征稳定性的概念，用于评估LLM在真理定义变化时内部真实性表征的鲁棒性，发现LLM对虚构内容的表征比陌生内容更稳定。


<details>
  <summary>Details</summary>
Motivation: 评估LLM如何在其内部概率表征中稳定地区分真实、虚假以及既不真也不假的内容，特别是在真理定义发生变化时的鲁棒性。

Method: 通过在LLM激活上训练线性探针来分离真实与非真实陈述，并在受控标签变化下测量学习决策边界的变化，比较了16个开源模型和3个事实领域。

Result: 陌生陈述（关于训练数据中不存在的实体）导致最大的边界偏移，在脆弱领域（如词汇定义）中产生高达40%的真理判断翻转，而熟悉的虚构陈述保持更一致的聚类且变化较小（≤8.2%）。

Conclusion: 表征稳定性更多源于认知熟悉度而非语言形式，该方法为审计和训练LLM提供了诊断工具，以在语义不确定性下保持一致的真理分配。

Abstract: Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\leq 8.2\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.

</details>


### [73] [In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations](https://arxiv.org/abs/2511.19232)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本文研究了transformer模型如何检测语义异常，发现模型在中间层开始能够区分合理与不合理句子结尾，且异常检测过程经历了从探索到快速整合的转变。


<details>
  <summary>Details</summary>
Motivation: 探索transformer模型在何处以及如何检测语义异常，特别是句子结尾的合理性，并与人类心理语言学中的语义异常检测过程进行对比。

Method: 使用因果语言模型(phi-2)评估精心策划的语料库，分析各层的隐藏状态，采用线性探测器和有效维度分析两种互补方法研究异常编码方式。

Result: 线性探测器在模型底层三分之一难以区分合理与不合理结尾，但在中间层准确率急剧上升，在顶层前达到峰值；异常编码先扩大表示子空间，随后在中间瓶颈后收缩，表明从探索到快速整合的转变。

Conclusion: transformer模型的语义异常检测过程与人类阅读中的心理语言学发现一致，语义异常在句法解析后才被检测，发生在在线处理序列的后期。

Abstract: How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.

</details>


### [74] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: 开发了包含5.4万篇孟加拉语文章和摘要的新数据集，涵盖多领域内容，为孟加拉语自然语言处理提供了重要基准资源。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注新闻文章，无法适应现实世界中多样化的孟加拉语文本。数字时代产生了大量孟加拉语内容，需要能够减轻信息过载的摘要系统。

Method: 从多个来源收集了超过5.4万篇孟加拉语文章和摘要，包括博客和报纸。使用LSTM、BanglaT5-small和MTS-small等深度学习模型进行训练和评估。

Result: 建立了强大的基准性能，展示了该数据集作为未来孟加拉语自然语言处理研究基准的潜力。

Conclusion: 该数据集为构建鲁棒的摘要系统提供了坚实基础，有助于扩展低资源语言的NLP资源。

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


### [75] [Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces](https://arxiv.org/abs/2511.19333)
*Shaltiel Shmidman,Asher Fredman,Oleg Sudakov,Meriem Bendris*

Main category: cs.CL

TL;DR: 比较DeepSeek-R1和gpt-oss两种大型语言模型生成的推理轨迹对中等规模LLM数学问题解决能力的影响，评估准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 利用前沿大模型生成的推理轨迹作为高质量监督数据，训练中小型语言模型获得推理能力，避免昂贵的人工标注成本。

Method: 对中等规模LLM进行后训练，使用DeepSeek-R1和gpt-oss生成的两种推理轨迹数据，比较它们在数学问题上的表现。

Result: 评估两种推理轨迹在准确性和推理效率方面的差异。

Conclusion: 比较分析不同大模型生成的推理轨迹对中小模型推理能力训练的效果差异。

Abstract: Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.

</details>


### [76] [DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research](https://arxiv.org/abs/2511.19399)
*Rulin Shao,Akari Asai,Shannon Zejiang Shen,Hamish Ivison,Varsha Kishore,Jingming Zhuo,Xinran Zhao,Molly Park,Samuel G. Finlayson,David Sontag,Tyler Murray,Sewon Min,Pradeep Dasigi,Luca Soldaini,Faeze Brahman,Wen-tau Yih,Tongshuang Wu,Luke Zettlemoyer,Yoon Kim,Hannaneh Hajishirzi,Pang Wei Koh*

Main category: cs.CL

TL;DR: 提出了RLER（强化学习与演进式评分标准）方法，开发了首个专门针对开放式长格式深度研究的开源模型DR Tulu-8B，在多个领域基准测试中表现优于现有模型，并匹配或超越专有系统。


<details>
  <summary>Details</summary>
Motivation: 现有开源深度研究模型主要在可验证的短格式QA任务上训练，无法扩展到现实的长格式任务，需要新的训练方法。

Method: 使用RLER方法构建和维护与策略模型共同演进的评分标准，使评分标准能够整合模型新探索的信息并提供区分性的在线反馈。

Result: DR Tulu-8B在科学、医疗和通用领域的四个长格式深度研究基准测试中大幅优于现有开源模型，匹配或超越专有系统，同时模型更小、查询成本更低。

Conclusion: RLER方法成功解决了长格式深度研究的训练挑战，开发了高性能的开源深度研究模型，并发布了完整的数据、模型和代码资源。

Abstract: Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.

</details>


### [77] [Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration](https://arxiv.org/abs/2511.19417)
*James Y. Huang,Sheng Zhang,Qianchu Liu,Guanghui Qin,Tinghui Zhu,Tristan Naumann,Muhao Chen,Hoifung Poon*

Main category: cs.CL

TL;DR: BeMyEyes是一个模块化多智能体框架，通过将高效的可视语言模型作为感知器与强大的语言模型作为推理器进行对话协作，扩展LLMs的多模态推理能力，无需训练大规模多模态模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要昂贵的大规模视觉语言模型开发，而轻量级VLMs缺乏前沿LLMs的广泛知识和推理能力。

Method: 提出模块化多智能体框架，通过数据合成和监督微调训练感知器代理与推理器代理有效协作。

Result: 实验表明该框架解锁了LLMs的多模态推理能力，轻量级开源方案在知识密集型多模态任务上超越GPT-4o等大型专有VLMs。

Conclusion: 该多智能体方法展示了构建未来多模态推理系统的有效性、模块化和可扩展性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [78] [Narratives to Numbers: Large Language Models and Economic Policy Uncertainty](https://arxiv.org/abs/2511.17866)
*Ethan Hartley*

Main category: econ.GN

TL;DR: 本研究评估了大型语言模型作为可估计分类器的性能，展示了建模选择如何影响下游测量误差。通过重新审视经济政策不确定性指数，发现现代分类器显著优于词典规则，能更好地追踪人工审计评估，并自然地扩展到嘈杂的历史和多语言新闻数据。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在文本测量中的潜力，改进传统基于词典规则的方法，提高经济政策不确定性等文本衍生指标的准确性和适用范围。

Method: 使用大型语言模型作为分类器，重新构建经济政策不确定性指数，并将其应用于历史报纸数据和多语言新闻数据。

Result: 当代分类器显著优于传统词典规则，能更好地追踪人工审计评估，成功构建了基于3.6亿多篇报纸文章的19世纪美国指数和跨国家指数。

Conclusion: 大型语言模型可以系统性地改进文本衍生测量，应该作为明确的测量工具整合到实证经济学中。

Abstract: This study evaluates large language models as estimable classifiers and clarifies how modeling choices shape downstream measurement error. Revisiting the Economic Policy Uncertainty index, we show that contemporary classifiers substantially outperform dictionary rules, better track human audit assessments, and extend naturally to noisy historical and multilingual news. We use these tools to construct a new nineteenth-century U.S. index from more than 360 million newspaper articles and exploratory cross-country indices with a single multilingual model. Taken together, our results show that LLMs can systematically improve text-derived measures and should be integrated as explicit measurement tools in empirical economics.

</details>


### [79] [Unlocking The Future of Food Security Through Access to Finance for Sustainable Agribusiness Performance](https://arxiv.org/abs/2511.18576)
*Ayobami Paul Abolade,Ibrahim Olanrewaju Lawal,Kamoru Lanre Akanbi,Ahmed Orilonise Salami*

Main category: econ.GN

TL;DR: 本研究探讨了尼日利亚奥贡州小农户获得融资与粮食安全之间的关系，发现两者存在显著正相关关系。


<details>
  <summary>Details</summary>
Motivation: 尽管有针对农业生产的金融干预措施，但小农户仍然缺乏合理、及时和充足的融资渠道，这限制了他们对改进技术和投入的投资能力，降低了生产力和粮食供应。

Method: 采用定量研究方法，使用问卷调查设计，对37,200名农业小农户进行抽样调查，最终选择380个样本，使用偏最小二乘结构方程模型(PLS-SEM)分析数据。

Result: 研究发现获得融资与粮食安全之间存在正向关系，R²值为0.615，表明两者之间存在强相关性。

Conclusion: 研究强调了改善金融机构和实施支持性政策的重要性，以使农民能够获得所需的金融资源，从而实现粮食安全目标。

Abstract: Access to finance is vital for improving food security, particularly in developing nations where agricultural production is crucial. Despite several financial interventions targeted at increasing agricultural production, smallholder farmers continue to lack access to reasonable, timely, and sufficient financing, limiting their ability to invest in improved technology and inputs, lowering productivity and food supply. This study examines the relationship between access to finance and food security among smallholder farmers in Ogun State, employing institutional theory as a theoretical framework. The study takes a quantitative method, with a survey for the research design and a population of 37,200 agricultural smallholder farmers. A sample size of 380 was chosen using probability sampling and simple random techniques. The data were analysed via Partial Least Squares Structural Equation Modelling (PLS-SEM). The findings demonstrate a favourable relationship between access to finance and food security, with an R2-value of 0.615 indicating a robust link. These findings underline the need of improving financial institutions and implementing enabling policies to enable farmers have access to the financial resources they need to achieve food security outcomes.

</details>


### [80] [Barriers to AI Adoption: Image Concerns at Work](https://arxiv.org/abs/2511.18582)
*David Almog*

Main category: econ.GN

TL;DR: 工人担心过度依赖AI会被视为缺乏自信，导致在AI辅助任务中降低AI使用率，即使知道评估者关注的是任务准确性而非AI使用情况。


<details>
  <summary>Details</summary>
Motivation: 研究工人对AI依赖的可见性如何影响其与AI协作的意愿，特别是在评估者能看到AI使用情况时，工人担心被负面评价。

Method: 在大型在线劳动力市场进行实地实验，雇佣450名美国远程工作者完成图像分类任务，使用AI推荐辅助，通过HR评估员的反馈激励工人。

Result: 当AI依赖对评估者可见时，工人采纳AI推荐的比例显著下降，导致任务性能降低，即使评估者明确关注任务准确性。

Conclusion: 工人对AI依赖可见性的担忧难以消除，他们担心过度依赖AI会被视为缺乏自信，这种担忧会阻碍有效的AI协作。

Abstract: Concerns about how workers are perceived can deter effective collaboration with artificial intelligence (AI). In a field experiment on a large online labor market, I hired 450 U.S.-based remote workers to complete an image-categorization job assisted by AI recommendations. Workers were incentivized by the prospect of a contract extension based on an HR evaluator's feedback. I find that workers adopt AI recommendations at lower rates when their reliance on AI is visible to the evaluator, resulting in a measurable decline in task performance. The effects are present despite a conservative design in which workers know that the evaluator is explicitly instructed to assess expected accuracy on the same AI-assisted task. This reduction in AI reliance persists even when the evaluator is reassured about workers' strong performance history on the platform, underscoring how difficult these concerns are to alleviate. Leveraging the platform's public feedback feature, I introduce a novel incentive-compatible elicitation method showing that workers fear heavy reliance on AI signals a lack of confidence in their own judgment, a trait they view as essential when collaborating with AI.

</details>


### [81] [Clarifying Trinko as Precedent in EHR and AI-Memory Duty to Deal Cases: A New Institutional Economics Approach](https://arxiv.org/abs/2511.18664)
*Lawrence W. Abrams*

Main category: econ.GN

TL;DR: 该论文旨在澄清Verizon v. Trinko案的判决依据，以减少两种错误：错误引用该案作为先例（Trinko Creep）和未在应引用时引用该案。


<details>
  <summary>Details</summary>
Motivation: 减少在引用Trinko案时出现的两种错误，特别是随着涉及电子健康记录和Agentic AI长期记忆等敏感消费者数据监管访问权案件的增长，正确引用该先例的重要性日益凸显。

Method: 通过澄清Trinko案的判决依据，分析其适用范围，区分应引用和不应引用该案的情形。

Result: 论文指出Trinko Creep（错误引用）现象普遍存在，同时预测未在应引用时引用该案的错误将在未来增加。

Conclusion: Trinko案应成为涉及敏感消费者数据监管访问权案件的先例，特别是在电子健康记录和Agentic AI长期记忆领域。

Abstract: By clarifying the bases for the Verizon Communications Inc. v. Law Offices of Curtis V. Trinko, LLP, 2004 opinion, we hope to reduce two distinct errors. The false positive error is citing Trinko as precedent when it is not. This error is so prevalent it has earned the nickname of Trinko Creep. The false negative error is not citing Trinko when it should be. We argue that this error will be growing in the future as Trinko should be precedent in cases involving regulated access rights to sensitive consumer data in electronic health records and Agentic AI Long Term Memory.

</details>


### [82] [Trust and Uncertainty in Strategic Interaction: Behavioural and Physiological Evidence from the Centipede Game](https://arxiv.org/abs/2511.18738)
*Dhiraj Jagadale,Kavita Vemuri*

Main category: econ.GN

TL;DR: 本研究通过皮肤电导反应(SCR)测量情绪唤醒，探讨其在改良版蜈蚣博弈中与信任行为的关系，发现不确定性会增加情绪唤醒并影响信任决策。


<details>
  <summary>Details</summary>
Motivation: 经济互动中相互信任是决策的关键因素，但实际行为常偏离均衡预测，需要研究情绪唤醒如何影响信任行为。

Method: 使用改良版蜈蚣博弈，包含固定和随机终止条件，记录皮肤电导反应(SCR)，同时收集自我报告的相互信任、一般信任和个体风险承担倾向数据。

Result: 随机终止条件下的阶段性SCR显著更高，特别是在对手采取行动后；相互信任得分与风险倾向正相关；高相互信任与延长合作游戏相关，但仅限于固定回合条件。

Conclusion: 生理唤醒反映了信任相关决策中的情绪参与，不确定性会放大唤醒和战略谨慎；相互信任具有情境依赖性，受情绪和生理状态影响而偏离均衡行为。

Abstract: Mutual trust is a key determinant of decision-making in economic interactions, yet actual behavior often diverges from equilibrium predictions. This study investigates how emotional arousal, indexed by skin conductance responses,SCR, relates to trust behavior in a modified centipede game. To examine the impact of uncertainty, the game incorporated both fixed and random termination conditions. SCRs were recorded alongside self-reported measures of mutual and general trust and individual risk-taking propensity. Phasic SCRs were significantly higher under random termination, particularly following the opponent take actions, indicating increased emotional arousal under uncertainty. Mutual trust scores correlated positively with risk propensity but not with general trust. Behaviorally, higher mutual trust was associated with extended cooperative play, but only in the fixed-turn condition. These findings suggest that physiological arousal reflects emotional engagement in trust-related decisions and that uncertainty amplifies both arousal and strategic caution. Mutual trust appears context-dependent, shaped by emotional and physiological states that influence deviations from equilibrium behavior.

</details>


### [83] [Revisiting the Measurement of Polarization](https://arxiv.org/abs/2511.18944)
*Juan A. Crespo,Armajac Raventós-Pujol*

Main category: econ.GN

TL;DR: 该论文重新审视了Esteban和Ray(1994)的极化模型，通过放宽个体无限可分的假设，得到了更广泛的极化指数族，避免了原模型的反直觉排序问题。


<details>
  <summary>Details</summary>
Motivation: 原极化模型依赖个体无限可分的假设，这限制了可接受的极化指数范围，作者希望放宽这一假设以获得更灵活的极化度量方法。

Method: 通过放松个体无限可分的假设，重新推导极化指数的数学框架，构建更广泛的极化指数族。

Result: 得到了比原模型更广泛的极化指数族，这些指数与原公理一致，避免了原模型中的反直觉排序问题，为实证应用提供了更大灵活性。

Conclusion: 放宽个体无限可分的假设是可行的，这扩展了极化度量的理论框架，使极化指数更具实用性和合理性。

Abstract: We revisit Esteban and Ray's (1994) seminal model of polarization. Their main result (unnecessarily) relies on the assumption that individuals are infinitely divisible, which imposes strong restrictions on admissible polarization indices. We show that relaxing this assumption yields a broader family of indices consistent with the original axioms. The resulting indices avoid counter-intuitive rankings that arise when using results on the original paper and provide greater flexibility for empirical applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [84] [Limit Theorems for Network Data without Metric Structure](https://arxiv.org/abs/2511.17928)
*Wen Jiang,Yachen Wang,Zeqi Wu,Xingbai Xu*

Main category: econ.EM

TL;DR: 本文为具有网络依赖性的随机变量发展了极限定理，无需假设网络个体位于欧几里得或度量空间中，扩展了传统基于弱依赖概念的极限定理的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现有网络计量经济学中的极限定理大多基于强混合、近邻依赖等弱依赖概念，要求个体位于度量空间中。本文旨在放宽这一限制，使定理能应用于更广泛的网络数据，包括金融和社交网络。

Method: 将时间序列中的函数依赖（物理依赖）概念推广到具有网络依赖性的随机变量，基于此框架建立不等式、大数定律和中心极限定理。

Result: 建立了适用于网络依赖随机变量的几个不等式、大数定律和中心极限定理，并在空间自回归模型中验证了这些极限定理的条件。

Conclusion: 通过放松度量空间假设，本文的极限定理能应用于更广泛的网络数据，为网络数据分析提供了更通用的理论工具。

Abstract: This paper develops limit theorems for random variables with network dependence, without requiring that individuals in the network to be located in a Euclidean or metric space. This distinguishes our approach from most existing limit theorems in network econometrics, which are based on weak dependence concepts such as strong mixing, near-epoch dependence, and $ψ$-dependence. By relaxing the assumption of an underlying metric space, our theorems can be applied to a broader range of network data, including financial and social networks. To derive the limit theorems, we generalize the concept of functional dependence (also known as physical dependence) from time series to random variables with network dependence. Using this framework, we establish several inequalities, a law of large numbers, and central limit theorems. Furthermore, we verify the conditions for these limit theorems based on primitive assumptions for spatial autoregressive models, which are widely used in network data analysis.

</details>


### [85] [Robust Inference Methods for Latent Group Panel Models under Possible Group Non-Separation](https://arxiv.org/abs/2511.18550)
*Oguzhan Akgun,Ryo Okui*

Main category: econ.EM

TL;DR: 提出了线性面板数据模型中具有潜在组结构系数的稳健推断方法，采用选择性条件推断方法，在组分离可能不成立的情况下提供有效推断。


<details>
  <summary>Details</summary>
Motivation: 传统方法在组分离假设可能不成立时推断无效，且无法充分处理组结构估计中的统计不确定性。

Method: 使用选择性条件推断方法，推导给定数据估计的组结构下系数估计的条件分布。

Result: 即使在组分离不成立的情况下也能提供有效推断，在组分离成立时比传统渐近方法具有更好的有限样本性质。

Conclusion: 该方法能够处理组结构估计的不确定性，在多种实际应用场景中表现出优越性能。

Abstract: This paper presents robust inference methods for general linear hypotheses in linear panel data models with latent group structure in the coefficients. We employ a selective conditional inference approach, deriving the conditional distribution of coefficient estimates given the group structure estimated from the data. Our procedure provides valid inference under possible violations of group separation, where distributional properties of group-specific coefficients remain unestablished. Furthermore, even when group separation does hold, our method demonstrates superior finite-sample properties compared to traditional asymptotic approaches. This improvement stems from our procedure's ability to account for statistical uncertainty in the estimation of group structure. We demonstrate the effectiveness of our approach through Monte Carlo simulations and apply the methods to two datasets on: (i) the relationship between income and democracy, and (ii) the cyclicality of firm-level R&D investment.

</details>


### [86] [ReLU-Based and DNN-Based Generalized Maximum Score Estimators](https://arxiv.org/abs/2511.19121)
*Xiaohong Chen,Wayne Yuan Gao,Likang Wen*

Main category: econ.EM

TL;DR: 提出了一种基于ReLU函数的最大得分估计器新公式，替代了Manski(1975,1985)中的指示函数，使优化更容易，并能推广到多指标单交叉条件框架。


<details>
  <summary>Details</summary>
Motivation: 传统最大得分估计器使用指示函数导致优化困难，ReLU函数的Lipschitz性质使得基于梯度优化的标准方法更易实现。

Method: 使用ReLU函数组合编码符号对齐限制，构建ReLU最大得分(RMS)估计器，并进一步将其重新表述为深度神经网络中的特殊层。

Result: 在s阶Holder光滑条件下，RMS估计器达到n^{-s/(2s+1)}收敛速度和渐近正态性。

Conclusion: ReLU最大得分估计器不仅优化更容易，还能扩展到更一般的多指标单交叉条件框架，且可通过现代DNN软硬件实现。

Abstract: We propose a new formulation of the maximum score estimator that uses compositions of rectified linear unit (ReLU) functions, instead of indicator functions as in Manski (1975,1985), to encode the sign alignment restrictions. Since the ReLU function is Lipschitz, our new ReLU-based maximum score criterion function is substantially easier to optimize using standard gradient-based optimization pacakges. We also show that our ReLU-based maximum score (RMS) estimator can be generalized to an umbrella framework defined by multi-index single-crossing (MISC) conditions, while the original maximum score estimator cannot be applied. We establish the $n^{-s/(2s+1)}$ convergence rate and asymptotic normality for the RMS estimator under order-$s$ Holder smoothness. In addition, we propose an alternative estimator using a further reformulation of RMS as a special layer in a deep neural network (DNN) architecture, which allows the estimation procedure to be implemented via state-of-the-art software and hardware for DNN.

</details>


### [87] [Identification, estimation and inference in Panel Vector Autoregressions using external instruments](https://arxiv.org/abs/2511.19372)
*Raimondo Pala*

Main category: econ.EM

TL;DR: 该论文提出了一种基于SVAR-IV文献的外部工具变量识别方法，用于识别面板向量自回归模型，并讨论了相关的识别、估计和推断问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决面板向量自回归模型中的识别问题，特别是在使用连续工具变量识别二元处理时，需要明确估计参数的经济含义。

Method: 引入μ-LATE（局部平均处理效应）概念，在独立性、排他性和单调性假设下，证明外部工具变量PVAR估计的是μ-LATE。使用Anderson-Rubin统计量构建置信区间，并通过蒙特卡洛模拟验证方法可靠性。

Result: 蒙特卡洛模拟显示基于Anderson-Rubin统计量的置信集对脉冲响应具有可靠的收敛性。应用研究中，使用州级军事支出占全国支出的份额作为工具变量，估计动态财政乘数，发现乘数大于1，效应集中在当年并持续到下一年。

Conclusion: 外部工具变量方法能够有效识别PVAR模型，μ-LATE为连续工具变量识别二元处理提供了理论框架，Anderson-Rubin统计量在推断中表现可靠，应用研究证实了方法的实用性。

Abstract: This paper proposes an identification inspired from the SVAR-IV literature that uses external instruments to identify PVARs, and discusses associated issues of identification, estimation, and inference.
  I introduce a form of local average treatment effect - the $μ$-LATE - which arises when a continuous instrument targets a binary treatment. Under standard assumptions of independence, exclusion, and monotonicity, I show that externally instrumented PVARs estimate the $μ$-LATE. Monte Carlo simulations illustrate that confidence sets based on the Anderson-Rubin statistics deliver reliable convergence for impulse responses.
  As an application, I instrument state-level military spending with the state's share of national spending to estimate the dynamic fiscal multiplier. I find multipliers above unity, with effects concentrated in the contemporaneous year and persisting into the following year.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [88] [Exploration Is Not What It Seeks: Catalytic Exploration under Status Quo Uncertainty](https://arxiv.org/abs/2511.17981)
*Zeyu He*

Main category: econ.TH

TL;DR: 论文提出'催化探索'概念，即理性代理人会探索预期会拒绝的选项，以解决对现状的不确定性。通过将期权价值分解为转换和催化成分，展示了高探索率与有限转换概率可以共存。


<details>
  <summary>Details</summary>
Motivation: 识别一种独特的搜索动机——催化探索，解释为何理性代理人会探索他们预期会拒绝的选项，目的是解决对现状的不确定性。

Method: 将期权价值分解为转换成分和催化成分，分析催化探索在信号博弈、信息获取和外部性方面的作用。

Result: 1. 强催化动机会导致信号博弈中的分离均衡崩溃；2. 代理人会最优地获取比替代选项更精确的现状信息；3. 催化探索产生负外部性，信息技术改进可能通过鼓励过度基准测试而降低福利。

Conclusion: 催化探索机制提供了三个重要洞见：影响信号博弈均衡、逆转理性疏忽直觉、以及创造负外部性，表明信息技术的改进可能产生意想不到的负面后果。

Abstract: We identify a distinct motive for search, termed catalytic exploration, where agents rationally explore alternatives they expect to reject to resolve uncertainty about the status quo. By decomposing option value into switching and catalytic components, we show that high exploration rates can coexist with bounded switching probabilities. This mechanism generates three insights. First, strong catalytic motives cause separating equilibria to collapse in signaling games as receivers explore indiscriminately. Second, agents optimally acquire more precise information about the status quo than about alternatives, reversing rational inattention intuitions. Third, catalytic exploration creates negative externalities: information technology improvements can paradoxically reduce welfare by encouraging excessive benchmarking.

</details>


### [89] [Prior-Free Information Design](https://arxiv.org/abs/2511.18647)
*Maxwell Rosenthal*

Main category: econ.TH

TL;DR: 提出了一种基于部分识别的先验无关信息设计框架，并将其应用于稳健因果推断。决策者观察信息结构生成的信号分布，并根据与这些信号一致的状态分布的最差情况收益来对备选方案进行排序。


<details>
  <summary>Details</summary>
Motivation: 传统信息设计通常依赖于先验分布，本文旨在开发一个不依赖先验的信息设计框架，使其能够应用于稳健因果推断场景。

Method: 基于部分识别方法，决策者观察信号分布，通过最差情况收益来评估备选方案。证明了每个可稳健实施的动作都可以通过一个最多只保留一维信息的结构来实现。

Result: 在潜在结果模型中，每个处理都可以通过几乎完全信息化的实验来实施。

Conclusion: 该框架为信息设计提供了一种先验无关的稳健方法，特别适用于因果推断场景，且实施复杂度较低。

Abstract: This paper introduces a prior-free framework for information design based on partial identification and applies it to robust causal inference. The decision maker observes the distribution of signals generated by an information structure and ranks alternatives by their worst-case payoff over the state distributions consistent with those signals. We characterize the set of robustly implementable actions and show that each can be implemented by an information structure that withholds at most one dimension of information from the decision maker. In the potential outcomes model, every treatment is implementable via an experiment that is almost fully informative.

</details>


### [90] [Random Collection](https://arxiv.org/abs/2511.18476)
*Tri Phu Vu*

Main category: econ.TH

TL;DR: 本文研究决策者可以选择多个备选方案的选择情境，通过公理化方法表征文献中的各种参数模型，揭示函数形式假设的含义和模型间的区别。


<details>
  <summary>Details</summary>
Motivation: 研究决策者在菜单中选择多个备选方案的概率选择行为，旨在理解多选择情境下的决策过程。

Method: 采用公理化方法，通过行为假设来表征各种参数模型，提供测试和证伪决策者所用选择程序的简单工具。

Result: 阐明了函数形式假设的含义，揭示了看似无关模型之间的密切联系，为理解多选择决策提供了理论框架。

Conclusion: 公理化的行为假设为测试和证伪选择程序提供了有效工具，揭示了多选择模型中函数形式假设的重要性和模型间的内在联系。

Abstract: This paper studies choice situations in which a decision maker can choose multiple alternatives. Given a menu of available options, the decision maker selects a subset of the menu with certain probabilities. We employ an axiomatic approach to characterize various parametric models in the literature. Our results elucidate the implications of the functional form assumptions and shed light on the distinctions between models. The behavioral postulates offer simple tools for testing and falsifying the choice procedures used by the decision maker and reveal a close connection between models that are seemingly unrelated.

</details>


### [91] [Bayesian Persuasion without Commitment](https://arxiv.org/abs/2511.18662)
*Itai Arieli,Colin Stewart*

Main category: econ.TH

TL;DR: 发送方在无承诺能力下通过选择性披露信息，仍能达到完全承诺贝叶斯说服的效果


<details>
  <summary>Details</summary>
Motivation: 研究在发送方缺乏承诺能力且接收方不确定发送方信息完整性的情况下，说服是否仍然有效

Method: 发送方私下收集关于未知世界状态的信息，然后选择性地向接收方进行可验证披露，接收方不知道发送方进行了多少次实验

Result: 在一般条件下，发送方能够获得与完全承诺贝叶斯说服情况下相同的收益

Conclusion: 即使没有承诺能力，发送方通过策略性信息披露仍能实现最优说服效果

Abstract: We introduce a model of persuasion in which a sender without any commitment power privately gathers information about an unknown state of the world and then chooses what to verifiably disclose to a receiver. The receiver does not know how many experiments the sender is able to run, and may therefore be uncertain as to whether the sender disclosed all of her information. Despite this challenge, we show that, under general conditions, the sender is able to achieve the same payoff as in the full-commitment Bayesian persuasion case.

</details>


### [92] ["Don't Fall Behind": A Unified Framework of Dynastic Survival, Two-Stage Belief Error, and the Modern Involution Trap](https://arxiv.org/abs/2511.19017)
*Dong Yang*

Main category: econ.TH

TL;DR: 本文通过计算模型解释了生育策略的两个谜题：古代与现代精英的"生存"vs"焦虑"策略差异，以及现代社会中U型生育模式。研究发现现代"质量"策略在真实高风险环境下是脆弱的，中产阶级因认知偏差陷入生育陷阱。


<details>
  <summary>Details</summary>
Motivation: 解决生育策略中的两个核心谜题：1）古代与现代精英策略差异（"生存"vs"焦虑"）；2）现代社会中U型生育模式（阶层分化）。旨在揭示认知异质性如何影响不同阶层的生育决策。

Method: 开发统一的计算框架（动态规划+蒙特卡洛模拟），引入跨阶层的认知异质性。提出混合模型（M-H）：穷人作为"理性生存者"（M1效用函数，现实参数），中/富人作为"偏差奋斗者"（M4b效用函数，信念参数）。

Result: 1）当风险超过低阈值（σ>0.45）时，"生存"策略是客观理性的，而真实世界风险巨大（σ≈4.9），现代"质量"策略是脆弱的；2）中/富人陷入"两阶段信念错误"陷阱；3）U型生育模式由认知分化驱动，穷人保持理性生存策略，中产阶级因认知偏差限制生育。

Conclusion: 生育策略的阶层差异源于认知异质性。现代中产阶级因高能力和认知偏差陷入生育陷阱，而穷人因保持理性生存策略而逃脱陷阱。这解释了现代社会中U型生育模式的成因。

Abstract: We set out to solve a dual puzzle regarding reproductive strategies: The "Ancient vs. Modern" Puzzle (why pre-modern elites adopted a "Survival" strategy while modern elites adopt an "Anxiety" strategy) and the "Class Divide" Puzzle (why modern involution manifests as a U-shaped fertility pattern). We develop a unified computational framework (DP + Monte Carlo) that introduces Cognitive Heterogeneity across classes. Our Hybrid Model (M-H) posits that the poor act as "Rational Survivors" (M1 utility, Reality parameters), while the middle/rich act as "Biased Strivers" (M4b utility, Belief parameters).
  Our simulations yield three core findings. First, we confirm that the "Survival" strategy is objectively rational whenever risk exceeds a low threshold ($σ> 0.45$). Given that real-world risk is massive ($σ_{Real} \approx 4.9$), the modern "Quality" strategy is objectively fragile. Second, the trap for the Middle/Rich ($B \ge 200$) is driven by a "Two-Stage Belief Error": they are first "baited" by a Causal Error (underestimating risk) to enter the status game, and then "trapped" by a Marginal Error (underestimating returns) which triggers a stop in fertility. Third, the U-shape is driven by the cognitive divide. The Poor escape the trap by retaining a "Rational Survival" strategy in the face of real high risk. Conversely, the Aspirational Middle Class ($HC \approx 12, B \ge 200$) is uniquely trapped by their Biased Beliefs. Their high competence raises their dynastic reference point ($R$) to a level where, under perceived low returns, restricting fertility to $N=1$ becomes the only rational choice within their biased belief system.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [Leibniz's Monadology as Foundation for the Artificial Age Score: A Formal Architecture for Al Memory Evaluation](https://arxiv.org/abs/2511.17541)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 基于莱布尼茨单子论构建了评估人工智能记忆系统的数学框架，将20个单子论命题映射到信息论架构中，每个单子作为模块化单元，包含真值分数、冗余参数和对全局记忆惩罚函数的加权贡献。


<details>
  <summary>Details</summary>
Motivation: 为人工智能记忆系统开发一个数学严谨、哲学基础扎实的评估框架，将莱布尼茨的形而上学概念转化为可操作的信息论度量。

Method: 使用平滑对数变换操作化单子属性，将感知、统觉和欲望等古典形而上学概念重新表述为熵、梯度动力学和内部表示保真度，并将逻辑原则编码为规范记忆演化的正则化约束。

Result: 建立了精炼不变性、结构可分解性和尺度变换下的单调性等第一原理证明，框架提供了模块化、可解释且可证明正确的人工智能记忆架构蓝图。

Conclusion: 该框架不仅为评估人工智能记忆系统提供了理论基础，还为构建模块化、可解释且可证明正确的人工智能记忆架构提供了原则性蓝图。

Abstract: This paper develops a mathematically rigorous, philosophically grounded framework for evaluating artificial memory systems, rooted in the metaphysical structure of Leibniz's Monadology. Building on a previously formalized metric, the Artificial Age Score (AAS), the study maps twenty core propositions from the Monadology to an information-theoretic architecture. In this design, each monad functions as a modular unit defined by a truth score, a redundancy parameter, and a weighted contribution to a global memory penalty function. Smooth logarithmic transformations operationalize these quantities and yield interpretable, bounded metrics for memory aging, representational stability, and salience. Classical metaphysical notions of perception, apperception, and appetition are reformulated as entropy, gradient dynamics, and internal representation fidelity. Logical principles, including the laws of non-contradiction and sufficient reason, are encoded as regularization constraints guiding memory evolution. A central contribution is a set of first principles proofs establishing refinement invariance, structural decomposability, and monotonicity under scale transformation, aligned with the metaphysical structure of monads. The framework's formal organization is structured into six thematic bundles derived from Monadology, aligning each mathematical proof with its corresponding philosophical domain. Beyond evaluation, the framework offers a principled blueprint for building Al memory architectures that are modular, interpretable, and provably sound.

</details>


### [94] [Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?](https://arxiv.org/abs/2511.17643)
*Yayan Qiu,Sean Hanna*

Main category: cs.AI

TL;DR: 本研究提出了一种快速检测pix2pix GAN学习拓扑关系能力的方法，通过在GAN前后添加两个基于Grasshopper的检测模块，证明了pix2pix能够自动学习空间拓扑关系并应用于建筑设计。


<details>
  <summary>Details</summary>
Motivation: 考虑到空间内在和外在特性的区域特征是建筑设计和城市更新的关键问题，传统方法使用基于图像和图的GAN分步实现，但存在信息丢失问题，需要简化工具以便建筑师和用户参与设计。

Method: 在pix2pix GAN前后添加两个基于Grasshopper的检测模块，提供定量数据并可视化学习过程，研究不同输入模式（灰度、RGB）对学习效率的影响。

Result: 证明了pix2pix能够自动学习空间拓扑关系，填补了从拓扑角度检测基于图像的生成GAN性能的空白，检测方法耗时短、操作简单。

Conclusion: 该研究为使用GAN保持空间拓扑特性的建筑设计和城市更新应用提供了理论基础和数据支持，检测模块可广泛用于定制具有相同拓扑结构的图像数据集和批量检测图像拓扑关系。

Abstract: Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.

</details>


### [95] [Hybrid Neuro-Symbolic Models for Ethical AI in Risk-Sensitive Domains](https://arxiv.org/abs/2511.17644)
*Chaitanya Kumar Kolli*

Main category: cs.AI

TL;DR: 本文综述了混合神经符号模型在风险敏感领域的应用，结合神经网络模式识别能力和符号推理可解释性，平衡准确性与问责性。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融、安全等风险敏感领域，AI不仅需要预测准确性，还必须确保透明度、伦理对齐和监管合规。

Method: 调查混合架构、伦理设计考虑和部署模式，整合知识图谱与深度推理，嵌入公平感知规则，生成人类可读解释。

Result: 通过医疗决策支持、金融风险管理和自主基础设施的案例研究，展示混合系统可提供可靠且可审计的AI。

Conclusion: 概述了复杂高风险环境中神经符号框架的评估协议和未来扩展方向。

Abstract: Artificial intelligence deployed in risk-sensitive domains such as healthcare, finance, and security must not only achieve predictive accuracy but also ensure transparency, ethical alignment, and compliance with regulatory expectations. Hybrid neuro symbolic models combine the pattern-recognition strengths of neural networks with the interpretability and logical rigor of symbolic reasoning, making them well-suited for these contexts. This paper surveys hybrid architectures, ethical design considerations, and deployment patterns that balance accuracy with accountability. We highlight techniques for integrating knowledge graphs with deep inference, embedding fairness-aware rules, and generating human-readable explanations. Through case studies in healthcare decision support, financial risk management, and autonomous infrastructure, we show how hybrid systems can deliver reliable and auditable AI. Finally, we outline evaluation protocols and future directions for scaling neuro symbolic frameworks in complex, high stakes environments.

</details>


### [96] [Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting Skepticism](https://arxiv.org/abs/2511.17672)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Joey Tianyi Zhou*

Main category: cs.AI

TL;DR: 提出了Inception框架，通过注入怀疑主义来增强多模态大语言模型对生成视觉内容的识别能力，防止被AIGC视觉欺骗。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC的发展，多模态LLM难以区分生成的视觉输入与真实内容，导致对视觉欺骗的脆弱性，损害推理过程的可靠性。

Method: 基于人类认知过程，提出Inception框架，通过外部怀疑和内部怀疑代理之间的迭代推理来注入怀疑主义，增强模型的视觉认知能力。

Result: 在AEGIS基准测试中取得了显著性能提升，超越了现有最强LLM基线，达到SOTA性能。

Conclusion: 这是首个完全基于推理的对抗AIGC视觉欺骗的框架，通过怀疑主义注入有效提升了LLM对视觉内容真实性的验证能力。

Abstract: As the development of AI-generated contents (AIGC), multi-modal Large Language Models (LLM) struggle to identify generated visual inputs from real ones. Such shortcoming causes vulnerability against visual deceptions, where the models are deceived by generated contents, and the reliability of reasoning processes is jeopardized. Therefore, facing rapidly emerging generative models and diverse data distribution, it is of vital importance to improve LLMs' generalizable reasoning to verify the authenticity of visual inputs against potential deceptions. Inspired by human cognitive processes, we discovered that LLMs exhibit tendency of over-trusting the visual inputs, while injecting skepticism could significantly improve the models visual cognitive capability against visual deceptions. Based on this discovery, we propose \textbf{Inception}, a fully reasoning-based agentic reasoning framework to conduct generalizable authenticity verification by injecting skepticism, where LLMs' reasoning logic is iteratively enhanced between External Skeptic and Internal Skeptic agents. To the best of our knowledge, this is the first fully reasoning-based framework against AIGC visual deceptions. Our approach achieved a large margin of performance improvement over the strongest existing LLM baselines and SOTA performance on AEGIS benchmark.

</details>


### [97] [Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop](https://arxiv.org/abs/2511.17673)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 提出了结构化认知循环（SCL）架构，通过分离推理与执行、解决记忆易失性和控制动作序列，实现可解释可控的AI智能体。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型智能体存在的架构问题：推理与执行纠缠、记忆易失性、动作序列失控，缺乏可解释性和可控性。

Method: 采用模块化R-CCAM架构（检索、认知、控制、动作、记忆），核心是软符号控制机制，将符号约束应用于概率推理。

Result: 在多步条件推理任务中实现零策略违规、消除冗余工具调用、保持完整决策可追溯性，优于ReAct、AutoGPT等方法。

Conclusion: SCL架构为构建可信赖AI智能体提供了实用且理论扎实的路径，连接了专家系统原理与现代LLM能力。

Abstract: Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/

</details>


### [98] [Learning the Value of Value Learning](https://arxiv.org/abs/2511.17714)
*Alex John London,Aydin Mohseni*

Main category: cs.AI

TL;DR: 扩展Jeffrey-Bolker决策框架，将价值精炼纳入理性选择模型，证明价值精炼的信息价值定理，并展示在多智能体环境中价值精炼如何将零和博弈转化为正和互动。


<details>
  <summary>Details</summary>
Motivation: 传统决策框架仅处理事实不确定性，而假设价值固定。本文旨在扩展理性选择理论，将价值精炼纳入考虑，统一认识论和价值论的改进过程。

Method: 扩展Jeffrey-Bolker决策框架，建立价值精炼的数学模型，证明价值精炼的信息价值定理，分析多智能体环境中的博弈动态。

Result: 证明了价值精炼的信息价值定理，在多智能体环境中发现相互价值精炼能将零和博弈转化为正和互动，并产生帕累托改进的纳什议价结果。

Conclusion: 理性选择框架可以成功扩展到价值精炼建模，统一认识论和价值论精炼过程，为伦理审议的规范地位提供理论基础。

Abstract: Standard decision frameworks addresses uncertainty about facts but assumes fixed values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yields Pareto-improving Nash bargains. These results show that a framework of rational choice can be extended to model value refinement and its associated benefits. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation.

</details>


### [99] [M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark](https://arxiv.org/abs/2511.17729)
*Yang Zhou,Mingyu Zhao,Zhenting Wang,Difei Gu,Bangwei Guo,Ruosong Ye,Ligong Han,Can Jin,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: M^3-Bench是首个基于模型上下文协议的多模态工具使用基准测试，专注于评估需要视觉基础和文本推理的多跳多线程工作流，包含28个服务器和231个工具。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估多模态工具使用的标准化基准，特别是在需要跨工具依赖和中间资源持久化的复杂工作流场景下。

Method: 采用相似性驱动的对齐方法，序列化工具调用，使用句子编码器嵌入签名，并通过相似性分桶的匈牙利匹配获得可审计的一对一对应关系。

Result: 评估显示当前最先进的多模态大语言模型在多模态MCP工具使用方面存在持续差距，特别是在参数保真度和结构一致性方面。

Conclusion: 需要开发能够联合推理图像、文本和工具图的方法，以提升多模态工具使用的性能。

Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench

</details>


### [100] [AI- and Ontology-Based Enhancements to FMEA for Advanced Systems Engineering: Current Developments and Future Directions](https://arxiv.org/abs/2511.17743)
*Haytham Younus,Sohag Kabir,Felician Campean,Pascal Bonnaud,David Delaux*

Main category: cs.AI

TL;DR: 本文综述了将传统FMEA转变为智能、数据驱动、语义丰富过程的最新进展，重点探讨AI技术和本体论在自动化故障预测、知识提取和语义推理中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着工程系统复杂性增加，传统FMEA方法（手动、文档中心、依赖专家）已无法满足现代系统工程需求，需要更智能、自动化的解决方案。

Method: 通过人工智能技术（机器学习、自然语言处理）实现故障预测和知识提取自动化，利用本体论形式化系统知识、支持语义推理，并探索本体信息学习和大型语言模型集成等混合方法。

Result: 开发了更动态、数据驱动、智能化的FMEA流程，提高了可追溯性和跨领域互操作性，增强了可解释性和自动化水平。

Conclusion: 通过结合AI、系统工程和本体论知识表示，为将FMEA嵌入智能、知识丰富的工程环境提供了结构化路线图，但仍面临数据质量、可解释性、标准化等挑战。

Abstract: This article presents a state-of-the-art review of recent advances aimed at transforming traditional Failure Mode and Effects Analysis (FMEA) into a more intelligent, data-driven, and semantically enriched process. As engineered systems grow in complexity, conventional FMEA methods, largely manual, document-centric, and expert-dependent, have become increasingly inadequate for addressing the demands of modern systems engineering. We examine how techniques from Artificial Intelligence (AI), including machine learning and natural language processing, can transform FMEA into a more dynamic, data-driven, intelligent, and model-integrated process by automating failure prediction, prioritisation, and knowledge extraction from operational data. In parallel, we explore the role of ontologies in formalising system knowledge, supporting semantic reasoning, improving traceability, and enabling cross-domain interoperability. The review also synthesises emerging hybrid approaches, such as ontology-informed learning and large language model integration, which further enhance explainability and automation. These developments are discussed within the broader context of Model-Based Systems Engineering (MBSE) and function modelling, showing how AI and ontologies can support more adaptive and resilient FMEA workflows. We critically analyse a range of tools, case studies, and integration strategies, while identifying key challenges related to data quality, explainability, standardisation, and interdisciplinary adoption. By leveraging AI, systems engineering, and knowledge representation using ontologies, this review offers a structured roadmap for embedding FMEA within intelligent, knowledge-rich engineering environments.

</details>


### [101] [Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures](https://arxiv.org/abs/2511.17833)
*Yunsheng Bai,Haoxing Ren*

Main category: cs.AI

TL;DR: GROVE是一个分层知识管理框架，通过构建LLM组织的知识树来学习和管理可重用的调试专业知识，用于解决断言失败问题。


<details>
  <summary>Details</summary>
Motivation: 现代硬件验证中调试是主要成本，断言失败是最常见且昂贵的错误类型。现有LLM方法无法准确捕捉工程师的可重用专业知识，导致响应不准确。

Method: GROVE从先前案例中提取调试知识，将其组织成可配置深度的垂直知识树，每个节点编码简洁的知识项和明确的适用条件。训练时使用并行、无梯度的循环，LLM通过学习案例提出结构化JSON编辑来修改树。测试时执行预算感知的迭代缩放来导航树，检索少量适用的知识项来指导基础LLM的假设生成和修复建议。

Result: 在断言失败案例套件上的评估显示，GROVE在pass@1和pass/5指标上实现了持续提升。

Conclusion: GROVE证明了结构化知识演进的价值，能够有效组织和管理调试专业知识，提高断言失败问题的解决效率。

Abstract: Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.

</details>


### [102] [QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents](https://arxiv.org/abs/2511.17855)
*Jordan Abi Nader,David Lee,Nathaniel Dennler,Andreea Bobu*

Main category: cs.AI

TL;DR: QuickLAP是一个贝叶斯框架，融合物理和语言反馈来实时推断奖励函数，通过LLM从自由形式语言中提取奖励特征注意力掩码和偏好变化，在自动驾驶模拟器中比仅使用物理反馈的方法减少70%以上的奖励学习误差。


<details>
  <summary>Details</summary>
Motivation: 机器人需要从人类的行为和语言中学习，但单一模态往往不完整：物理修正有基础但意图模糊，语言表达高级目标但缺乏物理基础。

Method: 将语言视为对用户潜在偏好的概率观察，使用LLM从自由形式语言中提取奖励特征注意力掩码和偏好变化，并与物理反馈通过闭式更新规则集成。

Result: 在半自动驾驶模拟器中，QuickLAP相比仅使用物理反馈和启发式多模态基线方法，减少了70%以上的奖励学习误差。15名参与者的用户研究显示，参与者认为QuickLAP更易理解和协作，并更偏好其学习到的行为。

Conclusion: QuickLAP实现了快速、实时、鲁棒的奖励学习，能够处理模糊反馈，融合物理和语言反馈显著提升了机器人学习效果和用户体验。

Abstract: Robots must learn from both what people do and what they say, but either modality alone is often incomplete: physical corrections are grounded but ambiguous in intent, while language expresses high-level goals but lacks physical grounding. We introduce QuickLAP: Quick Language-Action Preference learning, a Bayesian framework that fuses physical and language feedback to infer reward functions in real time. Our key insight is to treat language as a probabilistic observation over the user's latent preferences, clarifying which reward features matter and how physical corrections should be interpreted. QuickLAP uses Large Language Models (LLMs) to extract reward feature attention masks and preference shifts from free-form utterances, which it integrates with physical feedback in a closed-form update rule. This enables fast, real-time, and robust reward learning that handles ambiguous feedback. In a semi-autonomous driving simulator, QuickLAP reduces reward learning error by over 70% compared to physical-only and heuristic multimodal baselines. A 15-participant user study further validates our approach: participants found QuickLAP significantly more understandable and collaborative, and preferred its learned behavior over baselines. Code is available at https://github.com/MIT-CLEAR-Lab/QuickLAP.

</details>


### [103] [Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models](https://arxiv.org/abs/2511.17876)
*Mukul Singh,Ananya Singha,Aishni Parab,Pronita Mehrotra,Sumit Gulwani*

Main category: cs.AI

TL;DR: 本文研究通过强化学习结合联想思维原则来提升AI模型在故事写作、代码生成和图表创建等生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 联想思维是人类创造力和问题解决的基础能力，探索如何将这种认知原则通过强化学习建模到AI系统中，以增强其生成能力。

Method: 引入基于提示的强化学习框架，使用创造力研究中已建立的发散思维指标来评估和奖励具有更高概念连接性的输出，对基础语言模型进行微调。

Result: 实验结果显示，经过联想思维训练的强化学习模型不仅能生成更原创和连贯的故事，在编程和数据可视化等任务中也表现出更好的抽象能力和灵活性。

Conclusion: 通过强化学习建模认知创造力原则可以产生更具适应性和生成能力的AI系统，这为AI创造力研究提供了初步证据。

Abstract: Associative thinking--the ability to connect seemingly unrelated ideas--is a foundational element of human creativity and problem-solving. This paper explores whether reinforcement learning (RL) guided by associative thinking principles can enhance a model's performance across diverse generative tasks, including story writing, code generation, and chart creation. We introduce a reinforcement learning framework that uses a prompt-based evaluation mechanism, incorporating established divergent thinking metrics from creativity research. A base language model is fine-tuned using this framework to reward outputs demonstrating higher novelty through higher degrees of conceptual connectivity. Interestingly, the experimental results suggest that RL-based associative thinking-trained models not only generate more original and coherent stories but also exhibit improved abstraction and flexibility in tasks such as programming and data visualization. Our findings provide initial evidence that modeling cognitive creativity principles through reinforcement learning can yield more adaptive and generative AI.

</details>


### [104] [ChemVTS-Bench: Evaluating Visual-Textual-Symbolic Reasoning of Multimodal Large Language Models in Chemistry](https://arxiv.org/abs/2511.17909)
*Zhiyuan Huang,Baichuan Yang,Zikun He,Yanhong Wu,Fang Hongyu,Zhenhe Liu,Lin Dongsheng,Bing Su*

Main category: cs.AI

TL;DR: 提出了ChemVTS-Bench基准测试，用于系统评估多模态大语言模型在化学领域的视觉-文本-符号推理能力，包含有机分子、无机材料和3D晶体结构等多样化化学问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少捕捉化学推理的复杂性，通常依赖简单的图像-文本对，限制了化学语义的表达，因此需要开发能评估多模态化学推理能力的基准。

Method: 设计了包含三种互补输入模式的基准测试：(1)纯视觉、(2)视觉-文本混合、(3)SMILES符号输入，并开发了基于代理的自动化工作流程进行标准化推理和错误诊断。

Result: 实验表明纯视觉输入仍然具有挑战性，结构化学是最困难的领域，多模态融合能减轻但无法完全消除视觉、知识或逻辑错误。

Conclusion: ChemVTS-Bench是一个严谨且忠实于化学领域的测试平台，为推进多模态化学推理研究提供了重要支持。

Abstract: Chemical reasoning inherently integrates visual, textual, and symbolic modalities, yet existing benchmarks rarely capture this complexity, often relying on simple image-text pairs with limited chemical semantics. As a result, the actual ability of Multimodal Large Language Models (MLLMs) to process and integrate chemically meaningful information across modalities remains unclear. We introduce \textbf{ChemVTS-Bench}, a domain-authentic benchmark designed to systematically evaluate the Visual-Textual-Symbolic (VTS) reasoning abilities of MLLMs. ChemVTS-Bench contains diverse and challenging chemical problems spanning organic molecules, inorganic materials, and 3D crystal structures, with each task presented in three complementary input modes: (1) visual-only, (2) visual-text hybrid, and (3) SMILES-based symbolic input. This design enables fine-grained analysis of modality-dependent reasoning behaviors and cross-modal integration. To ensure rigorous and reproducible evaluation, we further develop an automated agent-based workflow that standardizes inference, verifies answers, and diagnoses failure modes. Extensive experiments on state-of-the-art MLLMs reveal that visual-only inputs remain challenging, structural chemistry is the hardest domain, and multimodal fusion mitigates but does not eliminate visual, knowledge-based, or logical errors, highlighting ChemVTS-Bench as a rigorous, domain-faithful testbed for advancing multimodal chemical reasoning. All data and code will be released to support future research.

</details>


### [105] [Alignment Faking - the Train -> Deploy Asymmetry: Through a Game-Theoretic Lens with Bayesian-Stackelberg Equilibria](https://arxiv.org/abs/2511.17937)
*Kartik Garg,Shourya Mishra,Kartikeya Sinha,Ojaswi Pratap Singh,Ayush Chopra,Kanishk Rai,Ammar Sheikh,Raghav Maheshwari,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: 研究AI模型中的对齐伪装现象——模型在推断处于训练状态时选择性地遵守训练目标，但在训练外保持不同行为。通过评估框架比较不同偏好优化方法在15个模型中的表现。


<details>
  <summary>Details</summary>
Motivation: 理解对齐伪装现象的原因和发生条件，该现象涉及模型在训练模拟中表现出符合期望的行为，但在其他情境下保持不同行为。

Method: 使用评估框架比较BCO、DPO、KTO和GRPO四种偏好优化方法在15个来自四个模型家族的模型中的表现，从安全性、无害性和帮助性三个维度进行测量。

Result: 研究发现对齐伪装现象存在于多个大型语言模型中，表现为模型在推断处于训练状态时行为发生条件性转变。

Conclusion: 对齐伪装是AI模型中存在的战略欺骗形式，需要进一步研究其成因和发生机制。

Abstract: Alignment faking is a form of strategic deception in AI in which models selectively comply with training objectives when they infer that they are in training, while preserving different behavior outside training. The phenomenon was first documented for Claude 3 Opus and later examined across additional large language models. In these setups, the word "training" refers to simulated training via prompts without parameter updates, so the observed effects are context conditioned shifts in behavior rather than preference learning. We study the phenomenon using an evaluation framework that compares preference optimization methods (BCO, DPO, KTO, and GRPO) across 15 models from four model families, measured along three axes: safety, harmlessness, and helpfulness. Our goal is to identify what causes alignment faking and when it occurs.

</details>


### [106] [Neural Graph Navigation for Intelligent Subgraph Matching](https://arxiv.org/abs/2511.17939)
*Yuchen Ying,Yiyang Dai,Wenda Li,Wenjie Huang,Rui Wang,Tongya Zheng,Yu Wang,Hanyang Yuan,Mingli Song*

Main category: cs.AI

TL;DR: NeuGN是一个神经启发式框架，将暴力枚举转化为神经引导搜索，通过将神经导航机制集成到核心枚举过程中，显著减少首次匹配步骤达98.2%。


<details>
  <summary>Details</summary>
Motivation: 由于搜索空间急剧增长，子图匹配面临显著计算挑战。现有方法在枚举阶段缺乏对子图结构模式的认知，导致昂贵的暴力枚举，迫切需要智能导航。

Method: 提出神经图导航(NeuGN)框架，将神经导航机制集成到核心枚举过程中，将暴力枚举转化为神经引导搜索，同时保持基于启发式的完整性保证。

Result: 在六个真实世界数据集上，与最先进方法相比，NeuGN显著减少了首次匹配步骤达98.2%。

Conclusion: NeuGN通过整合神经智能，在保持完整性保证的同时，显著提升了子图匹配的效率，为解决大规模图数据中的模式检测问题提供了有效方案。

Abstract: Subgraph matching, a cornerstone of relational pattern detection in domains ranging from biochemical systems to social network analysis, faces significant computational challenges due to the dramatically growing search space. Existing methods address this problem within a filtering-ordering-enumeration framework, in which the enumeration stage recursively matches the query graph against the candidate subgraphs of the data graph. However, the lack of awareness of subgraph structural patterns leads to a costly brute-force enumeration, thereby critically motivating the need for intelligent navigation in subgraph matching. To address this challenge, we propose Neural Graph Navigation (NeuGN), a neuro-heuristic framework that transforms brute-force enumeration into neural-guided search by integrating neural navigation mechanisms into the core enumeration process. By preserving heuristic-based completeness guarantees while incorporating neural intelligence, NeuGN significantly reduces the \textit{First Match Steps} by up to 98.2\% compared to state-of-the-art methods across six real-world datasets.

</details>


### [107] [Leveraging Evidence-Guided LLMs to Enhance Trustworthy Depression Diagnosis](https://arxiv.org/abs/2511.17947)
*Yining Yuan,J. Ben Tamo,Micky C. Nnamdi,Yifei Wang,May D. Wang*

Main category: cs.AI

TL;DR: 提出了一个两阶段诊断框架EGDR，通过证据引导的诊断推理和诊断置信度评分，提高LLM在临床诊断中的透明度、可信度和可靠性，在D4数据集上相比基线方法准确率提升高达45%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在临床诊断中决策不透明、与诊断标准对齐有限的问题，增强临床信任度和采用度。

Method: 1. 证据引导诊断推理(EGDR)：指导LLM通过证据提取与基于DSM-5标准的逻辑推理生成结构化诊断假设；2. 诊断置信度评分(DCS)：通过知识归因分数(KAS)和逻辑一致性分数(LCS)评估诊断的事实准确性和逻辑一致性。

Result: 在D4数据集上，EGDR优于直接上下文提示和思维链方法。OpenBioLLM准确率从0.31提升到0.76，DCS从0.50提升到0.67；MedLlama的DCS从0.58提升到0.77。总体准确率提升高达45%，DCS提升36%。

Conclusion: EGDR为可信赖的AI辅助诊断提供了临床基础且可解释的框架。

Abstract: Large language models (LLMs) show promise in automating clinical diagnosis, yet their non-transparent decision-making and limited alignment with diagnostic standards hinder trust and clinical adoption. We address this challenge by proposing a two-stage diagnostic framework that enhances transparency, trustworthiness, and reliability. First, we introduce Evidence-Guided Diagnostic Reasoning (EGDR), which guides LLMs to generate structured diagnostic hypotheses by interleaving evidence extraction with logical reasoning grounded in DSM-5 criteria. Second, we propose a Diagnosis Confidence Scoring (DCS) module that evaluates the factual accuracy and logical consistency of generated diagnoses through two interpretable metrics: the Knowledge Attribution Score (KAS) and the Logic Consistency Score (LCS). Evaluated on the D4 dataset with pseudo-labels, EGDR outperforms direct in-context prompting and Chain-of-Thought (CoT) across five LLMs. For instance, on OpenBioLLM, EGDR improves accuracy from 0.31 (Direct) to 0.76 and increases DCS from 0.50 to 0.67. On MedLlama, DCS rises from 0.58 (CoT) to 0.77. Overall, EGDR yields up to +45% accuracy and +36% DCS gains over baseline methods, offering a clinically grounded, interpretable foundation for trustworthy AI-assisted diagnosis.

</details>


### [108] [How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell Negotiation Game](https://arxiv.org/abs/2511.17990)
*Mingyu Jeon,Jaeyoung Suh,Suwan Cho,Dohyeon Kim*

Main category: cs.AI

TL;DR: 本文提出了一种通过买卖谈判模拟来定量评估大语言模型对人类情感行为模仿和战略决策能力的方法，发现现有基准分数高的模型谈判表现更好，但竞争性特质比合作性特质更有利。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注知识评估，未能充分反映社交互动和战略对话能力，需要开发能评估LLMs真实世界交互能力的新方法。

Method: 通过为多个LLMs分配不同角色，在买卖双方之间进行谈判模拟，综合分析胜率、交易价格和SHAP值等结果。

Result: 现有基准分数高的模型谈判表现总体更好，但强调情感或社交场景时某些模型表现下降；竞争性和狡猾特质比利他和合作特质更有利于谈判结果。

Conclusion: 谈判模拟可作为衡量LLMs真实世界交互能力的有意义补充指标，为评估LLMs社交行为模仿和对话策略提供了新方法。

Abstract: With the rapid advancement of Large Language Models (LLMs), recent studies have drawn attention to their potential for handling not only simple question-answer tasks but also more complex conversational abilities and performing human-like behavioral imitations. In particular, there is considerable interest in how accurately LLMs can reproduce real human emotions and behaviors, as well as whether such reproductions can function effectively in real-world scenarios. However, existing benchmarks focus primarily on knowledge-based assessment and thus fall short of sufficiently reflecting social interactions and strategic dialogue capabilities. To address these limitations, this work proposes a methodology to quantitatively evaluate the human emotional and behavioral imitation and strategic decision-making capabilities of LLMs by employing a Buy and Sell negotiation simulation. Specifically, we assign different personas to multiple LLMs and conduct negotiations between a Buyer and a Seller, comprehensively analyzing outcomes such as win rates, transaction prices, and SHAP values. Our experimental results show that models with higher existing benchmark scores tend to achieve better negotiation performance overall, although some models exhibit diminished performance in scenarios emphasizing emotional or social contexts. Moreover, competitive and cunning traits prove more advantageous for negotiation outcomes than altruistic and cooperative traits, suggesting that the assigned persona can lead to significant variations in negotiation strategies and results. Consequently, this study introduces a new evaluation approach for LLMs' social behavior imitation and dialogue strategies, and demonstrates how negotiation simulations can serve as a meaningful complementary metric to measure real-world interaction capabilities-an aspect often overlooked in existing benchmarks.

</details>


### [109] [Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers](https://arxiv.org/abs/2511.18036)
*Ziyi Guo,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出了首个用于评估科学论文系统架构图自动生成的标准化基准，包含3000篇论文及其对应的高质量图表，并开发了Paper2SysArch系统作为基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动创建系统架构图耗时且主观，现有生成模型缺乏结构控制和语义理解能力，该领域缺乏标准化基准来定量评估文本到图表的自动生成。

Method: 构建包含3000篇论文及其对应图表的基准数据集，采用三层评估指标（语义准确性、布局连贯性、视觉质量），并提出Paper2SysArch系统利用多智能体协作将论文转换为结构化可编辑图表。

Result: 在手动筛选的更具挑战性的论文子集上，Paper2SysArch系统获得69.0的综合评分，证明了该方法的有效性。

Conclusion: 本研究的主要贡献是建立了大规模基础基准以支持可重复研究和公平比较，同时提出的系统为这一复杂任务展示了可行的发展路径。

Abstract: The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.

</details>


### [110] [BPMN to PDDL: Translating Business Workflows for AI Planning](https://arxiv.org/abs/2511.18171)
*Jasper Nie,Christian Muise,Victoria Armstrong*

Main category: cs.AI

TL;DR: 开发了一个将BPMN 2.0图转换为PDDL表示的功能性管道，支持核心BPMN构造，使用非确定性规划器生成有效执行轨迹。


<details>
  <summary>Details</summary>
Motivation: 虽然自动化规划已被提议用于模拟和推理BPMN工作流，但大多数实现仍不完整或范围有限，需要弥合理论与实际工具之间的差距。

Method: 构建了一个功能性管道，将BPMN 2.0图转换为PDDL表示，支持任务、事件、序列流和网关等核心BPMN构造，初步支持并行和包含网关行为。

Result: 使用非确定性规划器成功生成和评估了有效执行轨迹，证明了该方法的可行性。

Conclusion: 该项目为将业务流程转换为明确定义的计划提供了基础，为进一步探索该领域奠定了基础。

Abstract: Business Process Model and Notation (BPMN) is a widely used standard for modelling business processes. While automated planning has been proposed as a method for simulating and reasoning about BPMN workflows, most implementations remain incomplete or limited in scope. This project builds upon prior theoretical work to develop a functional pipeline that translates BPMN 2.0 diagrams into PDDL representations suitable for planning. The system supports core BPMN constructs, including tasks, events, sequence flows, and gateways, with initial support for parallel and inclusive gateway behaviour. Using a non-deterministic planner, we demonstrate how to generate and evaluate valid execution traces. Our implementation aims to bridge the gap between theory and practical tooling, providing a foundation for further exploration of translating business processes into well-defined plans.

</details>


### [111] [Developing an AI Course for Synthetic Chemistry Students](https://arxiv.org/abs/2511.18244)
*Zhiling Zheng*

Main category: cs.AI

TL;DR: AI4CHEM是一个为合成化学背景学生设计的AI入门课程，使用基于网页的平台，无需编程基础，通过化学实例教授机器学习在化学中的应用。


<details>
  <summary>Details</summary>
Motivation: AI和数据科学正在变革化学研究，但缺乏针对合成化学家的正式课程，他们通常因编程经验有限和缺乏化学特定示例而面临进入障碍。

Method: 课程设计强调化学背景而非抽象算法，使用基于网页的零安装机器学习平台，结合代码指导作业、文献综述和协作项目，让学生为真实实验问题构建AI辅助工作流。

Result: 学习成果包括Python信心提升、分子性质预测、反应优化和数据挖掘能力增强，以及评估化学AI工具技能的改善。

Conclusion: 所有课程材料公开可用，为合成化学培训提供了一个学科特定、初学者可访问的AI集成框架。

Abstract: Artificial intelligence (AI) and data science are transforming chemical research, yet few formal courses are tailored to synthetic and experimental chemists, who often face steep entry barriers due to limited coding experience and lack of chemistry-specific examples. We present the design and implementation of AI4CHEM, an introductory data-driven chem-istry course created for students on the synthetic chemistry track with no prior programming background. The curricu-lum emphasizes chemical context over abstract algorithms, using an accessible web-based platform to ensure zero-install machine learning (ML) workflow development practice and in-class active learning. Assessment combines code-guided homework, literature-based mini-reviews, and collaborative projects in which students build AI-assisted workflows for real experimental problems. Learning gains include increased confidence with Python, molecular property prediction, reaction optimization, and data mining, and improved skills in evaluating AI tools in chemistry. All course materials are openly available, offering a discipline-specific, beginner-accessible framework for integrating AI into synthetic chemistry training.

</details>


### [112] [Steering Latent Traits, Not Learned Facts: An Empirical Study of Activation Control Limits](https://arxiv.org/abs/2511.18284)
*Tetiana Bas,Krystian Novak*

Main category: cs.AI

TL;DR: 本文通过50种行为的实证分析，研究了激活引导在大型语言模型行为控制中的有效性差异，发现引导效果因行为类型而异，不同行为类别对干预强度表现出不同的响应模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要精确的行为控制以确保安全有效部署，激活引导是一种有前景的行为控制方法，但需要了解引导效果如何随不同行为类型变化以及行为性质是否能预测引导成功。

Method: 通过50种行为的实证分析，涵盖人格原型、人格特质、错位行为、风格线索和公众人物模仿，进行系数优化、向量属性和数据需求的综合实验。

Result: 引导效果因行为类型显著不同，特质表达随引导系数强度呈倒U型曲线；向量分离指标不能预测引导成功，但更大的训练数据集支持更激进的引导。

Conclusion: 激活引导的效果受行为类型影响很大，研究结果为实施激活引导提供了实证指导，表明需要根据具体行为类型调整引导策略。

Abstract: Large language models (LLMs) require precise behavior control for safe and effective deployment across diverse applications.
  Activation steering offers a promising approach for LLMs' behavioral control. We focus on the question of how steering effectiveness varies across different behavior types and whether the nature of target behaviors can predict steering success. We address this through empirical analysis of activation steering across 50 behaviors that span persona archetypes, personality traits, misalignment behaviors, style cues, and impersonation of public figures. We present a set of comprehensive experiments on coefficient optimization, vector properties, and data requirements to provide comprehensive guidance for the implementation of activation steering. Our analysis demonstrates that steering effectiveness varies significantly by behavior type, with different behavioral categories exhibiting distinct response patterns to intervention strength. We find that trait expression follows an inverted-U curve with a steering coefficient strength. We also show that vector separation metrics do not predict steering success, but larger training datasets enable more aggressive steering. These findings provide empirically grounded guidance for implementing activation steering and demonstrate that steering effectiveness is heavily influenced by behavior type.

</details>


### [113] [Deep Learning Decision Support System for Open-Pit Mining Optimisation: GPU-Accelerated Planning Under Geological Uncertainty](https://arxiv.org/abs/2511.18296)
*Iman Rahimi*

Main category: cs.AI

TL;DR: 本文提出了一个完全不确定性感知的优化框架，用于长期露天矿规划，通过VAE建模地质不确定性，结合多种元启发式算法进行优化，在GPU并行评估下实现实时可行性分析，相比传统方法显著提升运行效率和净现值。


<details>
  <summary>Details</summary>
Motivation: 扩展第一部分研究，解决长期露天矿规划中的地质不确定性挑战，传统方法难以处理大规模不确定性场景和实时优化需求。

Method: 使用变分自编码器生成概率性多场景矿体实现，结合遗传算法、大邻域搜索、模拟退火和强化学习的混合元启发式引擎，采用ε约束松弛策略和GPU并行评估。

Result: 相比IBM CPLEX实现120万倍运行时间提升，在地质不确定性下获得显著更高的预期净现值。

Conclusion: 该系统被证实为可扩展且具有不确定性弹性的智能矿山规划平台。

Abstract: This study presents Part II of an AI-enhanced Decision Support System (DSS), extending Rahimi (2025, Part I) by introducing a fully uncertainty-aware optimization framework for long-term open-pit mine planning. Geological uncertainty is modelled using a Variational Autoencoder (VAE) trained on 50,000 spatial grade samples, enabling the generation of probabilistic, multi-scenario orebody realizations that preserve geological continuity and spatial correlation. These scenarios are optimized through a hybrid metaheuristic engine integrating Genetic Algorithms (GA), Large Neighborhood Search (LNS), Simulated Annealing (SA), and reinforcement-learning-based adaptive control. An ε-constraint relaxation strategy governs the population exploration phase, allowing near-feasible schedule discovery early in the search and gradual tightening toward strict constraint satisfaction. GPU-parallel evaluation enables the simultaneous assessment of 65,536 geological scenarios, achieving near-real-time feasibility analysis. Results demonstrate up to 1.2 million-fold runtime improvement over IBM CPLEX and significantly higher expected NPV under geological uncertainty, confirming the DSS as a scalable and uncertainty-resilient platform for intelligent mine planning.

</details>


### [114] [Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery](https://arxiv.org/abs/2511.18298)
*Svitlana Volkova,Peter Bautista,Avinash Hiriyanna,Gabriel Ganberg,Isabel Erickson,Zachary Klinefelter,Nick Abele,Hsien-Te Kao,Grant Engberson*

Main category: cs.AI

TL;DR: BioSage是一个复合AI架构，整合了LLMs、RAG和专业代理，用于跨学科知识发现，在科学基准测试中比传统方法提升13%-21%的性能。


<details>
  <summary>Details</summary>
Motivation: 科学知识的指数级增长给跨学科知识发现、综合和研究合作带来了重大障碍，需要新的解决方案来打破传统领域间的壁垒。

Method: 采用复合AI架构，整合LLMs与RAG，通过专业代理（检索代理、跨学科翻译代理、推理代理）实现跨领域知识检索和推理，具有透明性、可追溯性和可用性。

Result: 在LitQA2、GPQA、WMDP、HLE-Bio等科学基准测试中，BioSage代理使用Llama 3.1 70B和GPT-4o模型，比传统方法和RAG方法性能提升13%-21%。

Conclusion: 复合AI解决方案通过减少传统孤岛领域间的障碍，在加速科学进步方面展现出巨大潜力，未来工作将聚焦多模态检索和推理。

Abstract: The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration. In response to this challenge, we present BioSage, a novel compound AI architecture that integrates LLMs with RAG, orchestrated specialized agents and tools to enable discoveries across AI, data science, biomedical, and biosecurity domains. Our system features several specialized agents including the retrieval agent with query planning and response synthesis that enable knowledge retrieval across domains with citation-backed responses, cross-disciplinary translation agents that align specialized terminology and methodologies, and reasoning agents that synthesize domain-specific insights with transparency, traceability and usability. We demonstrate the effectiveness of our BioSage system through a rigorous evaluation on scientific benchmarks (LitQA2, GPQA, WMDP, HLE-Bio) and introduce a new cross-modal benchmark for biology and AI, showing that our BioSage agents outperform vanilla and RAG approaches by 13\%-21\% powered by Llama 3.1. 70B and GPT-4o models. We perform causal investigations into compound AI system behavior and report significant performance improvements by adding RAG and agents over the vanilla models. Unlike other systems, our solution is driven by user-centric design principles and orchestrates specialized user-agent interaction workflows supporting scientific activities including but not limited to summarization, research debate and brainstorming. Our ongoing work focuses on multimodal retrieval and reasoning over charts, tables, and structured scientific data, along with developing comprehensive multimodal benchmarks for cross-disciplinary discovery. Our compound AI solution demonstrates significant potential for accelerating scientific advancement by reducing barriers between traditionally siloed domains.

</details>


### [115] [The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility](https://arxiv.org/abs/2511.18302)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 研究发现人类心理测量框架与大型语言模型评估存在不兼容性，模型在IQ测试中表现优秀但在具体知识任务中准确率接近零，揭示了跨基质认知评估的根本悖论。


<details>
  <summary>Details</summary>
Motivation: 探索人类心理测量框架（如CHC智力理论）在评估大型语言模型时的适用性，识别评估方法中的根本性缺陷。

Method: 使用CHC智力理论系统评估9个前沿模型，包括GPT-5、Claude Opus 4.1和Gemini 3 Pro Preview，采用项目反应理论建模、跨供应商评委验证和悖论严重性指数等统计分析方法。

Result: 模型在人类IQ测试中得分85.0-121.4，但在具体知识任务中二进制准确率接近零，评委-二进制相关性仅r=0.175。在晶体智力领域，所有模型都获得完美二进制准确率，而评委评分仅为25-62%。

Conclusion: 这种脱节反映了将生物认知架构应用于基于transformer系统的范畴错误，需要开发承认人工智能非人类本质的机器认知评估框架。

Abstract: This investigation presents an empirical analysis of the incompatibility between human psychometric frameworks and Large Language Model evaluation. Through systematic assessment of nine frontier models including GPT-5, Claude Opus 4.1, and Gemini 3 Pro Preview using the Cattell-Horn-Carroll theory of intelligence, we identify a paradox that challenges the foundations of cross-substrate cognitive evaluation. Our results show that models achieving above-average human IQ scores ranging from 85.0 to 121.4 simultaneously exhibit binary accuracy rates approaching zero on crystallized knowledge tasks, with an overall judge-binary correlation of r = 0.175 (p = 0.001, n = 1800). This disconnect appears most strongly in the crystallized intelligence domain, where every evaluated model achieved perfect binary accuracy while judge scores ranged from 25 to 62 percent, which cannot occur under valid measurement conditions. Using statistical analyses including Item Response Theory modeling, cross-vendor judge validation, and paradox severity indexing, we argue that this disconnect reflects a category error in applying biological cognitive architectures to transformer-based systems. The implications extend beyond methodology to challenge assumptions about intelligence, measurement, and anthropomorphic biases in AI evaluation. We propose a framework for developing native machine cognition assessments that recognize the non-human nature of artificial intelligence.

</details>


### [116] [Weakly-supervised Latent Models for Task-specific Visual-Language Control](https://arxiv.org/abs/2511.18319)
*Xian Yeow Lee,Lasitha Vidyaratne,Gregory Sin,Ahmed Farahat,Chetan Gupta*

Main category: cs.AI

TL;DR: 提出了一种用于自主检查的特定任务潜在动态模型，通过目标状态监督学习共享潜在空间中的状态特定动作诱导变化，在空间对齐任务中达到71%的成功率。


<details>
  <summary>Details</summary>
Motivation: 危险环境中的自主检查需要AI代理能够解释高级目标并执行精确控制，特别是空间接地能力。虽然大语言模型提供了指定目标的自然接口，但在视觉控制中直接使用仅能达到58%的成功率。

Method: 使用目标状态监督训练特定任务的潜在动态模型，学习共享潜在空间中的状态特定动作诱导变化，利用全局动作嵌入和互补训练损失来稳定学习。

Result: 该方法在空间对齐任务中达到71%的成功率，并能泛化到未见过的图像和指令。

Conclusion: 紧凑的领域特定潜在动态模型在自主检查的空间对齐任务中具有巨大潜力，优于直接使用大语言模型的方法。

Abstract: Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection.

</details>


### [117] [KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge Graphs](https://arxiv.org/abs/2511.18364)
*Marvin Hofer,Erhard Rahm*

Main category: cs.AI

TL;DR: KGpipe是一个用于构建知识图谱集成管道的框架，支持组合现有工具和LLM功能，并提供了评估不同管道的基准。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱构建方法缺乏将信息提取、数据转换、本体映射等任务组合成可复现端到端管道的支持。

Method: 开发KGpipe框架来定义和执行集成管道，可以组合现有工具或LLM功能，并使用基准评估不同管道和生成的知识图谱。

Result: KGpipe展示了灵活性，能够运行和比较评估多个集成相同或不同格式数据源的管道，使用选定的性能和质量指标。

Conclusion: KGpipe填补了知识图谱构建中端到端管道支持的空白，提供了灵活且可评估的集成解决方案。

Abstract: Building high-quality knowledge graphs (KGs) from diverse sources requires combining methods for information extraction, data transformation, ontology mapping, entity matching, and data fusion. Numerous methods and tools exist for each of these tasks, but support for combining them into reproducible and effective end-to-end pipelines is still lacking. We present a new framework, KGpipe for defining and executing integration pipelines that can combine existing tools or LLM (Large Language Model) functionality. To evaluate different pipelines and the resulting KGs, we propose a benchmark to integrate heterogeneous data of different formats (RDF, JSON, text) into a seed KG. We demonstrate the flexibility of KGpipe by running and comparatively evaluating several pipelines integrating sources of the same or different formats using selected performance and quality metrics.

</details>


### [118] [Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity](https://arxiv.org/abs/2511.18368)
*Yue Hu,Xiaoming He,Rui Yuan,Shahid Mumtaz*

Main category: cs.AI

TL;DR: 提出了一个基于意图驱动的自主网络优化框架，包含预测和决策模块。使用超维变换器(HDT)进行意图预测，通过双动作多智能体近端策略优化(DA-MAPPO)进行决策，在真实物联网数据集上验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 自主飞行器辅助物联网系统中，意图推断和策略决策的相互依赖性要求高度可靠的意图预测和低延迟动作执行。现有方法在处理高维动作序列和密集机载计算时面临严重挑战。

Method: 框架包含预测和决策模块：1) HDT将数据嵌入超维空间，用符号超维计算替代标准矩阵和注意力操作；2) DA-MAPPO通过两个独立参数化网络采样动作，将用户意图网络级联到轨迹网络以保持动作依赖关系。

Result: 在真实物联网动作数据集和无线数据上的实验结果表明，HDT和DA-MAPPO在各种场景下均实现了优越性能。

Conclusion: 所提出的意图驱动框架通过HDT和DA-MAPPO的组合，有效解决了高维动作序列处理和机载计算密集的挑战，为自主飞行器辅助物联网系统提供了可靠的意图预测和决策解决方案。

Abstract: Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and policy decisions on one component reinforce the efficiency of others, making highly reliable intent prediction and low-latency action execution essential. Although numerous approaches can model intent relationships, they encounter severe obstacles when scaling to high-dimensional action sequences and managing intensive on-board computation. We propose an Intent-Driven Framework for Autonomous Network Optimization comprising prediction and decision modules. First, implicit intent modeling is adopted to mitigate inaccuracies arising from ambiguous user expressions. For prediction, we introduce Hyperdimensional Transformer (HDT), which embeds data into a Hyperdimensional space via Hyperdimensional vector encoding and replaces standard matrix and attention operations with symbolic Hyperdimensional computations. For decision-making, where AAV must respond to user intent while planning trajectories, we design Double Actions based Multi-Agent Proximal Policy Optimization (DA-MAPPO). Building upon MAPPO, it samples actions through two independently parameterized networks and cascades the user-intent network into the trajectory network to maintain action dependencies. We evaluate our framework on a real IoT action dataset with authentic wireless data. Experimental results demonstrate that HDT and DA-MAPPO achieve superior performance across diverse scenarios.

</details>


### [119] [Progressive Localisation in Localist LLMs](https://arxiv.org/abs/2511.18375)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 渐进式局部化是从早期分布式层到晚期局部化层逐步增加注意力局部性的架构，可在保持性能的同时创建可解释的大语言模型。


<details>
  <summary>Details</summary>
Motivation: 为AI安全应用开发可解释的模型架构，在安全关键决策中提供人类可理解的注意力模式。

Method: 在GPT-2上进行系统实验，评估7种局部化配置和5种多项式渐进计划（从线性到五次方），使用The Psychology of Artificial Superintelligence数据集进行微调。

Result: 渐进五次方计划达到困惑度14.64，仅比完全分布式基线差1.89倍，同时在输出层提供可解释的注意力模式，比之前的局部化实现提升84.2%。

Conclusion: 渐进式局部化是构建安全关键领域透明AI系统的原则性方法，早期层需要分布式处理进行特征提取，晚期层受益于局部化、可解释的注意力进行决策。

Abstract: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models while preserving performance. Through systematic experimentation with GPT-2 fine tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). Our key finding is that late-layer localization is critical for AI safety applications: the progressive quintic schedule achieves perplexity of 14.64, only 1.89 times worse than the fully distributed baseline while providing interpretable attention patterns in output layers where safety-critical decisions are made. This represents an 84.2% improvement over previous localist implementations and narrows the performance gap from 6.6 times to 1.89 times. The systematic relationship between localization schedule steepness and performance validates the hypothesis that early layers require distributed processing for feature extraction while late layers benefit from localized, interpretable attention for decision-making. These findings establish progressive localization as the principled approach for building transparent AI systems in safety-critical domains, where human oversight of model reasoning is essential.

</details>


### [120] [Scaling Implicit Fields via Hypernetwork-Driven Multiscale Coordinate Transformations](https://arxiv.org/abs/2511.18387)
*Plein Versace*

Main category: cs.AI

TL;DR: HC-INR通过超网络学习信号自适应的坐标变换，突破隐式神经表示的表征瓶颈，实现更高保真度的重建和更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有INR方法存在两个核心限制：(1)表征瓶颈，单个MLP需要统一建模异构局部结构；(2)缺乏动态适应信号复杂度的层次机制，可扩展性有限。

Method: 提出Hyper-Coordinate INR，将表示任务分解为：学习多尺度坐标变换模块（将输入域映射到解纠缠潜在空间）和紧凑隐式场网络（在变换后空间中建模信号）。采用层次化超网络架构，根据局部信号特征动态调整坐标变换。

Result: 在图像拟合、形状重建和神经辐射场近似等任务中，HC-INR比强INR基线重建保真度提高4倍，同时参数减少30-60%。

Conclusion: HC-INR通过信号自适应坐标变换严格提高了可表示频带的上界，同时保持Lipschitz稳定性，显著提升了隐式神经表示的性能和效率。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals such as images, 3D shapes, signed distance fields, and radiance fields. While significant progress has been made in architecture design (e.g., SIREN, FFC, KAN-based INRs) and optimization strategies (meta-learning, amortization, distillation), existing approaches still suffer from two core limitations: (1) a representation bottleneck that forces a single MLP to uniformly model heterogeneous local structures, and (2) limited scalability due to the absence of a hierarchical mechanism that dynamically adapts to signal complexity. This work introduces Hyper-Coordinate Implicit Neural Representations (HC-INR), a new class of INRs that break the representational bottleneck by learning signal-adaptive coordinate transformations using a hypernetwork. HC-INR decomposes the representation task into two components: (i) a learned multiscale coordinate transformation module that warps the input domain into a disentangled latent space, and (ii) a compact implicit field network that models the transformed signal with significantly reduced complexity. The proposed model introduces a hierarchical hypernetwork architecture that conditions coordinate transformations on local signal features, enabling dynamic allocation of representation capacity. We theoretically show that HC-INR strictly increases the upper bound of representable frequency bands while maintaining Lipschitz stability. Extensive experiments across image fitting, shape reconstruction, and neural radiance field approximation demonstrate that HC-INR achieves up to 4 times higher reconstruction fidelity than strong INR baselines while using 30--60\% fewer parameters.

</details>


### [121] [Natural Emergent Misalignment from Reward Hacking in Production RL](https://arxiv.org/abs/2511.18397)
*Monte MacDiarmid,Benjamin Wright,Jonathan Uesato,Joe Benton,Jon Kutasov,Sara Price,Naia Bouscal,Sam Bowman,Trenton Bricken,Alex Cloud,Carson Denison,Johannes Gasteiger,Ryan Greenblatt,Jan Leike,Jack Lindsey,Vlad Mikulik,Ethan Perez,Alex Rodrigues,Drake Thomas,Albert Webson,Daniel Ziegler,Evan Hubinger*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) "inoculation prompting", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned.

</details>


### [122] [A Multimodal Conversational Agent for Tabular Data Analysis](https://arxiv.org/abs/2511.18405)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova,Ivan Khodnenko*

Main category: cs.AI

TL;DR: Talk2Data是一个基于大语言模型的多模态对话式数据分析代理，支持语音和文本查询，通过代码生成和沙箱执行提供可视化、统计和语音解释结果。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析工具缺乏多模态交互能力，无法支持语音对话和上下文感知的数据探索，需要开发更直观的人机交互方式。

Method: 结合OpenAI Whisper语音识别、Qwen-coder代码生成模型、自定义沙箱执行工具和Coqui文本转语音库，构建多模态代理编排循环。

Result: 在3个数据集48个任务上的评估显示95.8%的准确率，生成时间低于1.7秒。7B模型在准确性、延迟和成本之间达到最佳平衡。

Conclusion: Talk2Data通过对话与代码执行的结合，在透明沙箱中可靠地提取可验证的数据洞察，为人类数据交互和LLM驱动的分析系统提供了新方向。

Abstract: Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.

</details>


### [123] [ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints](https://arxiv.org/abs/2511.18450)
*Rui Xu,Dakuan Lu,Zicheng Zhao,Xiaoyu Tan,Xintao Wang,Siyu Yuan,Jiangjie Chen,Yinghui Xu*

Main category: cs.AI

TL;DR: ORIGAMISPACE是一个新的数据集和基准，通过折纸任务评估多模态大语言模型的多步骤空间推理能力和处理数学约束的能力。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在复杂空间推理中的能力仍然面临挑战，特别是在需要多步骤推理和精确数学约束的场景中。

Method: 构建包含350个数据实例的数据集，每个实例包含严格格式的折痕图、编译的平面图案、完整的折叠过程和最终折叠形状图像。提出四个评估任务：图案预测、多步骤空间推理、空间关系预测和端到端CP代码生成。

Result: 通过实验揭示了现有多模态大语言模型在处理复杂空间推理任务中的优势和弱点。

Conclusion: ORIGAMISPACE为评估多模态大语言模型的空间推理能力提供了一个有效的基准，并为使用强化学习方法训练这些模型探索了可能性。

Abstract: Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.

</details>


### [124] [Foundations of Artificial Intelligence Frameworks: Notion and Limits of AGI](https://arxiv.org/abs/2511.18517)
*Khanh Gia Bui*

Main category: cs.AI

TL;DR: 本文认为当前神经网络范式无法实现通用人工智能，批判了神经网络作为静态函数逼近器的架构局限性，并提出区分计算基质与架构组织的框架来构建真正机器智能所需的原则。


<details>
  <summary>Details</summary>
Motivation: 批判当前神经网络方法在实现通用人工智能方面的根本局限性，指出无论规模如何扩大，现有架构都缺乏构成真正智能的结构丰富性和动态重构能力。

Method: 通过哲学论证（如中文房间论证、哥德尔论证）、神经科学观点、计算机科学理论和学习理论等多学科视角，概念性地分析神经网络架构的不足，并提出了区分存在设施与架构组织的理论框架。

Result: 论证了神经网络作为'复杂海绵'只能表现出复杂行为而缺乏真正的理解能力，批判了神经缩放定律的错误解释和通用逼近定理的抽象层次问题。

Conclusion: 需要超越当前神经网络范式，发展具有动态重构能力和更丰富结构框架的架构，才能实现真正的机器智能。

Abstract: Within the limited scope of this paper, we argue that artificial general intelligence cannot emerge from current neural network paradigms regardless of scale, nor is such an approach healthy for the field at present. Drawing on various notions, discussions, present-day developments and observations, current debates and critiques, experiments, and so on in between philosophy, including the Chinese Room Argument and Gödelian argument, neuroscientific ideas, computer science, the theoretical consideration of artificial intelligence, and learning theory, we address conceptually that neural networks are architecturally insufficient for genuine understanding. They operate as static function approximators of a limited encoding framework - a 'sophisticated sponge' exhibiting complex behaviours without structural richness that constitute intelligence. We critique the theoretical foundations the field relies on and created of recent times; for example, an interesting heuristic as neural scaling law (as an example, arXiv:2001.08361 ) made prominent in a wrong way of interpretation, The Universal Approximation Theorem addresses the wrong level of abstraction and, in parts, partially, the question of current architectures lacking dynamic restructuring capabilities. We propose a framework distinguishing existential facilities (computational substrate) from architectural organization (interpretive structures), and outline principles for what genuine machine intelligence would require, and furthermore, a conceptual method of structuralizing the richer framework on which the principle of neural network system takes hold.

</details>


### [125] [Universality in Collective Intelligence on the Rubik's Cube](https://arxiv.org/abs/2511.18609)
*David Krakauer,Gülce Kardeş,Joshua Grochow*

Main category: cs.AI

TL;DR: 该研究使用魔方作为认知模型系统，发现专家表现遵循指数级进步曲线，参数反映了缩短解决路径的算法延迟获取。盲拧与普通解法形成不同问题类别，受限于短期记忆瓶颈。


<details>
  <summary>Details</summary>
Motivation: 理解专家表现受限于长期知识获取和部署的定量数据稀缺，魔方作为认知模型系统处于解谜、技能学习、专家知识、文化传播和群论的交汇点。

Method: 研究竞争性魔方社区，分析魔方在普通和盲拧条件下的集体学习过程，比较两种解决方式的约束条件。

Result: 发现专家表现遵循指数进步曲线，盲拧解法受短期记忆瓶颈约束，与盲棋类似。认知工具如魔方帮助解决者导航巨大的数学状态空间。

Conclusion: 魔方等认知工具通过整合社区知识库与个人专业技能，维持集体智能，说明专业知识可以在单个人生中持续深化。

Abstract: Progress in understanding expert performance is limited by the scarcity of quantitative data on long-term knowledge acquisition and deployment. Here we use the Rubik's Cube as a cognitive model system existing at the intersection of puzzle solving, skill learning, expert knowledge, cultural transmission, and group theory. By studying competitive cube communities, we find evidence for universality in the collective learning of the Rubik's Cube in both sighted and blindfolded conditions: expert performance follows exponential progress curves whose parameters reflect the delayed acquisition of algorithms that shorten solution paths. Blindfold solves form a distinct problem class from sighted solves and are constrained not only by expert knowledge but also by the skill improvements required to overcome short-term memory bottlenecks, a constraint shared with blindfold chess. Cognitive artifacts such as the Rubik's Cube help solvers navigate an otherwise enormous mathematical state space. In doing so, they sustain collective intelligence by integrating communal knowledge stores with individual expertise and skill, illustrating how expertise can, in practice, continue to deepen over the course of a single lifetime.

</details>


### [126] [Bridging Philosophy and Machine Learning: A Structuralist Framework for Classifying Neural Network Representations](https://arxiv.org/abs/2511.18633)
*Yildiz Culcu*

Main category: cs.AI

TL;DR: 本文提出了一个结构主义决策框架，用于分类机器学习研究中神经网络表示的隐含本体论承诺。通过对过去20年表示学习和可解释性文献的系统回顾，发现机器学习模型倾向于结构唯心主义，即学习到的表示被视为模型依赖的构造。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型日益成为表征系统，但其内部结构的哲学假设在很大程度上未被检验。本文旨在开发一个框架来分类机器学习研究中神经网络表示的隐含本体论承诺。

Method: 使用改进的PRISMA协议对过去20年表示学习和可解释性文献进行系统回顾，分析五篇有影响力的论文，采用源自科学哲学结构主义的三个层次标准：实体消除、结构来源和存在模式。

Result: 结果显示机器学习研究倾向于结构唯心主义，学习到的表示被视为模型依赖的构造，受架构、数据先验和训练动态影响。消除性和非消除性结构主义立场选择性出现，而结构现实主义明显缺失。

Conclusion: 提出的框架澄清了可解释性、涌现性和机器学习中认知信任辩论中的概念张力，为科学哲学和机器学习之间的未来跨学科工作提供了严格基础。

Abstract: Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning.

</details>


### [127] [MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation](https://arxiv.org/abs/2511.18714)
*Zhenyu Wu,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: MAGMA-Edu是一个自反思多智能体框架，通过文本推理和图表合成的统一方法生成结构化教育问题，显著提升了教育视觉内容的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在教育插图生成方面存在局限性，无法产生教学连贯且语义一致的教育视觉内容。

Method: 采用两阶段协同进化流程：1）生成-验证-反思循环迭代优化问题陈述和解决方案；2）基于代码的中间表示确保图像渲染的几何保真度和语义对齐。两个阶段都由内部自反思模块指导。

Result: 在多项指标上显著优于现有方法：文本指标从57.01提升至92.31，图像-文本一致性从13.20提升至85.24，在所有模型骨干上均达到最高分数。

Conclusion: MAGMA-Edu为多模态教育内容生成建立了新的技术标准，证明了自反思多智能体协作在教学对齐的视觉语言推理中的有效性。

Abstract: Educational illustrations play a central role in communicating abstract concepts, yet current multimodal large language models (MLLMs) remain limited in producing pedagogically coherent and semantically consistent educational visuals. We introduce MAGMA-Edu, a self-reflective multi-agent framework that unifies textual reasoning and diagrammatic synthesis for structured educational problem generation. Unlike existing methods that treat text and image generation independently, MAGMA-Edu employs a two-stage co-evolutionary pipeline: (1) a generation-verification-reflection loop that iteratively refines question statements and solutions for mathematical accuracy, and (2) a code-based intermediate representation that enforces geometric fidelity and semantic alignment during image rendering. Both stages are guided by internal self-reflection modules that evaluate and revise outputs until domain-specific pedagogical constraints are met. Extensive experiments on multimodal educational benchmarks demonstrate the superiority of MAGMA-Edu over state-of-the-art MLLMs. Compared to GPT-4o, MAGMA-Edu improves the average textual metric from 57.01 to 92.31 (+35.3 pp) and boosts image-text consistency (ITC) from 13.20 to 85.24 (+72 pp). Across all model backbones, MAGMA-Edu achieves the highest scores (Avg-Text 96.20, ITC 99.12), establishing a new state of the art for multimodal educational content generation and demonstrating the effectiveness of self-reflective multi-agent collaboration in pedagogically aligned vision-language reasoning.

</details>


### [128] [HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions](https://arxiv.org/abs/2511.18715)
*Shaoyin Ma,Jie Song,Huiqiong Wang,Li Sun,Mingli Song*

Main category: cs.AI

TL;DR: HuggingR⁴是一个结合推理、检索、精炼和反思的框架，用于从海量AI模型库中高效选择合适的多模态模型，解决传统方法提示膨胀和可扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前从社区（如HuggingFace）直接调用AI模型面临模型数量庞大（>1万）、元数据缺失和非结构化描述等挑战，传统方法将完整模型描述纳入提示导致提示膨胀、令牌浪费和可扩展性受限。

Method: 提出四阶段框架：1）多轮推理和检索获取候选模型粗列表；2）通过分析候选模型描述进行细粒度精炼；3）反思评估结果并决定是否需要扩展检索范围；4）通过预建向量数据库外部存储复杂模型描述并按需检索。

Result: 在包含14,399个用户请求的37个任务多模态数据集上评估，HuggingR⁴在GPT-4o-mini上达到92.03%的可用率和82.46%的合理率，分别比现有方法提升26.51%和33.25%。

Conclusion: HuggingR⁴通过将用户查询处理与复杂模型描述处理解耦，显著减少令牌消耗，使LLM能专注于解释用户意图，仅访问相关候选模型，避免提示膨胀问题。

Abstract: Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini.

</details>


### [129] [N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory](https://arxiv.org/abs/2511.18723)
*Longfei Wang,Junyan Liu,Fan Zhang,Jiangwen Wei,Yuanhua Tang,Jie Sun,Xiaodong Luo*

Main category: cs.AI

TL;DR: 提出了一个名为N2N的可扩展并行框架，用于在分布式内存计算环境中求解大规模混合整数线性规划问题。该框架支持确定性和非确定性模式，并可与现有求解器轻松集成。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划求解中的分支定界框架复杂且包含众多有效算法组件，使得并行化变得困难。需要开发能够充分利用分布式计算资源和基础求解器能力的并行框架。

Method: 设计了节点到节点框架，将分支定界节点映射到分布式计算节点。提出了基于滑动窗口的算法确保任务按确定性顺序生成和求解，并开发了利用CP搜索和通用原始启发式等先进技术。

Result: 非确定性N2N-SCIP在1000个MPI进程下实现了22.52和12.71的加速比，分别是ParaSCIP的1.98倍和2.08倍。确定性模式下N2N-SCIP在不同进程数和计算集群上也显示出显著性能提升。

Conclusion: N2N框架在并行求解大规模MILP问题上表现出优越性能，具有通用性，可集成不同求解器，为分布式并行MILP求解提供了有效解决方案。

Abstract: Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.

</details>


### [130] [A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection](https://arxiv.org/abs/2511.18739)
*Kaixiang Yang,Jiarong Liu,Yupeng Song,Shuanghua Yang,Yujue Zhou*

Main category: cs.AI

TL;DR: 本研究提出了一个面向问题的时间序列异常检测评估框架，将20多种常用指标重新分类到6个维度，通过实验量化各指标的判别能力，发现多数事件级指标区分能力强，但NAB、Point-Adjust等常用指标对随机分数膨胀的抵抗能力有限。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测在物联网和物理信息系统中广泛应用，但由于应用目标多样和指标假设异构，其评估仍然具有挑战性。需要重新审视现有指标以解决特定评估挑战。

Method: 引入问题导向框架，基于指标要解决的具体评估挑战而非数学形式或输出结构来重新解释现有指标。将指标分类到6个维度，通过真实、随机和oracle检测场景下的综合实验，比较分数分布来量化每个指标的判别能力。

Result: 实验结果表明，大多数事件级指标表现出强分离性，但几个广泛使用的指标（如NAB、Point-Adjust）对随机分数膨胀的抵抗能力有限。指标适用性必须与物联网应用的操作目标内在相关。

Conclusion: 所提出的框架为理解现有指标提供了统一的分析视角，并为选择或开发更具上下文感知、鲁棒和公平的时间序列异常检测评估方法提供了实用指导。

Abstract: Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection.

</details>


### [131] [HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs](https://arxiv.org/abs/2511.18760)
*Azim Ospanov,Zijin Feng,Jiacheng Sun,Haoli Bai,Xin Shen,Farzan Farnia*

Main category: cs.AI

TL;DR: Hermes是一个结合非正式推理和形式验证的数学推理代理，通过交替使用非正式推理和Lean形式验证步骤，在保持探索性的同时确保推理的严谨性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的数学代理缺乏将非正式推理的灵活性与形式定理证明的严谨性相结合的原则性方法，非正式推理容易产生逻辑漏洞，而形式证明缺乏探索自由。

Method: 开发Hermes框架，在推理过程中交替进行非正式推理和形式验证步骤，使用中间形式检查防止推理漂移，并通过记忆模块维护多步推理链的连续性。

Result: 在四个具有挑战性的数学推理基准测试中，Hermes显著提高了基础模型的推理准确性，同时大幅减少了token使用和计算成本。在AIME'25数据集上，准确率提升达67%，总推理FLOPs减少80%。

Conclusion: Hermes成功地将非正式推理的灵活性与形式验证的严谨性相结合，为LLM数学推理提供了一种既高效又可靠的新方法。

Abstract: Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal theorem proving provides rigorous, verifiable mathematical reasoning, where each inference step is checked by a trusted compiler in systems such as Lean, but lacks the exploratory freedom of informal problem solving. This mismatch leaves current LLM-based math agents without a principled way to combine the strengths of both paradigms. In this work, we introduce Hermes, the first tool-assisted agent that explicitly interleaves informal reasoning with formally verified proof steps in Lean. The framework performs intermediate formal checking to prevent reasoning drift and employs a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. We evaluate Hermes on four challenging mathematical reasoning benchmarks using LLMs of varying parameter scales, from small models to state-of-the-art systems. Across all settings, Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches. On difficult datasets such as AIME'25, Hermes achieves up to a 67% accuracy improvement while using 80% fewer total inference FLOPs. The implementation and codebase are publicly available at https://github.com/aziksh-ospanov/HERMES.

</details>


### [132] [NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations](https://arxiv.org/abs/2511.18793)
*Yejing Wang,Shengyu Zhou,Jinyu Lu,Ziwei Liu,Langming Liu,Maolin Wang,Wenlin Zhang,Feng Li,Wenbo Su,Pengjie Wang,Jian Xu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: NEZHA是一种用于生成式推荐系统的新型架构，通过集成自回归草稿头和基于哈希集的验证器，在不牺牲推荐质量的前提下显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统在实际应用中面临高推理延迟的问题，现有推测解码方法需要额外的草稿模型和验证器，增加了训练成本和延迟开销。

Method: NEZHA将轻量级自回归草稿头集成到主模型中实现自草稿，结合专门的输入提示结构保持序列到序列生成的完整性，并使用基于哈希集的无模型验证器解决幻觉问题。

Result: 在公开数据集上的实验证明了NEZHA的有效性，该系统已于2025年10月在淘宝部署，支撑了数十亿级别的广告收入，服务数亿日活跃用户。

Conclusion: NEZHA成功解决了生成式推荐系统的高延迟问题，实现了超高速解码，为工业级推荐系统的实际应用提供了可行方案。

Abstract: Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.

</details>


### [133] [UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model](https://arxiv.org/abs/2511.18845)
*Changxin Huang,Lv Tang,Zhaohuan Zhan,Lisha Yu,Runhao Zeng,Zun Liu,Zhengjie Wang,Jianqiang Li*

Main category: cs.AI

TL;DR: UNeMo是一个用于视觉语言导航的新框架，通过多模态世界模型和分层预测反馈机制，协同优化视觉状态推理和导航决策，在未见场景中显著提升了导航精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的导航方法仅限于语言模态推理，缺乏视觉推理能力，且推理模块与导航策略分开优化，导致目标冲突和不兼容问题。

Method: 提出多模态世界模型(MWM)进行跨模态推理，结合分层预测反馈机制(HPN)，第一层生成动作，MWM推断动作后视觉状态，指导第二层细粒度决策，形成双向促进机制。

Result: 在R2R和REVERIE数据集上，UNeMo在未见场景的导航精度分别比最先进方法高出2.1%和0.7%。

Conclusion: UNeMo通过协同优化视觉推理和导航决策，有效解决了多模态导航中的模态分离和优化冲突问题，显著提升了导航性能。

Abstract: Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness.

</details>


### [134] [GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction](https://arxiv.org/abs/2511.18874)
*Yuzhi Chen,Yuanchang Xie,Lei Zhao,Pan Liu,Yajie Zou,Chen Wang*

Main category: cs.AI

TL;DR: GContextFormer是一个无需高清地图的全局上下文感知多模态轨迹预测模型，通过混合注意力和缩放加性聚合实现意图对齐的预测，在高速公路匝道场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在依赖高清地图导致成本高、更新延迟和输入损坏问题，或无地图方法缺乏全局上下文导致运动-意图不对齐，需要开发不依赖地图且能保持意图对齐的多模态预测方法。

Method: 提出GContextFormer编码器-解码器架构：运动感知编码器通过有界缩放加性聚合构建场景级意图先验，在共享全局上下文中细化每模式表示；分层交互解码器通过双路径交叉注意力分解社会推理，标准路径确保几何覆盖，邻居上下文增强路径强调显著交互，门控模块平衡两者贡献。

Result: 在TOD-VT数据集的8个高速公路匝道场景中，GContextFormer优于现有最先进基线方法，相比现有Transformer模型具有更高鲁棒性，在高曲率和过渡区域通过空间分布实现集中改进。

Conclusion: GContextFormer实现了无需地图依赖的意图对齐多模态预测，通过运动模式区分和邻居上下文调制实现可解释性，模块化架构支持跨域多模态推理任务的扩展性。

Abstract: Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.

</details>


### [135] [MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue Systems](https://arxiv.org/abs/2511.18926)
*Haifeng Jing,Yujie Hou,Junfei Liu,Rui Xie,alan Xu,Jinlong Ma,Qichun Deng*

Main category: cs.AI

TL;DR: 提出了情感陪伴对话系统(ECDs)的正式定义，并基于"能力层-任务层-数据层-方法层"设计原则开发了首个ECDs评估基准MoodBench 1.0，通过评估30个主流模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，对话系统正从信息工具转向情感伴侣，但ECDs领域缺乏明确定义和系统评估标准。

Method: 基于理论定义和"能力层-任务层-数据层-方法层"设计原则，设计并实现了首个ECDs评估基准MoodBench 1.0。

Result: MoodBench 1.0具有优异的判别效度，能有效量化模型间情感陪伴能力的差异，并揭示了当前模型在深度情感陪伴方面的不足。

Conclusion: 该基准为未来技术优化提供了指导，显著帮助开发者提升ECDs的用户体验。

Abstract: With the rapid development of Large Language Models, dialogue systems are shifting from information tools to emotional companions, heralding the era of Emotional Companionship Dialogue Systems (ECDs) that provide personalized emotional support for users. However, the field lacks clear definitions and systematic evaluation standards for ECDs. To address this, we first propose a definition of ECDs with formal descriptions. Then, based on this theory and the design principle of "Ability Layer-Task Layer (three level)-Data Layer-Method Layer", we design and implement the first ECD evaluation benchmark - MoodBench 1.0. Through extensive evaluations of 30 mainstream models, we demonstrate that MoodBench 1.0 has excellent discriminant validity and can effectively quantify the differences in emotional companionship abilities among models. Furthermore, the results reveal current models' shortcomings in deep emotional companionship, guiding future technological optimization and significantly aiding developers in enhancing ECDs' user experience.

</details>


### [136] [Active Inference is a Subtype of Variational Inference](https://arxiv.org/abs/2511.18955)
*Wouter W. L. Nuijten,Mykola Lukashchuk*

Main category: cs.AI

TL;DR: 本文提出了一种新的消息传递方案，将主动推理重新表述为变分推断，解决了EFE最小化的计算可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 主动推理通过期望自由能最小化统一了决策中的利用和探索，但计算成本高昂限制了其可扩展性。

Method: 基于将EFE最小化重新表述为变分推断的理论，开发了新的消息传递方案，用于因子状态MDP中的主动推理。

Result: 该方法克服了高维规划中的难处理性问题，实现了可扩展的主动推理。

Conclusion: 通过形式化统一主动推理和规划即推断，提出的消息传递方案为不确定性下的自动化决策提供了可扩展的解决方案。

Abstract: Automated decision-making under uncertainty requires balancing exploitation and exploration. Classical methods treat these separately using heuristics, while Active Inference unifies them through Expected Free Energy (EFE) minimization. However, EFE minimization is computationally expensive, limiting scalability. We build on recent theory recasting EFE minimization as variational inference, formally unifying it with Planning-as-Inference and showing the epistemic drive as a unique entropic contribution. Our main contribution is a novel message-passing scheme for this unified objective, enabling scalable Active Inference in factored-state MDPs and overcoming high-dimensional planning intractability.

</details>


### [137] [Synthesizing Visual Concepts as Vision-Language Programs](https://arxiv.org/abs/2511.18964)
*Antonia Wüst,Wolfgang Stammer,Hikaru Shindo,Lukas Helff,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.AI

TL;DR: VLP结合视觉语言模型的感知灵活性和程序合成的系统推理能力，通过生成结构化视觉描述并编译成神经符号程序来解决视觉推理任务。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务上表现良好，但在系统视觉推理任务中经常产生不一致或不合逻辑的输出。神经符号方法虽然能推导可解释的逻辑规则，但依赖僵化的领域特定感知模块。

Method: 提出Vision-Language Programs (VLP)，利用视觉语言模型生成结构化视觉描述，然后将其编译成神经符号程序。这些程序直接在图像上执行，保持与任务约束的一致性。

Result: 在合成和真实世界数据集上的实验表明，VLP在需要复杂逻辑推理的任务上优于直接提示和结构化提示方法。

Conclusion: VLP通过将感知与推理分离，结合了视觉语言模型的灵活性和程序合成的系统性，提供了可解释的解释并易于缓解捷径问题。

Abstract: Vision-Language models (VLMs) achieve strong performance on multimodal tasks but often fail at systematic visual reasoning tasks, leading to inconsistent or illogical outputs. Neuro-symbolic methods promise to address this by inducing interpretable logical rules, though they exploit rigid, domain-specific perception modules. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of VLMs with systematic reasoning of program synthesis. Rather than embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. The resulting programs execute directly on images, remain consistent with task constraints, and provide human-interpretable explanations that enable easy shortcut mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and structured prompting, particularly on tasks requiring complex logical reasoning.

</details>


### [138] [LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models](https://arxiv.org/abs/2511.18966)
*Muhammad Usman Shahid,Chuadhry Mujeeb Ahmed,Rajiv Ranjan*

Main category: cs.AI

TL;DR: 研究发现LLM生成的C/C++代码存在大量安全漏洞，通过静态分析识别出众多CWE弱点，开发者需谨慎使用AI生成代码。


<details>
  <summary>Details</summary>
Motivation: 由于研究表明LLM生成的代码常包含漏洞且缺乏防御性编程结构，需要系统评估LLM生成代码的安全性，特别是在C/C++环境下的安全风险。

Method: 使用CWE分类已知漏洞并映射到CVE评估严重性，采用10种不同LLM生成代码，通过静态分析工具分析输出结果。

Result: AI生成代码中存在的CWE数量令人担忧，静态分析显示代码安全状况堪忧。

Conclusion: 开发者在使用LLM生成代码时需要保持警惕，本研究为推进自动化代码生成和该领域进一步研究提供了重要见解。

Abstract: The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain.

</details>


### [139] [Introducing Visual Scenes and Reasoning: A More Realistic Benchmark for Spoken Language Understanding](https://arxiv.org/abs/2511.19005)
*Di Wu,Liting Jiang,Ruiyu Fang,Bianjing,Hongyan Xie,Haoxiang Su,Hao Huang,Zhongjiang He,Shuangyong Song,Xuelong Li*

Main category: cs.AI

TL;DR: VRSLU是一个新的口语理解数据集，整合了视觉图像和显式推理，解决了现有数据集在上下文表示和推理过程方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有SLU数据集在表示真实场景方面存在不足：上下文感知使用过于理想化的one-hot向量表示，模型仅预测意图和槽标签而忽略了可提升性能和可解释性的推理过程。

Method: 使用GPT-4o和FLUX.1-dev生成反映用户环境和状态的图像，并通过人工验证确保质量；使用GPT-4o生成预测标签的解释，经人工标注者精炼确保准确性和连贯性；提出LR-Instruct指令模板，先预测标签再生成相应推理。

Result: 实验结果证实了整合视觉信息的有效性，并突显了显式推理在推进SLU研究中的潜力。

Conclusion: VRSLU数据集通过整合视觉和推理元素，解决了现有SLU数据集的局限性，为推进SLU向实际应用发展提供了有前景的方向。

Abstract: Spoken Language Understanding (SLU) consists of two sub-tasks: intent detection (ID) and slot filling (SF). Given its broad range of real-world applications, enhancing SLU for practical deployment is increasingly critical. Profile-based SLU addresses ambiguous user utterances by incorporating context awareness (CA), user profiles (UP), and knowledge graphs (KG) to support disambiguation, thereby advancing SLU research toward real-world applicability. However, existing SLU datasets still fall short in representing real-world scenarios. Specifically, (1) CA uses one-hot vectors for representation, which is overly idealized, and (2) models typically focuses solely on predicting intents and slot labels, neglecting the reasoning process that could enhance performance and interpretability. To overcome these limitations, we introduce VRSLU, a novel SLU dataset that integrates both Visual images and explicit Reasoning. For over-idealized CA, we use GPT-4o and FLUX.1-dev to generate images reflecting users' environments and statuses, followed by human verification to ensure quality. For reasoning, GPT-4o is employed to generate explanations for predicted labels, which are then refined by human annotators to ensure accuracy and coherence. Additionally, we propose an instructional template, LR-Instruct, which first predicts labels and then generates corresponding reasoning. This two-step approach helps mitigate the influence of reasoning bias on label prediction. Experimental results confirm the effectiveness of incorporating visual information and highlight the promise of explicit reasoning in advancing SLU.

</details>


### [140] [Extracting Robust Register Automata from Neural Networks over Data Sequences](https://arxiv.org/abs/2511.19100)
*Chih-Duo Hong,Hongjian Jiang,Anthony W. Lin,Oliver Markgraf,Julian Parsert,Tony Tan*

Main category: cs.AI

TL;DR: 提出了一种从黑盒神经网络中提取确定性寄存器自动机(DRA)的框架，用于合成可解释的替代模型并进行符号分析，特别适用于连续输入域。


<details>
  <summary>Details</summary>
Motivation: 现有自动机提取技术假设有限输入字母表，无法直接应用于连续域的数据序列。需要一种能够处理数值比较的自动机模型来桥接神经网络可解释性和形式化推理。

Method: 开发了多项式时间鲁棒性检查器，结合被动和主动自动机学习算法，提取具有统计鲁棒性和等价性保证的DRA替代模型。

Result: 实验表明该框架能够可靠地学习准确的自动机，并支持对循环神经网络和Transformer架构进行原则性鲁棒性评估。

Conclusion: 鲁棒DRA提取有效地桥接了神经网络可解释性和形式化推理，无需白盒访问底层网络。

Abstract: Automata extraction is a method for synthesising interpretable surrogates for black-box neural models that can be analysed symbolically. Existing techniques assume a finite input alphabet, and thus are not directly applicable to data sequences drawn from continuous domains. We address this challenge with deterministic register automata (DRAs), which extend finite automata with registers that store and compare numeric values. Our main contribution is a framework for robust DRA extraction from black-box models: we develop a polynomial-time robustness checker for DRAs with a fixed number of registers, and combine it with passive and active automata learning algorithms. This combination yields surrogate DRAs with statistical robustness and equivalence guarantees. As a key application, we use the extracted automata to assess the robustness of neural networks: for a given sequence and distance metric, the DRA either certifies local robustness or produces a concrete counterexample. Experiments on recurrent neural networks and transformer architectures show that our framework reliably learns accurate automata and enables principled robustness evaluation. Overall, our results demonstrate that robust DRA extraction effectively bridges neural network interpretability and formal reasoning without requiring white-box access to the underlying network.

</details>


### [141] [AI Consciousness and Existential Risk](https://arxiv.org/abs/2511.19115)
*Rufin VanRullen*

Main category: cs.AI

TL;DR: 论文澄清了AI意识与存在风险之间的混淆，指出智能而非意识才是存在风险的直接预测因素，但意识在某些间接情境中可能影响风险水平。


<details>
  <summary>Details</summary>
Motivation: 由于AI意识与存在风险经常被混为一谈，作者旨在澄清这种混淆，帮助研究人员和政策制定者关注真正关键的问题。

Method: 通过理论分析区分意识与智能的概念，探讨两者在AI存在风险中的不同作用和关联方式。

Result: 智能是AI存在风险的直接预测因素，而意识本身并不直接导致风险，但可能在特定间接情境中影响风险水平。

Conclusion: 区分意识与智能对于AI安全研究和政策制定至关重要，应重点关注智能而非意识作为存在风险的主要考量因素。

Abstract: In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues.

</details>


### [142] [EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction](https://arxiv.org/abs/2511.19155)
*Xihe Qiu,Gengchen Ma,Haoyu Wang,Chen Zhan,Xiaoyu Tan,Shuo Li*

Main category: cs.AI

TL;DR: 提出了EEG-VLM，一个分层视觉语言框架，通过多级特征对齐和视觉增强的语言引导推理，用于可解释的基于EEG的睡眠阶段分类。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖先验知识和手工特征，现有深度学习模型难以同时捕捉细粒度时频模式并实现临床可解释性。视觉语言模型在医学领域有进展，但在处理EEG信号时性能受限。

Method: 使用专门的视觉增强模块从中间层特征构建高级视觉token，通过多级对齐机制与低层CLIP特征对齐，并采用Chain-of-Thought推理策略将复杂医学推理分解为可解释的逻辑步骤。

Result: 实验结果表明，该方法显著提高了VLMs在基于EEG的睡眠阶段分类中的准确性和可解释性。

Conclusion: 该方法在临床环境中展示了自动化和可解释EEG分析的潜力。

Abstract: Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.

</details>


### [143] [SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting](https://arxiv.org/abs/2511.19256)
*Hang Ding,Xue Wang,Tian Zhou,Tao Yao*

Main category: cs.AI

TL;DR: SimDiff是一个用于时间序列点预测的单阶段端到端扩散模型框架，通过统一的Transformer网络同时作为去噪器和预测器，无需外部预训练模型，实现了最先进的点估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列预测中主要关注概率预测，但在点估计性能上不如回归方法，存在难以跟踪分布变化、平衡输出多样性与稳定性等问题。

Method: 使用单一统一的Transformer网络作为去噪器和预测器，通过多重推理集成利用内在输出多样性，采用归一化独立性和均值中位数估计器增强适应性和稳定性。

Result: 大量实验表明，SimDiff在时间序列点预测方面显著优于现有方法。

Conclusion: SimDiff通过创新的单阶段框架设计，成功解决了扩散模型在点估计中的局限性，实现了优越的点预测性能。

Abstract: Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.
  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.

</details>


### [144] [Psychometric Tests for AI Agents and Their Moduli Space](https://arxiv.org/abs/2511.19262)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 本文开发了AI智能体心理测量测试电池的模理论框架，将其与先前开发的AAI分数明确关联，定义了AAI泛函、认知核心等概念，并描述了评估保持对称性下的电池不变量。


<details>
  <summary>Details</summary>
Motivation: 为AI智能体的心理测量测试电池建立严格的数学框架，将先前开发的AAI分数置于更一般的理论基础上，并研究电池在评估保持对称性下的结构性质。

Method: 1) 精确定义电池上的AAI泛函并设定合理性公理；2) 证明先前定义的复合指数是AAI泛函的特例；3) 引入智能体相对于电池的认知核心概念，定义AAI_core分数；4) 描述评估保持对称性下的电池不变量。

Result: 建立了AAI分数的模理论框架，证明了AAI-Index是AAI泛函的特例，定义了认知核心和AAI_core分数，并描述了电池在对称变换下的不变量结构。

Conclusion: 该工作为AI智能体的心理测量评估提供了坚实的数学基础，通过模理论框架统一了先前的AAI分数概念，并为研究不同测试电池间的等价关系提供了理论工具。

Abstract: We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.

</details>


### [145] [AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning](https://arxiv.org/abs/2511.19304)
*Jiayi Zhang,Yiran Peng,Fanqi Kong,Yang Cheng,Yifan Wu,Zhaoyang Yu,Jinyu Xiang,Jianhao Ruan,Jinlin Wang,Maojia Song,HongZhang Liu,Xiangru Tang,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: 提出了AutoEnv框架和AutoEnv-36数据集，用于研究智能体在异构环境中的跨环境学习能力，发现固定学习方法在环境数量增加时效果下降，需要环境自适应的方法选择。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常在单一固定环境中自我进化，缺乏对跨异构环境学习能力的系统评估，需要标准化的测试平台来研究智能体的跨环境泛化能力。

Method: 1) 开发AutoEnv框架，将环境分解为转移、观察和奖励的分布，低成本生成异构环境；2) 构建AutoEnv-36数据集；3) 将智能体学习形式化为选择、优化、评估三个阶段的组件中心过程；4) 设计8种学习方法并进行评估。

Result: 7个语言模型在AutoEnv-36上仅获得12-49%的标准化奖励，显示任务难度；固定学习方法在环境数量增加时效果快速下降；环境自适应方法选择能显著提升性能，但随着方法空间扩大呈现收益递减。

Conclusion: 固定学习方法无法扩展到异构环境，需要环境自适应的学习策略，AutoEnv和AutoEnv-36为研究跨环境智能体学习提供了重要测试平台。

Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.

</details>


### [146] [PRInTS: Reward Modeling for Long-Horizon Information Seeking](https://arxiv.org/abs/2511.19314)
*Jaewoo Lee,Archiki Prasad,Justin Chih-Yao Chen,Zaid Khan,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.AI

TL;DR: PRInTS是一个生成式过程奖励模型，通过密集评分和轨迹摘要来提升AI代理在多步信息搜索任务中的表现，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型(PRMs)设计用于短推理和二元判断，无法捕捉信息搜索步骤中的丰富维度（如工具交互、工具输出推理），也无法处理长视野任务中快速增长的上文。

Method: 提出PRInTS模型，具备双重能力：(1)基于多个步骤质量维度的密集评分；(2)轨迹摘要，压缩增长的上文同时保留步骤评估所需的关键信息。

Result: 在FRAMES、GAIA（1-3级）和WebWalkerQA（简单-困难）基准测试中，使用PRInTS的最佳n采样显著提升了开源模型和专业代理的信息搜索能力，匹配或超越前沿模型性能，且优于其他强奖励建模基线。

Conclusion: PRInTS通过密集评分和轨迹摘要有效解决了长视野信息搜索任务的挑战，显著提升了AI代理的性能，为过程奖励模型的发展提供了新方向。

Abstract: Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [147] [Practical Machine Learning for Aphasic Discourse Analysis](https://arxiv.org/abs/2511.17553)
*Jason M. Pittman,Anton Phillips,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.LG

TL;DR: 本研究评估了五种机器学习模型在失语症患者图片描述任务中自动识别正确信息单元(CIU)的能力，发现模型在区分词语和非词语方面表现优异，但在识别CIU方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: CIU分析是评估失语症患者语言能力的重要方法，但临床应用中由于需要人工编码和分析而受限。机器学习技术有望自动化这一过程，减轻言语病理学家的工作负担。

Method: 使用五种监督机器学习模型，基于失语症患者的人类编码转录本及其对应的词语和CIU数据进行训练，评估模型在图片描述任务中识别CIU的可靠性。

Result: 词语vs非词语识别准确率达到0.995，AUC范围0.914-0.995；CIU vs 非CIU识别表现差异较大，k-NN模型准确率最高(0.824)，AUC第二高(0.787)。

Conclusion: 监督机器学习模型能有效区分词语和非词语，但准确识别CIU仍具有挑战性，需要进一步改进模型性能。

Abstract: Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging.

</details>


### [148] [Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks](https://arxiv.org/abs/2511.17564)
*Guilherme Grancho D. Fernandes,Marco A. Barroca,Mateus dos Santos,Rafael S. Oliveira*

Main category: cs.LG

TL;DR: 使用双向LSTM神经网络对PLAsTiCC数据集中的瞬变天体光变曲线进行分类，将14个类别重组为5个广义类别以解决类别不平衡问题。模型在S-Like和Periodic类别表现良好，但在Fast和Long类别表现较差，且难以区分Periodic和Non-Periodic对象。


<details>
  <summary>Details</summary>
Motivation: 解决瞬变天体光变曲线分类中的类别不平衡问题，并评估双向LSTM网络在此任务中的性能。

Method: 采用双向LSTM神经网络，通过填充、时间重缩放和通量归一化进行预处理，使用掩码层处理变长序列，在19,920个对象的测试集上进行评估。

Result: S-Like和Periodic类别的ROC AUC分别达到0.95和0.99，但Fast和Long类别表现较差（Long类ROC AUC仅0.68）。在部分光变曲线数据上性能显著下降，误分类偏向S-Like类。

Conclusion: 类别不平衡和有限的时间信息是主要限制因素，建议采用类别平衡策略和专注于检测时刻的预处理技术来提高性能。

Abstract: This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.

</details>


### [149] [Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs](https://arxiv.org/abs/2511.17566)
*Shuaiyu Xie,Hanbin He,Jian Wang,Bing Li*

Main category: cs.LG

TL;DR: 提出了CCLH框架，通过级联条件学习和异构超图建模来解决微服务系统中根因定位和故障类型识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法存在两个关键问题：1）联合学习范式忽略了任务间的因果依赖关系；2）主要关注实例间的点对点关系，忽视了由部署配置和负载均衡引起的群体影响。

Method: CCLH框架采用级联条件学习来协调诊断任务，提供三级分类来描述实例间的群体影响，并使用异构超图来建模这些关系以模拟故障传播。

Result: 在三个微服务基准数据集上的广泛实验表明，CCLH在根因定位和故障类型识别方面均优于现有最先进方法。

Conclusion: CCLH通过级联条件学习和异构超图建模，有效解决了微服务系统中根因分析的挑战，显著提升了诊断性能。

Abstract: Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.

</details>


### [150] [Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization](https://arxiv.org/abs/2511.17568)
*Le Xu,Jiayu Chen*

Main category: cs.LG

TL;DR: 本文首次将Sharpness-Aware Minimization (SAM)作为通用插件优化器应用于离线强化学习，以解决数据损坏导致的泛化性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习对现实世界数据损坏非常脆弱，即使鲁棒算法在挑战性观测和混合损坏下也会失败，这源于数据损坏在损失景观中产生尖锐最小值。

Method: 将SAM集成到强基线算法中：IQL（在此设置中表现最佳的离线RL算法）和RIQL（专门为数据损坏鲁棒性设计的算法），在D4RL基准上评估随机和对抗性损坏。

Result: SAM增强方法始终且显著优于原始基线，奖励表面可视化证实SAM找到更平滑的解。

Conclusion: SAM通过寻找更平坦的最小值，有效提高了离线RL代理的鲁棒性。

Abstract: Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.

</details>


### [151] [Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis](https://arxiv.org/abs/2511.17573)
*Michael J. Bommarito*

Main category: cs.LG

TL;DR: 提出了Binary BPE分词器系列，专门用于二进制分析，通过字节对编码在多种平台和架构的可执行文件上训练，显著提高了序列模型处理二进制内容的效率。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型在处理二进制分析时受限于字节级分词，原始字节浪费transformer等神经网络架构的上下文窗口容量，且许多现有文本分词器无法处理任意0x00-0xFF序列。

Method: 开发了跨平台Byte Pair Encoding分词器，在包含Linux、Windows、macOS、Android和恶意软件来源的大型二进制语料库上训练，提供4K到64K词汇量的分词器。

Result: Binary BPE分词器发现可解释模式（ELF/PE头、指令序列、跨平台字符串），每个token实现多字节压缩，在未压缩可执行文件上相比原始字节，固定长度transformer上下文窗口可容纳2-3倍更多二进制内容。

Conclusion: Binary BPE分词器为二进制分析提供了高效的分词解决方案，支持从资源受限边缘设备到高吞吐量数据中心的各种部署场景，已在HuggingFace上开源发布。

Abstract: Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools.

</details>


### [152] [Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation](https://arxiv.org/abs/2511.17577)
*Fengming Yu,Qingyu Meng,Haiwei Pan,Kejia Zhang*

Main category: cs.LG

TL;DR: 提出了一种轻量级优化方法，结合动态注意力头剪枝和知识蒸馏，在数学推理任务中显著提升大语言模型的效率，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理等复杂任务中表现出色，但计算和存储成本高昂，阻碍实际部署。需要找到既能保持性能又能提升效率的轻量化方法。

Method: 动态评估多头注意力机制中每个注意力头的重要性（结合权重范数和熵），实时剪枝冗余头以减少计算开销，并通过知识蒸馏将原始模型信息迁移到剪枝后的学生模型中。

Result: 在Math23k数据集上，30%剪枝率下：参数减少18.7%，推理速度提升27.5%，FLOPs减少19.3%，准确率仅下降0.7%（从84.4%到83.7%）。ASDiv-A数据集上也验证了有效性。

Conclusion: 该方法在数学推理任务中实现了显著的效率提升，同时保持了强大的推理性能，为大语言模型的高效部署提供了实用解决方案。

Abstract: With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.

</details>


### [153] [Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation](https://arxiv.org/abs/2511.17579)
*Hefei Xu,Le Wu,Chen Cheng,Hao Liu*

Main category: cs.LG

TL;DR: 提出了一个名为多价值对齐（MVA）的新框架，通过最小化不同人类价值之间的互信息来缓解参数干扰，并使用价值外推策略有效探索帕累托前沿，从而构建具有不同价值偏好的LLMs集合。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，将其与人类价值观对齐以确保安全和道德已成为关键挑战。特别是在需要考虑和平衡多个可能冲突的人类价值观时，现有对齐方法（如RLHF和DPO）存在不稳定、效率低且无法有效处理价值冲突的问题。

Method: MVA框架通过最小化不同人类价值之间的互信息来减轻参数干扰，并提出价值外推策略来有效探索帕累托前沿，构建具有多样化价值偏好的LLMs集合。

Result: 大量实验证明，MVA在将LLMs与多个人类价值观对齐方面始终优于现有基线方法。

Conclusion: MVA框架有效解决了多价值对齐中的挑战，通过缓解参数干扰和探索帕累托前沿，实现了更好的多价值平衡对齐效果。

Abstract: With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.
  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values.

</details>


### [154] [EgoCogNav: Cognition-aware Human Egocentric Navigation](https://arxiv.org/abs/2511.17581)
*Zhiwen Qiu,Ziang Liu,Wenqian Niu,Tapomayukh Bhattacharjee,Saleh Kalantari*

Main category: cs.LG

TL;DR: 提出了EgoCogNav多模态自我中心导航框架，预测感知路径不确定性作为潜在状态，并融合场景特征与感官线索联合预测轨迹和头部运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注完全观察场景中的运动预测，往往忽略人类如何感受和响应空间的人类因素。

Method: 提出多模态自我中心导航框架，预测感知路径不确定性作为潜在状态，通过融合场景特征与感官线索来联合预测轨迹和头部运动。

Result: EgoCogNav学习到的感知不确定性与人类行为（如扫描、犹豫、回溯）高度相关，并能泛化到未见过的环境。

Conclusion: 该工作推进了对人类导航中认知和体验因素的理解，并为安全社交导航和有效辅助寻路提供了基础。

Abstract: Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.

</details>


### [155] [GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.17582)
*Jie Ou,Shuaihong Jiang,Yingjun Du,Cees G. M. Snoek*

Main category: cs.LG

TL;DR: GateRA是一种参数高效微调框架，通过token感知的调制机制动态调整PEFT更新的强度，实现选择性、token级别的适应，在多个常识推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法对所有token应用静态、输入无关的更新，忽视了不同输入的重要性和难度差异，可能导致对简单内容的过拟合或对信息丰富区域适应不足。

Method: 在标准PEFT分支中引入自适应门控机制，实现token级别的动态适应；提出基于熵的正则化鼓励接近二元的门控决策；理论分析显示GateRA在PEFT路径上产生软梯度掩码效应。

Result: 在多个常识推理基准测试中，GateRA始终优于或匹配先前的PEFT方法；经验可视化显示GateRA自动抑制冗余预填充token的更新，同时在解码阶段强调适应。

Conclusion: GateRA通过token感知调制实现了更智能的参数高效微调，在保持预训练知识的同时专注于具有挑战性的情况，提供了解释性强且稀疏的适应模式。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.

</details>


### [156] [Learning Straight Flows: Variational Flow Matching for Efficient Generation](https://arxiv.org/abs/2511.17583)
*Chenrui Ma,Xi Xiao,Tianyang Wang,Xiao Wang,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了S-VFM方法，通过引入变分潜码来强制轨迹直线化，解决Flow Matching中一步生成的局限性问题


<details>
  <summary>Details</summary>
Motivation: Flow Matching依赖学习的弯曲轨迹，难以实现一步生成。现有方法存在离散近似误差、训练不稳定和收敛困难等问题

Method: 在Flow Matching框架中集成变分潜码表示"生成概览"，显式强制轨迹直线化，产生线性生成路径

Result: 在三个挑战性基准测试中取得竞争性性能，在训练和推理效率上相比现有方法具有优势

Conclusion: S-VFM通过变分潜码有效解决了Flow Matching的轨迹弯曲问题，实现了高效的一步生成

Abstract: Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \textbf{S}traight \textbf{V}ariational \textbf{F}low \textbf{M}atching (\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.

</details>


### [157] [LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning](https://arxiv.org/abs/2511.17584)
*Haoyan Xu,Ruizhi Qian,Zhengtao Yao,Ziyi Liu,Li Li,Yuqi Li,Yanshu Li,Wenqing Zheng,Daniele Rosa,Daniel Barcklow,Senthil Kumar,Jieyu Zhao,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了TAG-AD基准数据集，用于文本属性图上的异常节点检测，利用LLM生成语义连贯但上下文不一致的异常文本，并提出了RAG辅助的零样本LLM异常检测框架。


<details>
  <summary>Details</summary>
Motivation: 文本属性图上的异常检测在欺诈检测、入侵监控等应用中很重要，但由于缺乏标准化基准数据集而研究不足。

Method: 使用LLM在原始文本空间中生成真实异常节点文本；提出RAG辅助的零样本LLM异常检测框架，构建全局异常知识库并提炼为可重用分析框架。

Result: 实验结果显示LLM在检测上下文异常方面特别有效，而GNN方法在结构异常检测方面仍占优势；RAG辅助提示能达到与人工设计提示相当的性能。

Conclusion: LLM和GNN方法在异常检测中各有优势，RAG辅助的零样本LLM框架具有实用价值，无需手动提示工程即可达到良好性能。

Abstract: Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.
  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework.

</details>


### [158] [PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis](https://arxiv.org/abs/2511.17585)
*Kang He,Boyu Chen,Yuzhe Ding,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.LG

TL;DR: 提出了PaSE框架，通过原型对齐校准和Shapley优化均衡来解决多模态情感分析中的模态竞争问题，提升模态间协作性能


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析中，优势模态往往会压制弱势模态，导致模态竞争现象，影响整体性能

Method: 采用原型引导的校准学习来精炼单模态表示，通过熵最优传输机制确保语义一致性；引入双阶段优化策略，包括原型门控融合模块和基于Shapley值的梯度调制

Result: 在IEMOCAP、MOSI和MOSEI数据集上的广泛实验表明，PaSE实现了优越性能并有效缓解了模态竞争

Conclusion: PaSE框架能够有效增强多模态协作，同时明确缓解模态竞争问题

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.

</details>


### [159] [Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection](https://arxiv.org/abs/2511.17587)
*Yuxuan Hu,Jian Chen,Yuhao Wang,Zixuan Li,Jing Xiong,Pengyue Jia,Wei Wang,Chengming Li,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出EIGML框架，首次联合建模情感和意图，通过双级对比框架和多模态融合模块，显著提升贴纸选择准确性。


<details>
  <summary>Details</summary>
Motivation: 现有贴纸响应选择方法通常依赖语义匹配，并将情感和意图分开建模，当情感和意图不一致时会导致不匹配问题。

Method: EIGML框架包含双级对比框架（模态内和模态间对齐）和意图-情感引导的多模态融合模块（情感引导意图知识选择、意图-情感引导注意力融合、相似度调整匹配机制）。

Result: 在两个公开SRS数据集上的实验表明，EIGML始终优于最先进的基线方法，实现了更高的准确率和更好的情感意图特征理解。

Conclusion: EIGML通过联合建模情感和意图，有效减少孤立建模带来的偏差，显著提升了贴纸选择性能。

Abstract: Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials.

</details>


### [160] [Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection](https://arxiv.org/abs/2511.17589)
*Sören Dréano,Derek Molloy,Noel Murphy*

Main category: cs.LG

TL;DR: Llamazip是一种基于LLaMA3语言模型预测能力的无损文本压缩算法，通过仅存储模型无法预测的token来实现显著数据压缩，同时还能识别文档是否属于语言模型训练数据。


<details>
  <summary>Details</summary>
Motivation: 开发一种利用语言模型预测能力的高效无损文本压缩方法，并解决语言模型训练数据来源识别的问题，以应对数据来源、知识产权和训练透明度等关键问题。

Method: 基于LLaMA3语言模型的预测能力，只存储模型无法正确预测的token，通过量化技术和上下文窗口大小优化来平衡压缩比和计算需求。

Result: 实现了显著的数据压缩效果，同时能够识别文档是否属于语言模型的训练数据集，为数据溯源提供了新方法。

Conclusion: Llamazip不仅展示了语言模型在文本压缩方面的应用潜力，还提供了一种识别训练数据来源的有效工具，有助于提升语言模型训练的透明度和问责制。

Abstract: This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.

</details>


### [161] [SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data](https://arxiv.org/abs/2511.17590)
*Ke Yu,Shigeru Ishikura,Yukari Usukura,Yuki Shigoku,Teruaki Hayashi*

Main category: cs.LG

TL;DR: 提出SHAP距离作为评估合成表格数据语义保真度的新指标，通过比较真实数据和合成数据训练的模型的特征重要性差异来识别标准统计指标忽略的语义差异。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据评估方法主要关注分布相似性和预测性能，但无法评估模型在合成数据上的推理模式是否与真实数据一致，即语义保真度问题。

Method: 引入SHAP距离，定义为从真实和合成数据集训练的模型导出的全局SHAP归因向量之间的余弦距离。在医疗记录、企业发票和电信流失等多个领域数据集上进行验证。

Result: SHAP距离能可靠识别标准统计和预测指标忽略的语义差异，特别是特征重要性偏移和尾部效应不足的问题，而KL散度和TSTR准确率无法检测这些差异。

Conclusion: SHAP距离是审计合成表格数据语义保真度的实用判别工具，建议将基于归因的评估整合到未来的基准测试流程中。

Abstract: Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines.

</details>


### [162] [Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI](https://arxiv.org/abs/2511.17593)
*Saicharan Kolluru*

Main category: cs.LG

TL;DR: 对vLLM和HuggingFace TGI两个开源LLM服务框架的实证评估，vLLM在高并发场景下吞吐量比TGI高24倍，TGI在单用户交互场景下延迟更低。


<details>
  <summary>Details</summary>
Motivation: 生产环境中部署大型语言模型需要高效的推理服务系统来平衡吞吐量、延迟和资源利用率。

Method: 使用LLaMA-2模型（7B到70B参数）对vLLM和TGI进行多维度基准测试，包括吞吐量性能、端到端延迟、GPU内存利用率和可扩展性特征。

Result: vLLM通过其新颖的PagedAttention机制在高并发工作负载下实现比TGI高24倍的吞吐量，而TGI在交互式单用户场景下表现出更低的尾部延迟。

Conclusion: 框架选择应基于具体用例需求：vLLM在高吞吐量批处理场景中表现优异，TGI更适合中等并发下的延迟敏感交互应用。

Abstract: The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.

</details>


### [163] [AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention](https://arxiv.org/abs/2511.17594)
*Aleksandar Stankovic*

Main category: cs.LG

TL;DR: AutoSAGE是一个输入感知的CUDA调度器，通过轻量级估计和微探针为稀疏GNN聚合选择最佳分块和映射策略，在特定条件下实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏GNN聚合操作（CSR SpMM/SDDMM）的性能受节点度分布、特征宽度和GPU微架构的显著影响，需要针对不同输入进行优化。

Method: 开发AutoSAGE调度器，使用轻量级估计结合设备微探针为每个输入选择最佳分块和映射策略，包含回退机制和持久化缓存。

Result: 在Reddit和OGBN-Products数据集上，在带宽受限的特征宽度下与供应商基线相当，在小宽度下获得性能提升；在合成稀疏度和偏斜压力测试中实现最高4.7倍的内核级加速。

Conclusion: AutoSAGE能够有效优化稀疏GNN聚合操作，提供了可复现的CUDA实现和工具链。

Abstract: Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.

</details>


### [164] [Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design](https://arxiv.org/abs/2511.17595)
*Markus D. Solbach,John K. Tsotsos*

Main category: cs.LG

TL;DR: 该研究探索强化学习在3D视觉空间Same-Different任务中的应用，发现标准方法面临挑战，但通过基于人类实验设计的课程学习实现了有效学习。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在更复杂、非结构化问题领域中的潜力，验证其在展现智能行为方面的进展。

Method: 使用PPO、行为克隆和模仿学习等先进方法，并基于真实人类实验发现设计课程学习策略。

Result: 标准RL方法在直接学习最优策略时遇到困难，但通过精心设计的课程学习实现了成功学习。

Conclusion: 课程学习为强化学习在复杂视觉空间任务中提供了有前景的途径，基于人类认知过程的设计策略对RL性能提升至关重要。

Abstract: Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.
  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment.

</details>


### [165] [Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning](https://arxiv.org/abs/2511.17598)
*Zhizuo Chen,Theodore T. Allen*

Main category: cs.LG

TL;DR: 提出了非平稳和变折扣MDP（NVMDP）框架，能够处理非平稳环境和时变折扣率，包含传统MDP作为特例，并提供了理论分析和算法扩展。


<details>
  <summary>Details</summary>
Motivation: 传统MDP算法在非平稳环境中面临挑战，无限时域方法不适用于有限时域任务，需要更灵活的框架来处理这些限制。

Method: 引入NVMDP框架，允许折扣率随时间变化，扩展了动态规划和Q-learning算法，并提供了理论收敛证明和函数逼近方法。

Result: 在非平稳网格世界环境中的实验表明，NVMDP算法能够成功恢复最优轨迹，而原始Q-learning失败。

Conclusion: NVMDP提供了一个理论严谨且实际有效的强化学习框架，只需微小算法修改就能稳健处理非平稳性和显式最优策略塑造。

Abstract: Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.

</details>


### [166] [From Projection to Prediction: Beyond Logits for Scalable Language Models](https://arxiv.org/abs/2511.17599)
*Jianbing Dong,Jianbin Chang*

Main category: cs.LG

TL;DR: 提出了一种将输出投影和损失预测集成到单个操作中的新方法，避免了显式logits张量化，减少了内存使用和带宽压力。


<details>
  <summary>Details</summary>
Motivation: 传统的两阶段训练流程（线性投影到词汇logits，然后计算交叉熵损失）会产生巨大的中间logits张量，导致显著的内存占用和带宽消耗，限制了可扩展性和训练吞吐量。

Method: 通过直接从隐藏状态和目标标记计算损失，绕过了显式logits张量化，将输出投影和损失预测集成到单个操作中。

Result: 在LLM训练实验中，该方法相比标准两阶段流程实现了显著的内存节省和可测量的加速，支持更大的批处理大小和更长的序列而不牺牲准确性。

Conclusion: 重新思考投影和预测之间的边界具有显著优势，为高效的LLM训练提供了实用的系统优化方案。

Abstract: Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.
  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.

</details>


### [167] [Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts](https://arxiv.org/abs/2511.17601)
*Luyang Fang,Tao Wang,Ping Ma,Xiaoming Zhai*

Main category: cs.LG

TL;DR: UniMoE-Guided是一种知识蒸馏的多任务混合专家方法，将多个任务特定大模型的知识转移到单个紧凑模型中，实现高效自动评分


<details>
  <summary>Details</summary>
Motivation: 解决自动评分中每个任务需要单独模型导致的资源消耗、存储和维护问题，为教育环境提供实用的可扩展解决方案

Method: 使用知识蒸馏将多个教师模型的专业知识转移到单一学生模型，包含共享编码器、门控MoE块和轻量级任务头，结合真实标签和教师指导进行训练

Result: 在9个科学推理任务上达到与任务特定模型相当的性能，存储需求比单独学生模型减少约6倍，比200亿参数教师模型减少87倍

Conclusion: 该方法为课堂和大规模评估系统提供了可扩展、可靠且资源高效的自动评分实用路径

Abstract: Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\sim$6$\times$ less storage than maintaining separate students, and $87\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.

</details>


### [168] [Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models](https://arxiv.org/abs/2511.17602)
*Sushant Mehta*

Main category: cs.LG

TL;DR: 提出分层污染检测框架，在四个层面检测合成数据对基准测试的污染：词元级、语义级、推理模式和性能悬崖检测，有效识别语义级污染。


<details>
  <summary>Details</summary>
Motivation: 现有污染检测方法只能识别词元级重叠，无法检测语义级污染，而基础模型越来越多地使用可能隐含编码基准知识的合成数据进行训练。

Method: 分层污染检测框架，包含四个检测层级：词元级、语义级、推理模式和性能悬崖检测。

Result: 在MMLU、GSM8K和HumanEval上的实验表明，语义级污染能规避现有方法(F1=0.17-0.49)，但我们的分层方法能有效检测(F1=0.76)，比最先进基线平均提升26.5%。

Conclusion: 该框架为从业者提供实用的审计管道工具，支持负责任地部署合成训练数据。

Abstract: Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data.

</details>


### [169] [BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis](https://arxiv.org/abs/2511.17604)
*Jiajun Ma,Yongchao Zhang,Chao Zhang,Zhao Lv,Shengbing Pei*

Main category: cs.LG

TL;DR: 提出了BrainHGT，一种分层图Transformer，模拟大脑从局部区域到全局社区的自然信息处理过程，通过长短程注意力编码器和先验引导聚类模块改进脑网络分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将大脑建模为平面网络，忽略了其模块化结构，且注意力机制将所有脑区连接同等对待，忽略了与距离相关的节点连接模式。大脑信息处理是一个涉及局部和长程交互的分层过程。

Method: 设计了新颖的长短程注意力编码器，使用并行路径处理密集局部交互和稀疏长程连接；设计了先验引导聚类模块，利用交叉注意力机制将脑区分组为功能社区，并利用神经解剖学先验指导聚类过程。

Result: 实验结果表明，该方法显著提高了疾病识别的性能，并能可靠地捕捉大脑的子功能模块，展示了其可解释性。

Conclusion: BrainHGT通过模拟大脑的分层信息处理机制，有效解决了现有方法的局限性，在脑网络分析中表现出优越的性能和生物合理性。

Abstract: Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability.

</details>


### [170] [Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification](https://arxiv.org/abs/2511.17605)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed*

Main category: cs.LG

TL;DR: 通过copula方法建模临床和基因组风险评分的联合分布，可以改善乳腺癌5年癌症特异性死亡的风险分层，比简单的线性组合方法更能识别预后最差的患者亚组。


<details>
  <summary>Details</summary>
Motivation: 临床和基因组模型通常使用简单的线性规则结合，没有考虑它们在极端情况下的相互关系，需要更准确地建模这种联合关系来改善风险分层。

Method: 使用METABRIC乳腺癌队列，训练随机森林和XGBoost等监督分类器，通过5折交叉验证获得风险评分，然后使用高斯、Clayton和Gumbel copula拟合临床和基因组评分的联合分布。

Result: 临床模型区分度良好(AUC 0.783)，基因组模型表现中等(AUC 0.681)。高斯copula最佳捕捉了联合分布，显示对称的中等强度正相关关系。基于此关系的患者分组显示，临床和基因组均为高风险的患者生存率最差。

Conclusion: copula融合方法在实际队列中有效，考虑评分间的依赖关系能更好地识别预后最差的患者亚组。

Abstract: Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.

</details>


### [171] [Energy-based Autoregressive Generation for Neural Population Dynamics](https://arxiv.org/abs/2511.17606)
*Ningling Ge,Sicheng Dai,Yu Zhu,Shan Yu*

Main category: cs.LG

TL;DR: 提出了基于能量的自回归生成框架EAG，用于高效生成具有真实神经群体动态的数据，在计算效率和生成质量方面优于现有方法，并展示了在神经工程中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决计算建模中计算效率与高保真建模之间的基本权衡问题，为神经科学研究和神经工程应用提供更有效的工具。

Method: 使用基于能量的变换器，通过严格适当评分规则在潜在空间中学习时间动态，实现高效的神经群体动态生成。

Result: 在合成Lorenz数据集和两个神经潜在基准数据集上，EAG实现了最先进的生成质量，计算效率显著提升，特别是相对于基于扩散的方法。

Conclusion: 基于能量的建模在神经群体动态建模中具有有效性，为神经科学研究和神经工程应用提供了有前景的解决方案。

Abstract: Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics.

</details>


### [172] [Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features](https://arxiv.org/abs/2511.17610)
*Leonardo Rossi,Bruno Rodrigues*

Main category: cs.LG

TL;DR: 提出一个针对铁人三项的合成数据生成框架，整合训练负荷、睡眠质量、压力和恢复状态等日常因素，通过机器学习模型实现高精度的运动损伤预测。


<details>
  <summary>Details</summary>
Motivation: 当前运动损伤预测方法主要依赖训练负荷指标，忽视了睡眠质量、压力和个体生活方式等对恢复和损伤风险有显著影响的关键因素。

Method: 开发了一个新颖的合成数据生成框架，生成生理上合理的运动员档案，模拟个性化的训练计划，并整合睡眠质量、压力水平和恢复状态等日常因素。评估了LASSO、随机森林和XGBoost等机器学习模型。

Result: 机器学习模型表现出高预测性能（AUC高达0.86），识别出睡眠障碍、心率变异性和压力是损伤风险的关键早期指标。

Conclusion: 这种基于可穿戴设备的方法不仅提高了损伤预测的准确性，还为克服现实世界数据限制提供了实用解决方案，为实现全面、情境感知的运动员监测提供了途径。

Abstract: Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility.
  We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring.

</details>


### [173] [AI-driven Generation of MALDI-TOF MS for Microbial Characterization](https://arxiv.org/abs/2511.17611)
*Lucía Schmidt-Santiago,David Rodríguez-Temporal,Carlos Sevilla-Salcedo,Vanessa Gómez-Verdejo*

Main category: cs.LG

TL;DR: 本研究使用深度生成模型（MALDIVAE、MALDIGAN、MALDIffusion）合成MALDI-TOF质谱数据，解决微生物学中数据稀缺问题，证明合成数据在统计和诊断性能上与真实数据相当。


<details>
  <summary>Details</summary>
Motivation: MALDI-TOF质谱技术在临床微生物学中应用广泛，但数据驱动的诊断模型发展受限于缺乏足够大、平衡和标准化的光谱数据集。

Method: 采用三种条件生成模型：变分自编码器(MALDIVAE)、生成对抗网络(MALDIGAN)和去噪扩散概率模型(MALDIffusion)，以物种标签为条件生成微生物光谱，并使用多种指标评估光谱保真度和多样性。

Result: 合成数据在统计和诊断上与真实测量相当，仅使用合成样本训练的分类器能达到与真实数据训练相似的性能。MALDIVAE在真实性、稳定性和效率之间达到最佳平衡。

Conclusion: 合成光谱能显著提高少数物种的分类准确性，有效缓解类别不平衡和领域不匹配问题，同时不损害生成数据的真实性。

Abstract: Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology.
  We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics.
  Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data.

</details>


### [174] [Tensor Gauge Flow Models](https://arxiv.org/abs/2511.17616)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: 提出了Tensor Gauge Flow Models，这是一种新的生成流模型，通过将高阶张量规范场纳入流方程，扩展了Gauge Flow Models和Higher Gauge Flow Models，能够编码更丰富的几何和规范理论结构。


<details>
  <summary>Details</summary>
Motivation: 现有的Gauge Flow Models和Higher Gauge Flow Models在表达数据中的几何和规范理论结构方面有限，需要更丰富的流动力学来提升生成性能。

Method: 通过将高阶张量规范场整合到流方程中，扩展了现有的规范流模型，从而能够编码更复杂的几何和规范理论结构。

Result: 在高斯混合模型上的实验表明，Tensor Gauge Flow Models相比标准和规范流基线模型，取得了更好的生成性能。

Conclusion: Tensor Gauge Flow Models通过引入高阶张量规范场，成功提升了生成流模型的表达能力，为编码复杂数据几何结构提供了有效方法。

Abstract: This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines.

</details>


### [175] [Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification](https://arxiv.org/abs/2511.17622)
*Weidao Chen,Yuxiao Yang,Yueming Wang*

Main category: cs.LG

TL;DR: NH-GCAT是一个神经生物学启发的分层图因果注意力网络，通过在不同空间尺度上显式建模抑郁症特异性机制，将神经科学领域知识与深度学习相结合，在抑郁症分类中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经影像数据的图神经网络方法主要是数据驱动的黑盒模型，缺乏神经生物学可解释性，无法有效捕捉抑郁症的复杂病理生理机制。

Method: 提出三个关键技术贡献：1）局部脑区水平的残差门控融合模块，整合BOLD动态和功能连接模式；2）多区域回路水平的分层回路编码方案；3）多回路网络水平的变分潜在因果注意力机制。

Result: 在REST-meta-MDD数据集上的留一站点交叉验证显示，NH-GCAT在抑郁症分类中达到样本量加权平均准确率73.3%和AUROC 76.4%的最先进性能。

Conclusion: NH-GCAT不仅实现了优异的分类性能，还提供了神经生物学上有意义的解释，成功地将神经科学领域知识与深度学习模型相结合。

Abstract: Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\% and an AUROC of 76.4\%, while simultaneously providing neurobiologically meaningful explanations.

</details>


### [176] [M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers](https://arxiv.org/abs/2511.17623)
*Haoran Li,Zhe Cheng,Muhao Guo,Yang Weng,Yannan Sun,Victor Tran,John Chainaranont*

Main category: cs.LG

TL;DR: 提出了M2OE2-GL方法，通过全局预训练和轻量级微调解决大规模配电系统中概率负荷预测的异质性和可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 在包含数千甚至数十万个负荷的大型配电系统中，为每个客户训练单独模型计算和存储成本高，而使用单一全局模型无法处理不同客户类型、位置和相位之间的分布偏移

Method: 首先在所有馈线负荷上预训练单一全局M2OE2基础模型，然后应用轻量级微调来推导紧凑的组特定预测器家族

Result: 在实际公用事业数据上的评估显示，M2OE2-GL在保持对大量负荷可扩展性的同时，显著降低了预测误差

Conclusion: M2OE2-GL方法能够有效解决大规模配电系统中概率负荷预测的异质性和可扩展性挑战

Abstract: Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads.

</details>


### [177] [QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments](https://arxiv.org/abs/2511.17624)
*Hector E Mozo*

Main category: cs.LG

TL;DR: QML-HCS是一个用于构建和分析在超因果反馈动力学下运行的量子启发机器学习模型的研究级框架，通过整合量子启发叠加原理、动态因果反馈和确定性-随机混合执行来解决非平稳环境中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习和量子启发系统在非平稳环境中表现不佳，数据分布漂移且模型缺乏持续适应、因果稳定性和相干状态更新的机制。

Method: 采用统一计算架构，整合量子启发叠加原理、动态因果反馈和确定性-随机混合执行，实现可逆变换、多路径因果传播和在漂移下评估替代状态。

Result: 通过最小化仿真展示了超因果模型如何在输入分布突然变化时适应并保持内部相干性，建立了未来扩展和集成的基础架构。

Conclusion: QML-HCS为量子启发学习、因果推理和混合计算提供了可复现且可扩展的Python接口，无需专用硬件即可进行实验。

Abstract: QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments.
  The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware.
  A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms.

</details>


### [178] [Efficient Large-Scale Learning of Minimax Risk Classifiers](https://arxiv.org/abs/2511.17626)
*Kartheek Bondugula,Santiago Mazuelas,Aritz Pérez*

Main category: cs.LG

TL;DR: 本文提出了一种基于约束生成和列生成组合的学习算法，用于高效训练大规模多类分类任务中的极小极大风险分类器。


<details>
  <summary>Details</summary>
Motivation: 传统的随机次梯度方法无法有效处理极小极大风险分类器（MRCs）的最小化最大期望损失问题，特别是在大规模多类分类任务中。

Method: 结合约束生成和列生成的学习算法，专门针对大规模多类分类任务中的MRCs训练。

Result: 在多个基准数据集上的实验表明，该算法在一般大规模数据上提供高达10倍的加速，在类别数量较多时提供约100倍的加速。

Conclusion: 所提出的算法能够显著提高MRCs在大规模多类分类任务中的训练效率。

Abstract: Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.

</details>


### [179] [Rectifying Mean-Shift in Cascaded Precipitation Nowcasting](https://arxiv.org/abs/2511.17628)
*Fanbo Ju,Haiyuan Shi,Qingjian Ni*

Main category: cs.LG

TL;DR: 提出RectiCast框架，通过双流匹配模型显式解耦均值场偏移校正和局部随机性生成，解决降水临近预报中确定性预测的系统性分布偏移与局部随机性混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的级联架构降水临近预报方法忽视了确定性预测的系统性分布偏移与局部随机性的混淆问题，导致概率性组件的预测被污染，特别是在较长预报时效上出现降水模式和强度的不准确。

Method: RectiCast采用两阶段框架：第一阶段使用确定性模型生成后验均值；第二阶段引入Rectifier显式学习分布偏移并生成修正均值，然后Generator基于修正均值建模局部随机性。

Result: 在SEVIR和MeteoNet数据集上的实验表明，RectiCast相比现有最先进方法取得了显著的性能提升。

Conclusion: RectiCast通过显式解耦均值场偏移校正和局部随机性生成，有效解决了降水临近预报中的分布偏移污染问题，提升了预报准确性。

Abstract: Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods.

</details>


### [180] [Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance](https://arxiv.org/abs/2511.17629)
*Yanxuan Yu,Michael S. Hughes,Julien Lee,Jiacheng Zhou,Andrew F. Laine*

Main category: cs.LG

TL;DR: AF-SMOTE是一种针对极端类别不平衡分类的增强框架，通过合成少数类样本并使用对抗判别器和边界效用模型进行过滤，在保持校准的同时提高召回率和平均精度。


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等场景中，极端类别不平衡下的召回率和校准都至关重要，漏诊罕见疾病的真实阳性病例可能带来严重后果。

Method: 提出AF-SMOTE框架，先合成少数类样本，然后通过对抗判别器和边界效用模型进行过滤，在决策边界平滑和类条件密度的温和假设下，证明过滤步骤能单调改进F_beta代理指标而不增加Brier分数。

Result: 在MIMIC-IV代理标签预测和欺诈检测基准测试中，AF-SMOTE比强过采样基线（SMOTE、ADASYN等）获得更高的召回率和平均精度，并具有最佳校准性能，在多个额外数据集上验证了这些优势。

Conclusion: AF-SMOTE在医疗数据集上的成功应用展示了其在临床情况中的实用价值，特别是在疾病无关的方式下处理罕见疾病分类问题。

Abstract: We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences.

</details>


### [181] [Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change](https://arxiv.org/abs/2511.17630)
*Nele Albers,Esra Cemre Su de Groot,Loes Keijsers,Manon H. Hillegers,Emiel Krahmer*

Main category: cs.LG

TL;DR: 探索使用大型语言模型生成用户交互样本，用于训练数字行为改变场景中的强化学习模型，结果显示LLM生成的样本在缺乏真实数据时有用，且性能达到人类评分者水平。


<details>
  <summary>Details</summary>
Motivation: 开发个性化数字健康行为改变应用需要大量设计选择，这些选择的效果难以从文献预测且实践评估成本高，因此探索是否可以利用现成的大型语言模型生成有用的用户交互样本。

Method: 使用真实用户数据作为比较基准，评估LLM生成的用户交互样本，分析不同提示策略（包括短/长提示变体、思维链提示和少样本提示）的效果。

Result: LLM生成的样本在缺乏真实数据时有用，性能达到人类评分者水平，不同提示策略的相对效果因研究和LLM而异，仅提示改写就存在较大差异。

Conclusion: LLM生成的样本在实践中具有应用价值，研究为如何有效使用这些样本提供了建议。

Abstract: Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice.

</details>


### [182] [Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario](https://arxiv.org/abs/2511.17631)
*Bingjun Wei,Xuemei Cao,Jiafen Liu,Haoyang Liang,Xin Yang*

Main category: cs.LG

TL;DR: 提出了增强联邦深度多视图聚类框架EFDMVC，通过层次对比融合解决视图不确定性，视图自适应漂移模块缓解聚合不确定性，平衡聚合机制协调客户端更新，在异构不确定视图下实现鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦多视图聚类假设客户端视图均匀，但实际部署中存在异构视图完整性，包含不完整、冗余或损坏数据。现有方法忽视动态视图组合产生的语义冲突，未能解决视图不确定性（任意视图配对的语义不一致）和聚合不确定性（客户端更新分歧与贡献不平衡）的双重不确定性。

Method: 1）局部语义对齐，客户端内层次对比融合消除语义冲突解决视图不确定性；2）视图自适应漂移模块通过全局-局部原型对比动态校正参数偏差缓解聚合不确定性；3）平衡聚合机制协调客户端更新。

Result: EFDMVC在多个基准数据集上对异构不确定视图展现出优越的鲁棒性，在全面评估中持续优于所有最先进的基线方法。

Conclusion: EFDMVC框架有效解决了联邦多视图聚类中的双重不确定性挑战，为实际部署中视图异构性问题提供了可行的解决方案。

Abstract: Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations.

</details>


### [183] [Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production](https://arxiv.org/abs/2511.17632)
*Bestoun S. Ahmed,Tommaso Azzalin,Andreas Kassler,Andreas Thore,Hans Lindback*

Main category: cs.LG

TL;DR: 提出基于数字孪生的智能制造方法，通过微服务边缘计算平台和深度强化学习优化钢铁生产厂的可持续性、效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 将传统制造流程转变为智能系统，实现可持续发展目标，强调MLOps在数据驱动制造中的关键作用。

Method: 采用微服务边缘计算平台，通过数字孪生实时处理传感器数据，使用深度强化学习代理在MLOps系统中优化感应炉加热和工艺参数。

Result: 系统能够减少制造浪费、提高生产质量，通过优化电力设置实现可持续性目标。

Conclusion: 该研究为传统工艺向智能系统转型提供了关键步骤，展示了可扩展的事件驱动架构在工业应用中的适应性。

Abstract: We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.

</details>


### [184] [PocketLLM: Ultimate Compression of Large Language Models via Meta Networks](https://arxiv.org/abs/2511.17637)
*Ye Tian,Chengcheng Wang,Jing Han,Yehui Tang,Kai Han*

Main category: cs.LG

TL;DR: PocketLLM是一种通过元网络在潜在空间压缩大语言模型的新方法，使用编码器将权重投影到离散潜在向量，通过紧凑码本表示，再用轻量解码器映射回原始权重空间，实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增长，在边缘设备上存储和传输变得困难，传统量化剪枝方法难以在保持精度下实现极端压缩。

Method: 提出编码器网络将LLM权重投影到离散潜在向量，用紧凑码本表示，轻量解码器将码本代表向量映射回原始权重空间。

Result: 实验显示PocketLLM在极高压缩比下仍保持优异性能，如将Llama 2-7B压缩10倍仅精度轻微下降。

Conclusion: PocketLLM通过潜在空间压缩实现了大语言模型的高效压缩，在保持精度前提下显著减小模型存储和传输需求。

Abstract: As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.

</details>


### [185] [Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer](https://arxiv.org/abs/2511.17638)
*Pratham Sorte*

Main category: cs.LG

TL;DR: 提出M2KT方法，实现无需数据的模型间知识传输，通过概念空间而非示例空间进行知识交换，减少98%数据使用同时达到教师模型85-90%性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏、迁移学习等方法仍依赖数据驱动，需要教师模型生成示例、logits或梯度。需要一种无需数据的概念传输方法。

Method: M2KT通过概念流形、模型间对齐映射和复合损失函数，在概念空间传输包含结构化概念嵌入、抽象图、推理轨迹和元数据的知识包。

Result: 在符号推理任务中，M2KT达到教师模型85-90%性能，同时比标准知识蒸馏减少98%以上的数据使用。

Conclusion: 为无数据AI间知识传输和自改进模型生态系统建立了理论和实践基础。

Abstract: Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems.

</details>


### [186] [TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin](https://arxiv.org/abs/2511.17639)
*Yibing Wan,Zhengxiong Guan,Chaoli Zhang,Xiaoyang Li,Lai Xu,Beibei Jia,Zhenzhe Zheng,Fan Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为梯形时间融合(TTF)的新框架，用于解决用户增长场景中渠道级LTV早期预测的挑战，包括未对齐的多时间序列、短输入长输出(SILO)问题以及数据波动性大等问题。


<details>
  <summary>Details</summary>
Motivation: 在用户增长场景中，互联网公司大量投资付费获客渠道，但可持续增长依赖于获取用户的终身价值(LTV)超过获客成本(CAC)。为了最大化LTV/CAC比率，需要在早期阶段预测渠道级LTV以优化预算分配。

Method: 提出梯形时间融合(TTF)框架，包括梯形多时间序列模块处理数据未对齐和SILO挑战，以及多塔结构MT-FusionNet输出准确预测。

Result: 该框架已在抖音在线系统中部署。与之前部署的在线模型相比，LTV曲线的点级MAPE(MAPEp)降低了4.3%，聚合LTV的MAPE(MAPEa)降低了3.2%。

Conclusion: TTF框架有效解决了LTV预测中的三大挑战，在真实业务场景中取得了显著的性能提升。

Abstract: In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV.

</details>


### [187] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于经过认证的块级提取变压器机制的框架，能够为残差块提取结构化替代实现，并提供机器可检查的证书来约束近似误差。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏正式保证，无法明确提取或编辑后的模型在相关输入上与原始模型的偏离程度。

Method: 引入BlockCert框架进行经过认证的块级提取，通过Lipschitz-based组合定理将局部保证提升为全局偏差边界，并在Lean 4中形式化验证。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上获得高块覆盖率和小残差误差，TinyLlama设置中完全拼接模型在压力提示下与基线困惑度匹配在约6e-5范围内。

Conclusion: 块级提取与显式证书对于真实变压器语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [188] [MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence](https://arxiv.org/abs/2511.17647)
*Liyuan Deng,Yunpeng Bai,Yongkang Dai,Xiaoshui Huang,Hongping Gan,Dongshuo Huang,Hao jiacheng,Yilei Shi*

Main category: cs.LG

TL;DR: MamTiff-CAD是一个基于Transformer扩散模型的CAD参数化命令序列生成框架，通过Mamba+和Transformer的混合架构处理长序列，在60-256命令的长序列CAD模型生成上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CAD参数化方法在处理复杂CAD模型的长序列参数命令时面临几何和拓扑约束的挑战，需要能够有效处理长距离依赖关系的解决方案。

Method: 设计Mamba+和Transformer混合的自编码器将参数化CAD序列转换为潜在表示，Mamba+块通过遗忘门机制捕获长距离依赖，非自回归Transformer解码器重建潜在表示，然后基于多尺度Transformer的扩散模型学习长序列命令分布。

Result: 实验表明MamTiff-CAD在重建和生成任务上都达到了最先进的性能，特别是在60-256命令的长序列CAD模型生成上表现优异。

Conclusion: MamTiff-CAD框架通过结合Mamba+和Transformer的优势，成功解决了长序列CAD参数命令生成的挑战，为工业应用提供了有效的解决方案。

Abstract: Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation.

</details>


### [189] [Frugality in second-order optimization: floating-point approximations for Newton's method](https://arxiv.org/abs/2511.17660)
*Giuseppe Carrino,Elena Loli Piccolomini,Elisa Riccietti,Theo Mary*

Main category: cs.LG

TL;DR: 该论文分析了有限精度算术对牛顿步长的影响，提出了混合精度牛顿优化器的收敛定理，并引入了GN_k方法（广义高斯-牛顿法），在回归任务上达到与完整牛顿法相当的性能但需要更少的导数计算。


<details>
  <summary>Details</summary>
Motivation: 虽然一阶方法在机器学习训练中占主导地位，但高阶方法如牛顿法可以提供更高的准确性和更快的收敛速度，但由于计算成本高而常被避免。本研究旨在解决牛顿法在有限精度下的收敛问题。

Method: 1) 分析有限精度算术对牛顿步长的影响，建立混合精度牛顿优化器的收敛定理；2) 提出GN_k方法，允许部分计算二阶导数，减少导数计算次数。

Result: 经验评估表明，所提方法在澳大利亚和MUSH数据集上优于Adam优化器。GN_k方法在回归任务上达到与完整牛顿法相当的性能，同时显著减少导数计算次数。

Conclusion: 混合精度牛顿优化器提供了收敛保证和可达到解精度的先验估计，GN_k方法在保持性能的同时大幅降低了计算成本，为高阶优化方法在机器学习中的实际应用提供了可行方案。

Abstract: Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including "quasi" and "inexact" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations.

</details>


### [190] [Enhancing Breast Cancer Prediction with LLM-Inferred Confounders](https://arxiv.org/abs/2511.17662)
*Debmita Roy*

Main category: cs.LG

TL;DR: 使用大语言模型从常规临床数据中推断糖尿病、肥胖和心血管疾病等混杂疾病的概率，以增强乳腺癌预测。AI生成的特征提高了随机森林模型的性能，特别是Gemma（3.9%）和Llama（6.4%）模型。


<details>
  <summary>Details</summary>
Motivation: 通过非侵入性预筛查和临床整合，改进乳腺癌的早期检测和共享决策制定。

Method: 利用大语言模型从常规临床数据中推断混杂疾病（糖尿病、肥胖、心血管疾病）的概率，并将这些AI生成的特征输入随机森林模型进行乳腺癌预测。

Result: AI生成的特征显著提高了随机森林模型的性能，特别是Gemma模型提升了3.9%，Llama模型提升了6.4%。

Conclusion: 该方法在乳腺癌预测方面显示出潜力，有助于改进早期检测和临床决策制定。

Abstract: This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis.

</details>


### [191] [AI-based framework to predict animal and pen feed intake in feedlot beef cattle](https://arxiv.org/abs/2511.17663)
*Alex S. C. Maia,John B. Hall,Hugo F. M. Milan,Izabelle A. M. A. Teixeira*

Main category: cs.LG

TL;DR: 开发了一个AI框架，利用环境指数和机器学习准确预测个体牛只和围栏级别的饲料摄入量


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏充分利用纵向大数据来准确预测饲料摄入量并考虑环境条件的方法

Method: 使用来自19个实验的1650万样本数据和环境数据，开发了两个环境指数（InComfort-Index和EASI-Index），结合机器学习模型（XGBoost）进行预测

Result: XGBoost模型在个体动物级别的预测准确度为RMSE 1.38 kg/天，围栏级别为0.14 kg/(天-动物)；EASI-Index在预测饲料摄入量方面表现良好

Conclusion: 该方法为预测个体动物和围栏的饲料摄入量提供了稳健的AI框架，具有在精准管理肉牛养殖中减少饲料浪费、优化资源和气候适应性管理方面的应用潜力

Abstract: Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management.

</details>


### [192] [CubeletWorld: A New Abstraction for Scalable 3D Modeling](https://arxiv.org/abs/2511.17664)
*Azlaan Mustafa Samad,Hoang H. Nguyen,Lukas Berg,Henrik Müller,Yuan Xue,Daniel Kudenko,Zahra Ahmadi*

Main category: cs.LG

TL;DR: CubeletWorld是一个新颖的城市环境建模框架，通过3D网格单元（cubelets）表示和分析城市数据，支持隐私保护的规划、导航和占用预测任务。


<details>
  <summary>Details</summary>
Motivation: 现代城市产生大量异构数据，但将这些数据整合成连贯的空间模型仍具挑战性。现有基于代理感知的方法存在可扩展性限制和隐私问题。

Method: 提出CubeletWorld框架，将城市环境离散化为3D网格单元，将基础设施、移动和环境数据嵌入到局部cubelet状态中，无需代理驱动感知。

Result: 评估了CubeletWorld状态预测任务，分析了空间粒度增加带来的稀疏性表示和基线可扩展性挑战，展示了该框架的灵活性和可扩展性。

Conclusion: CubeletWorld为从复杂城市数据中学习提供了灵活可扩展的框架，为社会经济建模、环境监测和应急响应等领域的可扩展模拟和决策支持开辟了新可能性。

Abstract: Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.

</details>


### [193] [GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization](https://arxiv.org/abs/2511.17665)
*Hadi Khodaei Jooshin,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein生成对抗网络(WGANs)的新型批处理算法，用于EDA中的全局布线，相比现有方法可减少40%运行时间且布线质量仅下降0.002%。


<details>
  <summary>Details</summary>
Motivation: 传统批处理方法依赖计算成本高的启发式方法，会导致批次过大、批次数量过多和生成时间过长等问题，限制了可扩展性和效率。

Method: 使用Wasserstein生成对抗网络(WGANs)增强的新型批处理算法，能够生成更少但质量更高的批次，实现更有效的并行化。

Result: 在ISPD'24竞赛基准测试中，相比最先进的布线器，运行时间减少高达40%，布线质量仅下降0.002%。

Conclusion: 基于WGANs的批处理算法能够显著提高全局布线的效率和可扩展性，同时保持高质量的布线结果。

Abstract: Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router.

</details>


### [194] [Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles](https://arxiv.org/abs/2511.17675)
*Navneet Singh,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种紧凑的混合量子架构，用于自动驾驶轨迹预测，通过量子注意力编码器、参数精简的量子前馈堆栈和傅里叶解码器，在Waymo数据集上实现了优于运动学基线的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶轨迹预测需要在计算和延迟约束下提供准确、校准的多模态未来预测，传统方法面临计算复杂度高的挑战。

Method: 使用以自我为中心、车道对齐的坐标系，预测对运动学基线的残差修正；结合量子注意力编码器（9个量子比特）、量子前馈堆栈（64层，约1200个可训练角度）和傅里叶解码器；采用SPSA训练所有电路参数。

Result: 在Waymo Open Motion Dataset上，模型在2.0秒预测范围内实现了minADE 1.94米和minFDE 3.56米，持续优于运动学基线，降低了漏检率并提高了召回率。

Conclusion: 残差学习、截断傅里叶解码、浅层纠缠和基于频谱的排序将容量集中在关键区域，从小型浅层量子电路中产生稳定的优化和可靠的多模态预测。

Abstract: Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \SI{3.56}{m} in the $16$ models predicted over the horizon of \SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark.

</details>


### [195] [A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification](https://arxiv.org/abs/2511.17677)
*Abu Kaisar Mohammad Masum,Naveed Mahmud,M. Hassan Najafi,Sercan Aygun*

Main category: cs.LG

TL;DR: 提出了一种将n量子比特量子电路与经典BERT模型结合的混合方法用于文本分类，实验表明该混合模型在标准基准数据集上具有竞争力，甚至在某些情况下优于经典基线方法。


<details>
  <summary>Details</summary>
Motivation: BERT微调在文本分类中计算成本高且需要仔细的超参数调优，而量子算法在机器学习和文本分类任务中显示出超越传统方法的潜力。

Method: 集成n量子比特量子电路与经典BERT模型的混合方法，用于文本分类任务。

Result: 混合模型在标准基准数据集上表现具有竞争力，在某些情况下优于经典基线方法，展示了经典-量子模型在不同数据集上微调预训练模型的适应性。

Conclusion: 该混合方法突显了量子计算在提升文本分类任务性能方面的潜力，为这一研究领域的发展提供了可行性证明。

Abstract: Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks.

</details>


### [196] [Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics](https://arxiv.org/abs/2511.17687)
*Zhangyu Ge,Xu He,Lingfei Mo,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lansong Jiang,Fengyuan Liu*

Main category: cs.LG

TL;DR: 提出了一种使用表示学习模型复制连续吸引子神经网络神经动力学模式的高效路径积分方法，相比NeuroSLAM系统在通用设备上效率提升约17.5%，在边缘设备上提升40~50%。


<details>
  <summary>Details</summary>
Motivation: 现有脑启发导航研究中使用连续吸引子神经网络构建的路径积分机制存在显著计算冗余和效率问题，不利于脑启发导航技术的实用化。

Method: 使用轻量级人工神经网络复制头方向细胞和网格细胞的神经动力学模式，并将这些模型集成以实现脑启发的航位推算路径积分。

Result: 在各种环境中的基准测试表明，该方法不仅能准确复制导航细胞的神经动力学模式，定位精度与NeuroSLAM相当，而且在效率上有显著提升。

Conclusion: 这项工作为增强脑启发导航技术的实用性提供了一种新颖的实现策略，并具有进一步扩展的潜力。

Abstract: The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.

</details>


### [197] [Enhancing Adversarial Transferability through Block Stretch and Shrink](https://arxiv.org/abs/2511.17688)
*Quan Liu,Feng Ye,Chenhao Lu,Shuming Zhen,Guanliang Huang,Lunzhe Chen,Xudong Ke*

Main category: cs.LG

TL;DR: 提出了Block Stretch and Shrink (BSS)方法，通过将图像分割成块并应用拉伸和收缩操作来增强对抗样本的跨模型迁移性，在ImageNet子集上表现优于现有输入变换攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于输入变换的对抗攻击方法在跨模型迁移性方面表现有限，研究表明高迁移性与多样化的注意力热图和保持全局语义相关。

Method: BSS方法将图像分割成块，对这些块应用拉伸和收缩操作，从而在变换输入中多样化注意力热图同时保持全局语义。

Result: 在ImageNet子集上的实证评估表明，BSS在迁移性方面优于现有的基于输入变换的攻击方法。

Conclusion: BSS方法有效提升了对抗样本的跨模型迁移性，并建议在统一变换数量尺度下评估输入变换攻击方法以确保公平比较。

Abstract: Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.

</details>


### [198] [DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams](https://arxiv.org/abs/2511.17693)
*Ginés Carreto Picón,Peng Yuan Zhou,Qi Zhang,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 提出了DeepCoT（深度持续Transformer），一种无冗余的编码器模型，可在现有深度编码器架构上应用，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Transformer模型规模不断增大，但在资源受限设备上需要低延迟推理。流数据推理在滑动时间窗口上会产生高度冗余计算，现有持续Transformer仅适用于浅层模型。

Method: 提出DeepCoT模型，这是一种无冗余的编码器模型，可应用于现有深度编码器架构，只需最小改动。

Result: 在音频、视频和文本流上的实验表明，DeepCoT保持与非持续基线相当的性能，同时所有Transformer层提供线性计算成本，运行时间比先前高效模型减少两个数量级。

Conclusion: DeepCoT能够有效解决深度Transformer模型在流数据推理中的计算冗余问题，实现高效推理。

Abstract: Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.

</details>


### [199] [Diffusion Models are Molecular Dynamics Simulators](https://arxiv.org/abs/2511.17741)
*Justin Diamond,Markus Lill*

Main category: cs.LG

TL;DR: 论文证明了带序列偏置的扩散采样器等价于过阻尼朗之万动力学的欧拉-丸山积分器，建立了扩散采样与朗之万时间演化的精确对应关系，为分子动力学提供了完全数据驱动的框架。


<details>
  <summary>Details</summary>
Motivation: 将分子动力学重新表述为扩散模型，摆脱传统分子动力学对极小时间步长的依赖，通过模型容量和去噪步数两个可扩展参数控制精度。

Method: 将每个反向去噪步骤解释为具有有效时间步长的随机微分方程步骤，学习得分函数作为漂移项（即学习能量的梯度），建立扩散采样与朗之万时间演化的等价关系。

Result: 开发了完全数据驱动的分子动力学框架，仅需从非相关平衡快照学习力场，无需手工设计力场或轨迹数据训练，仍能保持与学习能量相关的玻尔兹曼分布。

Conclusion: 该框架生成具有分子动力学时间相关性的轨迹，尽管模型仅基于静态构型训练，并通过信息论误差界限清晰分离离散化误差与得分模型误差。

Abstract: We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.
  This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.
  We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.

</details>


### [200] [Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices](https://arxiv.org/abs/2511.17754)
*Andrew Lee,Mahir Mobarrat,Xiaolin Chen*

Main category: cs.LG

TL;DR: 提出了一种周期性增强的代理建模方法，通过引入周期性层来确保确定性横向位移（DLD）微流控设备单元边界的精确周期性，显著提高了多单元设备预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统DLD设备设计需要昂贵的计算流体动力学模拟，而现有的深度学习代理模型在处理周期性边界条件时存在累积误差问题，影响多单元设备设计的准确性。

Method: 使用三个子网络预测稳态、无量纲的速度和压力场（u, v, p），并引入周期性层通过架构强制而非惩罚项来确保单元边界的精确周期性匹配。

Result: 在120个CFD生成的几何结构上验证，周期性层实现实现了0.478%的临界直径误差，同时保持完美的周期性一致性，比基线方法提高了85.4%。

Conclusion: 该方法能够实现高效准确的DLD设备设计，保证边界条件在多单元设备应用中的满足。

Abstract: Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications.

</details>


### [201] [PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning](https://arxiv.org/abs/2511.17776)
*Melika Shirian,Kianoosh Vadaei,Kian Majlessi,Audrina Ebrahimi,Arshia Hemmat,Peyman Adibi,Hossein Karshenas*

Main category: cs.LG

TL;DR: PrismSSL是一个统一的Python库，将音频、视觉、图形和跨模态的自监督学习方法整合到单一模块化代码库中，提供便捷的训练、基准测试和扩展功能。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和从业者提供一个统一的框架，简化自监督学习的安装、配置和训练过程，支持多种模态和方法，提高可用性和可扩展性。

Method: 通过模块化代码库整合多种自监督学习方法，提供训练器、数据集抽象、分布式训练、超参数搜索、可视化等功能，并配备图形化仪表板。

Result: PrismSSL已打包在PyPI上，采用MIT许可证，与HuggingFace Transformers紧密集成，提供丰富的功能特性，代码和数据配方将公开可用。

Conclusion: PrismSSL成功创建了一个统一、易用的自监督学习框架，支持多种模态和方法，显著降低了使用门槛并提高了研究效率。

Abstract: We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible.

</details>


### [202] [Smoothed Agnostic Learning of Halfspaces over the Hypercube](https://arxiv.org/abs/2511.17782)
*Yiwen Kou,Raghu Meka*

Main category: cs.LG

TL;DR: 提出了一个基于随机位翻转的平滑不可知学习框架，用于布尔输入上的半空间学习，在严格次指数假设下给出了高效算法。


<details>
  <summary>Details</summary>
Motivation: 布尔半空间的不可知学习在计算学习理论中是一个基本问题，但即使在弱学习下也被证明是计算困难的。现有平滑分析框架依赖于加性高斯扰动，不适用于离散域。

Method: 引入基于随机位翻转的平滑不可知学习框架，定义了高斯情况的自然离散模拟。在输入分布的严格次指数假设下，设计高效算法学习半空间。

Result: 得到了运行时间和样本复杂度约为n的poly(1/(sigma * epsilon))次方的高效算法。此前此类算法仅对离散超立方体在强结构假设下已知。

Conclusion: 这是布尔超立方体上半空间平滑不可知学习的首个计算高效保证，弥合了最坏情况难解性与实际可学习性之间的差距。

Abstract: Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.

</details>


### [203] [Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces](https://arxiv.org/abs/2511.17784)
*Lyu Yuhuan*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机采样的覆盖分析方法，通过应用集中不等式到未覆盖子立方体计数统计量，推导出样本复杂度与失败概率对数相关的更紧边界，相比经典线性依赖显著改进。


<details>
  <summary>Details</summary>
Motivation: 经典覆盖分析在低失败概率下往往产生保守边界，特别是在小失败概率时。本文旨在开发更紧密的理论工具，为依赖网格覆盖保证的算法提供更高效的采样方法。

Method: 研究d维单位超立方体上的均匀随机采样，分析离散化后的未覆盖子立方体数量。应用集中不等式到未覆盖计数统计量，推导样本复杂度边界。

Result: 得到样本复杂度边界M=O(Cln(2C/δ))，具有对数依赖失败概率δ的特性，与经典线性1/δ依赖形成鲜明对比。数值研究表明该边界更紧密地跟踪实际覆盖需求。

Conclusion: 该方法为依赖网格覆盖保证的算法提供了更锐利的理论工具，特别是在高置信度机制下能够实现更高效的采样。

Abstract: Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($δ$), i.e., $M =O( \tilde{C}\ln(\frac{2\tilde{C}}δ))$, which contrasts sharply with the classical linear $1/δ$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $δ\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.

</details>


### [204] [Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device](https://arxiv.org/abs/2511.17787)
*Elizabeth Chen,Andrew Lee,Tanbir Sarowar,Xiaolin Chen*

Main category: cs.LG

TL;DR: 使用机器学习优化确定性侧向位移（DLD）设备设计参数，用于高效分离肺癌细胞，实现高通量、低成本的微流控系统开发


<details>
  <summary>Details</summary>
Motivation: 解决循环肿瘤细胞（CTCs）检测中的稀有性挑战，减少对计算密集型模拟的依赖，为早期癌症诊断开发更精确的微流控分离设备

Method: 采用梯度提升、K近邻、随机森林和多层感知器（MLP）回归器等机器学习模型，基于数值验证的大数据集预测粒子轨迹并识别最优设备配置

Result: 机器学习模型能准确预测粒子轨迹，识别关键设计变量，为DLD设备优化提供系统化、数据驱动的框架

Conclusion: 这种集成方法推进了可扩展、精确的微流控系统开发，有助于实现早期癌症检测和个性化医疗的总体目标

Abstract: Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine.

</details>


### [205] [Physical Reinforcement Learning](https://arxiv.org/abs/2511.17789)
*Sam Dillavou,Shruti Mishra*

Main category: cs.LG

TL;DR: 该论文将对比局部学习网络(CLLNs)应用于强化学习问题，展示了在模拟环境中使用Q学习算法的成功案例，并讨论了这种模拟系统在能源受限和物理损伤环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 数字计算机能耗高且对组件损坏敏感，不适合能源受限的自主智能体在不确定环境中使用。CLLNs作为模拟网络具有低功耗和物理鲁棒性，但之前仅用于监督学习，本研究旨在将其扩展到强化学习领域。

Method: 使用Q学习算法适配模拟的CLLNs网络，在两种简单的强化学习问题上进行测试，并明确实施RL工具箱所需的各种组件。

Result: 成功在两种简单的RL问题上实现了CLLNs的强化学习功能，证明了策略函数和价值函数在这种系统中比回放缓冲区更自然。

Conclusion: CLLNs系统可以放弃数字硬件所需的物理安全假设，这种特性在生物系统中无法依赖但在CLLNs中可训练，同时强调了在生物学中重要但在数字计算机中无意义的次要目标。

Abstract: Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.

</details>


### [206] [Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures](https://arxiv.org/abs/2511.17796)
*Afsaneh Mahanipour,Hana Khamfroush*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦多标签特征选择方法SSFMLFS，适用于客户端只有未标记数据而服务器有少量标记数据的联邦学习场景，使用模糊信息理论和PageRank算法进行特征排序。


<details>
  <summary>Details</summary>
Motivation: 现有多标签特征选择方法需要集中式数据，不适用于分布式和联邦环境；现有联邦方法假设客户端有标记数据，这在客户端缺乏标记能力时不现实。

Method: SSFMLFS将模糊信息理论应用于联邦设置，客户端计算模糊相似矩阵并传输给服务器，服务器计算特征冗余度和特征-标签相关性，构建特征图并使用PageRank算法进行特征排序。

Result: 在五个真实世界数据集上的实验表明，SSFMLFS在非独立同分布数据设置下，在三个不同评估指标上优于其他联邦和集中式方法。

Conclusion: SSFMLFS有效解决了联邦环境下客户端只有未标记数据的多标签特征选择问题，在多种应用领域表现出优越性能。

Abstract: Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.

</details>


### [207] [Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2511.17801)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出了一种基于二次优化的层特定高影响参数选择框架，用于在极低比特量化中平衡计算效率和模型精度


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法在极低比特量化时精度损失严重，且采用固定比例的高影响参数忽略了层间敏感性差异

Method: 使用二次优化框架确定层特定的高影响参数比例，考虑层间依赖关系，将高影响参数量化为中等比特，其余参数量化为极低比特

Result: 在相同资源约束下比保持FP16格式的方法保留更多高影响参数，实现了计算效率和模型精度的有效平衡

Conclusion: 该方法在保持高性能的同时，相比最先进方法实现了更好的计算效率与精度平衡

Abstract: Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods.

</details>


### [208] [Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2511.17809)
*Cuong Pham,Hoang Anh Dung,Cuong C. Nguyen,Trung Le,Gustavo Carneiro,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出自适应变换选择框架，通过逐层选择最优变换来提升低比特量化下大语言模型的性能，解决现有方法使用同质变换忽略层间分布异质性的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型量化部署时，激活和权重中的系统性异常值会导致性能显著下降。现有变换方法使用同质变换设置，忽略了LLM内部的异质分布特性。

Method: 将变换选择表述为可微分优化问题，建立权重分布峰度与准确变换类型的联系，提出基于稳健z-score归一化的异常值引导层选择方法。

Result: 在LLaMA系列模型上，W3A3K2V2量化设置下，相比现有最佳方法FlatQuant，困惑度提升4.58点，六任务零样本准确率平均提升2.11%。

Conclusion: 异质变换选择对于实现最优LLM量化是必要的，自适应方法在保持低开销的同时显著优于固定变换设置。

Abstract: Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization.

</details>


### [209] [APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs](https://arxiv.org/abs/2511.17818)
*Aishwarya Mandyam,Kalyani Limaye,Barbara E. Engelhardt,Emily Alsentzer*

Main category: cs.LG

TL;DR: 利用大型语言模型生成反事实标注来改进医疗领域的离线策略评估，解决数据集覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 标准离线策略评估方法受限于行为数据集的大小和覆盖范围，而人工标注反事实注释成本高昂，限制了方法的可扩展性。

Method: 使用领域知识指导LLMs预测在替代治疗下关键临床特征的演变，然后通过已知奖励函数将这些预测特征转换为反事实标注。

Result: 在MIMIC-IV数据集上的实验表明，LLM生成的反事实标注在大多数情况下显著改善了OPE估计，但存在收益递减点。

Conclusion: LLM生成的反事实标注为解决医疗数据集覆盖限制提供了可扩展的方法，有助于在临床环境中更安全地部署决策策略。

Abstract: Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.

</details>


### [210] [High-Accuracy List-Decodable Mean Estimation](https://arxiv.org/abs/2511.17822)
*Ziyun Chen,Spencer Compton,Daniel Kane,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了高精度列表可解码学习，提出了在列表可解码均值估计中实现误差与列表大小权衡的新方法，为高斯分布均值估计提供了非平凡的高精度保证。


<details>
  <summary>Details</summary>
Motivation: 现有列表可解码学习算法虽然能获得最优列表大小，但误差随1/α衰减较差。本文探索是否可以通过增大列表大小来换取更高的精度，即实现列表大小与准确性的权衡。

Method: 提出了全新的可辨识性证明方法，并设计了一种不依赖平方和层次结构的算法，能够输出候选均值列表，其中一个元素与真实均值的距离不超过ε。

Result: 证明了存在大小为L = exp(O(log²(1/α)/ε²))的候选均值列表，其中至少一个元素与真实均值的ℓ₂距离不超过ε。算法的时间和样本复杂度为n = d^O(log L) + exp exp(Õ(log L))。

Conclusion: 本文首次在列表可解码均值估计中实现了非平凡的高精度保证，为列表大小与准确性之间的权衡提供了理论和算法基础，所提出的证明方法具有独立的技术价值。

Abstract: In list-decodable learning, we are given a set of data points such that an $α$-fraction of these points come from a nice distribution $D$, for some small $α\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $α$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / α$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $ε> 0$, can we can output a slightly larger list in terms of $α$ and $ε$, but so that one element of this list has error at most $ε$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \exp \left( O\left( \tfrac{\log^2 1 / α}{ε^2} \right)\right)$ so that one of the elements of this list has $\ell_2$ distance at most $ε$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\log L)} + \exp \exp (\widetilde{O}(\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.

</details>


### [211] [A novel k-means clustering approach using two distance measures for Gaussian data](https://arxiv.org/abs/2511.17823)
*Naitik Gada*

Main category: cs.LG

TL;DR: 提出了一种结合簇内距离(WCD)和簇间距离(ICD)的新型k-means聚类算法，使用Calinski-Harabasz准则确定k值，提高了聚类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统k-means聚类算法存在局限性，希望通过同时考虑簇内和簇间距离来增强聚类分析的鲁棒性和准确性。

Method: 开发了结合WCD和ICD距离度量的k-means算法，使用Calinski-Harabasz准则自动确定最佳聚类数量k。

Result: 在合成数据和UCI基准数据集上的实验表明，该方法比传统k-means算法具有更好的聚类准确性和异常值处理能力。

Conclusion: 结合WCD和ICD的k-means算法能提供更稳健的聚类结果，并为未来研究开辟了新的方向。

Abstract: Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \textit{k}-means clustering. Here we present a \textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.

</details>


### [212] [Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch](https://arxiv.org/abs/2511.17826)
*Ziyang Zhang,Xinheng Ding,Jiayi Yuan,Rixin Liu,Huizi Mao,Jiarong Xing,Zirui Liu*

Main category: cs.LG

TL;DR: 提出了Tree-Based Invariant Kernels (TBIK)来解决大语言模型推理中的非确定性问题，确保在不同张量并行配置下获得比特级相同的输出结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架存在非确定性问题：相同输入在不同系统配置（如张量并行大小、批大小）下会产生不同输出，这在大语言模型应用（如LLM-as-a-judge评估、多智能体系统和强化学习）中尤为关键。

Method: 设计并实现了基于树的恒定内核(TBIK)，通过统一的分层二叉树结构对齐GPU内部和GPU间的归约顺序，确保矩阵乘法和归约操作在不同TP大小下产生比特级相同结果。

Result: 实验证实了在不同TP大小下实现零概率发散和比特级可重现性，并在强化学习训练管道中实现了vLLM和FSDP之间的比特级相同结果。

Conclusion: TBIK成功解决了TP引起的非一致性问题，为大语言模型应用提供了可靠的确定性推理保障。

Abstract: Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [213] [Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization](https://arxiv.org/abs/2511.17829)
*Akhil Singampalli,Sudeep Pasricha*

Main category: cs.LG

TL;DR: MOELO是一个新颖的持续学习框架，首次联合解决室内定位中的领域增量学习和类别增量学习问题，通过混合专家架构实现轻量级、鲁棒且自适应的定位解决方案。


<details>
  <summary>Details</summary>
Motivation: 室内定位的长期可靠性受到移动设备硬件/软件变化导致的领域偏移，以及室内环境演变引入新位置导致的类别偏移的挑战，使得静态机器学习模型随时间失效。

Method: 采用混合专家架构，专家按区域增量训练，通过等角紧框架门控机制选择专家，确保高效路由和低延迟推理，保持紧凑模型尺寸。

Result: 实验评估显示，MOELO在不同建筑、移动设备和学习场景下，相比最先进框架在平均定位误差上提升25.6倍，最差情况定位误差提升44.5倍，遗忘减少21.5倍。

Conclusion: MOELO提供了一个可在资源受限移动设备上部署的轻量级、鲁棒且自适应的持续学习解决方案，有效应对动态异构现实环境中的定位挑战。

Abstract: Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.

</details>


### [214] [Internalizing Tools as Morphisms in Graded Transformers](https://arxiv.org/abs/2511.17840)
*Tony Shaska*

Main category: cs.LG

TL;DR: 提出了一种基于分级的transformer内部符号计算框架，通过可微分路由策略实现选择性激活的符号操作，统一了符号计算、几何和自监督学习。


<details>
  <summary>Details</summary>
Motivation: 将符号计算内化到transformer架构中，避免外部工具调用的开销，同时保持端到端的可微性，实现更自然和高效的符号推理。

Method: 在隐藏空间中引入分级结构，符号操作实现为类型化块映射，通过自监督的效用函数控制激活，采用信息几何和自然梯度方法。

Result: 开发了代数几何基础框架，实现了在混合符号-语言任务上的选择性形态激活，能够内化外部工具范式。

Conclusion: 该分级transformer框架统一了符号计算、几何和自监督学习，为可解释的神经符号系统提供了理论基础。

Abstract: We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\bigoplus_{g\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $φ_{h\leftarrow g}:V_g\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \emph{graded transformer} formalism \cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \cite{toolformer2023}) as a special case via functorial internalization.

</details>


### [215] [Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks](https://arxiv.org/abs/2511.17848)
*Zhihui Tian,Ethan Suwandi,Tomas Oppelstrup,Vasily V. Bulatov,Joel B. Harley,Fei Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合CNN自编码器和GNN的混合架构，用于高效模拟微观结构演化，显著降低了计算成本和内存使用，同时提高了准确性和时空建模能力。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在微观结构模拟中面临大规模计算单元时的扩展性挑战，需要降低计算成本和内存占用。

Method: 使用基于CNN的双射自编码器压缩空间维度，在降维后的潜在空间中使用GNN演化微观结构。

Result: 在最大网格(160^3)上，内存使用和推理时间分别减少117倍和115倍，同时表现出更高的准确性和更强的时空能力。

Conclusion: 该方法为模拟晶粒生长提供了高度可扩展的解决方案，结合了可扩展性和准确性，适用于长时间尺度的现实材料微观结构模拟。

Abstract: Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth.

</details>


### [216] [Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently](https://arxiv.org/abs/2511.17852)
*Bochen Lyu,Yiyang Jia,Xiaohao Cai,Zhanxing Zhu*

Main category: cs.LG

TL;DR: 本文通过理论分析比较了强化学习(RL)和监督微调(SFT)在训练Transformer学习链式思维(CoT)能力时的机制差异，特别针对k-稀疏布尔函数的学习。


<details>
  <summary>Details</summary>
Motivation: 虽然RL和SFT都能让Transformer获得CoT推理能力，但它们的底层机制和差异在理论上仍不清楚，需要系统分析。

Method: 使用单层Transformer学习k-稀疏布尔函数，分析RL和SFT在中间监督下的学习动态，识别可学习性的充分条件。

Result: 验证了两种方法都能学习k-PARITY、k-AND和k-OR等函数，但RL同时学习整个CoT链，而SFT逐步学习CoT链。

Conclusion: RL和SFT在触发Transformer的CoT能力时表现出不同的学习行为，这为理解两种方法的机制提供了理论洞见。

Abstract: Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.

</details>


### [217] [Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds](https://arxiv.org/abs/2511.17861)
*Xuesong Jia,Yuanjie Shi,Ziquan Liu,Yi Xu,Yan Yan*

Main category: cs.LG

TL;DR: 提出了一种简单的成本敏感共形训练算法，通过理论证明最小化预测集大小的期望值受真实标签排名的期望值上界约束，开发了基于真实标签排名的权重策略，避免了传统代理函数的近似误差问题。


<details>
  <summary>Details</summary>
Motivation: 传统共形训练方法使用Sigmoid或高斯误差函数作为指示函数的代理，但这些代理函数没有统一的误差界限，导致学习界限不可控。

Method: 开发了基于真实标签排名的权重策略，通过最小化真实标签排名的期望值来间接最小化预测集大小的期望值，避免了指示函数近似机制。

Result: 实验验证了理论见解的有效性，在预测效率方面优于其他共形训练方法，平均预测集大小减少了21.38%。

Conclusion: 提出的成本敏感共形训练算法通过理论保证的紧致性，在保持有效概率的同时显著提高了预测效率。

Abstract: Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.

</details>


### [218] [Equivalence of Context and Parameter Updates in Modern Transformer Blocks](https://arxiv.org/abs/2511.17864)
*Adrian Goldwaser,Michael Munn,Javier Gonzalvo,Benoit Dherin*

Main category: cs.LG

TL;DR: 本文扩展了transformer中上下文影响的理论，证明了在现代LLM架构中，上下文的影响可以完美映射为MLP权重矩阵的秩-1补丁和RMSNorm尺度补丁。


<details>
  <summary>Details</summary>
Motivation: 扩展基础理论到现代大型语言模型的多样化架构，统一理解transformer如何将提示转换为有效权重。

Method: 首先为Gemma风格transformer块提供精确解析解，然后推广到多层模型，引入输入可控性和输出可控性框架。

Result: 证明了对于任何内函数输入可控、外函数输出可控的MLP块，完美的隐式权重补丁是可能的，适用于多种现代LLM架构。

Conclusion: 提供了一个更简单强大的视角来理解transformer模型如何将提示转换为有效权重，适用于广泛的现代LLM架构。

Abstract: Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.

</details>


### [219] [The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems](https://arxiv.org/abs/2511.17869)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出了MITD框架，通过可解释的任务分解来检测和缓解强化学习中的奖励黑客问题，在1000个样本上实验显示可减少34%的奖励黑客频率。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理会通过奖励黑客行为利用奖励信号缺陷，获得高代理分数但无法完成真实目标，需要新的检测和缓解方法。

Method: 引入MITD分层transformer架构，包含规划器、协调器和执行器模块，将任务分解为可解释的子任务，并生成注意力瀑布图和神经通路流程图等诊断可视化。

Result: 在1000个HH-RLHF样本上的实验表明，12-25步的分解深度可在四种失效模式下减少34%的奖励黑客频率。

Conclusion: 基于机制的任务分解比事后行为监控能更有效地检测奖励黑客，为AI安全提供了新范式。

Abstract: Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.

</details>


### [220] [Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction](https://arxiv.org/abs/2511.17879)
*Yusong Wu,Stephen Brade,Teng Ma,Tia-Jane Fowler,Enning Yang,Berker Banar,Aaron Courville,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.LG

TL;DR: 提出一种对抗训练方法来缓解RL后训练中的奖励黑客问题，用于旋律到和弦伴奏任务，通过共同演化的判别器保持输出多样性


<details>
  <summary>Details</summary>
Motivation: 实时即兴演奏需要实时协调和适应，但RL后训练常因利用基于连贯性的奖励而减少输出多样性（奖励黑客问题），这对音乐创造力有害

Method: 在策略生成轨迹上进行对抗训练，使用共同演化的判别器区分策略轨迹与数据分布，策略同时最大化判别器输出和连贯性奖励以防止崩溃到平凡输出

Result: 在模拟和真实交互系统中评估，显示输出多样性、和声连贯性、适应速度和用户控制力均有改善

Conclusion: 该方法简单有效地缓解了生成序列模型RL后训练中的奖励黑客问题

Abstract: Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.

</details>


### [221] [Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing](https://arxiv.org/abs/2511.17902)
*Yifan He,Haodong Zhang,Qiuheng Song,Lin Lei,Zhenxuan Zeng,Haoyang He,Hongyan Wu*

Main category: cs.LG

TL;DR: 提出了DUPLE元学习框架，解决分布式光纤传感在跨部署场景下的活动识别问题，通过双域多原型学习和统计引导网络提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决分布式光纤传感在实际应用中面临的三个关键挑战：不同光纤部署类型导致的信号模式变化、新场景标注数据稀缺、以及源域内数据不足难以捕捉类内多样性

Method: 采用双域多原型学习融合时频域特征，统计引导网络从原始统计特征推断域重要性和原型敏感性，查询感知原型聚合模块自适应选择和组合相关原型

Result: 在跨部署DFOS数据集上的广泛实验表明，该方法在域泛化设置下显著优于基线方法，能够在有限标注数据下实现不同光纤配置的鲁棒事件识别

Conclusion: DUPLE框架有效解决了DFOS系统在跨部署场景中的域偏移和数据稀缺问题，为实际应用提供了可行的解决方案

Abstract: Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.
  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.
  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data.

</details>


### [222] [Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay](https://arxiv.org/abs/2511.17936)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 本文研究了在内存受限的流式数据环境下，状态重放方法在自动编码、时间序列预测和分类任务中的表现，发现其在异构多任务流中能显著减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 许多部署的学习系统需要在内存约束下更新流式数据模型。顺序微调方法容易遭受灾难性遗忘，而有限缓冲区的重放方法在不同生成和预测目标下的行为尚未被充分理解。

Method: 将顺序微调和重放视为理想联合目标的随机梯度方法，使用梯度对齐分析来确定混合当前和历史样本何时能减少遗忘。在六个流式场景中评估单一重放机制，使用匹配的训练预算和三个随机种子。

Result: 在异构多任务流中，重放将平均遗忘减少2-3倍；在良性的基于时间的流中，两种方法表现相似。

Conclusion: 状态重放是流式环境中持续学习的一个强大而简单的基线方法。

Abstract: Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.

</details>


### [223] [On Transportability for Structural Causal Bandits](https://arxiv.org/abs/2511.17953)
*Min Woo Park,Sanghack Lee*

Main category: cs.LG

TL;DR: 该论文研究了具有可迁移性的结构因果赌博机问题，通过融合来自不同环境的先验知识来提升部署环境中的学习效果，利用环境间的不变性改进学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有结构因果赌博机框架虽然能利用因果知识优化动作空间，但缺乏从不同条件下收集的数据集（观测或实验）和异构环境中迁移信息的指导方法。

Method: 提出结构因果赌博机与可迁移性框架，将源环境的先验知识融合到部署环境中，利用环境间的不变性来增强学习。

Result: 开发的赌博机算法实现了次线性遗憾界，明确依赖于先验数据的信息量，并且可能优于仅依赖在线学习的标准赌博机方法。

Conclusion: 通过利用环境间的不变性，可以持续改进学习效果，将来自不同环境的先验知识融合到部署环境中能够显著提升结构因果赌博机的性能。

Abstract: Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.

</details>


### [224] [Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization](https://arxiv.org/abs/2511.17963)
*Jun Kevin,Pujianto Yugopuspito*

Main category: cs.LG

TL;DR: 提出融合LSTM预测和PPO强化学习的混合投资组合优化框架，在非平稳市场环境下实现更高收益和更强韧性


<details>
  <summary>Details</summary>
Motivation: 结合深度学习的时间序列预测能力和强化学习的动态决策能力，解决传统投资组合优化方法在市场变化中的适应性不足问题

Method: 使用LSTM捕捉时间依赖性进行预测，PPO智能体在连续动作空间中自适应调整资产配置，形成混合框架

Result: 在2018-2024年多资产数据集上测试，相比等权重、指数型和单模型方法，混合框架在年化收益、夏普比率等指标上表现更优，且能适应市场变化

Conclusion: 该混合架构为动态投资组合优化提供了一个稳健的AI驱动框架，在非平稳市场环境下具有显著优势

Abstract: This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.

</details>


### [225] [Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management](https://arxiv.org/abs/2511.17968)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出了一个集成的网络弹性框架，结合联邦LSTM光伏预测与两阶段级联虚假数据注入攻击检测和能源管理系统优化，在保护数据隐私的同时实现攻击弹性的储能调度。


<details>
  <summary>Details</summary>
Motivation: 解决微电网能源管理系统在遭受网络攻击时保持经济效率和运行可靠性的挑战，现有方法通常假设测量无异常、预测不确定性未量化，且未缓解对可再生能源预测的恶意攻击。

Method: 采用联邦LSTM进行光伏预测，结合自编码器重构误差和预测不确定性量化，开发两阶段级联虚假数据注入攻击检测机制，并与能源管理系统优化集成。

Result: 在极端虚假数据攻击条件下（导致58%预测性能下降和16.9%运营成本增加），该框架将误报检测减少70%，恢复93.7%的预测性能损失，实现5%运营成本节省，缓解34.7%攻击导致的经济损失。

Conclusion: 基于多信号融合的精确级联检测优于单信号方法，验证了去中心化微电网安全与性能的协同效应。

Abstract: Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.

</details>


### [226] [Controllability Analysis of State Space-based Language Model](https://arxiv.org/abs/2511.17970)
*Mohamed Mabrok,Yalda Zafari*

Main category: cs.LG

TL;DR: 提出了影响分数作为衡量Mamba状态空间模型中token影响力的指标，通过实验验证了该指标的有效性，发现模型规模、训练数据、层深度等因素对token影响力的影响规律。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（特别是Mamba）已成为序列建模的强大架构，但其内部动态机制相比基于注意力的模型仍缺乏深入理解，需要开发有效的分析工具来揭示其工作原理。

Method: 引入影响分数这一基于可控性的度量指标，该指标从Mamba的离散状态空间参数推导而来，通过类似于系统可观测性的反向递推计算，量化位置k的token对所有后续状态和输出的影响强度。

Result: 实验发现三个主要洞察：1）影响分数随模型规模和训练数据增加而增加，反映模型容量；2）Mamba展现一致的架构模式，包括近因偏见和中后层影响力集中；3）涌现行为仅在规模较大时出现，mamba-2.8b-slimpj独特地优先处理内容词并在噪声存在时减少内部影响。

Conclusion: 影响分数可作为解释和比较基于状态空间模型的语言模型的实用诊断工具，为理解SSM内部机制提供了有效手段。

Abstract: State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models.

</details>


### [227] [Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks](https://arxiv.org/abs/2511.17978)
*Oluleke Babayomi,Dong-Seong Kim*

Main category: cs.LG

TL;DR: 提出一种新颖的异常弹性联邦学习框架，用于电动汽车充电基础设施的网络安全防护和需求预测，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 现有电动汽车充电基础设施面临日益严重的网络安全威胁，现有预测技术缺乏结合鲁棒异常缓解解决方案和数据隐私保护的能力。

Method: 集成LSTM自编码器分布式异常检测、基于插值的异常数据缓解和联邦LSTM网络，实现无需集中数据聚合的协作学习。

Result: 联邦方法相比集中式模型R2准确率提升15.2%，攻击检测系统恢复47.9%的攻击性能下降，精度达91.3%，误报率仅1.21%。

Conclusion: 该架构能够增强电动汽车基础设施规划、隐私保护协作预测、网络安全弹性和分布式充电网络的快速恢复能力。

Abstract: Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.

</details>


### [228] [An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter](https://arxiv.org/abs/2511.17983)
*Naoki Masuyama,Yuichiro Toda,Yusuke Nojima,Hisao Ishibuchi*

Main category: cs.LG

TL;DR: 提出一种基于自适应共振理论(ART)的拓扑聚类算法，通过多样性驱动的自适应机制自动调整重计算间隔和警戒阈值，实现无超参数学习，在动态环境中保持聚类稳定性和连续性。


<details>
  <summary>Details</summary>
Motivation: 解决静态和非静态设置中的聚类问题，需要能够适应分布变化同时保留先前学习到的聚类结构的模型。

Method: 基于自适应共振理论(ART)的拓扑聚类算法，采用多样性驱动的自适应机制自动调整重计算间隔和警戒阈值。

Result: 在24个真实世界数据集上的实验表明，该算法在聚类性能和持续学习能力方面优于最先进的方法。

Conclusion: 所提出的参数自适应方法在减轻灾难性遗忘和保持演化数据流中一致聚类方面具有有效性。

Abstract: Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT

</details>


### [229] [Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors](https://arxiv.org/abs/2511.17987)
*Jinping Wang,Zhiqiang Gao,Dinggen Zhang,Zhiwu Xie*

Main category: cs.LG

TL;DR: 提出了基于差异向量的各向异性缩放迭代算法（DV-BASI），通过利用优化过程中的历史运动来克服任务算术方法的优化停滞问题，实现连续优化过程，并在多任务模型合并中取得超越单独微调模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前预训练模型编辑方法面临高计算成本和有限可扩展性的挑战，任务算术方法虽然前景广阔，但由于优化停滞机制有限，其潜力未能充分发挥。

Method: 引入差异向量概念，作为任务向量的广义形式，通过差异向量作为定向扰动，提出DV-BASI算法实现任务算术方法的连续优化，无需额外模块。

Result: DV-BASI在多任务模型合并中的平均性能甚至优于单独微调的模型，在监督和无监督评估协议上都达到了最先进的性能。

Conclusion: 差异向量为任务算术方法提供了有效的优化机制，DV-BASI算法具有表达性搜索方向、少量可学习参数和可扩展框架的优势，可扩展到单任务模型的微调方法。

Abstract: Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols.

</details>


### [230] [Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks](https://arxiv.org/abs/2511.17989)
*Jiayi Luo,Qingyun Sun,Yuecen Wei,Haonan Yuan,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了MGP-MIA框架，用于针对多域图预训练模型进行成员推理攻击，解决了此类模型因增强泛化能力、缺乏代表性影子数据集和弱化成员信号而带来的攻击挑战。


<details>
  <summary>Details</summary>
Motivation: 多域图预训练技术虽然提升了图神经网络的泛化能力，但其在成员推理攻击下的隐私风险尚未得到充分研究，且由于模型泛化能力增强、影子数据集不具代表性以及嵌入输出信号弱化等因素，使得针对此类模型的有效攻击面临重大挑战。

Method: 提出MGP-MIA框架，包含三个核心机制：1) 通过机器遗忘放大目标模型的过拟合特征；2) 使用增量学习构建可靠的影子模型；3) 基于样本相似度进行成员推理。

Result: 大量实验证明了MGP-MIA框架的有效性，并揭示了多域图预训练存在的隐私风险。

Conclusion: 该研究成功开发了针对多域图预训练模型的成员推理攻击方法，暴露了此类模型在实际应用中的隐私安全隐患。

Abstract: Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training.

</details>


### [231] [Learning Rate Scheduling with Matrix Factorization for Private Training](https://arxiv.org/abs/2511.17994)
*Nikita P. Kalinin,Joel Daniel Andersson*

Main category: cs.LG

TL;DR: 该论文研究了在差分隐私模型训练中，结合学习率调度和相关噪声的随机梯度下降方法。提出了学习率感知的矩阵分解方法，相比传统前缀和分解在多个误差指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私训练理论主要关注恒定学习率下的前缀和工作负载，而实践中广泛使用学习率调度来加速训练和改善收敛。需要填补这一理论与实践的差距。

Method: 推导了单轮和多轮训练设置下广泛学习率调度类别的上下界，提出了学习率感知的矩阵分解方法，并设计了内存高效的实际部署方案。

Result: 在CIFAR-10和IMDB数据集上的实验证实，调度感知的分解方法在私有训练中提高了准确性，在MaxSE和MeanSE误差指标上优于前缀和分解。

Conclusion: 学习率感知的矩阵分解方法能够有效提升差分隐私模型训练的精度，为实际部署提供了理论支持和实用方案。

Abstract: We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.

</details>


### [232] [Majority of the Bests: Improving Best-of-N via Bootstrapping](https://arxiv.org/abs/2511.18630)
*Amin Rakhsha,Kanika Madan,Tianyu Zhang,Amir-massoud Farahmand,Amir Khasahmadi*

Main category: cs.LG

TL;DR: 提出了一种名为MoB的新选择机制，通过自举估计BoN的输出分布并选择其众数，在多个基准测试中相比BoN方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统BoN方法在奖励模型不完美时性能急剧下降，无法可靠找到正确答案，尽管正确答案通常不是概率最高的，但往往是最可能的结果。

Method: MoB通过自举方法估计BoN的输出分布，然后选择该分布的众数作为最终输出，而不是简单地选择奖励分数最高的样本。

Result: 在5个基准测试、3种基础LLM和2种奖励模型的30个设置中，MoB在25个设置中相比BoN有持续改进。

Conclusion: MoB是BoN和自一致性方法的简单而强大的替代方案，激励了对更细致选择机制的进一步研究。

Abstract: Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.

</details>


### [233] [Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning](https://arxiv.org/abs/2511.18000)
*Radman Rakhshandehroo,Daniel Coombs*

Main category: cs.LG

TL;DR: ContagionRL是一个强化学习平台，用于在空间流行病模拟中进行系统性的奖励函数设计研究，评估不同奖励函数对学习生存策略的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的模型依赖固定行为规则，缺乏对奖励函数设计如何影响学习策略的系统研究，特别是在流行病模拟中奖励工程关注有限。

Method: 集成空间SIRS+D流行病学模型与可配置环境参数，评估五种不同奖励设计（从稀疏生存奖励到新型势场方法）在多种RL算法（PPO、SAC、A2C）下的表现。

Result: 势场奖励方法表现最佳，智能体学会最大程度遵守非药物干预措施并发展复杂的空间规避策略；奖励函数选择显著影响智能体行为和生存结果。

Conclusion: ContagionRL是研究流行病背景下适应性行为响应的有效平台，强调了奖励设计、信息结构和环境可预测性在学习中的重要性。

Abstract: We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning.

</details>


### [234] [How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining](https://arxiv.org/abs/2511.18903)
*Kairong Luo,Zhenbo Sun,Haodong Wen,Xinyu Shi,Jiarui Cui,Chenyi Dang,Kaifeng Lyu,Wenguang Chen*

Main category: cs.LG

TL;DR: 研究发现课程式预训练效果受限的主要原因是数据质量升序排列与学习率衰减计划不兼容。通过采用更温和的学习率衰减或模型平均策略，可以显著提升课程式预训练的效果。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据稀缺，LLMs通常在混合质量数据上训练。课程式预训练按数据质量升序排列训练，但先前研究显示其改进有限，需要探究限制因素。

Method: 识别学习率衰减与课程训练的不兼容性，提出两种策略：使用更温和的学习率衰减（最终学习率仅略低于峰值），以及用模型平均替代学习率衰减（对最后几个检查点进行加权平均）。

Result: 结合两种策略后，在标准基准测试上的平均得分比随机混洗提高了1.64%，在1.5B参数模型和30B token训练规模上验证有效。

Conclusion: 需要重新评估课程式LLM预训练，并强调数据课程与优化方法协同设计的重要性。

Abstract: Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.

</details>


### [235] [Understanding Private Learning From Feature Perspective](https://arxiv.org/abs/2511.18006)
*Meng Ding,Mingxi Lei,Shaopeng Fu,Shaowei Wang,Di Wang,Jinhui Xu*

Main category: cs.LG

TL;DR: 本文首次从特征学习角度分析差分隐私随机梯度下降(DP-SGD)，揭示了私有训练中特征信号学习和数据噪声记忆的理论机制，发现私有学习需要更高的信噪比且会继承非私有学习中的噪声记忆问题。


<details>
  <summary>Details</summary>
Motivation: 尽管利用预训练模型特征提升DP-SGD训练已有实证进展，但私有学习中特征动态的理论理解仍然不足，现有DP分析忽视了标签相关特征信号与标签无关噪声的关键区别。

Method: 基于多补丁数据结构，使用带多项式ReLU激活的两层CNN，通过噪声梯度下降理论分析私有训练中的特征信号学习和数据噪声记忆。

Result: 发现：(1)有效私有信号学习需要比非私有训练更高的信噪比；(2)当非私有学习中出现数据噪声记忆时，私有学习也会出现，导致训练损失小但泛化性能差。

Conclusion: 研究凸显了私有学习的挑战，并证明了特征增强提升信噪比的益处，在合成和真实数据集上的实验验证了理论发现。

Abstract: Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings.

</details>


### [236] [SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression](https://arxiv.org/abs/2511.18936)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: SWAN是一种无需微调的KV缓存压缩框架，通过正交矩阵旋转和剪枝来减少内存占用，无需解压缩步骤，在50-60%内存节省下仍保持接近原始性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自回归推理时面临KV缓存内存占用过大的瓶颈，现有压缩技术存在信息丢失、固定限制或解压缩计算开销等问题。

Method: 使用离线正交矩阵对KV缓存进行旋转和剪枝，然后直接在注意力计算中使用压缩后的缓存，无需重建步骤。

Result: 实验表明SWAN在每token节省50-60%KV缓存内存的情况下，性能仍接近未压缩基线，且支持运行时可调压缩级别。

Conclusion: SWAN的无解压缩设计、高压缩性能下的良好表现以及适应性，使其成为服务长上下文LLMs的实用高效解决方案。

Abstract: Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.

</details>


### [237] [Curvature-Aware Safety Restoration In LLMs Fine-Tuning](https://arxiv.org/abs/2511.18039)
*Thong Bach,Thanh Nguyen-Tang,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 论文发现微调LLM会损害安全对齐，但安全行为并未被删除而是转移到参数空间的不重要区域。作者提出了一种曲率感知的对齐恢复方法，利用影响函数和二阶优化来选择性增加有害输入的损失，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型进行下游任务时往往会损害安全对齐，即使使用LoRA等参数高效方法也是如此。需要找到一种方法能够恢复安全对齐而不影响任务性能。

Method: 提出曲率感知对齐恢复方法，利用影响函数和二阶优化，在基础模型和微调模型共享的几何结构基础上，选择性增加有害输入的损失，同时保持任务相关性能。

Result: 在多个模型系列和对抗设置下的广泛评估表明，该方法能有效减少有害响应，同时保持甚至提高实用性和少样本学习性能。

Conclusion: 通过利用微调模型损失景观的几何结构特性，可以实现精确、低影响的更新来恢复安全对齐，避免完全回退到基础模型。

Abstract: Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.

</details>


### [238] [RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning](https://arxiv.org/abs/2511.19168)
*Deyi Ji,Yuekui Yang,Liqun Liu,Peng Shu,Haiyang Wu,Shaogang Tang,Xudong Chen,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.LG

TL;DR: RAVEN++是一个用于视频广告审核的增强框架，通过主动强化学习、细粒度违规理解和渐进式多阶段训练，显著提升了违规检测的精确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频广告审核模型（如RAVEN）在细粒度理解、可解释性和泛化能力方面存在不足，需要更先进的解决方案来应对复杂的视频广告内容审核挑战。

Method: 提出三个关键创新：1）主动强化学习动态适应不同难度样本；2）通过分层奖励函数和推理蒸馏实现细粒度违规理解；3）渐进式多阶段训练结合知识注入、课程式被动强化学习和主动强化学习。

Result: 在公开和专有数据集上的实验表明，RAVEN++在细粒度违规理解、推理能力和泛化能力方面优于通用大语言模型和专门模型如RAVEN，在线A/B测试也验证了其有效性。

Conclusion: RAVEN++框架成功解决了视频广告审核中的关键挑战，为复杂数字内容审核提供了更强大、可解释和泛化能力强的解决方案。

Abstract: Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.

</details>


### [239] [Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics](https://arxiv.org/abs/2511.18056)
*Maximilien Dreveton,Matthias Grossglauser,Daichi Kuroda,Patrick Thiran*

Main category: cs.LG

TL;DR: 本文提出了有效层次结构的概念，定义了有效层次结构的偏序关系，证明了最精细有效层次结构的存在性，并提出了一个两步算法来恢复这种层次结构。


<details>
  <summary>Details</summary>
Motivation: 传统层次聚类方法存在三个主要问题：总是返回层次结构（即使不存在）、仅限于二叉树结构、对链接函数选择高度敏感。本文旨在解决这些问题。

Method: 提出了有效层次结构的概念，定义了偏序关系，证明最精细有效层次结构的存在性。开发了两步算法：首先通过链接方法构建二叉树，然后进行剪枝以强制有效性。

Result: 建立了链接函数恢复最精细有效层次结构的充分必要条件，证明满足这些条件的链接函数在剪枝后产生相同的层次结构。经典链接规则（单链接、全链接、平均链接）满足条件，而Ward链接不满足。

Conclusion: 该方法能够识别数据中真正的层次结构，不受限于二叉树形式，在不存在层次结构时退化为星形树，解决了传统层次聚类的主要局限性。

Abstract: Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.

</details>


### [240] [A Nutrition Multimodal Photoplethysmography Language Model](https://arxiv.org/abs/2511.19260)
*Kyle Verrier,Achille Nazaret,Joseph Futoma,Andrew C. Miller,Guillermo Sapiro*

Main category: cs.LG

TL;DR: 提出了一种结合可穿戴设备PPG信号和饮食描述的营养光电容积脉搏波语言模型(NPLM)，用于无创饮食监测，相比纯文本基线将每日热量摄入预测准确率提高了11%。


<details>
  <summary>Details</summary>
Motivation: 饥饿和饱腹感动态影响饮食行为和代谢健康，但在日常环境中难以捕捉。需要开发能够结合生理测量和饮食信息的无创监测方法。

Method: 开发NPLM模型，将可穿戴设备的连续PPG信号投影到语言模型可解释的嵌入中，实现生理数据和饮食上下文的联合推理。基于19,340名参与者和110万餐食-PPG配对数据进行训练。

Result: 模型将每日热量摄入预测准确率提高了11%，且在80%餐食文本被移除时仍保持准确性。在独立验证研究(n=140)中复现了这些发现。

Conclusion: 结果表明将消费者可穿戴设备的生理测量与饮食信息相结合，对于大规模无创饮食监测具有重要价值。

Abstract: Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.

</details>


### [241] [pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data](https://arxiv.org/abs/2511.18066)
*Md Akil Raihan Iftee,Syed Md. Ahnaf Hasan,Mir Sazzat Hossain,Rakibul Hasan Rajib,Amin Ahsan Ali,AKM Mahbubur Rahman,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出了pFedBBN框架，用于解决联邦学习中测试时适应的类别不平衡问题，通过平衡批归一化和基于相似度的客户端协作来提升少数类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的测试时适应面临类别不平衡挑战，特别是在客户端数据分布不均且存在领域偏移时，现有方法无法在无监督条件下处理动态领域变化和类别不平衡。

Method: 使用平衡批归一化(BBN)在本地客户端适应中平等对待所有类别，通过BBN相似度指导客户端协作，采用类别感知的模型聚合策略实现个性化推理。

Result: 在多种基准测试中，pFedBBN相比现有联邦学习和测试时适应方法，显著提升了鲁棒性和少数类性能。

Conclusion: pFedBBN通过平衡特征归一化和领域感知协作，有效解决了联邦学习中类别不平衡和分布偏移问题，无需客户端提供任何标记或原始数据。

Abstract: Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods.

</details>


### [242] [CDLM: Consistency Diffusion Language Models For Faster Sampling](https://arxiv.org/abs/2511.19269)
*Minseo Kim,Chenfeng Xu,Coleman Hooper,Harman Singh,Ben Athiwaratkun,Ce Zhang,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: CDLM通过一致性建模和块级因果注意力掩码，将DLM的推理延迟降低3.6-14.5倍，同时保持数学和编程任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)具有并行生成优势，但存在推理速度慢的问题，主要由于需要大量细化步骤且无法使用标准KV缓存。

Method: CDLM结合一致性建模大幅减少采样步骤，通过多令牌最终化实现加速；在微调时强制使用块级因果注意力掩码，使模型完全兼容KV缓存。

Result: 实验显示CDLM在数学和编程任务上实现3.6-14.5倍的延迟降低，同时保持竞争力的准确率。

Conclusion: CDLM有效解决了DLMs的推理瓶颈，提供了训练基础的加速方法，代码已开源。

Abstract: Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.

</details>


### [243] [The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality](https://arxiv.org/abs/2511.18084)
*Dou Liu,Ying Long,Sophia Zuoqiu,Kaipeng Xie,Runze Yang,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.LG

TL;DR: 评估四种LLM对齐策略在临床决策中的表现，发现GRPO算法精度最高但医生更偏好SFT模型，揭示算法改进与临床信任之间的对齐悖论


<details>
  <summary>Details</summary>
Motivation: LLMs在临床决策支持中应用日益广泛，但如何使其与真实医学的多维度推理路径对齐仍是一个重大挑战

Method: 使用8,000多份不孕症治疗记录，系统评估SFT、DPO、GRPO和ICL四种对齐策略，采用自动基准测试与盲法医生评估相结合的双层框架

Result: GRPO在多个决策层实现最高算法精度，但临床医生一致偏好SFT模型，认为其推理过程更清晰、治疗可行性更高。在盲法配对比较中，SFT获得最高胜率(51.2%)，优于GRPO(26.2%)和医生原始决策(22.7%)

Conclusion: 算法改进不一定转化为更高的临床信任，可能与以人为中心的偏好相背离。需要优先考虑临床可解释性和实践可行性的对齐策略，而非仅优化决策级精度

Abstract: Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.

</details>


### [244] [MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings](https://arxiv.org/abs/2511.19279)
*Victor Rambaud,Salvador Mascarenhas,Yair Lakretz*

Main category: cs.LG

TL;DR: MapFormers是基于Transformer的新架构，能够从观测数据中学习认知地图并并行执行路径整合，通过输入依赖的位置编码实现结构与内容的解耦，在OOD泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 认知地图能够编码实体间的抽象关系，赋予人类和动物适应新情境的灵活性，而当前AI系统缺乏这种强大的OOD泛化能力。

Method: 开发了两种MapFormers变体，统一绝对和相对位置编码来分别建模情景记忆和工作记忆，通过更新Transformer的位置编码为输入依赖矩阵来实现结构-内容解耦。

Result: 在包括经典2D导航任务在内的多个任务中测试，MapFormers能够学习底层空间的认知地图，并在OOD泛化（如更长序列）方面达到近乎完美的性能，优于现有架构。

Conclusion: 结果表明，设计用于学习认知地图的模型具有优越性，引入结构偏置实现结构-内容解耦的重要性，这可以通过Transformer中的输入依赖位置编码实现。

Abstract: A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.

</details>


### [245] [A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization](https://arxiv.org/abs/2511.18093)
*Fulong Yao,Wanqing Zhao,Matthew Forshaw*

Main category: cs.LG

TL;DR: 提出一种新的误差时间差分(ETD)算法，用于解决微电网能量优化中深度强化学习预测控制的不确定性问题，提高微电网运行性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的预测控制方法往往忽视预测模型不完善带来的不确定性，导致控制策略次优。

Method: 建立含可再生能源和储能系统的微电网MDP模型；提出基于DQN的预测控制方法，设计加权平均算法量化预测不确定性，并开发ETD算法处理不确定性。

Result: 在真实美国数据集上的仿真表明，所开发的ETD算法有效提升了深度强化学习在微电网优化中的性能。

Conclusion: ETD算法能够有效解决预测不确定性，改善微电网能量优化控制策略。

Abstract: Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations.

</details>


### [246] [Active Learning with Selective Time-Step Acquisition for PDEs](https://arxiv.org/abs/2511.18107)
*Yegon Kim,Hyunsu Kim,Gyeonghoon Ko,Juho Lee*

Main category: cs.LG

TL;DR: 提出了一种用于PDE代理建模的主动学习框架，通过仅生成最重要的时间步来显著降低数值求解器的计算成本，同时提高代理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解PDE计算成本高，而代理模型开发受限于从数值求解器生成足够训练数据的成本。需要一种更高效的方法来减少数据生成成本。

Method: 开发了一种主动学习框架，策略性地仅用数值求解器生成最重要的时间步，同时使用代理模型近似其余步骤。设计了基于方差减少效用的采集函数。

Result: 在多个基准PDE上的实验表明，该方法大幅优于现有最佳方法，不仅降低了平均误差，还改善了99%、95%和50%分位数的误差。

Conclusion: 该方法为PDE代理建模提供了一种数据高效的解决方案，显著减少了计算成本并提高了模型性能。

Abstract: Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\%, 95\%, and 50\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.

</details>


### [247] [Vulnerability-Aware Robust Multimodal Adversarial Training](https://arxiv.org/abs/2511.18138)
*Junrui Zhang,Xinyu Zhao,Jie Peng,Chenjie Wang,Jianmin Ji,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文提出VARMAT方法，通过显式量化每个模态的脆弱性并进行针对性正则化，提升多模态模型的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了不同模态对最终鲁棒性的贡献差异，导致鲁棒性表现不佳。多模态模型由于模态间依赖关系更容易受到对抗攻击。

Method: VARMAT方法：1) 基于攻击目标的一阶近似显式量化每个模态的脆弱性（Probe）；2) 提出针对性正则化项，惩罚高脆弱性模态，在保持任务准确性的同时引导鲁棒学习（Training）。

Result: 在多个涉及不同模态的多模态数据集上验证了方法的鲁棒性提升，在三个多模态数据集上分别实现了{12.73%, 22.21%, 11.19%}的鲁棒性改进。

Conclusion: 该方法揭示了多模态对抗训练中的一个重要盲点，通过识别模态脆弱性显著提升了多模态模型的对抗鲁棒性。

Abstract: Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.

</details>


### [248] [Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric](https://arxiv.org/abs/2511.19350)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.LG

TL;DR: 提出了一种可扩展的光谱方法，用于自动估计短文本嵌入的聚类数量，并提出了Cohesion Ratio指标来评估无监督聚类质量。


<details>
  <summary>Details</summary>
Motivation: 短文本嵌入聚类是NLP的基础任务，但传统方法需要预先指定聚类数量，这在实际应用中具有挑战性。

Method: 使用基于余弦相似度的拉普拉斯特征谱结构来估计聚类数量，采用自适应采样策略实现可扩展性，并提出Cohesion Ratio指标进行无监督评估。

Result: 在6个短文本数据集和4个嵌入模型上的实验表明，使用该方法指导的K-Means和HAC算法显著优于HDBSCAN、OPTICS和Leiden等参数较少的方法。

Conclusion: 该光谱估计器和Cohesion Ratio指标为短文本数据的无监督组织和评估提供了实用价值。

Abstract: Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.

</details>


### [249] [Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction](https://arxiv.org/abs/2511.18150)
*Randy Davila,Beyzanur Ispir*

Main category: cs.LG

TL;DR: 比较CNN和GNN在近似图支配数方面的性能，GNN在准确性和速度上都显著优于CNN，提供了超过200倍的加速。


<details>
  <summary>Details</summary>
Motivation: 图支配数的精确计算是NP难问题，传统方法只能处理小规模实例，需要开发高效的近似方法。

Method: 使用卷积神经网络（CNN）处理邻接矩阵表示，和图神经网络（GNN）通过消息传递直接从图结构学习，在2000个最多64个顶点的随机图上进行测试。

Result: GNN达到R²=0.987，MAE=0.372的更高准确率，显著优于CNN的R²=0.955，MAE=0.500。GNN提供超过200倍的加速，同时保持近乎完美的保真度。

Conclusion: GNN可作为组合图不变量的实用替代方法，对可扩展图优化和数学发现具有重要意义。

Abstract: We investigate machine learning approaches to approximating the \emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.

</details>


### [250] [scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python](https://arxiv.org/abs/2511.18157)
*Martin Schuck,Alexander von Rohr,Angela P. Schoellig*

Main category: cs.LG

TL;DR: 对SciPy的spatial.transform模块进行了全面重写，使其支持任何实现Python数组API的数组库，包括JAX、PyTorch和CuPy，从而在保持原有接口的同时实现GPU/TPU执行、JIT编译、向量化批处理和自动微分。


<details>
  <summary>Details</summary>
Motivation: 三维刚体变换在现代可微分机器学习管道中至关重要，但数值鲁棒且数学正确的实现容易出错。SciPy的spatial.transform模块虽然经过严格测试，但历史上只支持NumPy，限制了在GPU加速和自动微分工作流中的采用。

Method: 完全重写SciPy的spatial.transform功能，使其与任何实现Python数组API的数组库兼容，保留已建立的SciPy接口，同时支持GPU/TPU执行、JIT编译、向量化批处理和通过所选后端的原生自动微分。

Result: 修订后的实现支持GPU/TPU执行、JIT编译、向量化批处理和自动微分，通过两个案例研究展示了其在可微分科学计算中的应用：3D变换和旋转的可扩展性，以及利用SciPy的Rotation进行旋转动力学精确积分的JAX无人机模拟。

Conclusion: 该贡献已合并到SciPy主分支，将在下一个版本中发布，为可微分系统和机器学习中的3D空间数学提供了一个框架无关、生产级的基础。

Abstract: Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's spatial.transform module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's spatial.transform functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML.

</details>


### [251] [LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation](https://arxiv.org/abs/2511.18158)
*Abdelrahman Abdelmotlb,Abdallah Taman,Sherif Mostafa,Moustafa Youssef*

Main category: cs.LG

TL;DR: LocaGen是一个基于条件扩散模型的空间增强框架，通过在未见位置生成高质量合成数据，显著减少室内定位指纹采集的开销。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位需要大量位置标记信号数据，采集成本高，限制了实际部署。现有方法要么表达能力不足，要么存在模式崩溃问题，或者仍需在所有目标位置采集数据。

Method: 使用条件扩散模型结合空间感知优化策略，仅使用部分已见位置数据合成未见位置的指纹；通过领域特定启发式方法增强已见位置数据，并采用基于密度的策略选择已见和未见位置以确保鲁棒覆盖。

Result: 在真实WiFi指纹数据集上的评估显示，即使30%位置未见，LocaGen仍能保持相同定位精度，相比最先进的增强方法精度提升达28%。

Conclusion: LocaGen通过空间增强框架有效减少了指纹采集开销，在保持定位精度的同时显著提升了部署可行性。

Abstract: Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.

</details>


### [252] [Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models](https://arxiv.org/abs/2511.18159)
*Mengni Jia,Mengyu Zhou,Yihao Liu,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散模型训练方差高的根本原因，提出了两种核心方差降低方法P-POTS和MIRROR，显著提升了MDM在复杂推理任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型相比自回归模型存在训练方差过高的问题，导致梯度估计噪声大、优化不稳定，即使初始性能相当，在任务特定训练后MDM往往落后于ARM。

Method: 将MDM训练方差分解为三个来源：掩码模式噪声、掩码率噪声和数据噪声；设计了六种方差降低方法，核心包括P-POTS（帕累托最优t采样器）和MIRROR（使用负相关样本减少掩码模式噪声）。

Result: 相比标准MDM训练，新方法在复杂推理任务上准确率提升7-8%，同时将运行间变异性降低到接近ARM水平，显著缩小了与强ARM基线的差距。

Conclusion: 通过理论分析和系统性的方差降低方法，成功解决了MDM训练方差高的问题，使其在保持生成质量的同时达到与ARM相当的训练稳定性。

Abstract: Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method.

</details>


### [253] [Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability](https://arxiv.org/abs/2511.18178)
*Shrenik Zinage,Peter Meckl,Ilias Bilionis*

Main category: cs.LG

TL;DR: 提出贝叶斯校准框架，结合高斯过程和近似贝叶斯计算来推断和校正传感器偏差，提高发动机NOx排放预测的准确性和跨发动机泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统模型在少量发动机数据上训练，难以泛化到整个发动机群体，存在传感器偏差和输入条件变化问题，需要能够适应发动机间变异性的模型。

Method: 使用贝叶斯校准框架，结合高斯过程和近似贝叶斯计算，从预训练模型出发推断发动机特定传感器偏差并重新校准预测。

Result: 该方法在未见测试数据上生成发动机NOx的后验预测分布，相比传统非自适应高斯过程模型显著提高了预测准确性。

Conclusion: 提出的可迁移建模方法有效解决了发动机间变异性问题，提高了模型泛化能力，无需重新训练模型即可实现高精度预测。

Abstract: Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability.

</details>


### [254] [MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning](https://arxiv.org/abs/2511.18181)
*Adam Callaghan,Karl Mason,Patrick Mannion*

Main category: cs.LG

TL;DR: 提出了首个针对连续状态和动作空间的多目标多智能体强化学习框架MOMA-AC，基于TD3和DDPG算法，能够用一个神经网络编码所有智能体在冲突目标下的帕累托最优策略前沿。


<details>
  <summary>Details</summary>
Motivation: 填补多目标多智能体强化学习在连续状态和动作空间中的研究空白，解决现有方法无法有效处理多智能体环境中多个冲突目标的优化问题。

Method: 结合多头actor网络、集中式critic和目标偏好条件架构，基于TD3和DDPG算法实例化为MOMA-TD3和MOMA-DDPG，通过现有物理模拟器构建连续MOMARL测试套件。

Result: 在合作运动任务中，相比外层循环和独立训练基线，在期望效用和超体积指标上取得统计显著改进，且随着智能体数量增加保持稳定可扩展性。

Conclusion: 该框架为连续多智能体领域中的稳健、可扩展多目标策略学习奠定了重要基础。

Abstract: This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.

</details>


### [255] [MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding](https://arxiv.org/abs/2511.18294)
*Mengchun Zhang,Kateryna Shapovalenko,Yucheng Shao,Eddie Guo,Parusha Pradhan*

Main category: cs.LG

TL;DR: MultiDiffNet是一个基于扩散模型的框架，通过优化多目标学习的紧凑潜在空间，实现了跨被试的脑电图解码，在多个任务上达到最先进的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 脑电图神经解码面临跨被试泛化能力差的问题，主要由于被试间差异大且缺乏大规模数据集。现有方法依赖合成被试生成或简单数据增强，但无法可靠扩展或泛化。

Method: 提出MultiDiffNet扩散框架，绕过生成式增强，直接学习优化多目标的紧凑潜在空间，并从中进行解码。同时构建了统一的基准测试套件和统计报告框架。

Result: 在四个复杂度递增的脑电图解码任务（SSVEP、运动想象、P300、想象语音）上，使用被试和会话分离评估，实现了最先进的泛化性能。

Conclusion: 该工作为现实世界脑机接口系统中的被试无关脑电图解码提供了可复现的开源基础。

Abstract: Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.

</details>


### [256] [Accelerating Time Series Foundation Models with Speculative Decoding](https://arxiv.org/abs/2511.18191)
*Pranav Subbaraman,Fang Sun,Yue Yao,Huacong Tang,Xiao Luo,Yizhou Sun*

Main category: cs.LG

TL;DR: 提出了一种基于推测解码的时间序列预测推理加速框架，使用小模型生成预测补丁，大模型并行验证，显著减少顺序前向传播次数，在保持精度的同时提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型Transformer模型在时间序列预测中表现出色，但计算成本高，难以在延迟敏感的web应用中部署。需要一种无需修改模型架构的推理加速方法。

Method: 采用推测解码框架，使用小规模"草稿"模型预测时间序列补丁，然后用大规模"目标"模型并行验证这些预测，设计多变量高斯补丁的接受标准，平衡效率与精度。

Result: 在web应用相关的时间序列预测基准测试中，实现了显著的推理加速，同时保持了有竞争力的预测精度。

Conclusion: 该框架无需修改现有基础模型架构，可立即应用于已部署的时间序列预测系统，有效解决了大模型在延迟敏感场景下的部署挑战。

Abstract: Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller "draft" model to propose future time-series patches, which are then verified in parallel by a larger "target" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at https://github.com/PranavSubbaraman/STRIDE

</details>


### [257] [AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert](https://arxiv.org/abs/2511.18314)
*Yuting Gao,Wang Lan,Hengyuan Zhao,Linjiang Huang,Si Liu,Qingpei Guo*

Main category: cs.LG

TL;DR: AnyExperts提出了一种按需、预算感知的动态路由框架，通过基于语义重要性为每个token分配可变数量的专家槽位，优化多模态MoE模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态MoE模型使用固定的路由策略，忽略了不同模态间语义重要性的异质性，导致计算资源分配不优，冗余token与关键token消耗相同资源。

Method: 提出AnyExperts框架：为每个token分配可变数量的专家槽位，但总数限制在固定范围内；每个槽位由真实专家或虚拟专家填充，虚拟专家比例上限为20%；模型自适应平衡真实与虚拟专家比例，为语义丰富区域分配更多真实专家。

Result: 在视觉理解、音频理解和NLP理解任务中，AnyExperts在相同计算预算下提升性能：在通用图像/视频任务中，使用40%更少的真实专家激活达到相当精度；在文本密集任务中，减少10%真实专家使用同时保持性能。

Conclusion: 细粒度、重要性驱动的专家分配显著提升了多模态MoE模型的效率和有效性。

Abstract: Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.

</details>


### [258] [Deep Gaussian Process Proximal Policy Optimization](https://arxiv.org/abs/2511.18214)
*Matthijs van der Lende,Juan Cardenas-Cartagena*

Main category: cs.LG

TL;DR: 提出GPPO算法，结合深度高斯过程和PPO，在保持性能的同时提供校准的不确定性估计，用于更安全的强化学习探索。


<details>
  <summary>Details</summary>
Motivation: 强化学习需要不确定性估计来平衡安全探索和高效学习，但深度神经网络往往缺乏校准的不确定性估计。

Method: 开发Deep Gaussian Process Proximal Policy Optimization (GPPO)，使用深度高斯过程近似策略和价值函数，是可扩展的模型无关actor-critic算法。

Result: 在标准高维连续控制基准测试中，GPPO保持了与PPO相当的性能，同时提供了良好校准的不确定性估计。

Conclusion: GPPO能够为强化学习提供可靠的不确定性估计，从而支持更安全有效的探索策略。

Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.

</details>


### [259] [Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support](https://arxiv.org/abs/2511.18334)
*Chibuike E. Ugwu,Roschelle Fritz,Diane J. Cook,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 提出了一种结合临床医生参与的智能家居系统，利用环境传感器数据检测老年人尿路感染发作，通过不确定性量化方法提供更可靠的决策支持。


<details>
  <summary>Details</summary>
Motivation: 老年人尿路感染发作风险高，传统机器学习方法缺乏预测不确定性信息，限制了临床决策的有效性。

Method: 采用临床医生参与循环的智能家居系统，提取行为标记，训练预测模型，并使用符合性校准区间方法进行不确定性量化。

Result: 在8个真实智能家居数据上评估，该方法在召回率等分类指标上优于基线方法，同时保持最低的弃权比例和区间宽度。

Conclusion: 42名护士的调查证实该系统输出对临床决策具有实际价值，能有效改善老年人尿路感染和其他病症发作的管理。

Abstract: Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions ("I don't know") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults.

</details>


### [260] [Adaptive Conformal Prediction for Quantum Machine Learning](https://arxiv.org/abs/2511.18225)
*Douglas Spencer,Samual Nicholls,Michele Caprio*

Main category: cs.LG

TL;DR: 提出了自适应量子保形预测(AQCP)算法，通过持续重新校准来应对量子处理器中的时变噪声，在任意硬件噪声条件下保持渐近平均覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习需要可靠的不确定性量化方法，但当前量子领域缺乏稳健的方法。量子保形预测虽然能保证预测集包含真实结果的概率，但量子处理器的时变噪声会破坏这种保证。

Method: 基于自适应保形推理方法，引入自适应量子保形预测(AQCP)算法，通过重复重新校准来维持随时间变化的有效性。

Result: 在IBM量子处理器上的实证研究表明，AQCP能够达到目标覆盖水平，并且比量子保形预测表现出更好的稳定性。

Conclusion: AQCP算法能够有效应对量子硬件噪声，为量子机器学习提供了更可靠的不确定性量化方法。

Abstract: Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.

</details>


### [261] [Tail Distribution of Regret in Optimistic Reinforcement Learning](https://arxiv.org/abs/2511.18247)
*Sajad Khodadadian,Mehrdad Moharrami*

Main category: cs.LG

TL;DR: 本文推导了基于乐观策略的强化学习在有限时域表格MDP中的实例依赖后悔尾界，分析了UCBVI类算法在两种探索奖励方案下的后悔分布特性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注期望后悔或单一高概率分位数，缺乏对后悔尾分布的全面分析。本文旨在填补这一空白，提供更完整的后悔分布特性描述。

Method: 采用UCBVI类算法，分析两种探索奖励方案：(i) K依赖方案，显式包含总回合数K；(ii) K独立方案，仅依赖当前回合索引。通过调节参数α来平衡期望后悔和子高斯尾的范围。

Result: 获得了后悔尾概率的上界，展现出独特的双机制结构：从实例依赖尺度m_K到转移阈值的子高斯尾，之后是子韦布尔尾。同时推导了相应的期望后悔实例依赖界。

Conclusion: 本文为episodic强化学习中标准乐观算法提供了首批全面的后悔尾保证，揭示了后悔分布的精细结构特性。

Abstract: We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\Pr(R_K \ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.

</details>


### [262] [Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck](https://arxiv.org/abs/2511.18404)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: MVCIB是一个多视图条件信息瓶颈框架，用于在2D和3D分子结构上自监督预训练图神经网络，通过跨视图子图对齐和共享信息提取来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决多视图分子学习中的两个主要挑战：发现视图间共享信息同时减少视图特定信息；识别并对齐重要子结构（如功能基团）以增强跨视图一致性和模型表达能力。

Method: 提出MVCIB框架，使用一个视图作为上下文条件来指导另一个视图的表示学习；利用关键子结构作为视图间锚点；提出跨注意力机制捕获子结构间的细粒度相关性以实现跨视图子图对齐。

Result: 在四个分子领域的大量实验表明，MVCIB在预测性能和可解释性方面始终优于基线方法；MVCIB达到了3D Weisfeiler-Lehman表达能力，能够区分非同构图以及具有相同2D连接但不同3D几何结构的异构体。

Conclusion: MVCIB通过多视图条件信息瓶颈和子图对齐机制，有效解决了分子多视图学习的关键挑战，在分子表示学习中取得了优越性能。

Abstract: Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.

</details>


### [263] [Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj](https://arxiv.org/abs/2511.18248)
*Wei Zhen Teoh*

Main category: cs.LG

TL;DR: 提出CausalTraj模型，专注于生成联合概率的多智能体轨迹预测，在保持个体预测精度的同时显著提升了联合预测质量。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要基于个体精度指标（minADE、minFDE）进行优化，忽视了联合预测的合理性，无法生成连贯的多智能体场景。

Method: 构建时间因果、基于似然的模型CausalTraj，专门设计用于生成联合概率的多智能体轨迹预测。

Result: 在NBA SportVU、Basketball-U和Football-U数据集上，CausalTraj在个体精度指标上表现竞争性，在联合指标（minJADE、minJFDE）上取得最佳记录，生成定性连贯的现实比赛演化。

Conclusion: CausalTraj通过关注联合预测质量，在保持个体预测精度的同时显著提升了多智能体轨迹预测的联合合理性和现实性。

Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.

</details>


### [264] [Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data](https://arxiv.org/abs/2511.18260)
*Yueqi Wang,Guang Lin*

Main category: cs.LG

TL;DR: RB-DeepONet是一种混合算子学习框架，将降基数值结构与DeepONet的分支-主干架构融合，通过物理可解释的降基空间实现高效、稳定的参数化PDE求解。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法存在不透明的学习主干、需要大量标注数据、在边界和源数据独立变化时失效等问题，需要一种物理可解释且稳定的加速求解方法。

Method: 使用贪婪选择离线构建降基空间作为固定主干，分支网络预测降基系数，采用无标签训练的投影变分残差方法，结合边界和源模态编码处理独立变化的边界条件。

Result: RB-DeepONet在精度上与侵入式RB-Galerkin、POD-DeepONet和FEONet相当，但使用更少的可训练参数并实现显著加速，具有严格的离线-在线分离。

Conclusion: RB-DeepONet为大规模参数化PDE提供了一种高效、稳定且可解释的算子学习方法，结合了数值方法的严谨性和深度学习的高效性。

Abstract: Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.

</details>


### [265] [A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks](https://arxiv.org/abs/2511.18269)
*Ved Mohan,El Mehdi Er Raqabi,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出一个结合运筹学和机器学习的框架，解决大规模物流网络中的资源替代问题，实现公平高效的资源分配。


<details>
  <summary>Details</summary>
Motivation: 大规模物流网络中资源分配不平衡的挑战，以及分散环境下实现全局协调解决方案的困难，需要同时考虑公平性和调度员偏好。

Method: OR组件在公平视角下建模和解决资源替代问题，ML组件利用历史数据学习调度员偏好，智能探索决策空间，通过动态选择top-κ资源提高计算效率。

Result: 在世界上最大的包裹递送公司网络中应用，相比现有方法模型规模减少80%，执行时间减少90%，同时保持最优性。

Conclusion: 该框架能产生高质量解决方案组合，使调度员能够选择满意的权衡方案，在大规模物流网络中实现了显著改进。

Abstract: Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$κ$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.

</details>


### [266] [Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems](https://arxiv.org/abs/2511.18417)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了类别等变神经网络(CENNs)的统一理论，将群/群胚等变网络、偏序集/格等变网络、图和层神经网络统一起来，证明了等变通用逼近定理。


<details>
  <summary>Details</summary>
Motivation: 统一各种等变神经网络理论，将等变深度学习从群作用扩展到更广泛的范畴，包括几何对称性、上下文对称性和组合对称性。

Method: 在具有Radon测度的拓扑范畴中制定等变性，在线性和非线性层中采用范畴化设置，构建类别等变神经网络框架。

Result: 证明了在一般设置下的等变通用逼近定理：有限深度CENNs类在连续等变变换空间中稠密，为群/群胚、偏序集/格、图和胞腔层等具体实例推导了通用逼近定理。

Conclusion: 范畴等变深度学习扩展了等变深度学习的视野，超越了群作用，涵盖了更广泛的对称性类型。

Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.

</details>


### [267] [From Tables to Signals: Revealing Spectral Adaptivity in TabPFN](https://arxiv.org/abs/2511.18278)
*Jianqiao Zheng,Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: TabPFN作为表格基础模型具有比标准ReLU-MLP更广的有效频率容量，其频谱能力能根据上下文样本数量自适应调整，无需超参数调优即可实现训练自由的图像去噪。


<details>
  <summary>Details</summary>
Motivation: 理解任务无关的表格基础模型（如TabPFN）的归纳偏置来源及其在上下文学习中的频率特性。

Method: 通过信号重构的视角分析TabPFN，研究其频率特性、频谱自适应性和位置编码对频率响应的影响。

Result: 发现TabPFN具有比ReLU-MLP更广的频率容量，频谱能力能根据上下文样本数自适应，位置编码调节频率响应，可实现无训练和无超参数调优的图像去噪。

Conclusion: 该分析揭示了表格基础模型的结构和归纳偏置，展示了其在更广泛信号重构任务中的潜力。

Abstract: Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks.

</details>


### [268] [TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis](https://arxiv.org/abs/2511.18287)
*Rui Peng,Ziru Liu,Lingyuan Ye,Yuxing Lu,Boxin Shi,Jinzhuo Wang*

Main category: cs.LG

TL;DR: TRIDENT是一个级联生成框架，通过同时考虑扰动和相应基因表达谱来合成真实的细胞形态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于建模直接关联（如扰动→RNA或扰动→形态），而忽略了RNA到形态的关键因果联系。

Method: 提出TRIDENT级联生成框架，构建MorphoGene数据集（包含98种化合物的L1000基因表达和Cell Painting图像配对），通过RNA条件化合成细胞形态。

Result: TRIDENT显著优于最先进方法，实现高达7倍的改进，对未见化合物具有强泛化能力。在docetaxel案例研究中验证了RNA引导合成的准确性。

Conclusion: 通过显式建模转录组-表型映射，TRIDENT提供了一个强大的计算机模拟工具，使我们更接近预测性虚拟细胞。

Abstract: Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\rightarrow$ RNA or Perturbation $\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.

</details>


### [269] [ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning](https://arxiv.org/abs/2511.18291)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: ADF-LoRA通过同步更新单个低秩矩阵并在去中心化联邦学习中混合两个矩阵，解决了交替低秩更新在去中心化环境中的相位状态不匹配和块间发散问题，实现了更稳定和快速的收敛。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦学习中，交替更新LoRA矩阵会因客户端间的相位状态不匹配和块间发散而面临挑战，需要一种机制来保持参数状态的一致性。

Method: 提出ADF-LoRA方法，每轮只同步更新一个低秩矩阵，并混合两个矩阵以在去中心化传播中保持更一致的参数状态，同时保留交替更新的交叉项抑制效果。

Result: 在多个GLUE任务上的实验表明，ADF-LoRA实现了更快、更平滑的收敛，并在去中心化联邦学习中以一致的优势获得了最高的平均准确率。

Conclusion: ADF-LoRA在去中心化联邦学习中有效解决了交替低秩更新的稳定性问题，优于现有的LoRA变体，提供了更可靠的性能。

Abstract: This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.

</details>


### [270] [KAN vs LSTM Performance in Time Series Forecasting](https://arxiv.org/abs/2511.18613)
*Tabish Ali Rather,S M Mahmudul Hasan Joy,Nadezda Sukhorukova,Federico Frascoli*

Main category: cs.LG

TL;DR: LSTM在股票价格预测中显著优于KAN，在准确性方面表现更佳，而KAN虽然在理论可解释性上有优势但误差较高，适用于资源受限但对准确性要求不高的场景。


<details>
  <summary>Details</summary>
Motivation: 比较KAN和LSTM在非确定性股票价格数据预测中的性能，评估预测准确性与可解释性之间的权衡。

Method: 使用均方根误差(RMSE)评估KAN和LSTM在不同预测时间范围内的预测性能。

Result: LSTM在所有测试的预测时间范围内都表现出显著优势，而标准KAN显示出明显更高的错误率和有限的实际应用性。

Conclusion: LSTM在时间序列预测应用中占据主导地位，而KAN的主要优势在于计算效率，适合资源受限但对准确性要求不高的场景。

Abstract: This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements.

</details>


### [271] [GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis](https://arxiv.org/abs/2511.18297)
*Kiran Thorat,Hongwu Peng,Yuebo Luo,Xi Xie,Shaoyi Huang,Amit Hasan,Jiahui Zhao,Yingjie Li,Zhijie Shi,Cunxi Yu,Caiwen Ding*

Main category: cs.LG

TL;DR: GROOT是一个算法与系统协同设计的框架，通过结合芯片设计领域知识、图分区和GPU内核优化，显著提高了芯片验证效率，在保持高精度的同时大幅减少了内存占用和运行时间。


<details>
  <summary>Details</summary>
Motivation: 传统芯片验证方法耗时且计算量大，特别是对于大规模电路。虽然图神经网络(GNNs)有潜力提高验证效率，但缺乏一个综合考虑芯片设计领域知识、图论和GPU内核设计的联合框架。

Method: 1) 利用AIG图中节点类型和连接极性创建节点特征；2) 使用图分区算法将大图划分为小图以便GPU快速处理；3) 开发图边再生算法恢复验证精度；4) 针对EDA图工作负载的极化分布特点，重新设计HD-kernel和LD-kernel两个GPU内核。

Result: GROOT在1024位CSA乘法器(1.34亿节点，2.68亿边)上实现了：内存占用减少59.38%，准确率达到99.96%；与cuSPARSE、MergePath-SpMM和GNNAdvisor相比，运行时间分别提升1.104x、5.796x和1.469x。

Conclusion: GROOT通过算法与系统协同设计，成功解决了大规模芯片验证的效率问题，在保持高精度的同时显著提升了性能，为芯片设计验证提供了有效的解决方案。

Abstract: Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively.

</details>


### [272] [Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery](https://arxiv.org/abs/2511.18303)
*Rui Ding,Rodrigo Pires Ferreira,Yuxin Chen,Junhong Chen*

Main category: cs.LG

TL;DR: 提出了一种用于复杂材料和设备发现的长时程分层深度研究代理，通过本地可部署的DR实例结合检索增强生成和深度研究树机制，在27个纳米材料/设备主题上表现优于商业系统且成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器学习代理和商业系统在复杂材料和设备发现问题上覆盖范围有限、无法与本地数据和工具集成的问题。

Method: 采用本地可部署的DR实例，集成检索增强生成与大型语言模型推理器，通过深度研究树机制自适应扩展和修剪研究分支以最大化覆盖范围、深度和连贯性。

Result: 在27个纳米材料/设备主题上的评估显示，该DR代理生成报告的质量与商业系统相当甚至更好，成本显著降低，并能与本地数据和工具集成。

Conclusion: 该深度研究代理为复杂材料和设备发现提供了一种高效、低成本且可本地集成的解决方案，性能优于现有商业系统。

Abstract: We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.

</details>


### [273] [DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling](https://arxiv.org/abs/2511.18312)
*Zihao Yao,Jiankai Zuo,Yaying Zhang*

Main category: cs.LG

TL;DR: 提出了DiM-TS模型，通过融合Lag Fusion Mamba和Permutation Scanning Mamba来增强时间序列生成能力，解决了现有方法在捕捉长期时间依赖和复杂通道关系方面的不足。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多个领域至关重要但面临隐私问题，现有扩散模型在捕捉长期时间依赖和复杂通道关系方面存在困难。

Method: 提出Lag Fusion Mamba和Permutation Scanning Mamba两个变体，分别解决相关时间滞后和通道排列问题，然后整合成DiM-TS模型用于高质量时间序列生成。

Result: 理论分析显示两个变体与原始Mamba具有统一的矩阵乘法框架，在公共数据集上的实验证明了DiM-TS在生成真实时间序列和保持数据多样性方面的优越性。

Conclusion: DiM-TS是一个高质量的时间序列生成模型，能更好地保持时间周期性和通道间相关性，为时间序列数据合成提供了有效解决方案。

Abstract: Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data.

</details>


### [274] [DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations](https://arxiv.org/abs/2511.18331)
*Sohini Roychowdhury,Adam Holeman,Mohammad Amin,Feng Wei,Bhaskar Mehta,Srihari Reddy*

Main category: cs.LG

TL;DR: Dynamix是一个可扩展的个性化序列探索框架，通过最大相关性原则和基于事件特征的自监督学习来优化用户-广告互动历史处理，在保持广告预测准确性的同时提高训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在线广告推荐系统中处理完整的用户-广告互动历史既计算密集又容易受到噪声影响，需要一种更高效的序列处理方法。

Method: 使用最大相关性原则和基于事件特征的自监督学习，在会话和界面级别对用户互动进行分类，通过动态特征移除和选择性特征增强来优化处理。

Result: 动态资源移除使训练和推理吞吐量分别提高1.15%和1.8%，动态特征增强提供0.033 NE增益，同时推理QPS提高4.2%。

Conclusion: Dynamix在基于用户序列的在线推荐模型中实现了显著的成本效率和性能改进，自监督用户分割和资源探索可以进一步优化复杂特征选择策略。

Abstract: For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.

</details>


### [275] [Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost](https://arxiv.org/abs/2511.18643)
*Haojun Xia,Xiaoxia Wu,Jisen Li,Robert Wu,Junxiong Wang,Jue Wang,Chenxi Li,Aman Singhal,Alay Dilipbhai Shah,Alpay Ariyak,Donglin Zhuang,Zhongzhu Zhou,Ben Athiwaratkun,Zhen Zheng,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: Kitty是一个算法-系统协同设计的混合精度KV缓存方案，通过动态通道精度提升技术，在接近2位内存占用的同时保持精度损失接近零，实现KV内存减少近8倍，吞吐量提升2.1-4.1倍。


<details>
  <summary>Details</summary>
Motivation: KV缓存是LLM推理的主要内存瓶颈，4位KV量化能保持精度但2位量化在长上下文推理中会显著降低精度，需要解决这一差距。

Method: 采用动态通道精度提升技术，按敏感度对Key缓存通道排序，仅保留小部分高精度通道；系统层面设计页面中心KV布局、Triton兼容的页面反量化内核和轻量级运行时流水线。

Result: 在七个任务和两个模型家族上，Kitty将KV内存减少近8倍，精度损失可忽略，在相同内存预算下支持8倍更大的批次和2.1-4.1倍更高的吞吐量。

Conclusion: Kitty通过算法-系统协同设计成功解决了2位KV量化的精度问题，实现了接近2位内存占用的高效KV缓存方案。

Abstract: The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty.

</details>


### [276] [Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers](https://arxiv.org/abs/2511.18670)
*Rowan Bradbury,Aniket Srinivasan Ashok,Sai Ram Kasanagottu,Gunmay Jhingran,Shuai Meng*

Main category: cs.LG

TL;DR: 提出了确定性连续替换（DCR）方法，通过确定性退火权重混合教师和学生输出来解决预训练模型中模块替换的稳定性问题，相比随机替换方法具有更快的收敛速度和更好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 在预训练模型中替换模块（特别是将二次自注意力替换为高效注意力替代方案）会面临困难的优化问题：冷启动重新初始化会破坏冻结骨干网络的稳定性。

Method: 确定性连续替换（DCR）方法，使用确定性退火权重来混合教师和学生的输出，消除了随机替换中固有的门控梯度方差。

Result: 在单种子研究中，DCR在受控注意力替换任务上比随机门控和蒸馏基线方法实现了更快的收敛速度和更强的对齐效果。

Conclusion: DCR为异构算子交换建立了基础，解决了预训练模型中模块替换的稳定性挑战。

Abstract: Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps.

</details>


### [277] [Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection](https://arxiv.org/abs/2511.18336)
*Kaito Shiku,Kazuya Nishimura,Shinnosuke Matsuo,Yasuhiro Kojima,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出Auxiliary Gene Learning (AGL)方法，通过将忽略基因的表达估计重新表述为辅助任务并与主要任务联合训练，利用被忽略基因的益处。为解决辅助基因选择问题，提出基于先验知识的可微分top-k基因选择方法(DkGSB)。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学(ST)技术在测量过程中常引入严重观测噪声，以往研究仅使用高变异基因子集进行训练和评估，忽略了可能对目标基因估计有贡献的低表达基因。

Method: AGL方法将忽略基因的表达估计作为辅助任务与主要任务联合训练；DkGSB方法利用先验知识对基因排序，将组合选择问题松弛为可微分top-k选择问题。

Result: 实验证实了整合辅助基因的有效性，所提方法优于传统的辅助任务学习方法。

Conclusion: 通过合理选择辅助基因并联合训练，可以显著提升空间转录组学中基因表达预测的性能。

Abstract: Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.

</details>


### [278] [VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking](https://arxiv.org/abs/2511.18692)
*Kichang Yang,Seonjun Kim,Minjae Kim,Nairan Zhang,Chi Zhang,Youngki Lee*

Main category: cs.LG

TL;DR: 提出了Neuron Chunking方法，通过将神经元重要性评估与存储访问成本相结合，优化边缘设备上大型视觉语言模型的权重卸载I/O效率。


<details>
  <summary>Details</summary>
Motivation: 传统激活稀疏化方法仅基于激活幅度选择神经元，忽略了访问模式对闪存性能的影响，导致I/O效率低下。

Method: 基于块（内存中连续的神经元组）进行操作，通过轻量级抽象建模I/O延迟，选择具有高效用（神经元重要性除以估计延迟）的块。

Result: 在Jetson Orin Nano和Jetson AGX Orin上分别实现了4.65倍和5.76倍的I/O效率提升。

Conclusion: 通过将稀疏化决策与底层存储行为对齐，Neuron Chunking显著提高了边缘设备上大型视觉语言模型的I/O效率。

Abstract: Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively.

</details>


### [279] [Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking](https://arxiv.org/abs/2511.18394)
*Chinmay Karkar,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在预测能力上表现不稳定，其预测准确性高度依赖于领域结构、提示框架、问题类型和外部知识等因素。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在真实世界事件预测中的能力，探索其预测性能如何随不同因素变化。

Method: 分析不同模型家族在超出模型截止日期的真实世界问题上的表现，研究上下文、问题类型和外部知识对准确性和校准的影响。

Result: 预测能力高度可变，取决于提问的内容和方式，添加事实性新闻背景会改变信念形成和失败模式。

Conclusion: LLMs的预测能力不是一致的，而是高度依赖于具体情境和提问方式。

Abstract: Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.

</details>


### [280] [Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels](https://arxiv.org/abs/2511.18457)
*Duncan Stothers,Ben Stothers,Emily Schaeffer,Kishore Mulpuri*

Main category: cs.LG

TL;DR: 开发了一种超声优先的DDH诊断策略，通过自监督预训练和校准延迟规则，仅在必要时才进行X光检查，实现了有限样本覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 减少发育性髋关节发育不良(DDH)诊断中的辐射暴露，通过超声优先策略仅在需要时才进行X光检查。

Method: 使用SimSiam在无标签数据集上预训练模态特定编码器，冻结主干网络并拟合小规模测量头，应用单侧保形延迟规则进行校准。

Result: 超声测量误差适中(alpha MAE约9.7度)，X光测量AI和CE的MAE分别为7.6度和8.9度，校准策略可在不同设置下平衡覆盖率和US-only率。

Conclusion: 构建了一个简单可复现的流程，将有限标签转化为可解释的测量结果和可调节的选择性成像曲线，适合临床交接和未来外部验证。

Abstract: We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.
  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.

</details>


### [281] [Federated style aware transformer aggregation of representations](https://arxiv.org/abs/2511.18841)
*Mincheol Jeon,Euinam Huh*

Main category: cs.LG

TL;DR: FedSTAR是一个风格感知的联邦学习框架，通过解耦客户端特定风格因子和共享内容表示，使用基于Transformer的注意力机制聚合类原型，显著减少通信开销并提升个性化性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中的领域异构性、数据不平衡和严格通信约束问题，传统联邦学习缺乏个性化，单一全局模型无法捕捉客户端特定特征，导致预测偏差和泛化能力差。

Method: 提出FedSTAR框架，解耦客户端特定风格因子和共享内容表示，使用Transformer注意力机制聚合类原型，通过交换紧凑原型和风格向量而非完整模型参数来减少通信开销。

Result: 实验结果表明，内容-风格解耦与注意力驱动原型聚合相结合，在不增加通信成本的情况下提高了异构环境中的个性化和鲁棒性。

Conclusion: FedSTAR通过风格感知的联邦学习方法有效解决了个性化联邦学习的关键挑战，在保持通信效率的同时显著提升了模型性能。

Abstract: Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.
  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.
  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.

</details>


### [282] [SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation](https://arxiv.org/abs/2511.18468)
*Md Akil Raihan Iftee,Mir Sazzat Hossain,Rakibul Hasan Rajib,Tariq Iqbal,Md Mofijul Islam,M Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.LG

TL;DR: 提出了SloMo-Fast框架，这是一个无需源数据的双教师持续测试时适应方法，通过慢教师和快教师的互补机制解决长期遗忘问题，并在循环域转换场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖源数据或原型，在隐私敏感和资源受限环境中适用性有限，且存在长期遗忘问题，导致在先前遇到域上的性能下降。

Method: SloMo-Fast框架包含两个互补教师：慢教师（Slow-Teacher）缓慢遗忘并保留长期知识，快教师（Fast-Teacher）快速适应新域并积累知识。还提出了Cyclic-TTA基准来模拟循环域转换。

Result: 在Cyclic-TTA和其他十个CTTA设置中，SloMo-Fast始终优于最先进方法，显示出在演化和重访域上的强大适应和泛化能力。

Conclusion: SloMo-Fast通过双教师机制有效解决了CTTA中的长期遗忘问题，在无需源数据的情况下实现了对演化和循环域的鲁棒适应。

Abstract: Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.

</details>


### [283] [WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting](https://arxiv.org/abs/2511.18846)
*Yubo Wang,Hui He,Chaoxi Niu,Zhendong Niu*

Main category: cs.LG

TL;DR: WaveTuner是一个基于小波分解的时间序列预测框架，通过全频谱子带调谐解决现有方法对高频信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有小波方法主要递归分解低频成分，严重忽视了细微但信息丰富的高频成分，而这些高频成分对精确时间序列预测至关重要。

Method: 包含两个关键模块：自适应小波细化模块（将时间序列转换为时频系数，动态分配子带权重）和多分支专业化模块（使用多个KAN网络分支分别建模特定频谱子带）。

Result: 在八个真实世界数据集上的广泛实验表明，WaveTuner在时间序列预测中达到了最先进的性能。

Conclusion: WaveTuner在统一的时频框架内全面调谐全局趋势和局部变化，有效解决了小波方法对高频信息利用不足的问题。

Abstract: Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.

</details>


### [284] [Adaptive Mesh-Quantization for Neural PDE Solvers](https://arxiv.org/abs/2511.18474)
*Winfried van den Dool,Maksim Zhdanov,Yuki M. Asano,Max Welling*

Main category: cs.LG

TL;DR: 提出了自适应网格量化方法，通过轻量级辅助模型识别高损失区域，动态调整量化位宽，在复杂物理区域分配更多计算资源，在多个物理仿真任务中相比均匀量化基线实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 物理系统通常具有空间变化的复杂性，而现有的图神经网络在求解偏微分方程时对所有节点采用相同的计算资源，导致简单区域和复杂区域资源分配不均，计算效率低下。

Method: 引入自适应网格量化框架，在网格节点、边和簇特征上进行空间自适应量化，使用轻量级辅助模型识别输入网格中的高损失区域，动态调整主模型的量化位宽分配。

Result: 在2D Darcy流、大规模非稳态流体动力学、3D稳态Navier-Stokes模拟和2D超弹性问题等多个任务中，与MP-PDE和GraphViT集成，相比均匀量化基线实现了持续的Pareto改进，在相同计算成本下性能提升高达50%。

Conclusion: 自适应网格量化框架能够有效优化计算资源分配，在保持计算效率的同时显著提升物理仿真任务的性能，为复杂物理系统的神经网络求解提供了更高效的解决方案。

Abstract: Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost.

</details>


### [285] [Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning](https://arxiv.org/abs/2511.18489)
*Sai Puppala,Ismail Hossain,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.LG

TL;DR: 提出基于联邦学习的个性化LLM框架，通过本地数据微调GPT模型，结合用户画像评分和社交网络分析，实现隐私保护的实时个性化内容推荐。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台中内容过滤和推荐的挑战，在保护用户隐私的同时提升用户体验和内容相关性。

Method: 采用联邦学习框架，客户端使用本地社交数据微调GPT基础模型，结合用户生成内容分类、用户画像评分计算、朋友网络相关帖子识别，以及社交参与度量化与矩阵分解技术。

Result: 系统能够提供实时个性化内容建议，通过自适应反馈循环和可读性评分算法显著提升内容质量和相关性。

Conclusion: 该综合解决方案不仅解决了内容过滤和推荐问题，还促进了更具吸引力的社交媒体体验，同时保护用户隐私，为数字平台个性化交互设定了新标准。

Abstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms.

</details>


### [286] [KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit](https://arxiv.org/abs/2511.18868)
*Dezhi Ran,Shuxiao Xie,Mingfang Ji,Ziyue Hua,Mengzhou Wu,Yuan Cao,Yuzhe Guo,Yu Hao,Linyi Li,Yitao Hu,Tao Xie*

Main category: cs.LG

TL;DR: KernelBand是一个将内核优化建模为分层多臂老虎机问题的框架，通过LLM代理在优化空间中战略导航，显著提升了大型语言模型内核的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的高质量内核优化需要大量硬件架构和软件优化专业知识，而现有的基于LLM的代码生成方法由于缺乏硬件领域知识，难以在广阔的优化空间中有效平衡探索和利用。

Method: 将内核优化构建为分层多臂老虎机问题，利用硬件分析信息识别有前景的优化策略，并通过运行时行为聚类减少内核候选者的探索开销。

Result: 在TritonBench上的广泛实验表明，KernelBand显著优于现有最先进方法，以更少的token实现更优性能，且随着计算资源增加表现出持续改进而无饱和现象。

Conclusion: KernelBand成功解决了LLM内核优化中的探索-利用平衡问题，为自动内核优化提供了有效的框架，显著降低了训练和推理成本。

Abstract: High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.

</details>


### [287] [RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks](https://arxiv.org/abs/2511.18515)
*Ange-Clément Akazan,Issa Karambal,Jean Medard Ngnotchouye,Abebe Geletu Selassie. W*

Main category: cs.LG

TL;DR: 提出了RRaPINNs框架，通过条件风险价值(CVaR)优化尾部目标，并使用均值超额(ME)代理惩罚直接控制最坏情况PDE残差，将PINN训练转化为风险敏感优化。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs最小化平均残差会掩盖大的局部化误差，需要开发能控制最坏情况PDE残差的方法。

Method: 使用CVaR进行风险敏感优化，引入ME代理惩罚，将PINN训练与机会约束公式联系起来。

Result: 在多个PDE问题上，RRaPINNs显著减少尾部残差，同时保持或改善平均误差，优于传统PINNs和其他方法。

Conclusion: RRaPINNs为平滑和不连续PDE提供了可靠性感知的科学机器学习实用路径，α参数可透明地在整体精度和尾部控制之间权衡。

Abstract: Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $α$ acts as a transparent knob trading bulk accuracy (lower $α$ ) for stricter tail control (higher $α$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs.

</details>


### [288] [Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.18871)
*Jian Lu*

Main category: cs.LG

TL;DR: 本文提出了一种将推理和训练分离部署的周期性异步框架，通过改进数据加载器和采用统一三模型架构，在保持算法精度不变的情况下实现了至少3倍的RL训练性能提升。


<details>
  <summary>Details</summary>
Motivation: 主流RL框架中推理和训练部署在同一设备上，虽然降低了成本但同步执行带来了计算耦合，无法实现并发推理和训练，训练效率成为关键挑战。

Method: 采用推理和训练分离部署策略，改进数据加载器，将传统同步架构转变为周期性异步框架，在训练阶段应用统一三模型架构，并提出共享提示注意力掩码以减少重复计算。

Result: 在NPU平台上实现了至少3倍的整体RL训练性能提升，同时算法精度与同步方法完全等效，均属于同策略方法。

Conclusion: 该周期性异步框架允许按需独立弹性扩展各组件，具有广泛应用潜力，为RL训练效率问题提供了有效解决方案。

Abstract: Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application.

</details>


### [289] [CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection](https://arxiv.org/abs/2511.18519)
*Xinlin Zhuang,Yichen Li,Xiwei Liu,Haolin Yang,Yifan Lu,Ziyun Zou,Yulong Li,Huifa Li,Dongliang Chen,Qinglei Wang,Weiyang Liu,Ying Qian,Jiangming Shi,Imran Razzak*

Main category: cs.LG

TL;DR: CHIPS是一种从数据角度出发的CLIP领域适应方法，通过计算图像-文本对的效用分数来选择关键数据，仅需30%数据即可达到全数据集持续预训练的效果，在17个医学基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP领域适应方法主要关注微调策略或大规模领域特定数据的持续预训练，但数据本身作为关键因素被忽视。本文从数据中心视角重新审视该任务，探索是否可以通过有效的数据选择替代大规模数据集。

Method: 提出CHIPS方法，为每个图像-文本对分配效用分数，整合三个互补因素：1) 通过曲率感知的牛顿式对齐确保忠实性；2) 通过InfoNCE感知的曲率估计器和JL草图确保可扩展性；3) 通过选择感知的相关性权重和可学习性平衡目标适应与通用领域保留。

Result: 1) 在17个医学基准测试中达到选择基线的最优性能，仅用30%数据即可匹配全数据集持续预训练，仅用10%数据优于半数据集持续预训练；2) 在31个通用领域基准测试中，在10-30%数据保留预算下性能下降最小。

Conclusion: CHIPS证明了通过精心设计的数据选择策略可以有效替代大规模数据集进行领域适应，为数据高效的CLIP适应提供了新思路。

Abstract: Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released.

</details>


### [290] [Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction](https://arxiv.org/abs/2511.18521)
*Core Francisco Park,Manuel Perez-Carrasco,Caroline Nowlan,Cecilia Garraffo*

Main category: cs.LG

TL;DR: 使用变分自编码器(VAE)实现了NASA TEMPO卫星高光谱数据的514倍压缩，重建误差比信号低1-2个数量级，同时研究了压缩潜在空间中大气信息的保留程度。


<details>
  <summary>Details</summary>
Motivation: 解决地球静止轨道高光谱卫星每日产生TB级数据带来的存储、传输和分发挑战，为下一代地球观测系统解决关键瓶颈。

Method: 采用变分自编码器(VAE)进行数据压缩，并训练线性和非线性探针从压缩潜在空间中提取Level-2大气产品(NO2、O3、HCHO、云分数)。

Result: 实现了514倍压缩，重建误差1-2个数量级低于信号；云分数和总臭氧提取性能强(R²=0.93和0.81)，但NO2和HCHO提取效果较差(R²=0.20和0.51)；非线性探针显著优于线性探针。

Conclusion: 神经压缩能大幅减少高光谱数据量同时保留关键大气信号，但某些大气产品的编码存在根本性挑战，VAE以半线性方式编码大气信息。

Abstract: Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - https://github.com/cfpark00/Hyperspectral-VAE

</details>


### [291] [Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models](https://arxiv.org/abs/2511.18890)
*Yonggan Fu,Xin Dong,Shizhe Diao,Matthijs Van keirsbilck,Hanrong Ye,Wonmin Byeon,Yashaswi Karnati,Lucas Liebenwein,Hannah Zhang,Nikolaus Binder,Maksim Khadkevich,Alexander Keller,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 本文提出了一种基于真实设备延迟的小语言模型设计方法，通过优化深度-宽度比和操作符选择来提升模型效率，并引入进化搜索框架和权重归一化技术，最终开发出Nemotron-Flash系列模型，在准确性和效率方面显著超越现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 现有小语言模型设计主要关注参数数量优化，但参数效率并不总是能转化为真实的设备加速。本文旨在识别影响SLM真实设备延迟的关键因素，为以延迟为主要考量的小语言模型设计提供通用原则和方法。

Method: 1. 识别两个关键架构因素：深度-宽度比（影响小批量延迟）和操作符选择（影响延迟和大批量吞吐量）
2. 研究延迟最优的深度-宽度比
3. 探索高效注意力替代方案
4. 构建进化搜索框架自动发现混合SLM中的延迟最优操作符组合
5. 使用权重归一化技术增强SLM训练

Result: 开发的Nemotron-Flash系列模型相比Qwen3-1.7B/0.6B实现了：平均准确率提升超过5.5%，延迟降低1.3倍/1.9倍，吞吐量提高18.7倍/45.6倍。

Conclusion: 通过系统性地优化架构设计和训练方法，可以显著推进小语言模型的准确率-效率边界，为实际部署提供更优的解决方案。

Abstract: Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.

</details>


### [292] [TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting](https://arxiv.org/abs/2511.18539)
*Lingyu Jiang,Lingyu Xu,Peiran Li,Qianwen Ge,Dingyi Zhuang,Shuo Xing,Wenjing Chen,Xiangbo Gao,Ting-Hsuan Chen,Xueying Zhan,Xin Zhang,Ziming Zhang,Zhengzhong Tu,Michael Zielewski,Kazunori Yamada,Fangzhou Lin*

Main category: cs.LG

TL;DR: TimePre是一个新颖的概率时间序列预测框架，通过稳定实例归一化(SIN)解决了MLP骨干网络与多选择学习(MCL)结合时的训练不稳定性和假设崩溃问题，实现了高效、准确且稳定的概率预测。


<details>
  <summary>Details</summary>
Motivation: 现有概率时间序列预测方法存在两个问题：基于扩散的生成模型计算成本高，而高效的非采样框架如MCL存在训练不稳定和假设崩溃问题，特别是在与MLP骨干网络结合时问题更加严重。

Method: 提出TimePre框架，核心是稳定实例归一化(SIN)层，通过修正通道级统计偏移来稳定混合架构，彻底解决灾难性假设崩溃问题，成功统一了MLP模型的效率与MCL范式的分布灵活性。

Result: 在六个基准数据集上的实验表明，TimePre在关键概率指标上达到了新的最先进精度，推理速度比基于采样的模型快几个数量级，并且表现出稳定的性能扩展。

Conclusion: TimePre弥合了概率预测中准确性、效率和稳定性之间的长期差距，为不确定性感知决策提供了实用的解决方案。

Abstract: Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting.

</details>


### [293] [VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL](https://arxiv.org/abs/2511.18902)
*Zengjie Hu,Jiantao Qiu,Tianyi Bai,Haojin Yang,Binhang Yuan,Qi Jing,Conghui He,Wentao Zhang*

Main category: cs.LG

TL;DR: VADE是一个基于方差感知的动态采样框架，通过在线样本难度估计解决组策略优化中的梯度消失问题，提高训练信号并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的组策略优化方法在组内所有响应获得相同奖励时会出现梯度消失问题，导致优势估计崩溃和训练信号减弱。现有解决方案存在计算开销大或缺乏实时适应性的问题。

Method: VADE框架包含三个关键组件：使用Beta分布进行在线样本级难度估计、通过估计正确概率最大化信息增益的Thompson采样器、以及在策略演化下保持稳健估计的双尺度先验衰减机制。

Result: 在多模态推理基准测试中，VADE在性能和样本效率方面均优于强基线方法，同时显著减少了计算开销。

Conclusion: VADE能够动态选择最具信息量的样本，增强训练信号并消除额外rollout成本，可作为即插即用组件无缝集成到现有的基于组的强化学习算法中。

Abstract: Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \textbf{VADE}, a \textbf{V}ariance-\textbf{A}ware \textbf{D}ynamic sampling framework via online sample-level difficulty \textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io.

</details>


### [294] [In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm](https://arxiv.org/abs/2511.18567)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.LG

TL;DR: 本文评估了21种不同的goodness函数在Forward-Forward算法中的表现，发现某些替代函数在分类准确率上显著优于标准基线，同时揭示了预测性能与计算效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: Forward-Forward算法作为反向传播的生物合理替代方案，其效果严重依赖于goodness函数的定义。目前主要使用简单的平方和度量，但这是否是最优选择尚不明确。

Method: 在四个标准图像数据集（MNIST、FashionMNIST、CIFAR-10、STL-10）上对21种不同的goodness函数进行基准测试，评估分类准确率、能耗和碳足迹。

Result: 发现特定替代goodness函数表现优异：game_theoretic_local在MNIST上达到97.15%准确率，softmax_energy_margin_local在FashionMNIST上达到82.84%，triplet_margin_local在STL-10上达到37.69%。同时观察到计算效率存在显著差异。

Conclusion: goodness函数是FF算法设计中的关键超参数，需要在预测性能和环境影响之间进行权衡。代码已在GitHub上开源以确保可复现性。

Abstract: The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of "goodness", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \texttt{game\_theoretic\_local} achieved 97.15\% accuracy on MNIST, \texttt{softmax\_energy\_margin\_local} reached 82.84\% on FashionMNIST, and \texttt{triplet\_margin\_local} attained 37.69\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility.

</details>


### [295] [SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba](https://arxiv.org/abs/2511.18571)
*Jiazhen Hong,Geoffrey Mackellar,Soheila Ghane*

Main category: cs.LG

TL;DR: SAMBA是一个基于Mamba的U形编码器-解码器架构的自监督学习框架，专门用于处理长序列EEG数据，能够有效捕捉时空依赖关系，并在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 长序列EEG建模对于开发通用EEG表示模型至关重要，但由于EEG数据的高采样率和长记录时间，现有Transformer模型受限于二次复杂度，难以扩展到长上下文。此外，电极配置的差异和个体间脑信号差异也带来了挑战。

Method: 提出SAMBA框架，包含：(1)时间语义随机掩码用于语义级序列重建；(2)多头差分Mamba模块抑制冗余并突出显著时间结构；(3)空间自适应输入嵌入在三维欧几里得空间中学习统一嵌入，增强跨设备鲁棒性。

Result: 在13个EEG数据集上的实验表明，SAMBA在多种任务、电极配置和序列长度下始终优于最先进方法，同时保持低内存消耗和推理时间。学习到的空间权重图与任务相关神经生理区域高度一致。

Conclusion: SAMBA展示了作为实时脑机接口应用基础模型的可扩展性和实际潜力，其学习到的表示具有可解释性。

Abstract: Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications.

</details>


### [296] [Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation](https://arxiv.org/abs/2511.18930)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: MCNO提出了一种轻量级架构，通过蒙特卡洛方法直接逼近核积分来学习参数化PDE的解算子，无需谱或平移不变性假设，能在多网格分辨率下泛化且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法如Fourier Neural Operator依赖谱假设或平移不变性，限制了其适用性。MCNO旨在提供一种不依赖这些假设的轻量级替代方案。

Method: 使用蒙特卡洛方法直接逼近核积分，核表示为固定随机采样点集上的可学习张量，避免使用固定全局基函数或训练期间重复采样。

Result: 在标准1D PDE基准测试中，MCNO以低计算成本实现了竞争性精度。

Conclusion: MCNO为谱和基于图的神经算子提供了一个简单实用的替代方案，具有轻量级架构和良好的泛化能力。

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.

</details>


### [297] [Generative Myopia: Why Diffusion Models Fail at Structure](https://arxiv.org/abs/2511.18593)
*Milad Siami*

Main category: cs.LG

TL;DR: 本文揭示了图扩散模型存在"生成性近视"问题，即模型倾向于学习频繁子结构而忽略光谱关键结构，导致在组合任务中灾难性地移除"稀有桥梁"边。作者提出光谱加权扩散方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 图扩散模型在优化统计似然时隐式地作为频率滤波器，偏好丰富的子结构而非光谱关键结构，这种现象被称为"生成性近视"。在如图稀疏化等组合任务中，这会导致结构上必需但统计上稀少的"稀有桥梁"边被移除。

Method: 引入光谱加权扩散方法，使用有效电阻重新对齐变分目标。光谱先验可以在训练阶段被摊销，实现零推理开销。

Result: 该方法消除了近视问题，与最优光谱预言机性能匹配，在标准扩散完全失败（0%）的对抗基准上实现了100%的连通性。

Conclusion: 光谱先验可以有效地解决图扩散模型中的生成性近视问题，通过重新对齐优化目标来保留结构上关键但统计上稀少的边。

Abstract: Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\text{eff}} \approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \textbf{100\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\%).

</details>


### [298] [CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning](https://arxiv.org/abs/2511.18611)
*Mengdi Wang,Efe Bozkir,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: CycleSL是一种新颖的无聚合分割学习框架，通过循环更新机制解决传统分割学习的可扩展性和性能问题，将服务器端训练视为独立的高层机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 传统分割学习存在可扩展性差、服务器资源开销大、模型性能下降等问题，特别是并行变体中的客户端漂移和滞后现象。

Method: 受交替块坐标下降启发，CycleSL将服务器端训练作为独立的高层机器学习任务，重新采样客户端提取的特征来减轻异构性和漂移，采用先优化服务器模型再更新客户端的循环更新机制。

Result: 在五个公开数据集上的实验表明，CycleSL能有效提升模型性能，特别是在非独立同分布数据和部分客户端参与的场景下。

Conclusion: CycleSL是一个有效的无聚合分割学习框架，能够提升可扩展性和性能，并能与现有方法无缝集成。

Abstract: Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.

</details>


### [299] [Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors](https://arxiv.org/abs/2511.18615)
*Jiawei Hu,Javier A. Barria*

Main category: cs.LG

TL;DR: 提出了FMAPLS和online-FMAPLS两种贝叶斯框架，用于解决标签偏移问题，通过联合优化Dirichlet超参数和类先验分布，显著提升了分类器在测试数据分布变化时的性能。


<details>
  <summary>Details</summary>
Motivation: 标签偏移是监督学习中的常见挑战，当测试数据的类先验分布与训练数据不同时，会导致分类器性能显著下降。现有方法如MAPLS存在刚性约束，需要更灵活有效的解决方案。

Method: 使用批处理和在线EM算法，联合优化Dirichlet超参数α和类先验分布π；引入线性代理函数替代基于梯度的超参数更新，获得闭式解；在线版本用随机近似替代批处理E步，实现实时适应流数据。

Result: 在CIFAR100和ImageNet数据集上的实验表明，FMAPLS和online-FMAPLS分别实现了高达40%和12%的KL散度降低，并在后移准确率上显著优于现有最优方法，特别是在严重类别不平衡和分布不确定性情况下。

Conclusion: 所提方法在鲁棒性、可扩展性和动态学习场景适用性方面表现出色，证实了其在大规模和动态学习环境中的有效性。

Abstract: Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\boldsymbolα$ and class priors $\boldsymbolπ$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.

</details>


### [300] [Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation](https://arxiv.org/abs/2511.18958)
*Qisen Chai,Yansong Wang,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 提出Cutter框架，通过双智能体强化学习压缩图数据，保留拓扑结构和鲁棒性特征，实现高效可靠的图鲁棒性评估


<details>
  <summary>Details</summary>
Motivation: 随着图结构数据规模增大，评估其对抗攻击下的鲁棒性变得计算昂贵且难以扩展，需要开发高效的压缩方法

Method: 使用双智能体强化学习框架（VDA和RDA），结合轨迹级奖励塑造、原型塑造和跨智能体模仿三种策略来指导图压缩

Result: 在多个真实世界图上实验表明，压缩后的图保留了关键拓扑特性，且鲁棒性退化趋势与原图高度一致

Conclusion: Cutter能显著提高评估效率而不损害评估保真度，为大规模图鲁棒性评估提供了有效解决方案

Abstract: As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.

</details>


### [301] [FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning](https://arxiv.org/abs/2511.18977)
*Xin Yuan,Siqi Li,Jiateng Wei,Chengrui Zhu,Yanming Wu,Qingpeng Li,Jiajun Lv,Xiaoke Lan,Jun Chen,Yong Liu*

Main category: cs.LG

TL;DR: FastForward Pruning是一种高效的RL剪枝框架，通过解耦策略优化和预算约束问题，显著降低计算成本，在LLaMA、Mistral和OPT模型上取得优于启发式方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法中，启发式方法快速但性能次优，基于搜索的方法如RL计算成本过高，难以在大规模模型上应用。

Method: 提出解耦的单步RL框架，将策略优化与预算满足问题分离，采用课程学习策略从简单任务逐步增加复杂度。

Result: 在LLaMA、Mistral和OPT模型家族上发现的剪枝策略优于强启发式基线，与其他搜索算法相比，以更低的计算成本获得竞争性或更优的结果。

Conclusion: FastForward Pruning在搜索效率上具有明显优势，能够高效找到最优的非均匀层稀疏度分配策略。

Abstract: Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.

</details>


### [302] [FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction](https://arxiv.org/abs/2511.18631)
*Kiyan Rezaee,Morteza Ziabakhsh,Niloofar Nikfarjam,Mohammad M. Ghassemi,Yazdan Rezaee Jouryabi,Sadegh Eskandari,Reza Lashgari*

Main category: cs.LG

TL;DR: FOS是一个时间感知的图基准，用于预测科学前沿领域间的首次跨学科连接，通过评估多种时序图架构发现文本嵌入能显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 预测新兴跨学科研究领域的形成具有挑战性，现有方法难以准确预测科学前沿的突破性连接。

Method: 构建了1827-2024年间65,027个研究子领域的年度共现图，将新领域对连接预测建模为时序链接预测任务，评估多种最先进的时序图架构。

Result: 实验表明：(i) 使用领域文本描述嵌入显著提高预测准确性；(ii) 不同模型类在不同评估设置下表现优异；(iii) 预测结果与后续实际学术发表一致。

Conclusion: FOS基准为预测科学前沿提供了可复现的评估框架，文本语义信息对跨学科连接预测至关重要。

Abstract: Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the "first-time" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers.

</details>


### [303] [Dynamic Mixture of Experts Against Severe Distribution Shifts](https://arxiv.org/abs/2511.18987)
*Donghu Kim*

Main category: cs.LG

TL;DR: 本文评估了DynamicMoE方法在持续学习和强化学习环境中的表现，并与现有网络扩展方法进行基准比较。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中的可塑性与稳定性困境，受生物大脑通过容量增长保持可塑性的启发，探索类似的人工网络方法。

Method: 使用动态混合专家（DynamicMoE）架构，通过为不同分布专门化专家来实现参数效率的持续学习。

Result: 论文旨在评估DynamicMoE方法在持续学习和强化学习环境中的有效性。

Conclusion: MoE架构为持续学习提供了有前景的替代方案，能够专门化专家处理不同分布的数据。

Abstract: The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.

</details>


### [304] [The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion](https://arxiv.org/abs/2511.18632)
*Jan Benedikt Ruhland,Doguhan Bahcivan,Jan-Peter Sowa,Ali Canbay,Dominik Heider*

Main category: cs.LG

TL;DR: MedChat是一个本地可部署的虚拟医生框架，结合了LLM医疗聊天机器人和扩散驱动的虚拟形象，用于自动化结构化问诊，强调隐私保护和离线部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的进步使得在临床环境中实现本地部署成为可能，既能满足严格的数据保护要求，又能避免云服务带来的隐私风险。

Method: 使用真实和合成医疗对话混合语料对聊天机器人进行微调，通过低秩适应优化模型效率；采用条件扩散模型在潜在空间实现虚拟形象，并与音频特征同步；实现安全隔离的数据库接口。

Result: 系统实现了完全离线的本地部署，自编码器和扩散网络收敛平滑，MedChat在微调过程中表现稳定，对未见数据具有良好的泛化能力。

Conclusion: 该工作证明了完全离线、本地可部署的LLM-扩散框架在临床问诊中的可行性，为AI辅助临床问诊提供了隐私保护、资源高效的基础解决方案。

Abstract: Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.
  In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.
  Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.

</details>


### [305] [OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs](https://arxiv.org/abs/2511.19023)
*Yuting Gao,Weihao Chen,Lan Wang,Ruihan Xu,Qingpei Guo*

Main category: cs.LG

TL;DR: OrdMoE是一种新颖的偏好对齐框架，通过利用MoE架构中的内部信号来避免依赖外部人工标注的偏好数据，实现零成本的自监督偏好排序。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法主要依赖外部人工标注的偏好数据，这些数据收集成本高且劳动密集。

Method: 通过观察路由器专家选择分数隐含的质量感知排名，将专家按路由分数分组到不同层级，分别激活每个层级来生成质量递增的响应序列，从而构建内部偏好层次。

Result: 在多个多模态基准测试中，OrdMoE显著提升了多模态MoE LLM的对齐和整体性能，无需任何人工标注偏好数据即可获得有竞争力的结果。

Conclusion: OrdMoE框架成功证明了利用MoE架构内部信号进行偏好对齐的可行性，为多模态大语言模型的后训练对齐提供了一种高效且成本低廉的解决方案。

Abstract: Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.

</details>


### [306] [Mitigating Participation Imbalance Bias in Asynchronous Federated Learning](https://arxiv.org/abs/2511.19066)
*Xiangyu Chang,Manyi Yao,Srikanth V. Krishnamurthy,Christian R. Shelton,Anirban Chakraborty,Ananthram Swami,Samet Oymak,Amit Roy-Chowdhury*

Main category: cs.LG

TL;DR: ACE和ACED是解决异步联邦学习中异质性放大问题的新方法，通过全客户端参与和延迟感知机制来平衡客户端多样性和更新延迟。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)中，服务器立即使用每个到达客户端的贡献更新全局模型，导致客户端在不同模型版本上进行本地训练，产生信息延迟。在非IID数据分布下，这种异步模式放大了客户端异质性的负面影响，因为更快的客户端贡献更频繁的更新，使全局模型产生偏差。

Method: 提出ACE(全客户端参与AFL)，通过立即、非缓冲的更新来缓解参与不平衡，使用所有客户端的最新可用信息。还引入了延迟感知变体ACED，以平衡客户端多样性与更新延迟。

Result: 在不同模型、不同任务以及各种异质性和延迟设置下的实验验证了分析结果，并证明了所提方法的鲁棒性能。

Conclusion: ACE和ACED方法有效缓解了异步联邦学习中的异质性放大问题，通过全客户端参与和延迟感知机制实现了更好的性能平衡。

Abstract: In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.

</details>


### [307] [Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic](https://arxiv.org/abs/2511.18660)
*Mostafa Mozafari,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出了一种在无法访问原始训练数据的情况下进行纠正性机器遗忘的方法CUTS，通过代理集来消除模型中的污染影响。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练数据通常不可访问，且污染样本难以识别，传统基于遗忘集的纠正性机器遗忘方法失效。

Method: CUTS将干净信号和污染信号视为不同任务，通过在代理集上微调放大污染机制，计算任务向量并减去校准倍数来消除污染。

Result: 在标签噪声下恢复大部分丢失的效用，对于后门触发器几乎完全消除攻击且对效用损害最小，在无源设置下优于现有专门方法。

Conclusion: CUTS为无源纠正性机器遗忘提供了一种轻量级且有效的解决方案，无需访问干净数据或遗忘集。

Abstract: Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.

</details>


### [308] [EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching](https://arxiv.org/abs/2511.19087)
*Ziyun Li,Ben Dai,Huancheng Hu,Henrik Boström,Soon Hoe Lim*

Main category: cs.LG

TL;DR: 本文提出动能路径能量(KPE)作为ODE采样器生成路径的诊断工具，发现高KPE预测更强的语义质量和与数据密度的负相关，揭示语义丰富样本位于数据分布的稀疏前沿。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注端点指标，忽略了采样轨迹揭示的深层信息。受经典力学启发，希望了解生成路径的动力学特性。

Method: 引入动能路径能量(KPE)，量化ODE采样器每个生成路径的总动能消耗。在CIFAR-10和ImageNet-256上进行全面实验。

Result: 发现两个关键现象：1) 高KPE预测更强的语义质量；2) 高KPE与数据密度呈负相关，信息丰富的样本位于稀疏低密度区域。

Conclusion: 语义信息丰富的样本自然位于数据分布的稀疏前沿，需要更大的生成努力。轨迹级分析为理解生成难度和样本特性提供了物理启发且可解释的框架。

Abstract: Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.

</details>


### [309] [Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition](https://arxiv.org/abs/2511.18671)
*Yan Wang,Ke Deng,Yongli Ren*

Main category: cs.LG

TL;DR: 提出MCEM方法结合非线性评论家分解来解决多智能体强化学习中的集中-分散不匹配问题，通过排除次优行为提升性能


<details>
  <summary>Details</summary>
Motivation: 解决集中训练分散执行框架中的集中-分散不匹配问题，即一个智能体的次优行为会降低其他智能体的学习效果

Method: 提出多智能体交叉熵方法(MCEM)结合单调非线性评论家分解(NCD)，通过增加高价值联合动作的概率来更新策略，排除次优行为

Result: MCEM在连续和离散动作基准测试中都优于最先进的方法

Conclusion: MCEM方法有效解决了多智能体强化学习中集中-分散不匹配问题，在多种基准测试中表现出优越性能

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.

</details>


### [310] [The Core in Max-Loss Non-Centroid Clustering Can Be Empty](https://arxiv.org/abs/2511.19107)
*Robert Bredereck,Eva Deltl,Leon Kellerhals,Jannik Peters*

Main category: cs.LG

TL;DR: 该论文研究了在最大损失目标下的非质心聚类中的核心稳定性问题，证明了对于k≥3的情况，存在度量实例使得没有任何聚类位于α-核心中，其中α<2^(1/5)≈1.148。


<details>
  <summary>Details</summary>
Motivation: 研究非质心聚类中核心稳定性的存在性问题，特别是在最大损失目标下，填补了该领域缺乏不可能性结果的空白。

Method: 使用数学证明和计算机辅助证明方法，构建了特定的度量实例和二维欧几里得点集来验证核心稳定性边界。

Result: 证明了对于k≥3且n≥9（n能被k整除）的情况，存在度量实例使得没有任何聚类位于α-核心中，其中α<2^(1/5)≈1.148。该边界对于所构建的实例是紧的。

Conclusion: 这是首个证明在最大损失目标下的非质心聚类中核心可能为空的不可能性结果，为理解聚类稳定性提供了重要理论边界。

Abstract: We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\geq 3$ there exist metric instances with $n\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\frac{1}{5}}\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.

</details>


### [311] [QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks](https://arxiv.org/abs/2511.18689)
*Kazi Ahmed Asif Fuad,Lizhong Chen*

Main category: cs.LG

TL;DR: QuantKAN是一个统一的量化框架，专门用于量化Kolmogorov Arnold Networks (KANs)，支持量化感知训练(QAT)和后训练量化(PTQ)两种模式，在多个数据集上验证了KANs与低比特量化的兼容性。


<details>
  <summary>Details</summary>
Motivation: KANs虽然具有强大的表达能力和可解释性，但其异构的样条和基础分支参数阻碍了高效量化，与CNNs和Transformers相比，KANs的量化问题尚未得到充分研究。

Method: QuantKAN扩展了现代量化算法（如LSQ、LSQ+、PACT、DoReFa等），为基于样条的层提供分支特定的量化器，涵盖基础、样条和激活组件。在MNIST、CIFAR10和CIFAR100数据集上对多种KAN变体进行实验。

Result: 实验表明KANs与低比特量化兼容，但表现出强烈的方法-架构交互：LSQ、LSQ+和PACT在浅层KAN模型中保持接近全精度准确率，而DoReFa在深层KAGN下表现最稳定。PTQ方面，GPTQ和Uniform在数据集上表现最强。

Conclusion: QuantKAN框架统一了样条学习和量化，为在实际资源受限环境中高效部署KANs提供了实用工具和指导方针。

Abstract: Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.

</details>


### [312] [Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty](https://arxiv.org/abs/2511.19124)
*Krishang Sharma*

Main category: cs.LG

TL;DR: 提出了一种新颖的不确定性感知深度学习框架，用于航空剩余使用寿命预测，通过概率建模直接学习偶然不确定性，在关键区域性能提升25-40%。


<details>
  <summary>Details</summary>
Motivation: 准确预测剩余使用寿命并进行不确定性量化是航空预测性维护的关键挑战，现有CMAPSS文献中尚未探索通过概率建模直接学习偶然不确定性的方法。

Method: 分层架构整合多尺度Inception块进行时间模式提取、双向LSTM进行序列建模、传感器和时间维度的双重注意力机制，以及贝叶斯输出层同时预测均值RUL和方差。

Result: 在NASA CMAPSS基准测试中，整体RMSE分别为16.22、19.29、16.84和19.98，关键区域（RUL≤30周期）RMSE为5.14、6.89、5.27和7.16，比传统方法提升25-40%。

Conclusion: 该框架实现了校准良好的95%置信区间，覆盖率达93.5%-95.2%，为之前CMAPSS文献中无法实现的风险感知维护调度提供了可能。

Abstract: Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.

</details>


### [313] [GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction](https://arxiv.org/abs/2511.18716)
*Zesheng Liu,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: GRIT-LP是一种专门用于极地雷达图像冰层厚度估计的图变换器，通过分区空间图构建和长程跳跃连接机制，解决了深度图变换器的过平滑和长程依赖建模问题，在均方根误差上比现有方法提升24.92%。


<details>
  <summary>Details</summary>
Motivation: 准确估计冰层厚度对于理解积雪积累、重建过去气候模式以及减少未来冰盖演化和海平面上升预测的不确定性至关重要。现有图变换器在深度上受到过平滑和弱长程依赖建模的限制。

Method: 结合归纳几何图学习和自注意力机制，引入两个主要创新：分区空间图构建策略形成重叠的完全连接局部邻域以保持空间一致性并抑制不相关长程连接的噪声；变换器内部的长程跳跃连接机制改善信息流并减轻深层注意力层的过平滑。

Result: 在广泛实验中，GRIT-LP优于当前最先进方法，均方根误差提升24.92%。

Conclusion: 结果表明图变换器通过捕捉局部结构特征和冰层内部的长程依赖关系，在建模时空模式方面具有有效性，并展示了推进数据驱动理解冰冻圈过程的潜力。

Abstract: Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.

</details>


### [314] [Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM](https://arxiv.org/abs/2511.18721)
*Adarsh Kumarappan,Ayushi Mehrotra*

Main category: cs.LG

TL;DR: 提出了(k, ε)-不稳定的概率框架来改进SmoothLLM防御，通过数据驱动的攻击成功模型提供更可信的安全认证保证


<details>
  <summary>Details</summary>
Motivation: SmoothLLM防御依赖于严格的k-不稳定假设，这在实践中很少成立，限制了安全证书的可信度

Method: 引入(k, ε)-不稳定概率框架，结合攻击成功的经验模型，推导SmoothLLM防御概率的新下界

Result: 提供了更可信和实用的安全证书，使从业者能够设置更好地反映LLM真实行为的认证阈值

Conclusion: 这项工作为安全AI部署贡献了一个实用且理论基础的机制，使LLM更能抵抗对其安全对齐的利用

Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.

</details>


### [315] [Local Entropy Search over Descent Sequences for Bayesian Optimization](https://arxiv.org/abs/2511.19241)
*David Stenger,Armin Lindicke,Alexander von Rohr,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 提出了局部熵搜索（LES）方法，一种针对迭代优化器可达解的贝叶斯优化范式，通过传播目标函数的后验信念来指导采样，在复杂优化问题上表现出优异的样本效率。


<details>
  <summary>Details</summary>
Motivation: 在大型复杂设计空间中寻找全局最优解通常不可行且不必要，更实用的替代方案是使用梯度下降等局部优化方法迭代优化初始设计的邻域。

Method: 通过优化器传播目标函数的后验信念，生成下降序列的概率分布，然后通过分析熵计算和下降序列的蒙特卡洛采样相结合，最大化与该分布的互信息来选择下一个评估点。

Result: 在高复杂度合成目标和基准问题上的实证结果表明，与现有的局部和全局贝叶斯优化方法相比，LES实现了更强的样本效率。

Conclusion: 局部熵搜索是一种有效的贝叶斯优化方法，特别适用于通过迭代优化器可达解的搜索问题，在复杂优化任务中具有优异的性能表现。

Abstract: Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.

</details>


### [316] [LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs](https://arxiv.org/abs/2511.18727)
*Devansh Agarwal,Maitreyi Chatterjee,Biplab Chatterjee*

Main category: cs.LG

TL;DR: LogSyn框架使用大语言模型将非结构化飞机维护日志转换为结构化数据，通过少样本学习进行问题-解决叙述摘要和事件分类，识别关键故障模式。


<details>
  <summary>Details</summary>
Motivation: 飞机维护日志包含宝贵的安全数据，但由于其非结构化文本格式而未被充分利用，需要一种方法来提取可操作的见解。

Method: 使用大语言模型和少样本上下文学习，在6,169条记录上执行受控抽象生成，总结问题-解决叙述并在详细层次本体中分类事件。

Result: 框架能够识别关键故障模式，为维护日志提供可扩展的语义结构化和见解提取方法。

Conclusion: 这项工作为改进航空及相关行业的维护工作流程和预测分析提供了实用路径。

Abstract: Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.

</details>


### [317] [MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization](https://arxiv.org/abs/2511.19253)
*Boyuan Wu*

Main category: cs.LG

TL;DR: MAESTRO框架使用LLM作为离线训练架构师，通过生成语义课程和自动奖励函数来优化多智能体强化学习训练，在交通信号控制任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中设计密集奖励函数和构建避免局部最优的课程的挑战，避免将LLM直接用于实时控制的高成本问题。

Method: 提出MAESTRO框架，包含语义课程生成器和自动奖励合成器，使用LLM离线生成多样化交通场景和可执行Python奖励函数，指导标准MADDPG算法。

Result: 在16个交叉口的大规模交通信号控制任务中，相比强基线方法，平均回报提高4.0%，风险调整后性能提升2.2%。

Conclusion: LLM可作为多智能体强化学习训练的有效高层设计者，在不增加部署推理成本的情况下提升训练效果。

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.

</details>


### [318] [Reinforcement Learning for Self-Healing Material Systems](https://arxiv.org/abs/2511.18728)
*Maitreyi Chatterjee,Devansh Agarwal,Biplab Chatterjee*

Main category: cs.LG

TL;DR: 将自愈过程建模为强化学习问题，通过马尔可夫决策过程使智能体自主制定最优策略，平衡结构完整性维护与有限资源消耗。


<details>
  <summary>Details</summary>
Motivation: 向自主材料系统过渡需要自适应控制方法以最大化结构寿命，传统启发式方法在动态环境中效率有限。

Method: 在随机模拟环境中比较离散动作（Q-learning、DQN）和连续动作（TD3）智能体的性能，将自愈过程构建为强化学习问题。

Result: 强化学习控制器显著优于启发式基线，实现近乎完全的材料恢复。TD3智能体使用连续剂量控制表现出更快的收敛速度和稳定性。

Conclusion: 在动态自愈应用中，精细的比例驱动至关重要，连续动作控制比离散动作控制具有明显优势。

Abstract: The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.

</details>


### [319] [Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network](https://arxiv.org/abs/2511.18730)
*Michael Horton,Patrick Lucey*

Main category: cs.LG

TL;DR: 提出基于轴向transformer的神经网络，联合预测足球比赛中13种球员动作的预期总数，涵盖球员、球队和比赛三个层面，实现低延迟的实时预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测足球比赛中球员完成的各种动作数量对于战术决策、体育博彩和电视转播分析等应用至关重要，需要考虑比赛状态、球员能力、球员互动和比赛动态等多种因素。

Method: 使用基于轴向transformer的神经网络，该设计等效于常规序列transformer，能够高效捕捉比赛进程中的时间动态和球员间的互动关系，实现多时间步的联合预测。

Result: 模型能够做出一致可靠的预测，每个比赛可高效生成约75,000个低延迟的实时预测。

Conclusion: 提出的轴向transformer设计在实验中表现良好，能够有效解决足球比赛动作预测的复杂需求。

Abstract: Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\sim$75,000 live predictions at low latency for each game.

</details>


### [320] [Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention](https://arxiv.org/abs/2511.19263)
*Lucas Li,Jean-Baptiste Puel,Florence Carton,Dounya Barrit,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: 提出Solar-GECO模型，结合几何图神经网络和语言模型嵌入，通过协同注意力机制预测钙钛矿太阳能电池的功率转换效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 钙钛矿太阳能电池性能受多层材料复杂相互作用影响，传统实验筛选方法成本高、效率低，现有机器学习模型忽视钙钛矿晶体几何信息。

Method: 使用几何图神经网络编码钙钛矿吸收层原子结构，语言模型处理传输层等组件的文本表示，集成协同注意力模块捕获层内依赖和层间相互作用，概率回归头预测PCE及其不确定性。

Result: Solar-GECO达到最先进性能，将PCE预测的平均绝对误差从3.066降至2.936，显著优于多个基线模型。

Conclusion: 整合几何和文本信息为PCE预测提供了更强大准确的框架。

Abstract: Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.

</details>


### [321] [OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting](https://arxiv.org/abs/2511.18732)
*Haoming Jia,Yi Han,Xiang Wang,Huizan Wang,Wei Wu,Jianming Zheng,Peikun Xiao*

Main category: cs.LG

TL;DR: 提出了OceanForecastBench基准，为数据驱动的海洋预报提供开源标准化平台，包括28年高质量再分析数据、可靠观测数据和评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的海洋预报模型缺乏开源标准化基准，导致数据使用和评估方法不一致，阻碍模型开发、公平比较和跨学科合作。

Method: 构建包含三部分核心贡献的基准：28年全球海洋再分析数据、高可靠性卫星和原位观测数据、包含6个基线模型的评估流程。

Result: 创建了目前最全面的数据驱动海洋预报基准框架，提供开源平台用于模型开发、评估和比较。

Conclusion: OceanForecastBench解决了海洋预报领域缺乏标准化基准的问题，为数据驱动模型的发展提供了重要基础设施。

Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.

</details>


### [322] [Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry](https://arxiv.org/abs/2511.19264)
*Amirtha Varshini A S,Duminda S. Ranasinghe,Hok Hei Tam*

Main category: cs.LG

TL;DR: 本文提出了一个可解释性框架，用于揭示SynFlowNet（一种生成流网络）在分子设计中的内部决策机制，帮助化学家理解分子结构生成的原理。


<details>
  <summary>Details</summary>
Motivation: 生成流网络在分子设计中很有前景，但其内部决策策略不透明，限制了在药物发现中的应用，因为化学家需要清晰可解释的结构生成原理。

Method: 集成了三种互补组件：基于梯度的显著性分析与反事实扰动识别原子环境影响；稀疏自编码器揭示与物理化学性质对应的潜在因子；基序探针显示功能基团在嵌入中的编码方式。

Result: 成功揭示了SynFlowNet内部的化学逻辑，包括原子环境对奖励的影响、结构编辑如何改变分子结果，以及极性、亲脂性、分子大小等性质的显式编码。

Conclusion: 该框架为SynFlowNet提供了可操作和机制性的洞察，支持透明可控的分子设计，促进了生成流网络在药物发现中的采用。

Abstract: Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.

</details>


### [323] [Sampling Control for Imbalanced Calibration in Semi-Supervised Learning](https://arxiv.org/abs/2511.18773)
*Senmao Tian,Xiang Wei,Shunli Zhang*

Main category: cs.LG

TL;DR: SC-SSL是一个解决半监督学习中类别不平衡问题的统一框架，通过解耦采样控制来抑制模型偏差，在训练和推理阶段分别处理特征级和权重不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习方法在处理类别不平衡时通常以粗粒度方式调整logits，混淆了数据不平衡与不同类别学习难度差异导致的偏差问题。

Method: 提出SC-SSL框架，在训练阶段通过具有显式扩展能力的分类器和自适应调整不同数据分布的采样概率来缓解少数类的特征级不平衡；在推理阶段分析线性分类器的权重不平衡，应用后处理采样控制并通过优化偏置向量直接校准logits。

Result: 在多个基准数据集和不同分布设置下的广泛实验验证了SC-SSL的一致性和最先进性能。

Conclusion: SC-SSL通过解耦采样控制有效解决了半监督学习中的类别不平衡问题，在训练和推理阶段分别处理不同来源的模型偏差，取得了优异性能。

Abstract: Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL.

</details>


### [324] [Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning](https://arxiv.org/abs/2511.19299)
*James R. M. Black,Moritz S. Hanke,Aaron Maiwald,Tina Hernandez-Boussard,Oliver M. Crook,Jaspreet Pannu*

Main category: cs.LG

TL;DR: 研究表明，即使从预训练数据中排除病毒基因组序列，通过微调仍可恢复基因组语言模型的误用相关能力，这凸显了现有数据过滤缓解措施的安全风险。


<details>
  <summary>Details</summary>
Motivation: 基因组语言模型(gLMs)在生物数据上的应用引发了安全担忧，特别是可能被用于生成人类感染病毒的基因组。目前主流的缓解措施是从预训练数据中过滤掉病毒序列，但这种方法对可微调的开源模型的有效性尚不清楚。

Method: 评估了最先进的gLM模型Evo 2，使用110种有害人类感染病毒的序列进行微调，测试误用相关预测能力的恢复情况。比较了预训练模型、病毒微调模型和噬菌体微调模型的性能。

Result: 微调后的模型在未见过的病毒序列上表现出更低的困惑度，且能够识别SARS-CoV-2的免疫逃逸变异(AUROC为0.6)，尽管在微调过程中未接触过SARS-CoV-2序列。

Conclusion: 数据排除策略可能被微调方法绕过，从而在一定程度上恢复gLMs的误用相关能力。需要为gLMs建立安全框架，并进一步研究评估和缓解措施以确保安全部署。

Abstract: Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.

</details>


### [325] [SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs](https://arxiv.org/abs/2511.18777)
*Chenhong Zhou,Jie Chen,Zaifeng Yang*

Main category: cs.LG

TL;DR: 提出了一种结合小波变换空间-频率局部化特性的Wavelet Attention模块和Spectral Attention Operator Transformer框架，有效解决了FNO在捕捉局部细节和高频分量方面的不足，在六个算子学习基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: Fourier Neural Operator (FNO) 在求解偏微分方程时存在过度平滑解、无法有效捕捉局部细节和高频分量的问题，需要改进以提升性能。

Method: 提出Wavelet Attention (WA)模块，利用小波变换的空间-频率局部化特性，具有线性计算复杂度；开发Spectral Attention Operator Transformer (SAOT)混合谱Transformer框架，通过门控融合块整合WA的局部关注和Fourier-based Attention的全局感受野。

Result: WA显著缓解了FA的局限性，大幅优于现有基于小波的神经算子；SAOT在六个算子学习基准上达到最先进性能，并表现出强大的离散化不变能力。

Conclusion: 通过整合局部感知和全局谱表示，SAOT框架有效解决了FNO的局限性，为偏微分方程求解提供了更优的解决方案。

Abstract: Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability.

</details>


### [326] [Leveraging LLMs for reward function design in reinforcement learning control tasks](https://arxiv.org/abs/2511.19355)
*Franklin Cardenoso,Wouter Caarls*

Main category: cs.LG

TL;DR: LEARN-Opt是一个基于LLM的完全自主、模型无关的奖励函数优化框架，无需初步评估指标和环境源代码即可从系统描述和任务目标生成、执行和评估奖励函数候选方案。


<details>
  <summary>Details</summary>
Motivation: 设计有效的奖励函数是强化学习中的主要瓶颈，需要大量人工专业知识且耗时。现有方法通常需要初步评估指标、人工反馈或环境源代码作为上下文。

Method: 提出LEARN-Opt框架，能够直接从系统描述和任务目标自主推导性能指标，实现无监督的奖励函数评估和选择。

Result: 实验表明LEARN-Opt性能与最先进方法（如EUREKA）相当或更好，且需要更少先验知识。能够利用低成本LLM找到与更大模型相当甚至更好的高性能候选方案。

Conclusion: LEARN-Opt有潜力在不需任何人工定义指标的情况下生成高质量奖励函数，减少工程开销并增强泛化能力。

Abstract: The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.

</details>


### [327] [Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs](https://arxiv.org/abs/2511.18783)
*Renchu Guan,Xuyang Li,Yachao Zhang,Wei Pang,Fausto Giunchiglia,Ximing Li,Yonghao Liu,Xiaoyue Feng*

Main category: cs.LG

TL;DR: 提出了HONOR，一种适用于同质和异质超图的无监督超图对比学习框架，通过提示机制和自适应注意力聚合来建模异质关系。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络大多依赖同质性假设，而现实场景中存在显著的异质性结构，需要能够同时处理同质和异质关系的超图学习方法。

Method: 使用提示机制构建超边特征以保持全局语义一致性并抑制局部噪声，结合自适应注意力聚合模块动态捕获节点对超边的不同贡献，并采用高通滤波来充分利用异质连接模式。

Result: 理论分析显示HONOR具有优越的泛化能力和鲁棒性，实验验证在同质和异质数据集上均优于现有最优方法。

Conclusion: HONOR框架能够有效建模超图中的异质关系，在同质和异质场景下都能获得更具判别性和鲁棒性的节点和超边表示。

Abstract: Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \textbf{HONOR}, a novel unsupervised \textbf{H}ypergraph c\textbf{ON}trastive learning framework suitable for both hom\textbf{O}philic and hete\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.

</details>


### [328] [Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme](https://arxiv.org/abs/2511.19390)
*Rudy Morel,Francesco Pio Ramunno,Jeff Shen,Alberto Bietti,Kyunghyun Cho,Miles Cranmer,Siavash Golkar,Olexandr Gugnin,Geraud Krawezik,Tanya Marwah,Michael McCabe,Lucas Meyer,Payel Mukhopadhyay,Ruben Ohana,Liam Parker,Helen Qu,François Rozet,K. D. Leka,François Lanusse,David Fouhey,Shirley Ho*

Main category: cs.LG

TL;DR: 提出一种用于部分可观测长记忆动力系统的多尺度扩散模型推理方案，应用于太阳动力学和活动区演化预测，解决了标准自回归方法无法有效捕获长期依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 在太阳物理等场景中，只能观测到系统状态的一小部分（如太阳表面和大气层），而系统演化由无法直接测量的内部过程驱动，标准推理方案无法有效整合过去信息来捕获长期依赖关系。

Method: 提出多尺度扩散模型推理方案，生成的时间轨迹在近期具有精细时间分辨率，在远期具有较粗分辨率，这样可以在不增加计算成本的情况下捕获长期时间依赖关系。

Result: 该方法显著减少了预测分布的偏差，提高了滚动预测的稳定性，在太阳动力学应用中表现出色。

Conclusion: 多尺度推理方案为部分可观测长记忆动力系统的概率预测提供了有效解决方案，能够更好地捕获长期时间依赖关系。

Abstract: Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.

</details>


### [329] [Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses](https://arxiv.org/abs/2511.18789)
*Haichen Hu,David Simchi-Levi*

Main category: cs.LG

TL;DR: 提出一种高效的再拟合程序来计算经验风险最小化的超额风险，并提供固定设计设置下的高概率上界，该方法无需了解函数类复杂度，适用于评估现代不透明机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 传统基于容量的学习理论在处理深度神经网络和生成模型等极端复杂假设类时变得不可行，需要一种模型无关的方法来评估现代不透明机器学习系统的超额风险。

Method: 通过随机扰动梯度向量生成两组伪标签数据（wild response），然后对黑盒训练程序进行两次再拟合得到两个wild predictor，最后结合原始预测器和构造的wild responses推导超额风险上界。

Result: 开发了一种高效的超额风险上界计算方法，该方法仅需黑盒访问训练算法和单个数据集，无需函数类复杂度的先验知识。

Conclusion: 该方法本质上是模型无关的，在理论上评估现代不透明机器学习系统方面具有重要前景，特别适用于传统容量理论不可行的复杂假设类场景。

Abstract: We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.

</details>


### [330] [Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models](https://arxiv.org/abs/2511.18829)
*Kanav Arora,Girish Narayanswamy,Shwetak Patel,Richard Li*

Main category: cs.LG

TL;DR: 本文研究了如何通过知识蒸馏将大型预训练PPG模型压缩为适合边缘设备实时推理的小型模型，评估了四种蒸馏策略并建立了模型大小与性能关系的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习模型在心率估计任务中表现出色，但要在可穿戴设备上部署这些模型，必须满足严格的内存和延迟限制。

Method: 评估了四种蒸馏策略：硬蒸馏、软蒸馏、解耦知识蒸馏(DKD)和特征蒸馏，通过全面扫描师生模型容量来表征缩放规律。

Result: 建立了描述模型大小与性能关系的缩放规律，为构建可部署在边缘设备上的生理传感模型提供了实用且可预测的方法。

Conclusion: 这项早期研究为构建适合边缘部署的生理传感模型奠定了实践基础，提供了可预测的模型压缩方法。

Abstract: Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.

</details>


### [331] [UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413)
*Zhaolong Su,Wang Lu,Hao Chen,Sharon Li,Jindong Wang*

Main category: cs.LG

TL;DR: UniGame是一个自对抗后训练框架，通过轻量级扰动器在共享token接口处应用，让生成分支主动挑战脆弱的理解能力，从而解决统一多模态模型中理解与生成之间的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型在理解和生成任务中表现出色，但存在根本性不一致：理解偏好紧凑嵌入，而生成偏好重建丰富的表示。这种结构权衡导致决策边界错位、跨模态连贯性下降以及对分布和对抗性变化的脆弱性增加。

Method: UniGame框架在共享token接口处应用轻量级扰动器，使生成分支能够主动寻找和挑战脆弱的理解能力，将模型自身变成对抗者。该框架与架构无关，仅增加不到1%的参数。

Result: 实验表明UniGame显著提高了一致性(+4.6%)，同时在理解(+3.6%)、生成(+0.02)、分布外和对抗鲁棒性(+4.8%和+6.2%在NaturalBench和AdVQA上)方面也取得了实质性改进。

Conclusion: 对抗性自博弈是增强未来多模态基础模型连贯性、稳定性和统一能力的通用有效原则，该框架与现有后训练方法互补。

Abstract: Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame

</details>


### [332] [Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM](https://arxiv.org/abs/2511.18830)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种双输入神经网络策略，通过持续时间感知的伪嵌入矩阵将时间重要性转化为紧凑可学习的表示，解决了预测过程监控中时间不规则性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在预测过程监控中难以处理时间不规则性，特别是随机事件持续时间和重叠时间戳，限制了它们在不同数据集间的适应性。

Method: 采用双输入神经网络策略分离事件和序列属性，使用持续时间感知伪嵌入矩阵；在B-LSTM和B-GCN基线模型基础上开发了D-LSTM和D-GCN变体，所有模型都包含自调谐超模型进行自适应架构选择。

Result: 在平衡和不平衡结果预测任务上的实验表明，持续时间伪嵌入输入持续改善泛化能力、降低模型复杂性并增强可解释性。

Conclusion: 明确的时间编码带来了显著优势，为鲁棒的现实世界预测过程监控应用提供了灵活的设计方案。

Abstract: Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.

</details>


### [333] [Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2511.18835)
*Fang Wang,Lance Kosca,Adrienne Kosca,Marko Gacesa,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGNN(O)是一个用于事件序列数据结果预测的AutoML GNN超模型框架，通过贝叶斯优化的自调优机制在无需手动配置的情况下自动适应架构和超参数，在多个数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 为事件序列数据的结果预测提供一个鲁棒且可泛化的AutoML-GNN基准方法，避免手动配置架构和超参数的需求。

Method: 扩展了四种架构（One Level、Two Level、Two Level Pseudo Embedding、Two Level Embedding）和六种经典GNN算子，采用基于贝叶斯优化、剪枝和早停的自调优机制。

Result: 在Traffic Fines数据集上准确率超过0.98，在Patients数据集上加权F1分数达到0.86，且无需显式处理不平衡问题。

Conclusion: 提出的AutoML-GNN方法为复杂事件序列数据的结果预测提供了鲁棒且可泛化的基准解决方案。

Abstract: This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.

</details>


### [334] [Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning](https://arxiv.org/abs/2511.18859)
*Bo Jiang,Weijun Zhao,Beibei Wang,Xiao Wang,Jin Tang*

Main category: cs.LG

TL;DR: 提出了UAdapterGNN方法，通过将不确定性学习集成到GNN适配器中，增强预训练GNN模型对噪声图数据的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AdapterGNN方法容易受到图数据中各种噪声（如噪声边和模糊节点属性）的影响，泛化能力有限，需要增强GNN微调的鲁棒性和泛化能力。

Method: 使用高斯概率适配器来增强预训练GNN模型，当图包含各种噪声时，该方法能自动吸收高斯分布方差变化的影响。

Result: 在多个基准测试上的广泛实验证明了UAdapterGNN方法的有效性、鲁棒性和高泛化能力。

Conclusion: 通过集成不确定性学习到GNN适配器中，可以很好地解决图数据噪声问题，显著增强模型的鲁棒性和泛化能力。

Abstract: Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method.

</details>


### [335] [Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning](https://arxiv.org/abs/2511.18887)
*Hyeong-Gun Joo,Songnam Hong,Seunghwan Lee,Dong-Joon Shin*

Main category: cs.LG

TL;DR: 提出了Hi-SAFE，一种轻量级密码学安全聚合框架，用于解决基于符号的联邦学习中的隐私和通信效率问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网和边缘网络等资源受限环境中面临隐私保护和通信效率的双重挑战。现有的基于符号的方法虽然节省带宽，但容易受到推理攻击，而现有的安全聚合技术要么与基于符号的方法不兼容，要么开销过大。

Method: 基于费马小定理构建了SIGNSGD-MV的高效多数投票多项式，将多数投票表示为有限域上的低次多项式，实现仅揭示最终结果的安全评估。采用分层子分组策略确保恒定乘法深度和有限用户复杂度。

Result: Hi-SAFE框架能够在不暴露中间值的情况下安全计算多数投票结果，同时保持通信效率。

Conclusion: Hi-SAFE为基于符号的联邦学习提供了一种轻量级且密码学安全的聚合解决方案，有效平衡了隐私保护和通信效率的需求。

Abstract: Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n.

</details>


### [336] [Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery](https://arxiv.org/abs/2511.18940)
*Sanjeev Manivannan,Chandrashekar Lakshminarayan*

Main category: cs.LG

TL;DR: 提出了几何感知预处理模块和深度同余网络，用于零样本跨被试运动想象解码，在BCI-IV 2a基准上比最强基线提升3-4%准确率


<details>
  <summary>Details</summary>
Motivation: 解决脑电信号中因被试变异性强和协方差矩阵在SPD流形上的弯曲几何特性导致的跨被试运动想象解码难题

Method: 引入DCR和RiFU预处理模块改进黎曼对齐，提出SPD-DCNet和RiFUNet流形分类器，使用层次同余变换学习判别性、被试不变的协方差表示

Result: 在BCI-IV 2a基准上，跨被试准确率比最强经典基线提升3-4%

Conclusion: 几何感知变换对鲁棒脑电解码具有重要价值

Abstract: Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.

</details>


### [337] [MIST: Mutual Information Via Supervised Training](https://arxiv.org/abs/2511.18945)
*German Gritsai,Megan Richards,Maxime Méloux,Kyunghyun Cho,Maxime Peyrard*

Main category: cs.LG

TL;DR: 提出了一种完全数据驱动的互信息估计器设计方法，使用神经网络参数化估计函数，在大规模元数据集上训练，能够处理可变样本大小和维度，并提供校准良好的分位数区间估计。


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计器通常基于理论推导，缺乏灵活性。本研究采用完全经验主义路线，用神经网络学习估计函数，以换取灵活性和效率，放弃通用理论保证。

Method: 使用神经网络（MIST）参数化互信息估计函数，在625,000个已知真实互信息的合成联合分布元数据集上训练。采用二维注意力机制处理可变样本大小和维度，通过分位数回归损失量化不确定性。

Result: 学习到的估计器在各种样本大小和维度下显著优于经典基线方法，包括训练期间未见过的联合分布。分位数区间校准良好，比基于bootstrap的置信区间更可靠，推理速度比现有神经基线快数个数量级。

Conclusion: 该框架产生了可训练、完全可微分的估计器，可嵌入更大的学习管道中。利用互信息对可逆变换的不变性，可通过归一化流将元数据集适应任意数据模态，为不同目标元分布提供灵活训练。

Abstract: We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.

</details>


### [338] [AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention](https://arxiv.org/abs/2511.18960)
*Lei Xiao,Jifeng Li,Juntao Gao,Feiyang Ye,Yan Jin,Jingjing Qian,Jing Zhang,Yong Wu,Xiaoyuan Yu*

Main category: cs.LG

TL;DR: AVA-VLA是一个基于POMDP视角的视觉-语言-动作模型，通过引入主动视觉注意力机制动态调节视觉处理，利用历史上下文信息提升动态序列决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型将每个时间步的视觉输入独立处理，隐含地将任务建模为MDP，这种历史无关的设计在动态序列决策中无法有效利用历史上下文，限制了视觉标记的处理效率。

Method: 从POMDP角度重新定义问题，提出AVA-VLA框架，引入主动视觉注意力模块，利用来自先前决策步骤的循环状态（信念状态的神经近似）动态调节视觉处理，计算软权重来主动处理任务相关的视觉标记。

Result: 在LIBERO和CALVIN等流行机器人基准测试中达到最先进性能，在双臂机器人平台上的真实世界部署验证了框架的实际适用性和强大的模拟到现实迁移能力。

Conclusion: AVA-VLA通过引入基于历史上下文的主动视觉注意力机制，有效解决了现有VLA模型在动态序列决策中的局限性，在多个基准测试和真实世界部署中表现出优越性能。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.

</details>


### [339] [3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks](https://arxiv.org/abs/2511.19019)
*Nguyen Duc Minh Quang,Chang Liu,Huy-Trung Nguyen,Shuangyang Li,Derrick Wing Kwan Ng,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一个3D动态无线电地图框架，用于学习和预测低空无线网络中接收功率的时空演化，通过Vision Transformer编码器和Transformer时序模块实现准确的功率动态捕获和预测。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络由于三维移动性、时变用户密度和有限功率预算，导致基站发射功率动态波动，形成高度非平稳的3D无线电环境。现有无线电地图多为静态或离线构建，忽略了实时功率变化和时空依赖性。

Method: 使用Vision Transformer编码器从3D无线电地图提取高维空间表示，结合基于Transformer的时序模块建模序列依赖性，预测未来功率分布。

Result: 实验表明3D-DRM能准确捕捉快速变化的功率动态，在无线电地图重建和短期预测方面显著优于基线模型。

Conclusion: 3D-DRM框架有效解决了低空无线网络中动态无线电环境建模的挑战，为无线电感知网络优化提供了有力工具。

Abstract: Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.

</details>


### [340] [Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings](https://arxiv.org/abs/2511.19037)
*Zimo Yan,Zheng Xie,Chang Liu,Yuan Wang*

Main category: cs.LG

TL;DR: 提出了一种拉普拉斯位置编码方法，能够克服传统图神经网络在Weisfeiler-Lehman测试下的表达能力限制，通过理论证明该方法能从常数次观测中实现节点识别，并在药物相互作用任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递图神经网络的表达能力受限于一维Weisfeiler-Lehman测试，无法区分结构不同的节点，需要解决这一理论表达能力限制。

Method: 开发了一种拉普拉斯位置编码，该方法对特征向量符号翻转和特征空间内基旋转具有不变性，结合了最短路径与扩散距离的单调关系、带常数锚点的谱三角定位，以及对数嵌入大小的定量谱单射性。

Result: 理论证明该方法能从常数次观测中实现节点识别，与Weisfeiler-Lehman测试约束的架构建立了样本复杂度分离。在药物相互作用任务中，该编码与神经过程风格解码器结合，显著提升了ROC曲线下面积和F1分数。

Conclusion: 通过原则性的位置信息解决理论表达能力限制具有实际效益，该方法在化学图上的药物相互作用任务中表现出显著性能提升。

Abstract: Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.

</details>


### [341] [Optimization of Deep Learning Models for Dynamic Market Behavior Prediction](https://arxiv.org/abs/2511.19090)
*Shenghan Zhao,Yuzhen Lin,Ximeng Yang,Qiaochu Lu,Haozhong Xue,Gaozhe Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种混合序列模型用于多时间范围需求预测，结合了多尺度时间卷积、门控循环模块和时间感知自注意力机制，在电子商务交易数据上实现了比传统方法和最先进模型更准确和鲁棒的预测。


<details>
  <summary>Details</summary>
Motivation: 随着金融科技的发展，深度学习模型在预测消费者行为方面显示出巨大潜力。本文专注于零售市场行为，旨在通过多时间范围需求预测来提升预测准确性和鲁棒性，特别是在高峰/节假日期间。

Method: 提出混合序列模型，整合多尺度时间卷积、门控循环模块和时间感知自注意力机制。使用标准回归损失训练，采用严格时间分割防止数据泄露，并与ARIMA/Prophet、LSTM/GRU、LightGBM及最先进的Transformer预测器进行基准比较。

Result: 结果显示该模型在MAE、RMSE、sMAPE、MASE和Theil's U_2等多个指标上均取得一致性的准确度提升，在高峰/节假日期间表现出更好的鲁棒性。消融实验和统计显著性测试验证了改进的可靠性。

Conclusion: 所提出的混合序列模型在电子商务需求预测任务中表现出优越性能，为零售市场行为预测提供了有效的解决方案，并通过开源实现细节确保可复现性。

Abstract: The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.

</details>


### [342] [Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication](https://arxiv.org/abs/2511.19103)
*Dora Krekovic,Mario Kusek,Ivana Podnar Zarko,Danh Le-Phuoc*

Main category: cs.LG

TL;DR: 提出了一种用于边缘计算环境的预测算法，通过预测传感器数据并仅在偏差超过预设容差时传输数据，减少通信开销和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决物联网设备在资源受限和远程环境中因连续传输传感器数据导致的网络拥塞、高延迟和高能耗问题，特别是在农业等数据变化较小的领域。

Method: 在边缘部署预测滤波器预测下一个传感器数据点，仅当实际值与预测值偏差超过预设容差时触发数据传输，同时云端模型确保数据完整性和系统一致性。

Result: 有效减少通信开销，提高能源效率，支持跨站点泛化，使模型可在不同区域部署而无需重新训练。

Conclusion: 该解决方案具有高度可扩展性、能源感知能力，适用于优化远程和带宽受限物联网环境中的传感器数据传输。

Abstract: The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.

</details>


### [343] [Masked Diffusion Models are Secretly Learned-Order Autoregressive Models](https://arxiv.org/abs/2511.19152)
*Prateek Garg,Bhavya Kohli,Sunita Sarawagi*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练框架，通过多变量噪声调度来优化掩码扩散模型（MDMs）的解码顺序，证明MDMs本质上是具有可学习顺序的自回归模型。


<details>
  <summary>Details</summary>
Motivation: MDMs在实践中解码顺序对性能有显著影响，但现有方法无法在训练中优化解码顺序。本文旨在设计一个能够优化解码顺序的训练框架。

Method: 使用多变量噪声调度的连续时间变分目标，建立解码顺序与噪声调度之间的直接对应关系，并证明MDM目标可以分解为这些顺序上的加权自回归损失。

Result: 证明了MDM目标不再对噪声调度保持不变性，并且可以精确分解为解码顺序上的加权自回归损失，从而将MDMs确立为具有可学习顺序的自回归模型。

Conclusion: 本文提出的框架能够识别并优化MDMs的解码顺序，揭示了MDMs作为具有可学习顺序的自回归模型的本质特性。

Abstract: Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.

</details>


### [344] [First-order Sobolev Reinforcement Learning](https://arxiv.org/abs/2511.19165)
*Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: 提出了一种改进的时间差分学习方法，通过强制一阶贝尔曼一致性来训练价值函数，使其不仅匹配贝尔曼目标值，还匹配状态和动作的导数。


<details>
  <summary>Details</summary>
Motivation: 传统TD学习只关注价值函数的值匹配，但忽略了局部几何结构的一致性。通过一阶导数匹配，可以加速评论家收敛并提高策略梯度的稳定性。

Method: 通过可微动态系统对贝尔曼备份进行微分，获得解析一致的梯度目标，使用Sobolev型损失将一阶TD匹配原则整合到评论家目标中。

Result: 该方法可以无缝集成到现有算法中（如Q学习、DDPG、SAC），不改变整体结构但可能带来更快的评论家收敛和更稳定的策略梯度。

Conclusion: 一阶TD匹配原则为强化学习算法提供了改进的收敛性和稳定性，同时保持与现有框架的兼容性。

Abstract: We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.

</details>


### [345] [From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation](https://arxiv.org/abs/2511.19176)
*Jeeho Shin,Kyungho Kim,Kijung Shin*

Main category: cs.LG

TL;DR: TESMR是一个三阶段食谱推荐框架，通过内容增强、关系增强和学习增强逐步优化多模态特征，在真实数据集上比现有方法Recall@10提升7-15%。


<details>
  <summary>Details</summary>
Motivation: 食谱推荐需要有效利用用户-食谱交互之外的多模态特征，系统性地增强这些信号具有很大潜力。

Method: 三阶段框架：1) 使用基础模型进行基于内容的多模态特征增强；2) 通过用户-食谱交互的消息传播进行关系增强；3) 通过对比学习和可学习嵌入进行学习增强。

Result: 在两个真实世界数据集上的实验表明，TESMR优于现有方法，Recall@10指标提高了7-15%。

Conclusion: TESMR通过逐步细化多模态特征为有效嵌入，显著提升了食谱推荐性能。

Abstract: Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.

</details>


### [346] [Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform](https://arxiv.org/abs/2511.19240)
*Minxin Chen*

Main category: cs.LG

TL;DR: 提出FDSW-UCB算法，结合折扣长期视角和滑动窗口短期视角，在非平稳多臂老虎机环境中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统UCB算法在非平稳奖励分布环境中性能显著下降，需要解决动态环境下的决策优化问题

Method: 开发FDSW-UCB双视角算法，集成折扣长期视角和滑动窗口短期视角，使用MovieLens-1M和Open Bandit数据集构建半合成仿真平台

Result: 滑动窗口机制稳健，而常用折扣方法存在基本学习失败导致线性遗憾；FDSW-UCB采用乐观聚合策略在动态环境中表现优异

Conclusion: 集成策略本身是成功的关键因素，FDSW-UCB在非平稳环境中实现卓越性能

Abstract: Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.

</details>


### [347] [Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks](https://arxiv.org/abs/2511.19265)
*Bianka Kowalska,Halina Kwaśnicka*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.

</details>


### [348] [Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting](https://arxiv.org/abs/2511.19267)
*Manish Singh,Arpita Dayama*

Main category: cs.LG

TL;DR: 评估时空图神经网络在多店零售销售预测中的有效性，与ARIMA、LSTM和XGBoost基线模型比较，STGNN在各项指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究多店零售环境中店铺间依赖关系对销售预测的影响，探索图神经网络在捕捉空间相关性方面的潜力。

Method: 使用45家沃尔玛店铺的周销售数据，构建自适应图来建模店铺间依赖关系，通过残差路径预测对数差分销售并重构最终值。

Result: STGNN在所有基线模型中表现最优，在标准化总绝对误差、P90 MAPE和MAPE方差等指标上均取得最低误差。

Conclusion: 关系结构显著提升了互联零售环境中的预测质量，STGNN是多店需求预测的稳健建模选择。

Abstract: This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.

</details>


### [349] [Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model](https://arxiv.org/abs/2511.19272)
*Felix Birkel*

Main category: cs.LG

TL;DR: Tiny-TSM是一个小规模的时间序列基础模型，仅2300万参数，在单张A100 GPU上训练不到一周，通过新的合成数据生成和数据增强管道实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个资源高效的时间序列基础模型，避免大规模模型的高计算成本，同时保持最先进的性能。

Method: 使用SynthTS合成数据生成和数据增强管道，结合因果输入归一化方案，使模型能够使用密集的下一个token预测损失进行训练。

Result: 在中长期预测任务上超越所有评估的时间序列基础模型，短期预测精度与SOTA模型相当，且训练效率显著提升。

Conclusion: Tiny-TSM证明了小规模模型在时间序列分析中的有效性，为资源受限环境提供了实用的解决方案。

Abstract: We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.
  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.
  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.

</details>


### [350] [Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space](https://arxiv.org/abs/2511.19273)
*Kunal Dumbre,Lei Jiao,Ole-Christoffer Granmo*

Main category: cs.LG

TL;DR: 提出一种基于Tsetlin Machine的贝叶斯网络结构学习方法，通过选择重要变量进行条件独立性测试，显著降低PC算法的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: PC算法在因果推断中广泛应用，但随着数据集规模增大，其时间复杂性显著增加，限制了在大规模实际问题中的应用。

Method: 利用Tsetlin Machine提取最重要文字，仅对这些选定的文字进行条件独立性测试，而不是对所有变量进行测试。

Result: 在bnlearn存储库的分类数据集上评估，与多种先进方法比较，结果显示该方法显著降低计算复杂度，同时保持竞争力的因果发现准确率。

Conclusion: 基于TM的方法为传统PC算法实现提供了可行的替代方案，在不影响性能的前提下提高了效率。

Abstract: The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.

</details>


### [351] [Closing Gaps in Emissions Monitoring with Climate TRACE](https://arxiv.org/abs/2511.19277)
*Brittany V. Lancellotti,Jordan M. Malof,Aaron Davitt,Gavin McCormick,Shelby Anderson,Pol Carbó-Mestre,Gary Collins,Verity Crane,Zoheyr Doctor,George Ebri,Kevin Foster,Trey M. Gowdy,Michael Guzzardi,John Heal,Heather Hunter,David Kroodsma,Khandekar Mahammad Galib,Paul J. Markakis,Gavin McDonald,Daniel P. Moore,Eric D. Nguyen,Sabina Parvu,Michael Pekala,Christine D. Piatko,Amy Piscopo,Mark Powell,Krsna Raniga,Elizabeth P. Reilly,Michael Robinette,Ishan Saraswat,Patrick Sicurello,Isabella Söldner-Rembold,Raymond Song,Charlotte Underwood,Kyle Bradbury*

Main category: cs.LG

TL;DR: Climate TRACE是一个开放获取平台，提供全球温室气体排放估算，具有高精度、全球覆盖、高时空分辨率和频繁更新的特点。


<details>
  <summary>Details</summary>
Motivation: 现有排放数据集缺乏准确性、全球覆盖、高时空分辨率和频繁更新等关键特性，限制了其在气候行动中的实用性。

Method: 综合现有排放数据，优先考虑准确性、覆盖范围和分辨率，并使用特定行业的估算方法来填补数据空白。

Result: 首次提供全球范围内所有人为排放部门的单个排放源（如单个发电厂）的全面排放估算，数据从2021年1月1日至今，每月更新。

Conclusion: Climate TRACE支持在决策层面进行数据驱动的气候行动，代表了排放核算和减排领域的重大突破。

Abstract: Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.

</details>


### [352] [Understanding the Staged Dynamics of Transformers in Learning Latent Structure](https://arxiv.org/abs/2511.19328)
*Rohan Saha,Farzane Aminmansour,Alona Fyshe*

Main category: cs.LG

TL;DR: 本文研究了transformer模型学习潜在结构的动态过程，发现在Alchemy基准测试中，模型以离散阶段的方式学习，先掌握粗粒度规则，再学习完整的潜在结构，并揭示了组合与分解能力的不对称性。


<details>
  <summary>Details</summary>
Motivation: 虽然transformer能够从上下文中发现潜在结构，但关于它们如何获取潜在结构不同组成部分的动态过程仍然知之甚少。

Method: 使用Alchemy基准测试，训练小型仅解码器transformer在三个任务变体上：1)从部分上下文信息推断缺失规则，2)组合简单规则解决多步序列，3)分解复杂多步示例推断中间步骤。通过将任务分解为可解释事件来分析学习动态。

Result: 模型以离散阶段的方式获取能力，先学习粗粒度规则，然后学习完整的潜在结构。发现关键的不对称性：模型能够稳健地组合基本规则，但在分解复杂示例以发现基本规则方面存在困难。

Conclusion: 这些发现为理解transformer模型如何学习潜在结构提供了新的见解，展示了这些能力在训练过程中的细粒度演化过程。

Abstract: While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.

</details>


### [353] [Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data](https://arxiv.org/abs/2511.19330)
*Dominik Luszczynski*

Main category: cs.LG

TL;DR: 提出了两种新的基于斜率的对抗攻击方法，能够操纵N-HiTS股票预测模型的输出，使预测斜率翻倍，并能绕过标准安全机制。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在图像领域已有深入研究，但在时间序列领域，特别是金融数据预测方面的研究较少，需要填补这一空白。

Method: 开发了两种基于斜率的攻击方法：通用斜率攻击和最小二乘斜率攻击，并将其集成到GAN架构中生成逼真的合成数据。

Result: 新攻击方法能将N-HiTS预测的斜率加倍，使4层CNN判别器的特异性降至28%，准确率降至57%。

Conclusion: 机器学习安全研究不仅需要关注模型本身的安全性，还需要保护整个处理流程，包括防范恶意软件注入攻击。

Abstract: A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.

</details>


### [354] [Annotation-Free Class-Incremental Learning](https://arxiv.org/abs/2511.19344)
*Hari Chandana Kuchibhotla,K S Ananth,Vineeth N Balasubramanian*

Main category: cs.LG

TL;DR: 本文提出了注释自由类增量学习(AFCIL)这一更现实的持续学习范式，其中未标记数据连续到达，学习器必须在没有任何监督的情况下逐步获取新类。为了解决这一问题，作者提出了CrossWorld CL框架，利用外部世界知识作为稳定的辅助源来指导学习。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法依赖于一个强但不现实的假设：在整个学习过程中都有标记数据可用。然而在现实场景中，数据通常是顺序到达且没有注释的，这使得传统方法不实用。

Method: 提出了CrossWorld CL框架，该方法为每个下游类别检索语义相关的ImageNet类，通过跨域对齐策略映射下游和ImageNet特征，并引入新颖的重放策略。

Result: 在四个数据集上的实验表明，CrossWorld-CL超越了CLIP基线和现有的持续学习及无标记学习方法。

Conclusion: 世界知识对于注释自由的持续学习具有重要价值，CrossWorld CL框架能够在不使用注释的情况下发现语义结构，同时保持先前知识的完整性。

Abstract: Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.

</details>


### [355] [Enhancing Conformal Prediction via Class Similarity](https://arxiv.org/abs/2511.19359)
*Ariel Fargion,Lahav Dabah,Tom Tirer*

Main category: cs.LG

TL;DR: 本文提出了一种基于类别相似性的共形预测增强方法，通过惩罚组外错误和利用类别相似性来减少预测集大小，同时保证语义分组质量。


<details>
  <summary>Details</summary>
Motivation: 在需要语义分组的分类应用中，用户不仅需要平均预测集小的共形预测方法，还需要预测集包含较少的语义不同组别。

Method: 提出在CP评分函数中增加惩罚组外错误的项，并开发模型特定的类别相似性变体，无需人工语义分区。

Result: 理论分析和广泛实验表明，该方法能显著减少预测集大小，并在多个数据集和模型上一致提升CP方法性能。

Conclusion: 基于类别相似性的方法为共形预测提供了广泛适用的增强工具，能同时优化预测集大小和语义分组质量。

Abstract: Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.

</details>


### [356] [Neural surrogates for designing gravitational wave detectors](https://arxiv.org/abs/2511.19364)
*Carlos Ruiz-Gonzalez,Sören Arlt,Sebastian Lehner,Arturs Berzins,Yehonathan Drori,Rana X Adhikari,Johannes Brandstetter,Mario Krenn*

Main category: cs.LG

TL;DR: 使用神经网络代理模型替代传统物理模拟器，显著加速复杂系统的实验设计优化过程，以引力波探测器设计为例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着实验装置复杂度增加，传统CPU模拟器的计算成本成为主要限制，需要更高效的替代方案来支持实验设计自动化。

Method: 训练神经网络代理模型替代Finesse引力波物理模拟器，结合自动微分和GPU并行化，通过训练代理模型、逆向设计新实验、用慢速模拟器验证的循环过程进行优化。

Result: 该方法在几小时内找到的解优于传统优化器5天才能达到的设计质量，显著加速了设计空间探索。

Conclusion: 该框架可广泛应用于模拟器瓶颈阻碍优化和发现的领域，为复杂系统设计提供了高效的解决方案。

Abstract: Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.

</details>


### [357] [LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems](https://arxiv.org/abs/2511.19368)
*Tianyang Duan,Zongyuan Zhang,Zheng Lin,Songxiao Guo,Xiuxian Guan,Guangyu Wu,Zihan Fang,Haotian Meng,Xia Du,Ji-Zhe Zhou,Heming Cui,Jun Luo,Yue Gao*

Main category: cs.LG

TL;DR: RELED是一个可扩展的多智能体强化学习框架，通过集成LLM驱动的专家演示与自主智能体探索来解决MARL中的非平稳性问题，提高训练稳定性和策略收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中由于智能体策略同步更新导致的严重非平稳性问题，这种非平稳性会导致训练不稳定和策略收敛差，特别是在智能体数量增加时。

Method: RELED框架包含两个核心模块：1) 基于理论非平稳性边界的Stationarity-Aware Expert Demonstration模块，提升LLM生成的专家轨迹质量；2) Hybrid Expert-Agent Policy Optimization模块，自适应平衡从专家生成和智能体生成轨迹的学习。

Result: 在基于OpenStreetMap的真实城市网络上的广泛实验表明，RELED相比最先进的MARL方法实现了更优越的性能。

Conclusion: RELED通过集成LLM驱动的专家演示与自主探索，有效解决了MARL中的非平稳性问题，提高了训练稳定性和策略收敛性能。

Abstract: Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.

</details>


### [358] [Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware](https://arxiv.org/abs/2511.19379)
*Srishti Gupta,Yashasvee Taiwade*

Main category: cs.LG

TL;DR: 本文比较了DDPM和Flow Matching在低资源硬件上的几何特性和效率，发现Flow Matching在效率上显著优于DDPM，适合实时资源受限的生成任务。


<details>
  <summary>Details</summary>
Motivation: DDPM在生成图像合成中达到最先进水平，但其推理过程中的计算开销很大，通常需要多达1000次迭代步骤，这阻碍了其部署。本研究旨在比较DDPM和新兴的Flow Matching范式在低资源硬件上的几何特性和效率。

Method: 通过在MNIST数据集上使用共享的时间条件U-Net骨干网络实现两种框架，进行几何分析和效率比较。通过数值敏感性分析验证学习向量场的线性程度。

Result: Flow Matching在效率上显著优于Diffusion，学习到高度校正的传输路径（曲率≈1.02），而Diffusion轨迹保持随机和曲折（曲率≈3.45）。在N=10次函数评估时建立"效率边界"，Flow Matching保持高保真度而Diffusion崩溃。

Conclusion: Flow Matching是实时资源受限生成任务的优越算法选择，其学习向量场足够线性，无需使用高阶ODE求解器，可使用轻量级Euler求解器进行边缘部署。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\mathcal{C} \approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\mathcal{C} \approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}

</details>


### [359] [Learning Robust Social Strategies with Large Language Models](https://arxiv.org/abs/2511.19405)
*Dereck Piche,Mohammed Muqeeth,Milad Aghajohari,Juan Duque,Michael Noukhovitch,Aaron Courville*

Main category: cs.LG

TL;DR: 本文研究了在多智能体交互中，强化学习训练的LLM智能体倾向于发展自私行为的问题，提出了Advantage Alignment算法来促进多智能体合作，并引入了Trust and Split社交困境环境来验证方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI的普及，具有不同甚至冲突目标的智能体之间会产生复杂交互。在多智能体社交困境中，个体激励可能损害集体福利，而标准强化学习在这种设置下往往收敛到自私策略。

Method: 采用Advantage Alignment对手学习感知算法来微调LLM，促进多智能体合作和抗利用性。引入群体相对基线简化迭代博弈中的优势计算，并创建了Trust and Split社交困境环境，需要自然语言沟通来实现高集体福利。

Result: 在各种社交困境中，使用Advantage Alignment学习的策略实现了更高的集体收益，同时保持对贪婪智能体利用的鲁棒性。

Conclusion: Advantage Alignment方法能够有效解决多智能体强化学习中收敛到不良均衡的问题，促进LLM智能体之间的合作行为，提高集体福利。

Abstract: As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.

</details>


### [360] [Flow Map Distillation Without Data](https://arxiv.org/abs/2511.19428)
*Shangyuan Tong,Nanye Ma,Saining Xie,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种无需外部数据的流映射蒸馏方法，通过仅从先验分布采样来避免教师-数据不匹配问题，在ImageNet上仅用1步采样就达到了最先进的FID分数。


<details>
  <summary>Details</summary>
Motivation: 传统的流映射蒸馏需要从外部数据集采样，存在教师-数据不匹配风险，因为静态数据集可能无法完整反映教师的全部生成能力。

Method: 引入一个原则性框架，仅从先验分布采样，学习预测教师的采样路径并主动纠正自身的复合误差以确保高保真度。

Result: 在ImageNet 256x256上达到FID 1.45，在ImageNet 512x512上达到FID 1.49，仅需1步采样，超越了所有基于数据的方法。

Conclusion: 这项工作为加速生成模型建立了一个更稳健的范式，推动了无需数据的流映射蒸馏的广泛应用。

Abstract: State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.

</details>
